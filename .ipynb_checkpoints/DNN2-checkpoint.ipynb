{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "#epochs = 2000\n",
    "use_gpu = True\n",
    "lr = 0.001\n",
    "weight_decay = 10\n",
    "\n",
    "# Batch size and learning rate is hyperparameters in deep learning\n",
    "# suggest batch_size is reduced, lr is also reduced which will reduce concussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('./dataset-0510/train.csv')\n",
    "X_test = pd.read_csv('./dataset-0510/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['total_price']\n",
    "X = X.drop(columns=['building_id', 'total_price'], axis=1)\n",
    "\n",
    "X_test = X_test.drop(columns=['building_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scale y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = StandardScaler()\n",
    "y_train = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_eval = y_scaler.fit_transform(y_eval.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputer, Scaler, Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/islab/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# step1. Imputation transformer for completing missing values.\n",
    "step1 = ('Imputer', Imputer())\n",
    "# step2. MinMaxScaler\n",
    "step2 = ('MinMaxScaler', MinMaxScaler())\n",
    "# step3. feature selection\n",
    "#step3 = ('FeatureSelection', SelectFromModel(RandomForestRegressor()))\n",
    "step3 = ('FeatureSelection', VarianceThreshold())\n",
    "\n",
    "pipeline = Pipeline(steps=[step1, step2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 233)\n",
      "(10000, 233)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train = pipeline.fit_transform(X_train)\n",
    "X_eval = pipeline.fit_transform(X_eval)\n",
    "\n",
    "print(X_test.shape)\n",
    "X_test = pipeline.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_eval = torch.from_numpy(X_eval).float().to(device)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).float().to(device)\n",
    "y_eval = torch.from_numpy(y_eval).float().to(device)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42000, 233])\n",
      "torch.Size([10000, 233])\n",
      "torch.Size([42000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data.TensorDataset(X_train, y_train)\n",
    "loader = Data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "eval_dataset = Data.TensorDataset(X_eval, y_eval)\n",
    "loader = Data.DataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(233, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        # fc1\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = F.relu(x)\n",
    "        # fc2\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = F.relu(x)\n",
    "        # fc3\n",
    "        x = self.fc3(x)\n",
    "        '''\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optim = optim.Adam(model.parameters(), lr= lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(model, loader, epochs=10):\n",
    "    model.train()\n",
    "    for e in range(epochs):\n",
    "        train_loss = []\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            optim.zero_grad()\n",
    "            pred = model(batch_x)\n",
    "            loss = criterion(batch_y, pred)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        print('training loss', np.array(train_loss).mean())\n",
    "    return model\n",
    "\n",
    "def eval_func(model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            pred = model(batch_x)\n",
    "            loss = criterion(batch_y, pred)\n",
    "        print('testing loss', loss.item())\n",
    "\n",
    "def test_func(model, X, y_scaler=None):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(X)\n",
    "        pred = pred.cpu().numpy()\n",
    "        \n",
    "        if y_scaler != None:\n",
    "            pred = y_scaler.inverse_transform(pred)\n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 0\n",
      "training loss 0.9501436736957303\n",
      "training loss 0.9233199489581488\n",
      "training loss 0.9064398968114064\n",
      "training loss 0.8836419139281173\n",
      "training loss 0.8366680365052468\n",
      "training loss 0.771222911463093\n",
      "training loss 0.75759209820114\n",
      "training loss 0.736790798985828\n",
      "training loss 0.7432348111860205\n",
      "training loss 0.7207435961083116\n",
      "epochs 1\n",
      "training loss 0.6984301521624928\n",
      "training loss 0.6777804596303697\n",
      "training loss 0.6726904134093714\n",
      "training loss 0.6803147826298908\n",
      "training loss 0.6530774530291549\n",
      "training loss 0.6554120853077692\n",
      "training loss 0.6363395146880658\n",
      "training loss 0.6279896232595754\n",
      "training loss 0.6197491216432028\n",
      "training loss 0.6125582458116302\n",
      "epochs 2\n",
      "training loss 0.6098270424652111\n",
      "training loss 0.5935704218102241\n",
      "training loss 0.5812477491851915\n",
      "training loss 0.5897203585360667\n",
      "training loss 0.5921173570331565\n",
      "training loss 0.5740297626913574\n",
      "training loss 0.5815515164149602\n",
      "training loss 0.5680268438226549\n",
      "training loss 0.5636804933662125\n",
      "training loss 0.5589320784622194\n",
      "epochs 3\n",
      "training loss 0.5515480272548705\n",
      "training loss 0.5580288016770255\n",
      "training loss 0.5432449320408185\n",
      "training loss 0.5317189201444532\n",
      "training loss 0.5210367090572827\n",
      "training loss 0.530061021658342\n",
      "training loss 0.5345323424053082\n",
      "training loss 0.5301889115634266\n",
      "training loss 0.471733131055757\n",
      "training loss 0.46221478381605197\n",
      "epochs 4\n",
      "training loss 0.45746602546154386\n",
      "training loss 0.47751102195287776\n",
      "training loss 0.45241771982806106\n",
      "training loss 0.3974956922442521\n",
      "training loss 0.4025130588440752\n",
      "training loss 0.4584052669108306\n",
      "training loss 0.5206094086826081\n",
      "training loss 0.3532177797048336\n",
      "training loss 0.3283337568486827\n",
      "training loss 0.33553466977207647\n",
      "epochs 5\n",
      "training loss 0.3208153384762067\n",
      "training loss 0.23855172085841891\n",
      "training loss 0.26534915973090906\n",
      "training loss 0.22639650607889458\n",
      "training loss 0.26009658177150385\n",
      "training loss 0.20988180132732914\n",
      "training loss 0.21386757042977703\n",
      "training loss 0.2779676514672579\n",
      "training loss 0.2862580058191453\n",
      "training loss 0.24104843802223241\n",
      "epochs 6\n",
      "training loss 0.2761300346170002\n",
      "training loss 0.417007692351788\n",
      "training loss 0.2455070420082377\n",
      "training loss 0.19145901824212458\n",
      "training loss 0.4594632976011779\n",
      "training loss 0.490803507680448\n",
      "training loss 0.2753508837890795\n",
      "training loss 0.3019159958328568\n",
      "training loss 0.24983493878167104\n",
      "training loss 0.22619789901120332\n",
      "epochs 7\n",
      "training loss 0.19591794732288084\n",
      "training loss 0.21193283297509116\n",
      "training loss 0.2601591384645896\n",
      "training loss 0.3598908435613416\n",
      "training loss 0.2420224039463539\n",
      "training loss 0.3320805047591976\n",
      "training loss 0.22606322266231987\n",
      "training loss 0.2796447201403871\n",
      "training loss 0.2734338954638908\n",
      "training loss 0.20909849221676896\n",
      "epochs 8\n",
      "training loss 0.209737518389411\n",
      "training loss 0.22760015863423078\n",
      "training loss 0.19109153723918762\n",
      "training loss 0.1797251614416726\n",
      "training loss 0.1813378644775178\n",
      "training loss 0.21515692840917813\n",
      "training loss 0.22448759428148318\n",
      "training loss 0.16449247067357006\n",
      "training loss 0.1766855765281349\n",
      "training loss 0.22527774644213125\n",
      "epochs 9\n",
      "training loss 0.19293638372592536\n",
      "training loss 0.18037979953848873\n",
      "training loss 0.21058055047897536\n",
      "training loss 0.28563239968368437\n",
      "training loss 0.3314377225577229\n",
      "training loss 0.33132784347499444\n",
      "training loss 0.21382593380781764\n",
      "training loss 0.22201604913372575\n",
      "training loss 0.1943141154637828\n",
      "training loss 0.24295033367424626\n"
     ]
    }
   ],
   "source": [
    "for t in range(10):\n",
    "    print('epochs', t)\n",
    "    model = train_func(model, train_dataset)\n",
    "    #eval_func(model, eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vaildation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-20aaa26f569b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-4f99a11354ec>\u001b[0m in \u001b[0;36meval_func\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'testing loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_scaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "eval_func(model, eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test_func(model, X_test, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission = pd.read_csv('./dataset-0510/submit_test.csv')\n",
    "submission['total_price'] = pred\n",
    "submission.to_csv('submission/DNN2_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size use 128 or 32 , learning rate use 0.003 which find loss will stock in 0.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
