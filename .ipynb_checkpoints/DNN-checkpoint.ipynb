{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building_id\n",
    "#columns = X.columns\n",
    "data_train = pd.read_csv('./dataset-0510/train.csv')\n",
    "X_test = pd.read_csv('./dataset-0510/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "data = data_train.append(X_test, ignore_index=True, sort=False)\n",
    "data = data.drop(['building_id'], axis=1)\n",
    "print(data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill the nan values\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "sale_price = data['total_price'].values\n",
    "data = data.drop('total_price', axis=1)\n",
    "\n",
    "columns = data.columns\n",
    "# check has any nan value in data\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_material</th>\n",
       "      <th>city</th>\n",
       "      <th>txn_dt</th>\n",
       "      <th>total_floor</th>\n",
       "      <th>building_type</th>\n",
       "      <th>building_use</th>\n",
       "      <th>building_complete_dt</th>\n",
       "      <th>parking_way</th>\n",
       "      <th>parking_area</th>\n",
       "      <th>parking_price</th>\n",
       "      <th>...</th>\n",
       "      <th>XIV_250</th>\n",
       "      <th>XIV_500</th>\n",
       "      <th>XIV_index_500</th>\n",
       "      <th>XIV_1000</th>\n",
       "      <th>XIV_index_1000</th>\n",
       "      <th>XIV_5000</th>\n",
       "      <th>XIV_index_5000</th>\n",
       "      <th>XIV_10000</th>\n",
       "      <th>XIV_index_10000</th>\n",
       "      <th>XIV_MIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.220133</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.300577</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037838</td>\n",
       "      <td>0.034503</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.045336</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.135021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.269487</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.379486</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.016657</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033208</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.868705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.979105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.461026</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.288697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048649</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.061219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.862568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.979317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890325</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.889899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.079516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.011898</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.139679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996866</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.330351</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013572</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.140720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 233 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_material      city    txn_dt  total_floor  building_type  \\\n",
       "0                0.7  1.000000  0.220133     0.107143           0.75   \n",
       "1                0.7  0.222222  0.269487     0.142857           0.25   \n",
       "2                0.7  0.222222  0.461026     0.107143           0.25   \n",
       "3                0.7  1.000000  0.890325     0.821429           0.00   \n",
       "4                0.0  1.000000  0.996866     0.035714           1.00   \n",
       "\n",
       "   building_use  building_complete_dt  parking_way  parking_area  \\\n",
       "0           0.2              0.300577          1.0      0.005952   \n",
       "1           0.2              0.379486          1.0      0.005952   \n",
       "2           0.2              0.288697          1.0      0.005952   \n",
       "3           0.2              0.889899          0.0      0.005952   \n",
       "4           0.2              0.330351          1.0      0.005952   \n",
       "\n",
       "   parking_price  ...   XIV_250   XIV_500  XIV_index_500  XIV_1000  \\\n",
       "0       0.042916  ...  0.037838  0.034503            1.0  0.045336   \n",
       "1       0.042916  ...  0.012613  0.016657            1.0  0.033208   \n",
       "2       0.042916  ...  0.048649  0.046401            1.0  0.061219   \n",
       "3       0.079516  ...  0.003604  0.011898            1.0  0.036096   \n",
       "4       0.042916  ...  0.003604  0.010708            1.0  0.013572   \n",
       "\n",
       "   XIV_index_1000  XIV_5000  XIV_index_5000  XIV_10000  XIV_index_10000  \\\n",
       "0             1.0  0.135021             0.0   0.181915              0.0   \n",
       "1             1.0  0.868705             0.0   0.979105              0.0   \n",
       "2             1.0  0.862568             0.0   0.979317              0.0   \n",
       "3             1.0  0.139679             0.0   0.220238              0.0   \n",
       "4             1.0  0.140720             0.0   0.225440              0.0   \n",
       "\n",
       "    XIV_MIN  \n",
       "0  0.031126  \n",
       "1  0.036191  \n",
       "2  0.049197  \n",
       "3  0.090022  \n",
       "4  0.112169  \n",
       "\n",
       "[5 rows x 233 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "scaling_data = x_scaler.fit_transform(data)\n",
    "scaling_sale_price = y_scaler.fit_transform(sale_price.reshape(-1, 1))\n",
    "\n",
    "data = pd.DataFrame(scaling_data, columns = columns)\n",
    "sale_price = pd.DataFrame(scaling_sale_price, columns=['total_price'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_x = data.iloc[:60000]\n",
    "test_x  = data.iloc[60000:]\n",
    "\n",
    "train_y = sale_price.iloc[:60000]\n",
    "test_y = sale_price.iloc[60000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        self.fc1 = nn.Linear(233, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        '''\n",
    "        \n",
    "        self.fc1 = nn.Linear(233, 144)\n",
    "        self.fc2 = nn.Linear(144, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        '''\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# takes in a module and applies the specified weight initialization\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "## takes in a module and applies the specified weight initialization\n",
    "def weights_init_normal(m):\n",
    "    '''Takes in a module and initializes all linear layers with weight\n",
    "       values taken from a normal distribution.'''\n",
    "\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model\n",
    "    if classname.find('Linear') != -1:\n",
    "        y = m.in_features\n",
    "    # m.weight.data shoud be taken from a normal distribution\n",
    "        m.weight.data.normal_(0.0,1/np.sqrt(y))\n",
    "    # m.bias.data should be 0\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None: \n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training(X_train, y_train):\n",
    "    train_batch = np.array_split(X_train, 50)\n",
    "    label_batch = np.array_split(y_train, 50)\n",
    "\n",
    "    for i in range(len(train_batch)):\n",
    "        train_batch[i] = torch.from_numpy(train_batch[i].values).float().to(device)\n",
    "    for i in range(len(label_batch)):\n",
    "        label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1).to(device)\n",
    "    return train_batch, label_batch\n",
    "\n",
    "#train_batch = torch.from_numpy(X_train.values).float().to(device)\n",
    "#label_batch = torch.from_numpy(y_train).float().view(-1, 1).to(device)\n",
    "train_batch, label_batch = batch_training(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = torch.from_numpy(X_val.values).float().to(device)\n",
    "y_val = torch.from_numpy(y_val.values).float().view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Regressor().to(device)\n",
    "ps = model(train_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50000..  Training Loss: 0.01461171..  Test Loss: 0.01448576.. \n",
      "Epoch: 51/50000..  Training Loss: 0.00007689..  Test Loss: 0.00922441.. \n",
      "Epoch: 101/50000..  Training Loss: 0.00005529..  Test Loss: 0.00842781.. \n",
      "Epoch: 151/50000..  Training Loss: 0.00001940..  Test Loss: 0.00819568.. \n",
      "Epoch: 201/50000..  Training Loss: 0.00001289..  Test Loss: 0.00753177.. \n",
      "Epoch: 251/50000..  Training Loss: 0.00001451..  Test Loss: 0.00757649.. \n",
      "Epoch: 301/50000..  Training Loss: 0.00001360..  Test Loss: 0.00751261.. \n",
      "Epoch: 351/50000..  Training Loss: 0.00001256..  Test Loss: 0.00748613.. \n",
      "Epoch: 401/50000..  Training Loss: 0.00001175..  Test Loss: 0.00734017.. \n",
      "Epoch: 451/50000..  Training Loss: 0.00001167..  Test Loss: 0.00745296.. \n",
      "Epoch: 501/50000..  Training Loss: 0.00001080..  Test Loss: 0.00713583.. \n",
      "Epoch: 551/50000..  Training Loss: 0.00001042..  Test Loss: 0.00712339.. \n",
      "Epoch: 601/50000..  Training Loss: 0.00001060..  Test Loss: 0.00722564.. \n",
      "Epoch: 651/50000..  Training Loss: 0.00000966..  Test Loss: 0.00717961.. \n",
      "Epoch: 701/50000..  Training Loss: 0.00000940..  Test Loss: 0.00714953.. \n",
      "Epoch: 751/50000..  Training Loss: 0.00001004..  Test Loss: 0.00708175.. \n",
      "Epoch: 801/50000..  Training Loss: 0.00000870..  Test Loss: 0.00708355.. \n",
      "Epoch: 851/50000..  Training Loss: 0.00001003..  Test Loss: 0.00708598.. \n",
      "Epoch: 901/50000..  Training Loss: 0.00000952..  Test Loss: 0.00719555.. \n",
      "Epoch: 951/50000..  Training Loss: 0.00001028..  Test Loss: 0.00761110.. \n",
      "Epoch: 1001/50000..  Training Loss: 0.00000986..  Test Loss: 0.00729993.. \n",
      "Epoch: 1051/50000..  Training Loss: 0.00000870..  Test Loss: 0.00724665.. \n",
      "Epoch: 1101/50000..  Training Loss: 0.00000851..  Test Loss: 0.00712975.. \n",
      "Epoch: 1151/50000..  Training Loss: 0.00001071..  Test Loss: 0.00789392.. \n",
      "Epoch: 1201/50000..  Training Loss: 0.00000821..  Test Loss: 0.00726675.. \n",
      "Epoch: 1251/50000..  Training Loss: 0.00000862..  Test Loss: 0.00738890.. \n",
      "Epoch: 1301/50000..  Training Loss: 0.00000800..  Test Loss: 0.00723947.. \n",
      "Epoch: 1351/50000..  Training Loss: 0.00000798..  Test Loss: 0.00722296.. \n",
      "Epoch: 1401/50000..  Training Loss: 0.00000854..  Test Loss: 0.00765203.. \n",
      "Epoch: 1451/50000..  Training Loss: 0.00000786..  Test Loss: 0.00720016.. \n",
      "Epoch: 1501/50000..  Training Loss: 0.00000793..  Test Loss: 0.00747551.. \n",
      "Epoch: 1551/50000..  Training Loss: 0.00000790..  Test Loss: 0.00722830.. \n",
      "Epoch: 1601/50000..  Training Loss: 0.00000797..  Test Loss: 0.00732552.. \n",
      "Epoch: 1651/50000..  Training Loss: 0.00000758..  Test Loss: 0.00724497.. \n",
      "Epoch: 1701/50000..  Training Loss: 0.00000724..  Test Loss: 0.00725599.. \n",
      "Epoch: 1751/50000..  Training Loss: 0.00000716..  Test Loss: 0.00717758.. \n",
      "Epoch: 1801/50000..  Training Loss: 0.00000739..  Test Loss: 0.00717513.. \n",
      "Epoch: 1851/50000..  Training Loss: 0.00000759..  Test Loss: 0.00735855.. \n",
      "Epoch: 1901/50000..  Training Loss: 0.00000842..  Test Loss: 0.00742849.. \n",
      "Epoch: 1951/50000..  Training Loss: 0.00000743..  Test Loss: 0.00727705.. \n",
      "Epoch: 2001/50000..  Training Loss: 0.00000712..  Test Loss: 0.00722861.. \n",
      "Epoch: 2051/50000..  Training Loss: 0.00000733..  Test Loss: 0.00729850.. \n",
      "Epoch: 2101/50000..  Training Loss: 0.00000676..  Test Loss: 0.00736186.. \n",
      "Epoch: 2151/50000..  Training Loss: 0.00000731..  Test Loss: 0.00729706.. \n",
      "Epoch: 2201/50000..  Training Loss: 0.00000680..  Test Loss: 0.00727170.. \n",
      "Epoch: 2251/50000..  Training Loss: 0.00000737..  Test Loss: 0.00728446.. \n",
      "Epoch: 2301/50000..  Training Loss: 0.00000703..  Test Loss: 0.00729584.. \n",
      "Epoch: 2351/50000..  Training Loss: 0.00000908..  Test Loss: 0.00772689.. \n",
      "Epoch: 2401/50000..  Training Loss: 0.00000659..  Test Loss: 0.00756180.. \n",
      "Epoch: 2451/50000..  Training Loss: 0.00000777..  Test Loss: 0.00766364.. \n",
      "Epoch: 2501/50000..  Training Loss: 0.00000863..  Test Loss: 0.00748887.. \n",
      "Epoch: 2551/50000..  Training Loss: 0.00000668..  Test Loss: 0.00729871.. \n",
      "Epoch: 2601/50000..  Training Loss: 0.00000847..  Test Loss: 0.00736709.. \n",
      "Epoch: 2651/50000..  Training Loss: 0.00000705..  Test Loss: 0.00726175.. \n",
      "Epoch: 2701/50000..  Training Loss: 0.00000753..  Test Loss: 0.00731747.. \n",
      "Epoch: 2751/50000..  Training Loss: 0.00000672..  Test Loss: 0.00763768.. \n",
      "Epoch: 2801/50000..  Training Loss: 0.00000619..  Test Loss: 0.00731889.. \n",
      "Epoch: 2851/50000..  Training Loss: 0.00000590..  Test Loss: 0.00733374.. \n",
      "Epoch: 2901/50000..  Training Loss: 0.00000617..  Test Loss: 0.00765213.. \n",
      "Epoch: 2951/50000..  Training Loss: 0.00000657..  Test Loss: 0.00751128.. \n",
      "Epoch: 3001/50000..  Training Loss: 0.00000670..  Test Loss: 0.00741791.. \n",
      "Epoch: 3051/50000..  Training Loss: 0.00000644..  Test Loss: 0.00759197.. \n",
      "Epoch: 3101/50000..  Training Loss: 0.00000604..  Test Loss: 0.00744026.. \n",
      "Epoch: 3151/50000..  Training Loss: 0.00000571..  Test Loss: 0.00741143.. \n",
      "Epoch: 3201/50000..  Training Loss: 0.00000601..  Test Loss: 0.00730295.. \n",
      "Epoch: 3251/50000..  Training Loss: 0.00000647..  Test Loss: 0.00740510.. \n",
      "Epoch: 3301/50000..  Training Loss: 0.00000644..  Test Loss: 0.00739205.. \n",
      "Epoch: 3351/50000..  Training Loss: 0.00000570..  Test Loss: 0.00757377.. \n",
      "Epoch: 3401/50000..  Training Loss: 0.00000598..  Test Loss: 0.00746074.. \n",
      "Epoch: 3451/50000..  Training Loss: 0.00000577..  Test Loss: 0.00748001.. \n",
      "Epoch: 3501/50000..  Training Loss: 0.00000686..  Test Loss: 0.00751180.. \n",
      "Epoch: 3551/50000..  Training Loss: 0.00000571..  Test Loss: 0.00736921.. \n",
      "Epoch: 3601/50000..  Training Loss: 0.00000617..  Test Loss: 0.00738749.. \n",
      "Epoch: 3651/50000..  Training Loss: 0.00000585..  Test Loss: 0.00740753.. \n",
      "Epoch: 3701/50000..  Training Loss: 0.00000811..  Test Loss: 0.00802109.. \n",
      "Epoch: 3751/50000..  Training Loss: 0.00000677..  Test Loss: 0.00750717.. \n",
      "Epoch: 3801/50000..  Training Loss: 0.00000598..  Test Loss: 0.00759130.. \n",
      "Epoch: 3851/50000..  Training Loss: 0.00000682..  Test Loss: 0.00752450.. \n",
      "Epoch: 3901/50000..  Training Loss: 0.00000662..  Test Loss: 0.00743435.. \n",
      "Epoch: 3951/50000..  Training Loss: 0.00000525..  Test Loss: 0.00745782.. \n",
      "Epoch: 4001/50000..  Training Loss: 0.00000502..  Test Loss: 0.00735393.. \n",
      "Epoch: 4051/50000..  Training Loss: 0.00000606..  Test Loss: 0.00747777.. \n",
      "Epoch: 4101/50000..  Training Loss: 0.00000579..  Test Loss: 0.00742218.. \n",
      "Epoch: 4151/50000..  Training Loss: 0.00000684..  Test Loss: 0.00759177.. \n",
      "Epoch: 4201/50000..  Training Loss: 0.00000607..  Test Loss: 0.00752759.. \n",
      "Epoch: 4251/50000..  Training Loss: 0.00000620..  Test Loss: 0.00749131.. \n",
      "Epoch: 4301/50000..  Training Loss: 0.00000558..  Test Loss: 0.00742738.. \n",
      "Epoch: 4351/50000..  Training Loss: 0.00000645..  Test Loss: 0.00744147.. \n",
      "Epoch: 4401/50000..  Training Loss: 0.00000593..  Test Loss: 0.00748561.. \n",
      "Epoch: 4451/50000..  Training Loss: 0.00000580..  Test Loss: 0.00758759.. \n",
      "Epoch: 4501/50000..  Training Loss: 0.00000608..  Test Loss: 0.00762242.. \n",
      "Epoch: 4551/50000..  Training Loss: 0.00000807..  Test Loss: 0.00818361.. \n",
      "Epoch: 4601/50000..  Training Loss: 0.00000523..  Test Loss: 0.00748785.. \n",
      "Epoch: 4651/50000..  Training Loss: 0.00000583..  Test Loss: 0.00751617.. \n",
      "Epoch: 4701/50000..  Training Loss: 0.00000671..  Test Loss: 0.00753064.. \n",
      "Epoch: 4751/50000..  Training Loss: 0.00000514..  Test Loss: 0.00741984.. \n",
      "Epoch: 4801/50000..  Training Loss: 0.00000574..  Test Loss: 0.00745860.. \n",
      "Epoch: 4851/50000..  Training Loss: 0.00000831..  Test Loss: 0.00758265.. \n",
      "Epoch: 4901/50000..  Training Loss: 0.00000583..  Test Loss: 0.00750940.. \n",
      "Epoch: 4951/50000..  Training Loss: 0.00000524..  Test Loss: 0.00738331.. \n",
      "Epoch: 5001/50000..  Training Loss: 0.00000587..  Test Loss: 0.00750937.. \n",
      "Epoch: 5051/50000..  Training Loss: 0.00000584..  Test Loss: 0.00753706.. \n",
      "Epoch: 5101/50000..  Training Loss: 0.00000610..  Test Loss: 0.00743584.. \n",
      "Epoch: 5151/50000..  Training Loss: 0.00000499..  Test Loss: 0.00742351.. \n",
      "Epoch: 5201/50000..  Training Loss: 0.00000662..  Test Loss: 0.00743968.. \n",
      "Epoch: 5251/50000..  Training Loss: 0.00000624..  Test Loss: 0.00737998.. \n",
      "Epoch: 5301/50000..  Training Loss: 0.00000627..  Test Loss: 0.00752601.. \n",
      "Epoch: 5351/50000..  Training Loss: 0.00000653..  Test Loss: 0.00769400.. \n",
      "Epoch: 5401/50000..  Training Loss: 0.00000574..  Test Loss: 0.00738433.. \n",
      "Epoch: 5451/50000..  Training Loss: 0.00000569..  Test Loss: 0.00767037.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5501/50000..  Training Loss: 0.00000699..  Test Loss: 0.00758274.. \n",
      "Epoch: 5551/50000..  Training Loss: 0.00000614..  Test Loss: 0.00762782.. \n",
      "Epoch: 5601/50000..  Training Loss: 0.00000651..  Test Loss: 0.00762330.. \n",
      "Epoch: 5651/50000..  Training Loss: 0.00000517..  Test Loss: 0.00759048.. \n",
      "Epoch: 5701/50000..  Training Loss: 0.00000643..  Test Loss: 0.00766645.. \n",
      "Epoch: 5751/50000..  Training Loss: 0.00000618..  Test Loss: 0.00759539.. \n",
      "Epoch: 5801/50000..  Training Loss: 0.00000635..  Test Loss: 0.00753318.. \n",
      "Epoch: 5851/50000..  Training Loss: 0.00000488..  Test Loss: 0.00746536.. \n",
      "Epoch: 5901/50000..  Training Loss: 0.00000493..  Test Loss: 0.00758369.. \n",
      "Epoch: 5951/50000..  Training Loss: 0.00000496..  Test Loss: 0.00742580.. \n",
      "Epoch: 6001/50000..  Training Loss: 0.00000731..  Test Loss: 0.00755210.. \n",
      "Epoch: 6051/50000..  Training Loss: 0.00000553..  Test Loss: 0.00736851.. \n",
      "Epoch: 6101/50000..  Training Loss: 0.00000834..  Test Loss: 0.00787389.. \n",
      "Epoch: 6151/50000..  Training Loss: 0.00000597..  Test Loss: 0.00735766.. \n",
      "Epoch: 6201/50000..  Training Loss: 0.00000581..  Test Loss: 0.00740448.. \n",
      "Epoch: 6251/50000..  Training Loss: 0.00000540..  Test Loss: 0.00728824.. \n",
      "Epoch: 6301/50000..  Training Loss: 0.00000570..  Test Loss: 0.00744447.. \n",
      "Epoch: 6351/50000..  Training Loss: 0.00000543..  Test Loss: 0.00755067.. \n",
      "Epoch: 6401/50000..  Training Loss: 0.00000646..  Test Loss: 0.00725197.. \n",
      "Epoch: 6451/50000..  Training Loss: 0.00000565..  Test Loss: 0.00748405.. \n",
      "Epoch: 6501/50000..  Training Loss: 0.00000561..  Test Loss: 0.00730311.. \n",
      "Epoch: 6551/50000..  Training Loss: 0.00000551..  Test Loss: 0.00737267.. \n",
      "Epoch: 6601/50000..  Training Loss: 0.00000439..  Test Loss: 0.00727982.. \n",
      "Epoch: 6651/50000..  Training Loss: 0.00000538..  Test Loss: 0.00728228.. \n",
      "Epoch: 6701/50000..  Training Loss: 0.00000590..  Test Loss: 0.00748491.. \n",
      "Epoch: 6751/50000..  Training Loss: 0.00000568..  Test Loss: 0.00731205.. \n",
      "Epoch: 6801/50000..  Training Loss: 0.00000533..  Test Loss: 0.00745689.. \n",
      "Epoch: 6851/50000..  Training Loss: 0.00000632..  Test Loss: 0.00718209.. \n",
      "Epoch: 6901/50000..  Training Loss: 0.00000589..  Test Loss: 0.00754717.. \n",
      "Epoch: 6951/50000..  Training Loss: 0.00000509..  Test Loss: 0.00734246.. \n",
      "Epoch: 7001/50000..  Training Loss: 0.00000558..  Test Loss: 0.00719303.. \n",
      "Epoch: 7051/50000..  Training Loss: 0.00000516..  Test Loss: 0.00737971.. \n",
      "Epoch: 7101/50000..  Training Loss: 0.00000564..  Test Loss: 0.00728501.. \n",
      "Epoch: 7151/50000..  Training Loss: 0.00000464..  Test Loss: 0.00720725.. \n",
      "Epoch: 7201/50000..  Training Loss: 0.00000560..  Test Loss: 0.00743365.. \n",
      "Epoch: 7251/50000..  Training Loss: 0.00000646..  Test Loss: 0.00744598.. \n",
      "Epoch: 7301/50000..  Training Loss: 0.00000521..  Test Loss: 0.00718774.. \n",
      "Epoch: 7351/50000..  Training Loss: 0.00000502..  Test Loss: 0.00734726.. \n",
      "Epoch: 7401/50000..  Training Loss: 0.00000521..  Test Loss: 0.00747943.. \n",
      "Epoch: 7451/50000..  Training Loss: 0.00000598..  Test Loss: 0.00744256.. \n",
      "Epoch: 7501/50000..  Training Loss: 0.00000602..  Test Loss: 0.00725146.. \n",
      "Epoch: 7551/50000..  Training Loss: 0.00000615..  Test Loss: 0.00717344.. \n",
      "Epoch: 7601/50000..  Training Loss: 0.00000657..  Test Loss: 0.00750966.. \n",
      "Epoch: 7651/50000..  Training Loss: 0.00000617..  Test Loss: 0.00752313.. \n",
      "Epoch: 7701/50000..  Training Loss: 0.00000578..  Test Loss: 0.00724826.. \n",
      "Epoch: 7751/50000..  Training Loss: 0.00000663..  Test Loss: 0.00717087.. \n",
      "Epoch: 7801/50000..  Training Loss: 0.00000442..  Test Loss: 0.00738336.. \n",
      "Epoch: 7851/50000..  Training Loss: 0.00000510..  Test Loss: 0.00729711.. \n",
      "Epoch: 7901/50000..  Training Loss: 0.00000523..  Test Loss: 0.00727270.. \n",
      "Epoch: 7951/50000..  Training Loss: 0.00000468..  Test Loss: 0.00723401.. \n",
      "Epoch: 8001/50000..  Training Loss: 0.00000573..  Test Loss: 0.00726414.. \n",
      "Epoch: 8051/50000..  Training Loss: 0.00000486..  Test Loss: 0.00735300.. \n",
      "Epoch: 8101/50000..  Training Loss: 0.00000496..  Test Loss: 0.00732219.. \n",
      "Epoch: 8151/50000..  Training Loss: 0.00000498..  Test Loss: 0.00728222.. \n",
      "Epoch: 8201/50000..  Training Loss: 0.00000453..  Test Loss: 0.00730377.. \n",
      "Epoch: 8251/50000..  Training Loss: 0.00000500..  Test Loss: 0.00731775.. \n",
      "Epoch: 8301/50000..  Training Loss: 0.00000561..  Test Loss: 0.00730765.. \n",
      "Epoch: 8351/50000..  Training Loss: 0.00000464..  Test Loss: 0.00729694.. \n",
      "Epoch: 8401/50000..  Training Loss: 0.00000470..  Test Loss: 0.00730508.. \n",
      "Epoch: 8451/50000..  Training Loss: 0.00000530..  Test Loss: 0.00722224.. \n",
      "Epoch: 8501/50000..  Training Loss: 0.00000507..  Test Loss: 0.00736559.. \n",
      "Epoch: 8551/50000..  Training Loss: 0.00000455..  Test Loss: 0.00758962.. \n",
      "Epoch: 8601/50000..  Training Loss: 0.00000534..  Test Loss: 0.00747799.. \n",
      "Epoch: 8651/50000..  Training Loss: 0.00000557..  Test Loss: 0.00742974.. \n",
      "Epoch: 8701/50000..  Training Loss: 0.00000668..  Test Loss: 0.00759708.. \n",
      "Epoch: 8751/50000..  Training Loss: 0.00000601..  Test Loss: 0.00734961.. \n",
      "Epoch: 8801/50000..  Training Loss: 0.00000487..  Test Loss: 0.00725680.. \n",
      "Epoch: 8851/50000..  Training Loss: 0.00000548..  Test Loss: 0.00726757.. \n",
      "Epoch: 8901/50000..  Training Loss: 0.00000434..  Test Loss: 0.00732290.. \n",
      "Epoch: 8951/50000..  Training Loss: 0.00000474..  Test Loss: 0.00723076.. \n",
      "Epoch: 9001/50000..  Training Loss: 0.00000529..  Test Loss: 0.00724594.. \n",
      "Epoch: 9051/50000..  Training Loss: 0.00000433..  Test Loss: 0.00728807.. \n",
      "Epoch: 9101/50000..  Training Loss: 0.00000605..  Test Loss: 0.00748593.. \n",
      "Epoch: 9151/50000..  Training Loss: 0.00000611..  Test Loss: 0.00743913.. \n",
      "Epoch: 9201/50000..  Training Loss: 0.00000592..  Test Loss: 0.00753967.. \n",
      "Epoch: 9251/50000..  Training Loss: 0.00000666..  Test Loss: 0.00754788.. \n",
      "Epoch: 9301/50000..  Training Loss: 0.00000523..  Test Loss: 0.00749798.. \n",
      "Epoch: 9351/50000..  Training Loss: 0.00000671..  Test Loss: 0.00755916.. \n",
      "Epoch: 9401/50000..  Training Loss: 0.00000616..  Test Loss: 0.00758701.. \n",
      "Epoch: 9451/50000..  Training Loss: 0.00000593..  Test Loss: 0.00757119.. \n",
      "Epoch: 9501/50000..  Training Loss: 0.00000757..  Test Loss: 0.00769381.. \n",
      "Epoch: 9551/50000..  Training Loss: 0.00000621..  Test Loss: 0.00765205.. \n",
      "Epoch: 9601/50000..  Training Loss: 0.00000519..  Test Loss: 0.00744586.. \n",
      "Epoch: 9651/50000..  Training Loss: 0.00000696..  Test Loss: 0.00753034.. \n",
      "Epoch: 9701/50000..  Training Loss: 0.00000533..  Test Loss: 0.00727411.. \n",
      "Epoch: 9751/50000..  Training Loss: 0.00000498..  Test Loss: 0.00732562.. \n",
      "Epoch: 9801/50000..  Training Loss: 0.00000401..  Test Loss: 0.00730405.. \n",
      "Epoch: 9851/50000..  Training Loss: 0.00000508..  Test Loss: 0.00731383.. \n",
      "Epoch: 9901/50000..  Training Loss: 0.00000539..  Test Loss: 0.00741385.. \n",
      "Epoch: 9951/50000..  Training Loss: 0.00000551..  Test Loss: 0.00735640.. \n",
      "Epoch: 10001/50000..  Training Loss: 0.00000556..  Test Loss: 0.00723598.. \n",
      "Epoch: 10051/50000..  Training Loss: 0.00000401..  Test Loss: 0.00728221.. \n",
      "Epoch: 10101/50000..  Training Loss: 0.00000402..  Test Loss: 0.00730645.. \n",
      "Epoch: 10151/50000..  Training Loss: 0.00000457..  Test Loss: 0.00735302.. \n",
      "Epoch: 10201/50000..  Training Loss: 0.00000543..  Test Loss: 0.00741324.. \n",
      "Epoch: 10251/50000..  Training Loss: 0.00000512..  Test Loss: 0.00748109.. \n",
      "Epoch: 10301/50000..  Training Loss: 0.00000460..  Test Loss: 0.00740726.. \n",
      "Epoch: 10351/50000..  Training Loss: 0.00000825..  Test Loss: 0.00758282.. \n",
      "Epoch: 10401/50000..  Training Loss: 0.00000479..  Test Loss: 0.00747198.. \n",
      "Epoch: 10451/50000..  Training Loss: 0.00000401..  Test Loss: 0.00742000.. \n",
      "Epoch: 10501/50000..  Training Loss: 0.00000508..  Test Loss: 0.00742396.. \n",
      "Epoch: 10551/50000..  Training Loss: 0.00000453..  Test Loss: 0.00740758.. \n",
      "Epoch: 10601/50000..  Training Loss: 0.00000707..  Test Loss: 0.00758075.. \n",
      "Epoch: 10651/50000..  Training Loss: 0.00000539..  Test Loss: 0.00743489.. \n",
      "Epoch: 10701/50000..  Training Loss: 0.00000510..  Test Loss: 0.00736921.. \n",
      "Epoch: 10751/50000..  Training Loss: 0.00000460..  Test Loss: 0.00727797.. \n",
      "Epoch: 10801/50000..  Training Loss: 0.00000411..  Test Loss: 0.00733134.. \n",
      "Epoch: 10851/50000..  Training Loss: 0.00000522..  Test Loss: 0.00732942.. \n",
      "Epoch: 10901/50000..  Training Loss: 0.00000536..  Test Loss: 0.00723741.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10951/50000..  Training Loss: 0.00000496..  Test Loss: 0.00737242.. \n",
      "Epoch: 11001/50000..  Training Loss: 0.00000525..  Test Loss: 0.00723017.. \n",
      "Epoch: 11051/50000..  Training Loss: 0.00000446..  Test Loss: 0.00734100.. \n",
      "Epoch: 11101/50000..  Training Loss: 0.00000499..  Test Loss: 0.00730290.. \n",
      "Epoch: 11151/50000..  Training Loss: 0.00000424..  Test Loss: 0.00727661.. \n",
      "Epoch: 11201/50000..  Training Loss: 0.00000372..  Test Loss: 0.00730706.. \n",
      "Epoch: 11251/50000..  Training Loss: 0.00000538..  Test Loss: 0.00720411.. \n",
      "Epoch: 11301/50000..  Training Loss: 0.00000443..  Test Loss: 0.00734812.. \n",
      "Epoch: 11351/50000..  Training Loss: 0.00000378..  Test Loss: 0.00731613.. \n",
      "Epoch: 11401/50000..  Training Loss: 0.00000564..  Test Loss: 0.00728617.. \n",
      "Epoch: 11451/50000..  Training Loss: 0.00000441..  Test Loss: 0.00739326.. \n",
      "Epoch: 11501/50000..  Training Loss: 0.00000595..  Test Loss: 0.00726350.. \n",
      "Epoch: 11551/50000..  Training Loss: 0.00000472..  Test Loss: 0.00734795.. \n",
      "Epoch: 11601/50000..  Training Loss: 0.00000530..  Test Loss: 0.00731474.. \n",
      "Epoch: 11651/50000..  Training Loss: 0.00000418..  Test Loss: 0.00732544.. \n",
      "Epoch: 11701/50000..  Training Loss: 0.00000572..  Test Loss: 0.00749949.. \n",
      "Epoch: 11751/50000..  Training Loss: 0.00000441..  Test Loss: 0.00731281.. \n",
      "Epoch: 11801/50000..  Training Loss: 0.00000505..  Test Loss: 0.00727196.. \n",
      "Epoch: 11851/50000..  Training Loss: 0.00000394..  Test Loss: 0.00732469.. \n",
      "Epoch: 11901/50000..  Training Loss: 0.00000483..  Test Loss: 0.00741479.. \n",
      "Epoch: 11951/50000..  Training Loss: 0.00000444..  Test Loss: 0.00733662.. \n",
      "Epoch: 12001/50000..  Training Loss: 0.00000610..  Test Loss: 0.00733503.. \n",
      "Epoch: 12051/50000..  Training Loss: 0.00000416..  Test Loss: 0.00734532.. \n",
      "Epoch: 12101/50000..  Training Loss: 0.00000408..  Test Loss: 0.00740651.. \n",
      "Epoch: 12151/50000..  Training Loss: 0.00000573..  Test Loss: 0.00738799.. \n",
      "Epoch: 12201/50000..  Training Loss: 0.00000480..  Test Loss: 0.00748328.. \n",
      "Epoch: 12251/50000..  Training Loss: 0.00000703..  Test Loss: 0.00739601.. \n",
      "Epoch: 12301/50000..  Training Loss: 0.00000600..  Test Loss: 0.00750628.. \n",
      "Epoch: 12351/50000..  Training Loss: 0.00000644..  Test Loss: 0.00749403.. \n",
      "Epoch: 12401/50000..  Training Loss: 0.00000455..  Test Loss: 0.00724504.. \n",
      "Epoch: 12451/50000..  Training Loss: 0.00000546..  Test Loss: 0.00751673.. \n",
      "Epoch: 12501/50000..  Training Loss: 0.00000520..  Test Loss: 0.00741874.. \n",
      "Epoch: 12551/50000..  Training Loss: 0.00000542..  Test Loss: 0.00750892.. \n",
      "Epoch: 12601/50000..  Training Loss: 0.00000420..  Test Loss: 0.00742847.. \n",
      "Epoch: 12651/50000..  Training Loss: 0.00000461..  Test Loss: 0.00725771.. \n",
      "Epoch: 12701/50000..  Training Loss: 0.00000434..  Test Loss: 0.00734762.. \n",
      "Epoch: 12751/50000..  Training Loss: 0.00000477..  Test Loss: 0.00723960.. \n",
      "Epoch: 12801/50000..  Training Loss: 0.00000678..  Test Loss: 0.00738596.. \n",
      "Epoch: 12851/50000..  Training Loss: 0.00000384..  Test Loss: 0.00730730.. \n",
      "Epoch: 12901/50000..  Training Loss: 0.00000464..  Test Loss: 0.00735280.. \n",
      "Epoch: 12951/50000..  Training Loss: 0.00000464..  Test Loss: 0.00744725.. \n",
      "Epoch: 13001/50000..  Training Loss: 0.00000601..  Test Loss: 0.00732333.. \n",
      "Epoch: 13051/50000..  Training Loss: 0.00000439..  Test Loss: 0.00729831.. \n",
      "Epoch: 13101/50000..  Training Loss: 0.00000425..  Test Loss: 0.00730878.. \n",
      "Epoch: 13151/50000..  Training Loss: 0.00000455..  Test Loss: 0.00727888.. \n",
      "Epoch: 13201/50000..  Training Loss: 0.00000521..  Test Loss: 0.00727839.. \n",
      "Epoch: 13251/50000..  Training Loss: 0.00000447..  Test Loss: 0.00729229.. \n",
      "Epoch: 13301/50000..  Training Loss: 0.00000523..  Test Loss: 0.00732329.. \n",
      "Epoch: 13351/50000..  Training Loss: 0.00000528..  Test Loss: 0.00741159.. \n",
      "Epoch: 13401/50000..  Training Loss: 0.00000411..  Test Loss: 0.00731706.. \n",
      "Epoch: 13451/50000..  Training Loss: 0.00000427..  Test Loss: 0.00729731.. \n",
      "Epoch: 13501/50000..  Training Loss: 0.00000413..  Test Loss: 0.00738117.. \n",
      "Epoch: 13551/50000..  Training Loss: 0.00000641..  Test Loss: 0.00765647.. \n",
      "Epoch: 13601/50000..  Training Loss: 0.00000466..  Test Loss: 0.00739113.. \n",
      "Epoch: 13651/50000..  Training Loss: 0.00000609..  Test Loss: 0.00754115.. \n",
      "Epoch: 13701/50000..  Training Loss: 0.00000514..  Test Loss: 0.00753317.. \n",
      "Epoch: 13751/50000..  Training Loss: 0.00000648..  Test Loss: 0.00746601.. \n",
      "Epoch: 13801/50000..  Training Loss: 0.00000614..  Test Loss: 0.00748545.. \n",
      "Epoch: 13851/50000..  Training Loss: 0.00000540..  Test Loss: 0.00735331.. \n",
      "Epoch: 13901/50000..  Training Loss: 0.00000819..  Test Loss: 0.00783918.. \n",
      "Epoch: 13951/50000..  Training Loss: 0.00000420..  Test Loss: 0.00742464.. \n",
      "Epoch: 14001/50000..  Training Loss: 0.00000451..  Test Loss: 0.00741289.. \n",
      "Epoch: 14051/50000..  Training Loss: 0.00000372..  Test Loss: 0.00731575.. \n",
      "Epoch: 14101/50000..  Training Loss: 0.00000449..  Test Loss: 0.00725157.. \n",
      "Epoch: 14151/50000..  Training Loss: 0.00000420..  Test Loss: 0.00727845.. \n",
      "Epoch: 14201/50000..  Training Loss: 0.00000420..  Test Loss: 0.00728751.. \n",
      "Epoch: 14251/50000..  Training Loss: 0.00000432..  Test Loss: 0.00728454.. \n",
      "Epoch: 14301/50000..  Training Loss: 0.00000452..  Test Loss: 0.00741873.. \n",
      "Epoch: 14351/50000..  Training Loss: 0.00000457..  Test Loss: 0.00729412.. \n",
      "Epoch: 14401/50000..  Training Loss: 0.00000452..  Test Loss: 0.00729117.. \n",
      "Epoch: 14451/50000..  Training Loss: 0.00000460..  Test Loss: 0.00727961.. \n",
      "Epoch: 14501/50000..  Training Loss: 0.00000476..  Test Loss: 0.00735362.. \n",
      "Epoch: 14551/50000..  Training Loss: 0.00000479..  Test Loss: 0.00758735.. \n",
      "Epoch: 14601/50000..  Training Loss: 0.00000403..  Test Loss: 0.00729509.. \n",
      "Epoch: 14651/50000..  Training Loss: 0.00000507..  Test Loss: 0.00727624.. \n",
      "Epoch: 14701/50000..  Training Loss: 0.00000415..  Test Loss: 0.00740120.. \n",
      "Epoch: 14751/50000..  Training Loss: 0.00000435..  Test Loss: 0.00731338.. \n",
      "Epoch: 14801/50000..  Training Loss: 0.00000417..  Test Loss: 0.00733891.. \n",
      "Epoch: 14851/50000..  Training Loss: 0.00000466..  Test Loss: 0.00728751.. \n",
      "Epoch: 14901/50000..  Training Loss: 0.00000438..  Test Loss: 0.00727501.. \n",
      "Epoch: 14951/50000..  Training Loss: 0.00000482..  Test Loss: 0.00725158.. \n",
      "Epoch: 15001/50000..  Training Loss: 0.00000478..  Test Loss: 0.00734433.. \n",
      "Epoch: 15051/50000..  Training Loss: 0.00000549..  Test Loss: 0.00734937.. \n",
      "Epoch: 15101/50000..  Training Loss: 0.00000452..  Test Loss: 0.00726495.. \n",
      "Epoch: 15151/50000..  Training Loss: 0.00000471..  Test Loss: 0.00733298.. \n",
      "Epoch: 15201/50000..  Training Loss: 0.00000439..  Test Loss: 0.00729764.. \n",
      "Epoch: 15251/50000..  Training Loss: 0.00000524..  Test Loss: 0.00731537.. \n",
      "Epoch: 15301/50000..  Training Loss: 0.00000419..  Test Loss: 0.00730473.. \n",
      "Epoch: 15351/50000..  Training Loss: 0.00000443..  Test Loss: 0.00729091.. \n",
      "Epoch: 15401/50000..  Training Loss: 0.00000458..  Test Loss: 0.00725718.. \n",
      "Epoch: 15451/50000..  Training Loss: 0.00000427..  Test Loss: 0.00729237.. \n",
      "Epoch: 15501/50000..  Training Loss: 0.00000453..  Test Loss: 0.00731288.. \n",
      "Epoch: 15551/50000..  Training Loss: 0.00000414..  Test Loss: 0.00738650.. \n",
      "Epoch: 15601/50000..  Training Loss: 0.00000474..  Test Loss: 0.00732176.. \n",
      "Epoch: 15651/50000..  Training Loss: 0.00000404..  Test Loss: 0.00737861.. \n",
      "Epoch: 15701/50000..  Training Loss: 0.00000483..  Test Loss: 0.00740776.. \n",
      "Epoch: 15751/50000..  Training Loss: 0.00000400..  Test Loss: 0.00740431.. \n",
      "Epoch: 15801/50000..  Training Loss: 0.00000437..  Test Loss: 0.00734002.. \n",
      "Epoch: 15851/50000..  Training Loss: 0.00000426..  Test Loss: 0.00730457.. \n",
      "Epoch: 15901/50000..  Training Loss: 0.00000459..  Test Loss: 0.00742299.. \n",
      "Epoch: 15951/50000..  Training Loss: 0.00000546..  Test Loss: 0.00743023.. \n",
      "Epoch: 16001/50000..  Training Loss: 0.00000482..  Test Loss: 0.00747139.. \n",
      "Epoch: 16051/50000..  Training Loss: 0.00000449..  Test Loss: 0.00749097.. \n",
      "Epoch: 16101/50000..  Training Loss: 0.00000431..  Test Loss: 0.00744465.. \n",
      "Epoch: 16151/50000..  Training Loss: 0.00000519..  Test Loss: 0.00736370.. \n",
      "Epoch: 16201/50000..  Training Loss: 0.00000503..  Test Loss: 0.00747155.. \n",
      "Epoch: 16251/50000..  Training Loss: 0.00000517..  Test Loss: 0.00735662.. \n",
      "Epoch: 16301/50000..  Training Loss: 0.00000460..  Test Loss: 0.00732413.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16351/50000..  Training Loss: 0.00000451..  Test Loss: 0.00727642.. \n",
      "Epoch: 16401/50000..  Training Loss: 0.00000456..  Test Loss: 0.00729834.. \n",
      "Epoch: 16451/50000..  Training Loss: 0.00000491..  Test Loss: 0.00724113.. \n",
      "Epoch: 16501/50000..  Training Loss: 0.00000487..  Test Loss: 0.00728311.. \n",
      "Epoch: 16551/50000..  Training Loss: 0.00000492..  Test Loss: 0.00734120.. \n",
      "Epoch: 16601/50000..  Training Loss: 0.00000444..  Test Loss: 0.00752602.. \n",
      "Epoch: 16651/50000..  Training Loss: 0.00000511..  Test Loss: 0.00749659.. \n",
      "Epoch: 16701/50000..  Training Loss: 0.00000541..  Test Loss: 0.00748708.. \n",
      "Epoch: 16751/50000..  Training Loss: 0.00000497..  Test Loss: 0.00744697.. \n",
      "Epoch: 16801/50000..  Training Loss: 0.00000509..  Test Loss: 0.00740944.. \n",
      "Epoch: 16851/50000..  Training Loss: 0.00000406..  Test Loss: 0.00740546.. \n",
      "Epoch: 16901/50000..  Training Loss: 0.00000659..  Test Loss: 0.00750594.. \n",
      "Epoch: 16951/50000..  Training Loss: 0.00000480..  Test Loss: 0.00745984.. \n",
      "Epoch: 17001/50000..  Training Loss: 0.00000422..  Test Loss: 0.00739806.. \n",
      "Epoch: 17051/50000..  Training Loss: 0.00000542..  Test Loss: 0.00750970.. \n",
      "Epoch: 17101/50000..  Training Loss: 0.00000430..  Test Loss: 0.00744876.. \n",
      "Epoch: 17151/50000..  Training Loss: 0.00000489..  Test Loss: 0.00744865.. \n",
      "Epoch: 17201/50000..  Training Loss: 0.00000601..  Test Loss: 0.00757359.. \n",
      "Epoch: 17251/50000..  Training Loss: 0.00000627..  Test Loss: 0.00761437.. \n",
      "Epoch: 17301/50000..  Training Loss: 0.00000551..  Test Loss: 0.00746737.. \n",
      "Epoch: 17351/50000..  Training Loss: 0.00000544..  Test Loss: 0.00736217.. \n",
      "Epoch: 17401/50000..  Training Loss: 0.00000543..  Test Loss: 0.00755164.. \n",
      "Epoch: 17451/50000..  Training Loss: 0.00000487..  Test Loss: 0.00737839.. \n",
      "Epoch: 17501/50000..  Training Loss: 0.00000533..  Test Loss: 0.00751486.. \n",
      "Epoch: 17551/50000..  Training Loss: 0.00000576..  Test Loss: 0.00746920.. \n",
      "Epoch: 17601/50000..  Training Loss: 0.00000705..  Test Loss: 0.00751021.. \n",
      "Epoch: 17651/50000..  Training Loss: 0.00000511..  Test Loss: 0.00744708.. \n",
      "Epoch: 17701/50000..  Training Loss: 0.00000485..  Test Loss: 0.00746213.. \n",
      "Epoch: 17751/50000..  Training Loss: 0.00000476..  Test Loss: 0.00727006.. \n",
      "Epoch: 17801/50000..  Training Loss: 0.00000503..  Test Loss: 0.00739649.. \n",
      "Epoch: 17851/50000..  Training Loss: 0.00000444..  Test Loss: 0.00725861.. \n",
      "Epoch: 17901/50000..  Training Loss: 0.00000707..  Test Loss: 0.00751628.. \n",
      "Epoch: 17951/50000..  Training Loss: 0.00000423..  Test Loss: 0.00740968.. \n",
      "Epoch: 18001/50000..  Training Loss: 0.00000377..  Test Loss: 0.00736263.. \n",
      "Epoch: 18051/50000..  Training Loss: 0.00000434..  Test Loss: 0.00727366.. \n",
      "Epoch: 18101/50000..  Training Loss: 0.00000471..  Test Loss: 0.00729125.. \n",
      "Epoch: 18151/50000..  Training Loss: 0.00000443..  Test Loss: 0.00729811.. \n",
      "Epoch: 18201/50000..  Training Loss: 0.00000571..  Test Loss: 0.00751591.. \n",
      "Epoch: 18251/50000..  Training Loss: 0.00000786..  Test Loss: 0.00792176.. \n",
      "Epoch: 18301/50000..  Training Loss: 0.00000479..  Test Loss: 0.00727002.. \n",
      "Epoch: 18351/50000..  Training Loss: 0.00000427..  Test Loss: 0.00727363.. \n",
      "Epoch: 18401/50000..  Training Loss: 0.00000549..  Test Loss: 0.00736210.. \n",
      "Epoch: 18451/50000..  Training Loss: 0.00000504..  Test Loss: 0.00750520.. \n",
      "Epoch: 18501/50000..  Training Loss: 0.00000467..  Test Loss: 0.00744338.. \n",
      "Epoch: 18551/50000..  Training Loss: 0.00000536..  Test Loss: 0.00741279.. \n",
      "Epoch: 18601/50000..  Training Loss: 0.00000409..  Test Loss: 0.00740008.. \n",
      "Epoch: 18651/50000..  Training Loss: 0.00000524..  Test Loss: 0.00748934.. \n",
      "Epoch: 18701/50000..  Training Loss: 0.00000716..  Test Loss: 0.00746005.. \n",
      "Epoch: 18751/50000..  Training Loss: 0.00000460..  Test Loss: 0.00747323.. \n",
      "Epoch: 18801/50000..  Training Loss: 0.00000525..  Test Loss: 0.00728833.. \n",
      "Epoch: 18851/50000..  Training Loss: 0.00000421..  Test Loss: 0.00725795.. \n",
      "Epoch: 18901/50000..  Training Loss: 0.00000367..  Test Loss: 0.00730164.. \n",
      "Epoch: 18951/50000..  Training Loss: 0.00000428..  Test Loss: 0.00737454.. \n",
      "Epoch: 19001/50000..  Training Loss: 0.00000420..  Test Loss: 0.00734043.. \n",
      "Epoch: 19051/50000..  Training Loss: 0.00000411..  Test Loss: 0.00741737.. \n",
      "Epoch: 19101/50000..  Training Loss: 0.00000417..  Test Loss: 0.00729804.. \n",
      "Epoch: 19151/50000..  Training Loss: 0.00000425..  Test Loss: 0.00727247.. \n",
      "Epoch: 19201/50000..  Training Loss: 0.00000427..  Test Loss: 0.00724110.. \n",
      "Epoch: 19251/50000..  Training Loss: 0.00000525..  Test Loss: 0.00748705.. \n",
      "Epoch: 19301/50000..  Training Loss: 0.00000433..  Test Loss: 0.00738388.. \n",
      "Epoch: 19351/50000..  Training Loss: 0.00000474..  Test Loss: 0.00747233.. \n",
      "Epoch: 19401/50000..  Training Loss: 0.00000821..  Test Loss: 0.00789722.. \n",
      "Epoch: 19451/50000..  Training Loss: 0.00000477..  Test Loss: 0.00737106.. \n",
      "Epoch: 19501/50000..  Training Loss: 0.00000458..  Test Loss: 0.00741997.. \n",
      "Epoch: 19551/50000..  Training Loss: 0.00000558..  Test Loss: 0.00745905.. \n",
      "Epoch: 19601/50000..  Training Loss: 0.00000522..  Test Loss: 0.00737937.. \n",
      "Epoch: 19651/50000..  Training Loss: 0.00000539..  Test Loss: 0.00747335.. \n",
      "Epoch: 19701/50000..  Training Loss: 0.00000559..  Test Loss: 0.00746902.. \n",
      "Epoch: 19751/50000..  Training Loss: 0.00000524..  Test Loss: 0.00736237.. \n",
      "Epoch: 19801/50000..  Training Loss: 0.00000454..  Test Loss: 0.00746289.. \n",
      "Epoch: 19851/50000..  Training Loss: 0.00000546..  Test Loss: 0.00739855.. \n",
      "Epoch: 19901/50000..  Training Loss: 0.00000411..  Test Loss: 0.00739050.. \n",
      "Epoch: 19951/50000..  Training Loss: 0.00000600..  Test Loss: 0.00733642.. \n",
      "Epoch: 20001/50000..  Training Loss: 0.00000494..  Test Loss: 0.00725200.. \n",
      "Epoch: 20051/50000..  Training Loss: 0.00000444..  Test Loss: 0.00720773.. \n",
      "Epoch: 20101/50000..  Training Loss: 0.00000475..  Test Loss: 0.00724885.. \n",
      "Epoch: 20151/50000..  Training Loss: 0.00000406..  Test Loss: 0.00723831.. \n",
      "Epoch: 20201/50000..  Training Loss: 0.00000452..  Test Loss: 0.00734478.. \n",
      "Epoch: 20251/50000..  Training Loss: 0.00000374..  Test Loss: 0.00726326.. \n",
      "Epoch: 20301/50000..  Training Loss: 0.00000537..  Test Loss: 0.00722090.. \n",
      "Epoch: 20351/50000..  Training Loss: 0.00000443..  Test Loss: 0.00738312.. \n",
      "Epoch: 20401/50000..  Training Loss: 0.00000444..  Test Loss: 0.00723709.. \n",
      "Epoch: 20451/50000..  Training Loss: 0.00000393..  Test Loss: 0.00742097.. \n",
      "Epoch: 20501/50000..  Training Loss: 0.00000422..  Test Loss: 0.00733938.. \n",
      "Epoch: 20551/50000..  Training Loss: 0.00000506..  Test Loss: 0.00738171.. \n",
      "Epoch: 20601/50000..  Training Loss: 0.00000382..  Test Loss: 0.00741813.. \n",
      "Epoch: 20651/50000..  Training Loss: 0.00000477..  Test Loss: 0.00726648.. \n",
      "Epoch: 20701/50000..  Training Loss: 0.00000470..  Test Loss: 0.00746085.. \n",
      "Epoch: 20751/50000..  Training Loss: 0.00000618..  Test Loss: 0.00742381.. \n",
      "Epoch: 20801/50000..  Training Loss: 0.00000455..  Test Loss: 0.00751157.. \n",
      "Epoch: 20851/50000..  Training Loss: 0.00000569..  Test Loss: 0.00743301.. \n",
      "Epoch: 20901/50000..  Training Loss: 0.00000566..  Test Loss: 0.00747460.. \n",
      "Epoch: 20951/50000..  Training Loss: 0.00000504..  Test Loss: 0.00731465.. \n",
      "Epoch: 21001/50000..  Training Loss: 0.00000983..  Test Loss: 0.00725924.. \n",
      "Epoch: 21051/50000..  Training Loss: 0.00000476..  Test Loss: 0.00739122.. \n",
      "Epoch: 21101/50000..  Training Loss: 0.00000382..  Test Loss: 0.00739457.. \n",
      "Epoch: 21151/50000..  Training Loss: 0.00000599..  Test Loss: 0.00737640.. \n",
      "Epoch: 21201/50000..  Training Loss: 0.00000621..  Test Loss: 0.00747030.. \n",
      "Epoch: 21251/50000..  Training Loss: 0.00000526..  Test Loss: 0.00719497.. \n",
      "Epoch: 21301/50000..  Training Loss: 0.00000469..  Test Loss: 0.00730978.. \n",
      "Epoch: 21351/50000..  Training Loss: 0.00000512..  Test Loss: 0.00723803.. \n",
      "Epoch: 21401/50000..  Training Loss: 0.00000552..  Test Loss: 0.00731942.. \n",
      "Epoch: 21451/50000..  Training Loss: 0.00000529..  Test Loss: 0.00748892.. \n",
      "Epoch: 21501/50000..  Training Loss: 0.00000385..  Test Loss: 0.00732084.. \n",
      "Epoch: 21551/50000..  Training Loss: 0.00000471..  Test Loss: 0.00744737.. \n",
      "Epoch: 21601/50000..  Training Loss: 0.00000389..  Test Loss: 0.00723372.. \n",
      "Epoch: 21651/50000..  Training Loss: 0.00000438..  Test Loss: 0.00735862.. \n",
      "Epoch: 21701/50000..  Training Loss: 0.00000444..  Test Loss: 0.00732818.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21751/50000..  Training Loss: 0.00000420..  Test Loss: 0.00719231.. \n",
      "Epoch: 21801/50000..  Training Loss: 0.00000400..  Test Loss: 0.00723666.. \n",
      "Epoch: 21851/50000..  Training Loss: 0.00000564..  Test Loss: 0.00732034.. \n",
      "Epoch: 21901/50000..  Training Loss: 0.00000613..  Test Loss: 0.00748868.. \n",
      "Epoch: 21951/50000..  Training Loss: 0.00000426..  Test Loss: 0.00733901.. \n",
      "Epoch: 22001/50000..  Training Loss: 0.00000432..  Test Loss: 0.00732424.. \n",
      "Epoch: 22051/50000..  Training Loss: 0.00000444..  Test Loss: 0.00733986.. \n",
      "Epoch: 22101/50000..  Training Loss: 0.00000541..  Test Loss: 0.00739488.. \n",
      "Epoch: 22151/50000..  Training Loss: 0.00000527..  Test Loss: 0.00730590.. \n",
      "Epoch: 22201/50000..  Training Loss: 0.00000536..  Test Loss: 0.00744360.. \n",
      "Epoch: 22251/50000..  Training Loss: 0.00000493..  Test Loss: 0.00733963.. \n",
      "Epoch: 22301/50000..  Training Loss: 0.00000427..  Test Loss: 0.00727880.. \n",
      "Epoch: 22351/50000..  Training Loss: 0.00000477..  Test Loss: 0.00748938.. \n",
      "Epoch: 22401/50000..  Training Loss: 0.00000926..  Test Loss: 0.00759243.. \n",
      "Epoch: 22451/50000..  Training Loss: 0.00000482..  Test Loss: 0.00726384.. \n",
      "Epoch: 22501/50000..  Training Loss: 0.00000738..  Test Loss: 0.00719847.. \n",
      "Epoch: 22551/50000..  Training Loss: 0.00000549..  Test Loss: 0.00736304.. \n",
      "Epoch: 22601/50000..  Training Loss: 0.00000594..  Test Loss: 0.00742986.. \n",
      "Epoch: 22651/50000..  Training Loss: 0.00000371..  Test Loss: 0.00726870.. \n",
      "Epoch: 22701/50000..  Training Loss: 0.00000528..  Test Loss: 0.00746011.. \n",
      "Epoch: 22751/50000..  Training Loss: 0.00000484..  Test Loss: 0.00736876.. \n",
      "Epoch: 22801/50000..  Training Loss: 0.00000643..  Test Loss: 0.00717109.. \n",
      "Epoch: 22851/50000..  Training Loss: 0.00000442..  Test Loss: 0.00731289.. \n",
      "Epoch: 22901/50000..  Training Loss: 0.00000462..  Test Loss: 0.00722133.. \n",
      "Epoch: 22951/50000..  Training Loss: 0.00000408..  Test Loss: 0.00730341.. \n",
      "Epoch: 23001/50000..  Training Loss: 0.00000723..  Test Loss: 0.00732408.. \n",
      "Epoch: 23051/50000..  Training Loss: 0.00000479..  Test Loss: 0.00735629.. \n",
      "Epoch: 23101/50000..  Training Loss: 0.00000525..  Test Loss: 0.00738483.. \n",
      "Epoch: 23151/50000..  Training Loss: 0.00000414..  Test Loss: 0.00740471.. \n",
      "Epoch: 23201/50000..  Training Loss: 0.00000360..  Test Loss: 0.00724480.. \n",
      "Epoch: 23251/50000..  Training Loss: 0.00000355..  Test Loss: 0.00720156.. \n",
      "Epoch: 23301/50000..  Training Loss: 0.00000547..  Test Loss: 0.00729413.. \n",
      "Epoch: 23351/50000..  Training Loss: 0.00000456..  Test Loss: 0.00726345.. \n",
      "Epoch: 23401/50000..  Training Loss: 0.00000509..  Test Loss: 0.00771154.. \n",
      "Epoch: 23451/50000..  Training Loss: 0.00000570..  Test Loss: 0.00741533.. \n",
      "Epoch: 23501/50000..  Training Loss: 0.00000482..  Test Loss: 0.00736352.. \n",
      "Epoch: 23551/50000..  Training Loss: 0.00000405..  Test Loss: 0.00725392.. \n",
      "Epoch: 23601/50000..  Training Loss: 0.00000458..  Test Loss: 0.00729696.. \n",
      "Epoch: 23651/50000..  Training Loss: 0.00000442..  Test Loss: 0.00756149.. \n",
      "Epoch: 23701/50000..  Training Loss: 0.00000464..  Test Loss: 0.00730794.. \n",
      "Epoch: 23751/50000..  Training Loss: 0.00000444..  Test Loss: 0.00729092.. \n",
      "Epoch: 23801/50000..  Training Loss: 0.00000420..  Test Loss: 0.00720217.. \n",
      "Epoch: 23851/50000..  Training Loss: 0.00000402..  Test Loss: 0.00718819.. \n",
      "Epoch: 23901/50000..  Training Loss: 0.00000490..  Test Loss: 0.00737809.. \n",
      "Epoch: 23951/50000..  Training Loss: 0.00000416..  Test Loss: 0.00743209.. \n",
      "Epoch: 24001/50000..  Training Loss: 0.00000440..  Test Loss: 0.00717980.. \n",
      "Epoch: 24051/50000..  Training Loss: 0.00000437..  Test Loss: 0.00727474.. \n",
      "Epoch: 24101/50000..  Training Loss: 0.00000367..  Test Loss: 0.00721822.. \n",
      "Epoch: 24151/50000..  Training Loss: 0.00000393..  Test Loss: 0.00725854.. \n",
      "Epoch: 24201/50000..  Training Loss: 0.00000427..  Test Loss: 0.00730150.. \n",
      "Epoch: 24251/50000..  Training Loss: 0.00000511..  Test Loss: 0.00766349.. \n",
      "Epoch: 24301/50000..  Training Loss: 0.00000526..  Test Loss: 0.00742214.. \n",
      "Epoch: 24351/50000..  Training Loss: 0.00000533..  Test Loss: 0.00760358.. \n",
      "Epoch: 24401/50000..  Training Loss: 0.00000489..  Test Loss: 0.00731889.. \n",
      "Epoch: 24451/50000..  Training Loss: 0.00000429..  Test Loss: 0.00724474.. \n",
      "Epoch: 24501/50000..  Training Loss: 0.00000412..  Test Loss: 0.00731147.. \n",
      "Epoch: 24551/50000..  Training Loss: 0.00000524..  Test Loss: 0.00722635.. \n",
      "Epoch: 24601/50000..  Training Loss: 0.00000496..  Test Loss: 0.00766342.. \n",
      "Epoch: 24651/50000..  Training Loss: 0.00000570..  Test Loss: 0.00733208.. \n",
      "Epoch: 24701/50000..  Training Loss: 0.00000383..  Test Loss: 0.00729070.. \n",
      "Epoch: 24751/50000..  Training Loss: 0.00000566..  Test Loss: 0.00730576.. \n",
      "Epoch: 24801/50000..  Training Loss: 0.00000512..  Test Loss: 0.00729301.. \n",
      "Epoch: 24851/50000..  Training Loss: 0.00000392..  Test Loss: 0.00722086.. \n",
      "Epoch: 24901/50000..  Training Loss: 0.00000621..  Test Loss: 0.00724300.. \n",
      "Epoch: 24951/50000..  Training Loss: 0.00000412..  Test Loss: 0.00718287.. \n",
      "Epoch: 25001/50000..  Training Loss: 0.00000387..  Test Loss: 0.00721153.. \n",
      "Epoch: 25051/50000..  Training Loss: 0.00000387..  Test Loss: 0.00724773.. \n",
      "Epoch: 25101/50000..  Training Loss: 0.00000524..  Test Loss: 0.00734717.. \n",
      "Epoch: 25151/50000..  Training Loss: 0.00000492..  Test Loss: 0.00716816.. \n",
      "Epoch: 25201/50000..  Training Loss: 0.00000428..  Test Loss: 0.00724592.. \n",
      "Epoch: 25251/50000..  Training Loss: 0.00000540..  Test Loss: 0.00735870.. \n",
      "Epoch: 25301/50000..  Training Loss: 0.00000511..  Test Loss: 0.00718344.. \n",
      "Epoch: 25351/50000..  Training Loss: 0.00000454..  Test Loss: 0.00753071.. \n",
      "Epoch: 25401/50000..  Training Loss: 0.00000484..  Test Loss: 0.00724632.. \n",
      "Epoch: 25451/50000..  Training Loss: 0.00000386..  Test Loss: 0.00718331.. \n",
      "Epoch: 25501/50000..  Training Loss: 0.00000604..  Test Loss: 0.00725064.. \n",
      "Epoch: 25551/50000..  Training Loss: 0.00000575..  Test Loss: 0.00743374.. \n",
      "Epoch: 25601/50000..  Training Loss: 0.00000463..  Test Loss: 0.00719707.. \n",
      "Epoch: 25651/50000..  Training Loss: 0.00000700..  Test Loss: 0.00763183.. \n",
      "Epoch: 25701/50000..  Training Loss: 0.00000463..  Test Loss: 0.00729579.. \n",
      "Epoch: 25751/50000..  Training Loss: 0.00000439..  Test Loss: 0.00729853.. \n",
      "Epoch: 25801/50000..  Training Loss: 0.00000447..  Test Loss: 0.00723759.. \n",
      "Epoch: 25851/50000..  Training Loss: 0.00000477..  Test Loss: 0.00733127.. \n",
      "Epoch: 25901/50000..  Training Loss: 0.00000571..  Test Loss: 0.00739948.. \n",
      "Epoch: 25951/50000..  Training Loss: 0.00000448..  Test Loss: 0.00731894.. \n",
      "Epoch: 26001/50000..  Training Loss: 0.00000481..  Test Loss: 0.00721486.. \n",
      "Epoch: 26051/50000..  Training Loss: 0.00000488..  Test Loss: 0.00717368.. \n",
      "Epoch: 26101/50000..  Training Loss: 0.00000485..  Test Loss: 0.00720292.. \n",
      "Epoch: 26151/50000..  Training Loss: 0.00000357..  Test Loss: 0.00714733.. \n",
      "Epoch: 26201/50000..  Training Loss: 0.00000418..  Test Loss: 0.00713458.. \n",
      "Epoch: 26251/50000..  Training Loss: 0.00000602..  Test Loss: 0.00736493.. \n",
      "Epoch: 26301/50000..  Training Loss: 0.00000385..  Test Loss: 0.00717570.. \n",
      "Epoch: 26351/50000..  Training Loss: 0.00000400..  Test Loss: 0.00732090.. \n",
      "Epoch: 26401/50000..  Training Loss: 0.00000423..  Test Loss: 0.00718203.. \n",
      "Epoch: 26451/50000..  Training Loss: 0.00000421..  Test Loss: 0.00719201.. \n",
      "Epoch: 26501/50000..  Training Loss: 0.00000398..  Test Loss: 0.00723817.. \n",
      "Epoch: 26551/50000..  Training Loss: 0.00000707..  Test Loss: 0.00750459.. \n",
      "Epoch: 26601/50000..  Training Loss: 0.00000381..  Test Loss: 0.00714738.. \n",
      "Epoch: 26651/50000..  Training Loss: 0.00000395..  Test Loss: 0.00742716.. \n",
      "Epoch: 26701/50000..  Training Loss: 0.00000513..  Test Loss: 0.00738111.. \n",
      "Epoch: 26751/50000..  Training Loss: 0.00000401..  Test Loss: 0.00718759.. \n",
      "Epoch: 26801/50000..  Training Loss: 0.00000540..  Test Loss: 0.00729162.. \n",
      "Epoch: 26851/50000..  Training Loss: 0.00000360..  Test Loss: 0.00725012.. \n",
      "Epoch: 26901/50000..  Training Loss: 0.00000449..  Test Loss: 0.00723305.. \n",
      "Epoch: 26951/50000..  Training Loss: 0.00000478..  Test Loss: 0.00739338.. \n",
      "Epoch: 27001/50000..  Training Loss: 0.00000564..  Test Loss: 0.00774707.. \n",
      "Epoch: 27051/50000..  Training Loss: 0.00000417..  Test Loss: 0.00719919.. \n",
      "Epoch: 27101/50000..  Training Loss: 0.00000366..  Test Loss: 0.00724964.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27151/50000..  Training Loss: 0.00000462..  Test Loss: 0.00723933.. \n",
      "Epoch: 27201/50000..  Training Loss: 0.00000470..  Test Loss: 0.00743951.. \n",
      "Epoch: 27251/50000..  Training Loss: 0.00000551..  Test Loss: 0.00727326.. \n",
      "Epoch: 27301/50000..  Training Loss: 0.00000483..  Test Loss: 0.00733044.. \n",
      "Epoch: 27351/50000..  Training Loss: 0.00000436..  Test Loss: 0.00716568.. \n",
      "Epoch: 27401/50000..  Training Loss: 0.00000466..  Test Loss: 0.00733677.. \n",
      "Epoch: 27451/50000..  Training Loss: 0.00000458..  Test Loss: 0.00726073.. \n",
      "Epoch: 27501/50000..  Training Loss: 0.00000673..  Test Loss: 0.00744011.. \n",
      "Epoch: 27551/50000..  Training Loss: 0.00000410..  Test Loss: 0.00735903.. \n",
      "Epoch: 27601/50000..  Training Loss: 0.00000394..  Test Loss: 0.00726287.. \n",
      "Epoch: 27651/50000..  Training Loss: 0.00000447..  Test Loss: 0.00728012.. \n",
      "Epoch: 27701/50000..  Training Loss: 0.00000432..  Test Loss: 0.00736514.. \n",
      "Epoch: 27751/50000..  Training Loss: 0.00000453..  Test Loss: 0.00723740.. \n",
      "Epoch: 27801/50000..  Training Loss: 0.00000425..  Test Loss: 0.00724005.. \n",
      "Epoch: 27851/50000..  Training Loss: 0.00000492..  Test Loss: 0.00733933.. \n",
      "Epoch: 27901/50000..  Training Loss: 0.00000482..  Test Loss: 0.00730666.. \n",
      "Epoch: 27951/50000..  Training Loss: 0.00000401..  Test Loss: 0.00716937.. \n",
      "Epoch: 28001/50000..  Training Loss: 0.00000494..  Test Loss: 0.00742841.. \n",
      "Epoch: 28051/50000..  Training Loss: 0.00000489..  Test Loss: 0.00724292.. \n",
      "Epoch: 28101/50000..  Training Loss: 0.00000458..  Test Loss: 0.00729579.. \n",
      "Epoch: 28151/50000..  Training Loss: 0.00000556..  Test Loss: 0.00735888.. \n",
      "Epoch: 28201/50000..  Training Loss: 0.00000393..  Test Loss: 0.00716165.. \n",
      "Epoch: 28251/50000..  Training Loss: 0.00000414..  Test Loss: 0.00717413.. \n",
      "Epoch: 28301/50000..  Training Loss: 0.00000543..  Test Loss: 0.00720432.. \n",
      "Epoch: 28351/50000..  Training Loss: 0.00000495..  Test Loss: 0.00733458.. \n",
      "Epoch: 28401/50000..  Training Loss: 0.00000462..  Test Loss: 0.00742754.. \n",
      "Epoch: 28451/50000..  Training Loss: 0.00000466..  Test Loss: 0.00731041.. \n",
      "Epoch: 28501/50000..  Training Loss: 0.00000477..  Test Loss: 0.00731994.. \n",
      "Epoch: 28551/50000..  Training Loss: 0.00000480..  Test Loss: 0.00731791.. \n",
      "Epoch: 28601/50000..  Training Loss: 0.00000461..  Test Loss: 0.00730347.. \n",
      "Epoch: 28651/50000..  Training Loss: 0.00000508..  Test Loss: 0.00744498.. \n",
      "Epoch: 28701/50000..  Training Loss: 0.00000404..  Test Loss: 0.00720807.. \n",
      "Epoch: 28751/50000..  Training Loss: 0.00000443..  Test Loss: 0.00728444.. \n",
      "Epoch: 28801/50000..  Training Loss: 0.00000479..  Test Loss: 0.00731112.. \n",
      "Epoch: 28851/50000..  Training Loss: 0.00000501..  Test Loss: 0.00740204.. \n",
      "Epoch: 28901/50000..  Training Loss: 0.00000715..  Test Loss: 0.00734776.. \n",
      "Epoch: 28951/50000..  Training Loss: 0.00000379..  Test Loss: 0.00719955.. \n",
      "Epoch: 29001/50000..  Training Loss: 0.00000460..  Test Loss: 0.00744235.. \n",
      "Epoch: 29051/50000..  Training Loss: 0.00000374..  Test Loss: 0.00719572.. \n",
      "Epoch: 29101/50000..  Training Loss: 0.00000462..  Test Loss: 0.00729313.. \n",
      "Epoch: 29151/50000..  Training Loss: 0.00000476..  Test Loss: 0.00728334.. \n",
      "Epoch: 29201/50000..  Training Loss: 0.00000549..  Test Loss: 0.00734639.. \n",
      "Epoch: 29251/50000..  Training Loss: 0.00000418..  Test Loss: 0.00729311.. \n",
      "Epoch: 29301/50000..  Training Loss: 0.00000432..  Test Loss: 0.00716932.. \n",
      "Epoch: 29351/50000..  Training Loss: 0.00000458..  Test Loss: 0.00740857.. \n",
      "Epoch: 29401/50000..  Training Loss: 0.00000528..  Test Loss: 0.00737993.. \n",
      "Epoch: 29451/50000..  Training Loss: 0.00000434..  Test Loss: 0.00736546.. \n",
      "Epoch: 29501/50000..  Training Loss: 0.00000423..  Test Loss: 0.00719504.. \n",
      "Epoch: 29551/50000..  Training Loss: 0.00000468..  Test Loss: 0.00718839.. \n",
      "Epoch: 29601/50000..  Training Loss: 0.00000525..  Test Loss: 0.00737277.. \n",
      "Epoch: 29651/50000..  Training Loss: 0.00000459..  Test Loss: 0.00716613.. \n",
      "Epoch: 29701/50000..  Training Loss: 0.00000417..  Test Loss: 0.00722758.. \n",
      "Epoch: 29751/50000..  Training Loss: 0.00000381..  Test Loss: 0.00717270.. \n",
      "Epoch: 29801/50000..  Training Loss: 0.00000457..  Test Loss: 0.00728860.. \n",
      "Epoch: 29851/50000..  Training Loss: 0.00000484..  Test Loss: 0.00733664.. \n",
      "Epoch: 29901/50000..  Training Loss: 0.00000389..  Test Loss: 0.00723696.. \n",
      "Epoch: 29951/50000..  Training Loss: 0.00000370..  Test Loss: 0.00725437.. \n",
      "Epoch: 30001/50000..  Training Loss: 0.00000560..  Test Loss: 0.00740401.. \n",
      "Epoch: 30051/50000..  Training Loss: 0.00000396..  Test Loss: 0.00721821.. \n",
      "Epoch: 30101/50000..  Training Loss: 0.00000466..  Test Loss: 0.00723309.. \n",
      "Epoch: 30151/50000..  Training Loss: 0.00000425..  Test Loss: 0.00713067.. \n",
      "Epoch: 30201/50000..  Training Loss: 0.00000455..  Test Loss: 0.00717586.. \n",
      "Epoch: 30251/50000..  Training Loss: 0.00000493..  Test Loss: 0.00713150.. \n",
      "Epoch: 30301/50000..  Training Loss: 0.00000395..  Test Loss: 0.00719345.. \n",
      "Epoch: 30351/50000..  Training Loss: 0.00000455..  Test Loss: 0.00734924.. \n",
      "Epoch: 30401/50000..  Training Loss: 0.00000426..  Test Loss: 0.00730486.. \n",
      "Epoch: 30451/50000..  Training Loss: 0.00000417..  Test Loss: 0.00727094.. \n",
      "Epoch: 30501/50000..  Training Loss: 0.00000476..  Test Loss: 0.00732256.. \n",
      "Epoch: 30551/50000..  Training Loss: 0.00000762..  Test Loss: 0.00740833.. \n",
      "Epoch: 30601/50000..  Training Loss: 0.00000469..  Test Loss: 0.00738536.. \n",
      "Epoch: 30651/50000..  Training Loss: 0.00000379..  Test Loss: 0.00715060.. \n",
      "Epoch: 30701/50000..  Training Loss: 0.00000442..  Test Loss: 0.00730973.. \n",
      "Epoch: 30751/50000..  Training Loss: 0.00000410..  Test Loss: 0.00736208.. \n",
      "Epoch: 30801/50000..  Training Loss: 0.00000434..  Test Loss: 0.00728944.. \n",
      "Epoch: 30851/50000..  Training Loss: 0.00000439..  Test Loss: 0.00732441.. \n",
      "Epoch: 30901/50000..  Training Loss: 0.00000554..  Test Loss: 0.00733487.. \n",
      "Epoch: 30951/50000..  Training Loss: 0.00000441..  Test Loss: 0.00738909.. \n",
      "Epoch: 31001/50000..  Training Loss: 0.00000457..  Test Loss: 0.00715297.. \n",
      "Epoch: 31051/50000..  Training Loss: 0.00000521..  Test Loss: 0.00742172.. \n",
      "Epoch: 31101/50000..  Training Loss: 0.00000553..  Test Loss: 0.00740636.. \n",
      "Epoch: 31151/50000..  Training Loss: 0.00000538..  Test Loss: 0.00732830.. \n",
      "Epoch: 31201/50000..  Training Loss: 0.00000408..  Test Loss: 0.00728039.. \n",
      "Epoch: 31251/50000..  Training Loss: 0.00000469..  Test Loss: 0.00734748.. \n",
      "Epoch: 31301/50000..  Training Loss: 0.00000533..  Test Loss: 0.00752752.. \n",
      "Epoch: 31351/50000..  Training Loss: 0.00000478..  Test Loss: 0.00733787.. \n",
      "Epoch: 31401/50000..  Training Loss: 0.00000523..  Test Loss: 0.00735898.. \n",
      "Epoch: 31451/50000..  Training Loss: 0.00000440..  Test Loss: 0.00730511.. \n",
      "Epoch: 31501/50000..  Training Loss: 0.00000453..  Test Loss: 0.00736783.. \n",
      "Epoch: 31551/50000..  Training Loss: 0.00000415..  Test Loss: 0.00728041.. \n",
      "Epoch: 31601/50000..  Training Loss: 0.00000458..  Test Loss: 0.00733361.. \n",
      "Epoch: 31651/50000..  Training Loss: 0.00000460..  Test Loss: 0.00729243.. \n",
      "Epoch: 31701/50000..  Training Loss: 0.00000467..  Test Loss: 0.00725148.. \n",
      "Epoch: 31751/50000..  Training Loss: 0.00000384..  Test Loss: 0.00716947.. \n",
      "Epoch: 31801/50000..  Training Loss: 0.00000455..  Test Loss: 0.00718472.. \n",
      "Epoch: 31851/50000..  Training Loss: 0.00000432..  Test Loss: 0.00731713.. \n",
      "Epoch: 31901/50000..  Training Loss: 0.00000418..  Test Loss: 0.00731677.. \n",
      "Epoch: 31951/50000..  Training Loss: 0.00000423..  Test Loss: 0.00719816.. \n",
      "Epoch: 32001/50000..  Training Loss: 0.00000382..  Test Loss: 0.00723824.. \n",
      "Epoch: 32051/50000..  Training Loss: 0.00000489..  Test Loss: 0.00721949.. \n",
      "Epoch: 32101/50000..  Training Loss: 0.00000581..  Test Loss: 0.00724881.. \n",
      "Epoch: 32151/50000..  Training Loss: 0.00000372..  Test Loss: 0.00721765.. \n",
      "Epoch: 32201/50000..  Training Loss: 0.00000411..  Test Loss: 0.00729810.. \n",
      "Epoch: 32251/50000..  Training Loss: 0.00000542..  Test Loss: 0.00731153.. \n",
      "Epoch: 32301/50000..  Training Loss: 0.00000492..  Test Loss: 0.00720350.. \n",
      "Epoch: 32351/50000..  Training Loss: 0.00000406..  Test Loss: 0.00714691.. \n",
      "Epoch: 32401/50000..  Training Loss: 0.00000387..  Test Loss: 0.00722022.. \n",
      "Epoch: 32451/50000..  Training Loss: 0.00001223..  Test Loss: 0.00720755.. \n",
      "Epoch: 32501/50000..  Training Loss: 0.00000376..  Test Loss: 0.00717396.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32551/50000..  Training Loss: 0.00000486..  Test Loss: 0.00723377.. \n",
      "Epoch: 32601/50000..  Training Loss: 0.00000448..  Test Loss: 0.00732418.. \n",
      "Epoch: 32651/50000..  Training Loss: 0.00000452..  Test Loss: 0.00709157.. \n",
      "Epoch: 32701/50000..  Training Loss: 0.00000443..  Test Loss: 0.00727479.. \n",
      "Epoch: 32751/50000..  Training Loss: 0.00000480..  Test Loss: 0.00741442.. \n",
      "Epoch: 32801/50000..  Training Loss: 0.00000454..  Test Loss: 0.00727706.. \n",
      "Epoch: 32851/50000..  Training Loss: 0.00000474..  Test Loss: 0.00721452.. \n",
      "Epoch: 32901/50000..  Training Loss: 0.00000390..  Test Loss: 0.00723729.. \n",
      "Epoch: 32951/50000..  Training Loss: 0.00000477..  Test Loss: 0.00744727.. \n",
      "Epoch: 33001/50000..  Training Loss: 0.00000638..  Test Loss: 0.00740908.. \n",
      "Epoch: 33051/50000..  Training Loss: 0.00000461..  Test Loss: 0.00731638.. \n",
      "Epoch: 33101/50000..  Training Loss: 0.00000426..  Test Loss: 0.00717563.. \n",
      "Epoch: 33151/50000..  Training Loss: 0.00000382..  Test Loss: 0.00718308.. \n",
      "Epoch: 33201/50000..  Training Loss: 0.00000461..  Test Loss: 0.00729534.. \n",
      "Epoch: 33251/50000..  Training Loss: 0.00000748..  Test Loss: 0.00734721.. \n",
      "Epoch: 33301/50000..  Training Loss: 0.00000417..  Test Loss: 0.00719476.. \n",
      "Epoch: 33351/50000..  Training Loss: 0.00000398..  Test Loss: 0.00710364.. \n",
      "Epoch: 33401/50000..  Training Loss: 0.00000483..  Test Loss: 0.00718364.. \n",
      "Epoch: 33451/50000..  Training Loss: 0.00000496..  Test Loss: 0.00742258.. \n",
      "Epoch: 33501/50000..  Training Loss: 0.00000437..  Test Loss: 0.00727356.. \n",
      "Epoch: 33551/50000..  Training Loss: 0.00000378..  Test Loss: 0.00713217.. \n",
      "Epoch: 33601/50000..  Training Loss: 0.00000380..  Test Loss: 0.00722942.. \n",
      "Epoch: 33651/50000..  Training Loss: 0.00000390..  Test Loss: 0.00725869.. \n",
      "Epoch: 33701/50000..  Training Loss: 0.00000640..  Test Loss: 0.00709583.. \n",
      "Epoch: 33751/50000..  Training Loss: 0.00000388..  Test Loss: 0.00717427.. \n",
      "Epoch: 33801/50000..  Training Loss: 0.00000370..  Test Loss: 0.00712743.. \n",
      "Epoch: 33851/50000..  Training Loss: 0.00000398..  Test Loss: 0.00719662.. \n",
      "Epoch: 33901/50000..  Training Loss: 0.00000412..  Test Loss: 0.00726926.. \n",
      "Epoch: 33951/50000..  Training Loss: 0.00000438..  Test Loss: 0.00709043.. \n",
      "Epoch: 34001/50000..  Training Loss: 0.00000404..  Test Loss: 0.00720040.. \n",
      "Epoch: 34051/50000..  Training Loss: 0.00000439..  Test Loss: 0.00730994.. \n",
      "Epoch: 34101/50000..  Training Loss: 0.00000510..  Test Loss: 0.00710414.. \n",
      "Epoch: 34151/50000..  Training Loss: 0.00000416..  Test Loss: 0.00711067.. \n",
      "Epoch: 34201/50000..  Training Loss: 0.00000362..  Test Loss: 0.00719301.. \n",
      "Epoch: 34251/50000..  Training Loss: 0.00000369..  Test Loss: 0.00719495.. \n",
      "Epoch: 34301/50000..  Training Loss: 0.00000419..  Test Loss: 0.00735611.. \n",
      "Epoch: 34351/50000..  Training Loss: 0.00000736..  Test Loss: 0.00729478.. \n",
      "Epoch: 34401/50000..  Training Loss: 0.00000442..  Test Loss: 0.00721191.. \n",
      "Epoch: 34451/50000..  Training Loss: 0.00000425..  Test Loss: 0.00719442.. \n",
      "Epoch: 34501/50000..  Training Loss: 0.00000396..  Test Loss: 0.00727820.. \n",
      "Epoch: 34551/50000..  Training Loss: 0.00000492..  Test Loss: 0.00727668.. \n",
      "Epoch: 34601/50000..  Training Loss: 0.00000516..  Test Loss: 0.00714749.. \n",
      "Epoch: 34651/50000..  Training Loss: 0.00000431..  Test Loss: 0.00736917.. \n",
      "Epoch: 34701/50000..  Training Loss: 0.00000560..  Test Loss: 0.00725233.. \n",
      "Epoch: 34751/50000..  Training Loss: 0.00000426..  Test Loss: 0.00714407.. \n",
      "Epoch: 34801/50000..  Training Loss: 0.00000410..  Test Loss: 0.00711543.. \n",
      "Epoch: 34851/50000..  Training Loss: 0.00000445..  Test Loss: 0.00732464.. \n",
      "Epoch: 34901/50000..  Training Loss: 0.00000599..  Test Loss: 0.00729092.. \n",
      "Epoch: 34951/50000..  Training Loss: 0.00000365..  Test Loss: 0.00723197.. \n",
      "Epoch: 35001/50000..  Training Loss: 0.00000479..  Test Loss: 0.00728781.. \n",
      "Epoch: 35051/50000..  Training Loss: 0.00000505..  Test Loss: 0.00740357.. \n",
      "Epoch: 35101/50000..  Training Loss: 0.00000478..  Test Loss: 0.00740391.. \n",
      "Epoch: 35151/50000..  Training Loss: 0.00000477..  Test Loss: 0.00720101.. \n",
      "Epoch: 35201/50000..  Training Loss: 0.00000398..  Test Loss: 0.00723327.. \n",
      "Epoch: 35251/50000..  Training Loss: 0.00000442..  Test Loss: 0.00717536.. \n",
      "Epoch: 35301/50000..  Training Loss: 0.00000596..  Test Loss: 0.00719994.. \n",
      "Epoch: 35351/50000..  Training Loss: 0.00000402..  Test Loss: 0.00726100.. \n",
      "Epoch: 35401/50000..  Training Loss: 0.00000362..  Test Loss: 0.00710399.. \n",
      "Epoch: 35451/50000..  Training Loss: 0.00000521..  Test Loss: 0.00731174.. \n",
      "Epoch: 35501/50000..  Training Loss: 0.00000467..  Test Loss: 0.00711040.. \n",
      "Epoch: 35551/50000..  Training Loss: 0.00000444..  Test Loss: 0.00708399.. \n",
      "Epoch: 35601/50000..  Training Loss: 0.00000441..  Test Loss: 0.00711086.. \n",
      "Epoch: 35651/50000..  Training Loss: 0.00000463..  Test Loss: 0.00741280.. \n",
      "Epoch: 35701/50000..  Training Loss: 0.00000438..  Test Loss: 0.00730361.. \n",
      "Epoch: 35751/50000..  Training Loss: 0.00000428..  Test Loss: 0.00718369.. \n",
      "Epoch: 35801/50000..  Training Loss: 0.00000491..  Test Loss: 0.00728059.. \n",
      "Epoch: 35851/50000..  Training Loss: 0.00000418..  Test Loss: 0.00721815.. \n",
      "Epoch: 35901/50000..  Training Loss: 0.00000451..  Test Loss: 0.00713617.. \n",
      "Epoch: 35951/50000..  Training Loss: 0.00000519..  Test Loss: 0.00733149.. \n",
      "Epoch: 36001/50000..  Training Loss: 0.00000472..  Test Loss: 0.00729668.. \n",
      "Epoch: 36051/50000..  Training Loss: 0.00000594..  Test Loss: 0.00711627.. \n",
      "Epoch: 36101/50000..  Training Loss: 0.00000609..  Test Loss: 0.00742782.. \n",
      "Epoch: 36151/50000..  Training Loss: 0.00000406..  Test Loss: 0.00731824.. \n",
      "Epoch: 36201/50000..  Training Loss: 0.00000517..  Test Loss: 0.00738103.. \n",
      "Epoch: 36251/50000..  Training Loss: 0.00000445..  Test Loss: 0.00713770.. \n",
      "Epoch: 36301/50000..  Training Loss: 0.00000508..  Test Loss: 0.00712712.. \n",
      "Epoch: 36351/50000..  Training Loss: 0.00000499..  Test Loss: 0.00722280.. \n",
      "Epoch: 36401/50000..  Training Loss: 0.00000624..  Test Loss: 0.00731534.. \n",
      "Epoch: 36451/50000..  Training Loss: 0.00000396..  Test Loss: 0.00718887.. \n",
      "Epoch: 36501/50000..  Training Loss: 0.00000458..  Test Loss: 0.00729722.. \n",
      "Epoch: 36551/50000..  Training Loss: 0.00000377..  Test Loss: 0.00717004.. \n",
      "Epoch: 36601/50000..  Training Loss: 0.00000383..  Test Loss: 0.00719697.. \n",
      "Epoch: 36651/50000..  Training Loss: 0.00000477..  Test Loss: 0.00734484.. \n",
      "Epoch: 36701/50000..  Training Loss: 0.00000419..  Test Loss: 0.00716974.. \n",
      "Epoch: 36751/50000..  Training Loss: 0.00000351..  Test Loss: 0.00724284.. \n",
      "Epoch: 36801/50000..  Training Loss: 0.00000414..  Test Loss: 0.00710669.. \n",
      "Epoch: 36851/50000..  Training Loss: 0.00000413..  Test Loss: 0.00724436.. \n",
      "Epoch: 36901/50000..  Training Loss: 0.00000598..  Test Loss: 0.00728401.. \n",
      "Epoch: 36951/50000..  Training Loss: 0.00000377..  Test Loss: 0.00717690.. \n",
      "Epoch: 37001/50000..  Training Loss: 0.00000444..  Test Loss: 0.00727142.. \n",
      "Epoch: 37051/50000..  Training Loss: 0.00000416..  Test Loss: 0.00732836.. \n",
      "Epoch: 37101/50000..  Training Loss: 0.00000888..  Test Loss: 0.00736452.. \n",
      "Epoch: 37151/50000..  Training Loss: 0.00000461..  Test Loss: 0.00730244.. \n",
      "Epoch: 37201/50000..  Training Loss: 0.00000407..  Test Loss: 0.00724673.. \n",
      "Epoch: 37251/50000..  Training Loss: 0.00000483..  Test Loss: 0.00739929.. \n",
      "Epoch: 37301/50000..  Training Loss: 0.00000415..  Test Loss: 0.00712480.. \n",
      "Epoch: 37351/50000..  Training Loss: 0.00000345..  Test Loss: 0.00714647.. \n",
      "Epoch: 37401/50000..  Training Loss: 0.00000358..  Test Loss: 0.00711406.. \n",
      "Epoch: 37451/50000..  Training Loss: 0.00000389..  Test Loss: 0.00716925.. \n",
      "Epoch: 37501/50000..  Training Loss: 0.00000452..  Test Loss: 0.00714532.. \n",
      "Epoch: 37551/50000..  Training Loss: 0.00000537..  Test Loss: 0.00711353.. \n",
      "Epoch: 37601/50000..  Training Loss: 0.00000480..  Test Loss: 0.00709854.. \n",
      "Epoch: 37651/50000..  Training Loss: 0.00000384..  Test Loss: 0.00712692.. \n",
      "Epoch: 37701/50000..  Training Loss: 0.00000475..  Test Loss: 0.00725499.. \n",
      "Epoch: 37751/50000..  Training Loss: 0.00000385..  Test Loss: 0.00726930.. \n",
      "Epoch: 37801/50000..  Training Loss: 0.00000410..  Test Loss: 0.00719309.. \n",
      "Epoch: 37851/50000..  Training Loss: 0.00000458..  Test Loss: 0.00728402.. \n",
      "Epoch: 37901/50000..  Training Loss: 0.00000429..  Test Loss: 0.00729251.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37951/50000..  Training Loss: 0.00000393..  Test Loss: 0.00722984.. \n",
      "Epoch: 38001/50000..  Training Loss: 0.00000480..  Test Loss: 0.00721423.. \n",
      "Epoch: 38051/50000..  Training Loss: 0.00000414..  Test Loss: 0.00723321.. \n",
      "Epoch: 38101/50000..  Training Loss: 0.00000407..  Test Loss: 0.00724082.. \n",
      "Epoch: 38151/50000..  Training Loss: 0.00000468..  Test Loss: 0.00718470.. \n",
      "Epoch: 38201/50000..  Training Loss: 0.00000726..  Test Loss: 0.00729258.. \n",
      "Epoch: 38251/50000..  Training Loss: 0.00000386..  Test Loss: 0.00715813.. \n",
      "Epoch: 38301/50000..  Training Loss: 0.00000442..  Test Loss: 0.00716708.. \n",
      "Epoch: 38351/50000..  Training Loss: 0.00000435..  Test Loss: 0.00721016.. \n",
      "Epoch: 38401/50000..  Training Loss: 0.00000546..  Test Loss: 0.00735591.. \n",
      "Epoch: 38451/50000..  Training Loss: 0.00000498..  Test Loss: 0.00729731.. \n",
      "Epoch: 38501/50000..  Training Loss: 0.00000423..  Test Loss: 0.00722288.. \n",
      "Epoch: 38551/50000..  Training Loss: 0.00000593..  Test Loss: 0.00717294.. \n",
      "Epoch: 38601/50000..  Training Loss: 0.00000462..  Test Loss: 0.00725108.. \n",
      "Epoch: 38651/50000..  Training Loss: 0.00000403..  Test Loss: 0.00721376.. \n",
      "Epoch: 38701/50000..  Training Loss: 0.00000460..  Test Loss: 0.00727871.. \n",
      "Epoch: 38751/50000..  Training Loss: 0.00000475..  Test Loss: 0.00726822.. \n",
      "Epoch: 38801/50000..  Training Loss: 0.00000437..  Test Loss: 0.00716969.. \n",
      "Epoch: 38851/50000..  Training Loss: 0.00000445..  Test Loss: 0.00720477.. \n",
      "Epoch: 38901/50000..  Training Loss: 0.00000444..  Test Loss: 0.00728783.. \n",
      "Epoch: 38951/50000..  Training Loss: 0.00000476..  Test Loss: 0.00720464.. \n",
      "Epoch: 39001/50000..  Training Loss: 0.00000596..  Test Loss: 0.00718425.. \n",
      "Epoch: 39051/50000..  Training Loss: 0.00000401..  Test Loss: 0.00724812.. \n",
      "Epoch: 39101/50000..  Training Loss: 0.00000470..  Test Loss: 0.00722387.. \n",
      "Epoch: 39151/50000..  Training Loss: 0.00000436..  Test Loss: 0.00723848.. \n",
      "Epoch: 39201/50000..  Training Loss: 0.00000431..  Test Loss: 0.00726953.. \n",
      "Epoch: 39251/50000..  Training Loss: 0.00000377..  Test Loss: 0.00720120.. \n",
      "Epoch: 39301/50000..  Training Loss: 0.00000470..  Test Loss: 0.00732266.. \n",
      "Epoch: 39351/50000..  Training Loss: 0.00000451..  Test Loss: 0.00718366.. \n",
      "Epoch: 39401/50000..  Training Loss: 0.00000378..  Test Loss: 0.00711284.. \n",
      "Epoch: 39451/50000..  Training Loss: 0.00000399..  Test Loss: 0.00718821.. \n",
      "Epoch: 39501/50000..  Training Loss: 0.00000452..  Test Loss: 0.00716349.. \n",
      "Epoch: 39551/50000..  Training Loss: 0.00000736..  Test Loss: 0.00731764.. \n",
      "Epoch: 39601/50000..  Training Loss: 0.00000442..  Test Loss: 0.00735718.. \n",
      "Epoch: 39651/50000..  Training Loss: 0.00000388..  Test Loss: 0.00722422.. \n",
      "Epoch: 39701/50000..  Training Loss: 0.00000624..  Test Loss: 0.00727283.. \n",
      "Epoch: 39751/50000..  Training Loss: 0.00000396..  Test Loss: 0.00717391.. \n",
      "Epoch: 39801/50000..  Training Loss: 0.00000348..  Test Loss: 0.00718278.. \n",
      "Epoch: 39851/50000..  Training Loss: 0.00000621..  Test Loss: 0.00720021.. \n",
      "Epoch: 39901/50000..  Training Loss: 0.00000454..  Test Loss: 0.00714552.. \n",
      "Epoch: 39951/50000..  Training Loss: 0.00000450..  Test Loss: 0.00727204.. \n",
      "Epoch: 40001/50000..  Training Loss: 0.00000509..  Test Loss: 0.00744623.. \n",
      "Epoch: 40051/50000..  Training Loss: 0.00000459..  Test Loss: 0.00728678.. \n",
      "Epoch: 40101/50000..  Training Loss: 0.00000443..  Test Loss: 0.00729505.. \n",
      "Epoch: 40151/50000..  Training Loss: 0.00000537..  Test Loss: 0.00728306.. \n",
      "Epoch: 40201/50000..  Training Loss: 0.00000360..  Test Loss: 0.00713572.. \n",
      "Epoch: 40251/50000..  Training Loss: 0.00000430..  Test Loss: 0.00721126.. \n",
      "Epoch: 40301/50000..  Training Loss: 0.00000422..  Test Loss: 0.00709312.. \n",
      "Epoch: 40351/50000..  Training Loss: 0.00000424..  Test Loss: 0.00716022.. \n",
      "Epoch: 40401/50000..  Training Loss: 0.00000351..  Test Loss: 0.00716213.. \n",
      "Epoch: 40451/50000..  Training Loss: 0.00000459..  Test Loss: 0.00721235.. \n",
      "Epoch: 40501/50000..  Training Loss: 0.00000484..  Test Loss: 0.00716033.. \n",
      "Epoch: 40551/50000..  Training Loss: 0.00000448..  Test Loss: 0.00714788.. \n",
      "Epoch: 40601/50000..  Training Loss: 0.00000537..  Test Loss: 0.00723219.. \n",
      "Epoch: 40651/50000..  Training Loss: 0.00000481..  Test Loss: 0.00724587.. \n",
      "Epoch: 40701/50000..  Training Loss: 0.00000393..  Test Loss: 0.00724803.. \n",
      "Epoch: 40751/50000..  Training Loss: 0.00000447..  Test Loss: 0.00716501.. \n",
      "Epoch: 40801/50000..  Training Loss: 0.00000553..  Test Loss: 0.00729154.. \n",
      "Epoch: 40851/50000..  Training Loss: 0.00000457..  Test Loss: 0.00722124.. \n",
      "Epoch: 40901/50000..  Training Loss: 0.00000404..  Test Loss: 0.00718256.. \n",
      "Epoch: 40951/50000..  Training Loss: 0.00000564..  Test Loss: 0.00737561.. \n",
      "Epoch: 41001/50000..  Training Loss: 0.00000646..  Test Loss: 0.00734835.. \n",
      "Epoch: 41051/50000..  Training Loss: 0.00000457..  Test Loss: 0.00719258.. \n",
      "Epoch: 41101/50000..  Training Loss: 0.00000414..  Test Loss: 0.00723753.. \n",
      "Epoch: 41151/50000..  Training Loss: 0.00000409..  Test Loss: 0.00718537.. \n",
      "Epoch: 41201/50000..  Training Loss: 0.00000451..  Test Loss: 0.00729688.. \n",
      "Epoch: 41251/50000..  Training Loss: 0.00000558..  Test Loss: 0.00714025.. \n",
      "Epoch: 41301/50000..  Training Loss: 0.00000545..  Test Loss: 0.00738201.. \n",
      "Epoch: 41351/50000..  Training Loss: 0.00000465..  Test Loss: 0.00725150.. \n",
      "Epoch: 41401/50000..  Training Loss: 0.00000514..  Test Loss: 0.00733904.. \n",
      "Epoch: 41451/50000..  Training Loss: 0.00000465..  Test Loss: 0.00723251.. \n",
      "Epoch: 41501/50000..  Training Loss: 0.00000600..  Test Loss: 0.00739248.. \n",
      "Epoch: 41551/50000..  Training Loss: 0.00000470..  Test Loss: 0.00723766.. \n",
      "Epoch: 41601/50000..  Training Loss: 0.00000428..  Test Loss: 0.00720738.. \n",
      "Epoch: 41651/50000..  Training Loss: 0.00000440..  Test Loss: 0.00720105.. \n",
      "Epoch: 41701/50000..  Training Loss: 0.00000433..  Test Loss: 0.00718849.. \n",
      "Epoch: 41751/50000..  Training Loss: 0.00000495..  Test Loss: 0.00715258.. \n",
      "Epoch: 41801/50000..  Training Loss: 0.00000376..  Test Loss: 0.00717836.. \n",
      "Epoch: 41851/50000..  Training Loss: 0.00000453..  Test Loss: 0.00722981.. \n",
      "Epoch: 41901/50000..  Training Loss: 0.00000510..  Test Loss: 0.00734395.. \n",
      "Epoch: 41951/50000..  Training Loss: 0.00000419..  Test Loss: 0.00720699.. \n",
      "Epoch: 42001/50000..  Training Loss: 0.00000524..  Test Loss: 0.00703556.. \n",
      "Epoch: 42051/50000..  Training Loss: 0.00000408..  Test Loss: 0.00727705.. \n",
      "Epoch: 42101/50000..  Training Loss: 0.00000482..  Test Loss: 0.00744353.. \n",
      "Epoch: 42151/50000..  Training Loss: 0.00000366..  Test Loss: 0.00717416.. \n",
      "Epoch: 42201/50000..  Training Loss: 0.00000448..  Test Loss: 0.00712687.. \n",
      "Epoch: 42251/50000..  Training Loss: 0.00000475..  Test Loss: 0.00732949.. \n",
      "Epoch: 42301/50000..  Training Loss: 0.00000461..  Test Loss: 0.00732814.. \n",
      "Epoch: 42351/50000..  Training Loss: 0.00000414..  Test Loss: 0.00722800.. \n",
      "Epoch: 42401/50000..  Training Loss: 0.00000474..  Test Loss: 0.00735022.. \n",
      "Epoch: 42451/50000..  Training Loss: 0.00000501..  Test Loss: 0.00719647.. \n",
      "Epoch: 42501/50000..  Training Loss: 0.00000485..  Test Loss: 0.00711138.. \n",
      "Epoch: 42551/50000..  Training Loss: 0.00000428..  Test Loss: 0.00725117.. \n",
      "Epoch: 42601/50000..  Training Loss: 0.00000410..  Test Loss: 0.00720921.. \n",
      "Epoch: 42651/50000..  Training Loss: 0.00000417..  Test Loss: 0.00716393.. \n",
      "Epoch: 42701/50000..  Training Loss: 0.00000476..  Test Loss: 0.00713190.. \n",
      "Epoch: 42751/50000..  Training Loss: 0.00000452..  Test Loss: 0.00719076.. \n",
      "Epoch: 42801/50000..  Training Loss: 0.00000416..  Test Loss: 0.00726865.. \n",
      "Epoch: 42851/50000..  Training Loss: 0.00000514..  Test Loss: 0.00734413.. \n",
      "Epoch: 42901/50000..  Training Loss: 0.00000528..  Test Loss: 0.00708597.. \n",
      "Epoch: 42951/50000..  Training Loss: 0.00000480..  Test Loss: 0.00721430.. \n",
      "Epoch: 43001/50000..  Training Loss: 0.00000400..  Test Loss: 0.00735593.. \n",
      "Epoch: 43051/50000..  Training Loss: 0.00000393..  Test Loss: 0.00712729.. \n",
      "Epoch: 43101/50000..  Training Loss: 0.00000439..  Test Loss: 0.00717865.. \n",
      "Epoch: 43151/50000..  Training Loss: 0.00000539..  Test Loss: 0.00717236.. \n",
      "Epoch: 43201/50000..  Training Loss: 0.00000568..  Test Loss: 0.00730704.. \n",
      "Epoch: 43251/50000..  Training Loss: 0.00000513..  Test Loss: 0.00746068.. \n",
      "Epoch: 43301/50000..  Training Loss: 0.00000487..  Test Loss: 0.00726624.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43351/50000..  Training Loss: 0.00000476..  Test Loss: 0.00730092.. \n",
      "Epoch: 43401/50000..  Training Loss: 0.00000459..  Test Loss: 0.00720429.. \n",
      "Epoch: 43451/50000..  Training Loss: 0.00000460..  Test Loss: 0.00738962.. \n",
      "Epoch: 43501/50000..  Training Loss: 0.00000524..  Test Loss: 0.00728467.. \n",
      "Epoch: 43551/50000..  Training Loss: 0.00000471..  Test Loss: 0.00722180.. \n",
      "Epoch: 43601/50000..  Training Loss: 0.00000383..  Test Loss: 0.00710807.. \n",
      "Epoch: 43651/50000..  Training Loss: 0.00000394..  Test Loss: 0.00714263.. \n",
      "Epoch: 43701/50000..  Training Loss: 0.00000471..  Test Loss: 0.00715563.. \n",
      "Epoch: 43751/50000..  Training Loss: 0.00000411..  Test Loss: 0.00721035.. \n",
      "Epoch: 43801/50000..  Training Loss: 0.00000372..  Test Loss: 0.00720932.. \n",
      "Epoch: 43851/50000..  Training Loss: 0.00000488..  Test Loss: 0.00724222.. \n",
      "Epoch: 43901/50000..  Training Loss: 0.00000451..  Test Loss: 0.00718847.. \n",
      "Epoch: 43951/50000..  Training Loss: 0.00000344..  Test Loss: 0.00714853.. \n",
      "Epoch: 44001/50000..  Training Loss: 0.00000506..  Test Loss: 0.00758606.. \n",
      "Epoch: 44051/50000..  Training Loss: 0.00000416..  Test Loss: 0.00723221.. \n",
      "Epoch: 44101/50000..  Training Loss: 0.00000531..  Test Loss: 0.00729018.. \n",
      "Epoch: 44151/50000..  Training Loss: 0.00000432..  Test Loss: 0.00727888.. \n",
      "Epoch: 44201/50000..  Training Loss: 0.00000484..  Test Loss: 0.00714980.. \n",
      "Epoch: 44251/50000..  Training Loss: 0.00000450..  Test Loss: 0.00724882.. \n",
      "Epoch: 44301/50000..  Training Loss: 0.00000399..  Test Loss: 0.00717207.. \n",
      "Epoch: 44351/50000..  Training Loss: 0.00000421..  Test Loss: 0.00720579.. \n",
      "Epoch: 44401/50000..  Training Loss: 0.00000338..  Test Loss: 0.00714480.. \n",
      "Epoch: 44451/50000..  Training Loss: 0.00000421..  Test Loss: 0.00717813.. \n",
      "Epoch: 44501/50000..  Training Loss: 0.00000457..  Test Loss: 0.00719608.. \n",
      "Epoch: 44551/50000..  Training Loss: 0.00000392..  Test Loss: 0.00715693.. \n",
      "Epoch: 44601/50000..  Training Loss: 0.00000460..  Test Loss: 0.00726448.. \n",
      "Epoch: 44651/50000..  Training Loss: 0.00000452..  Test Loss: 0.00716830.. \n",
      "Epoch: 44701/50000..  Training Loss: 0.00000462..  Test Loss: 0.00733343.. \n",
      "Epoch: 44751/50000..  Training Loss: 0.00000385..  Test Loss: 0.00712813.. \n",
      "Epoch: 44801/50000..  Training Loss: 0.00000400..  Test Loss: 0.00725979.. \n",
      "Epoch: 44851/50000..  Training Loss: 0.00000397..  Test Loss: 0.00721093.. \n",
      "Epoch: 44901/50000..  Training Loss: 0.00000468..  Test Loss: 0.00724835.. \n",
      "Epoch: 44951/50000..  Training Loss: 0.00000444..  Test Loss: 0.00719102.. \n",
      "Epoch: 45001/50000..  Training Loss: 0.00000346..  Test Loss: 0.00716252.. \n",
      "Epoch: 45051/50000..  Training Loss: 0.00000374..  Test Loss: 0.00716689.. \n",
      "Epoch: 45101/50000..  Training Loss: 0.00000438..  Test Loss: 0.00738525.. \n",
      "Epoch: 45151/50000..  Training Loss: 0.00001146..  Test Loss: 0.00773165.. \n",
      "Epoch: 45201/50000..  Training Loss: 0.00000352..  Test Loss: 0.00713097.. \n",
      "Epoch: 45251/50000..  Training Loss: 0.00000469..  Test Loss: 0.00720808.. \n",
      "Epoch: 45301/50000..  Training Loss: 0.00000388..  Test Loss: 0.00723632.. \n",
      "Epoch: 45351/50000..  Training Loss: 0.00000600..  Test Loss: 0.00761676.. \n",
      "Epoch: 45401/50000..  Training Loss: 0.00000357..  Test Loss: 0.00713687.. \n",
      "Epoch: 45451/50000..  Training Loss: 0.00000449..  Test Loss: 0.00723721.. \n",
      "Epoch: 45501/50000..  Training Loss: 0.00000478..  Test Loss: 0.00725128.. \n",
      "Epoch: 45551/50000..  Training Loss: 0.00000492..  Test Loss: 0.00718903.. \n",
      "Epoch: 45601/50000..  Training Loss: 0.00000433..  Test Loss: 0.00715905.. \n",
      "Epoch: 45651/50000..  Training Loss: 0.00000451..  Test Loss: 0.00726297.. \n",
      "Epoch: 45701/50000..  Training Loss: 0.00000443..  Test Loss: 0.00719368.. \n",
      "Epoch: 45751/50000..  Training Loss: 0.00000507..  Test Loss: 0.00721768.. \n",
      "Epoch: 45801/50000..  Training Loss: 0.00000408..  Test Loss: 0.00719993.. \n",
      "Epoch: 45851/50000..  Training Loss: 0.00000538..  Test Loss: 0.00720664.. \n",
      "Epoch: 45901/50000..  Training Loss: 0.00000488..  Test Loss: 0.00718925.. \n",
      "Epoch: 45951/50000..  Training Loss: 0.00000568..  Test Loss: 0.00721684.. \n",
      "Epoch: 46001/50000..  Training Loss: 0.00000460..  Test Loss: 0.00727677.. \n",
      "Epoch: 46051/50000..  Training Loss: 0.00000639..  Test Loss: 0.00709656.. \n",
      "Epoch: 46101/50000..  Training Loss: 0.00000485..  Test Loss: 0.00721221.. \n",
      "Epoch: 46151/50000..  Training Loss: 0.00000428..  Test Loss: 0.00714095.. \n",
      "Epoch: 46201/50000..  Training Loss: 0.00000416..  Test Loss: 0.00724817.. \n",
      "Epoch: 46251/50000..  Training Loss: 0.00000681..  Test Loss: 0.00762787.. \n",
      "Epoch: 46301/50000..  Training Loss: 0.00000362..  Test Loss: 0.00718217.. \n",
      "Epoch: 46351/50000..  Training Loss: 0.00000418..  Test Loss: 0.00720170.. \n",
      "Epoch: 46401/50000..  Training Loss: 0.00000473..  Test Loss: 0.00720159.. \n",
      "Epoch: 46451/50000..  Training Loss: 0.00000477..  Test Loss: 0.00721750.. \n",
      "Epoch: 46501/50000..  Training Loss: 0.00000513..  Test Loss: 0.00723655.. \n",
      "Epoch: 46551/50000..  Training Loss: 0.00000378..  Test Loss: 0.00716267.. \n",
      "Epoch: 46601/50000..  Training Loss: 0.00000383..  Test Loss: 0.00714818.. \n",
      "Epoch: 46651/50000..  Training Loss: 0.00000350..  Test Loss: 0.00717356.. \n",
      "Epoch: 46701/50000..  Training Loss: 0.00000434..  Test Loss: 0.00728057.. \n",
      "Epoch: 46751/50000..  Training Loss: 0.00000430..  Test Loss: 0.00713351.. \n",
      "Epoch: 46801/50000..  Training Loss: 0.00000460..  Test Loss: 0.00723342.. \n",
      "Epoch: 46851/50000..  Training Loss: 0.00000391..  Test Loss: 0.00710251.. \n",
      "Epoch: 46901/50000..  Training Loss: 0.00000408..  Test Loss: 0.00722078.. \n",
      "Epoch: 46951/50000..  Training Loss: 0.00000364..  Test Loss: 0.00720205.. \n",
      "Epoch: 47001/50000..  Training Loss: 0.00000479..  Test Loss: 0.00722977.. \n",
      "Epoch: 47051/50000..  Training Loss: 0.00000384..  Test Loss: 0.00718288.. \n",
      "Epoch: 47101/50000..  Training Loss: 0.00000803..  Test Loss: 0.00710323.. \n",
      "Epoch: 47151/50000..  Training Loss: 0.00000640..  Test Loss: 0.00723279.. \n",
      "Epoch: 47201/50000..  Training Loss: 0.00000381..  Test Loss: 0.00714256.. \n",
      "Epoch: 47251/50000..  Training Loss: 0.00000475..  Test Loss: 0.00726870.. \n",
      "Epoch: 47301/50000..  Training Loss: 0.00000410..  Test Loss: 0.00714827.. \n",
      "Epoch: 47351/50000..  Training Loss: 0.00000474..  Test Loss: 0.00709667.. \n",
      "Epoch: 47401/50000..  Training Loss: 0.00000594..  Test Loss: 0.00722029.. \n",
      "Epoch: 47451/50000..  Training Loss: 0.00000489..  Test Loss: 0.00714583.. \n",
      "Epoch: 47501/50000..  Training Loss: 0.00000497..  Test Loss: 0.00715636.. \n",
      "Epoch: 47551/50000..  Training Loss: 0.00000830..  Test Loss: 0.00711733.. \n",
      "Epoch: 47601/50000..  Training Loss: 0.00000387..  Test Loss: 0.00716596.. \n",
      "Epoch: 47651/50000..  Training Loss: 0.00000415..  Test Loss: 0.00721589.. \n",
      "Epoch: 47701/50000..  Training Loss: 0.00000366..  Test Loss: 0.00713460.. \n",
      "Epoch: 47751/50000..  Training Loss: 0.00000388..  Test Loss: 0.00720039.. \n",
      "Epoch: 47801/50000..  Training Loss: 0.00000430..  Test Loss: 0.00725907.. \n",
      "Epoch: 47851/50000..  Training Loss: 0.00000468..  Test Loss: 0.00716556.. \n",
      "Epoch: 47901/50000..  Training Loss: 0.00000387..  Test Loss: 0.00717922.. \n",
      "Epoch: 47951/50000..  Training Loss: 0.00000393..  Test Loss: 0.00724609.. \n",
      "Epoch: 48001/50000..  Training Loss: 0.00000438..  Test Loss: 0.00724987.. \n",
      "Epoch: 48051/50000..  Training Loss: 0.00000429..  Test Loss: 0.00716980.. \n",
      "Epoch: 48101/50000..  Training Loss: 0.00000350..  Test Loss: 0.00719197.. \n",
      "Epoch: 48151/50000..  Training Loss: 0.00000377..  Test Loss: 0.00725568.. \n",
      "Epoch: 48201/50000..  Training Loss: 0.00000534..  Test Loss: 0.00751039.. \n",
      "Epoch: 48251/50000..  Training Loss: 0.00000669..  Test Loss: 0.00714747.. \n",
      "Epoch: 48301/50000..  Training Loss: 0.00000474..  Test Loss: 0.00718512.. \n",
      "Epoch: 48351/50000..  Training Loss: 0.00000385..  Test Loss: 0.00715804.. \n",
      "Epoch: 48401/50000..  Training Loss: 0.00000451..  Test Loss: 0.00719415.. \n",
      "Epoch: 48451/50000..  Training Loss: 0.00000405..  Test Loss: 0.00715310.. \n",
      "Epoch: 48501/50000..  Training Loss: 0.00000432..  Test Loss: 0.00717957.. \n",
      "Epoch: 48551/50000..  Training Loss: 0.00000468..  Test Loss: 0.00716161.. \n",
      "Epoch: 48601/50000..  Training Loss: 0.00000402..  Test Loss: 0.00726619.. \n",
      "Epoch: 48651/50000..  Training Loss: 0.00000484..  Test Loss: 0.00711652.. \n",
      "Epoch: 48701/50000..  Training Loss: 0.00000371..  Test Loss: 0.00715839.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48751/50000..  Training Loss: 0.00000462..  Test Loss: 0.00720197.. \n",
      "Epoch: 48801/50000..  Training Loss: 0.00000638..  Test Loss: 0.00715671.. \n",
      "Epoch: 48851/50000..  Training Loss: 0.00000963..  Test Loss: 0.00740387.. \n",
      "Epoch: 48901/50000..  Training Loss: 0.00000407..  Test Loss: 0.00734960.. \n",
      "Epoch: 48951/50000..  Training Loss: 0.00000488..  Test Loss: 0.00743305.. \n",
      "Epoch: 49001/50000..  Training Loss: 0.00000370..  Test Loss: 0.00725858.. \n",
      "Epoch: 49051/50000..  Training Loss: 0.00000408..  Test Loss: 0.00725633.. \n",
      "Epoch: 49101/50000..  Training Loss: 0.00000557..  Test Loss: 0.00725855.. \n",
      "Epoch: 49151/50000..  Training Loss: 0.00000388..  Test Loss: 0.00717402.. \n",
      "Epoch: 49201/50000..  Training Loss: 0.00000468..  Test Loss: 0.00714649.. \n",
      "Epoch: 49251/50000..  Training Loss: 0.00000395..  Test Loss: 0.00725449.. \n",
      "Epoch: 49301/50000..  Training Loss: 0.00000506..  Test Loss: 0.00718368.. \n",
      "Epoch: 49351/50000..  Training Loss: 0.00000442..  Test Loss: 0.00728180.. \n",
      "Epoch: 49401/50000..  Training Loss: 0.00000462..  Test Loss: 0.00711933.. \n",
      "Epoch: 49451/50000..  Training Loss: 0.00000600..  Test Loss: 0.00715689.. \n",
      "Epoch: 49501/50000..  Training Loss: 0.00000508..  Test Loss: 0.00721399.. \n",
      "Epoch: 49551/50000..  Training Loss: 0.00000670..  Test Loss: 0.00730873.. \n",
      "Epoch: 49601/50000..  Training Loss: 0.00000387..  Test Loss: 0.00719830.. \n",
      "Epoch: 49651/50000..  Training Loss: 0.00000384..  Test Loss: 0.00717293.. \n",
      "Epoch: 49701/50000..  Training Loss: 0.00000376..  Test Loss: 0.00716302.. \n",
      "Epoch: 49751/50000..  Training Loss: 0.00000415..  Test Loss: 0.00721462.. \n",
      "Epoch: 49801/50000..  Training Loss: 0.00000496..  Test Loss: 0.00722055.. \n",
      "Epoch: 49851/50000..  Training Loss: 0.00000425..  Test Loss: 0.00723265.. \n",
      "Epoch: 49901/50000..  Training Loss: 0.00000558..  Test Loss: 0.00723455.. \n",
      "Epoch: 49951/50000..  Training Loss: 0.00000558..  Test Loss: 0.00708961.. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Regressor().to(device)\n",
    "#model.apply(weights_init)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.003)\n",
    "\n",
    "epochs = 50000\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(train_batch)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_batch[i])\n",
    "        #MSE(output, label_batch[i])\n",
    "        #loss = torch.sqrt(criterion(torch.log(output), torch.log(label_batch[i])))\n",
    "        loss = criterion(output, label_batch[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    if e%50 == 0:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = model(X_val)\n",
    "            #test_loss += torch.sqrt(criterion(torch.log(predictions), torch.log(y_val)))\n",
    "            test_loss += torch.sqrt(criterion(predictions, y_val))\n",
    "                \n",
    "        train_losses.append(train_loss/len(train_batch))\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.8f}.. \".format(train_loss/len(train_batch)),\n",
    "              \"Test Loss: {:.8f}.. \".format(test_loss))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc51faeedd8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "# loss 0.24 -> 2800\n",
    "# loss 0.14 -> 3300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.from_numpy(test_x.values).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model.forward(test_x)\n",
    "    \n",
    "\n",
    "output.shape\n",
    "output = output.cpu().numpy()\n",
    "#output_col = pd.DataFrame(test, columns = columns)\n",
    "\n",
    "output = y_scaler.inverse_transform(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./dataset-0510/submit_test.csv')\n",
    "submission['total_price'] = output\n",
    "submission.to_csv('submission/DNN_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
