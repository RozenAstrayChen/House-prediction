{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "#epochs = 2000\n",
    "use_gpu = True\n",
    "lr = 0.001\n",
    "weight_decay = 10\n",
    "\n",
    "# Batch size and learning rate is hyperparameters in deep learning\n",
    "# suggest batch_size is reduced, lr is also reduced which will reduce concussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('./dataset-0510/train.csv')\n",
    "X_test = pd.read_csv('./dataset-0510/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['total_price']\n",
    "X = X.drop(columns=['building_id', 'total_price'], axis=1)\n",
    "\n",
    "X_test = X_test.drop(columns=['building_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scale y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = StandardScaler()\n",
    "y_train = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_eval = y_scaler.fit_transform(y_eval.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputer, Scaler, Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/islab/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# step1. Imputation transformer for completing missing values.\n",
    "step1 = ('Imputer', Imputer())\n",
    "# step2. MinMaxScaler\n",
    "step2 = ('MinMaxScaler', MinMaxScaler())\n",
    "# step3. feature selection\n",
    "#step3 = ('FeatureSelection', SelectFromModel(RandomForestRegressor()))\n",
    "step3 = ('FeatureSelection', VarianceThreshold())\n",
    "\n",
    "pipeline = Pipeline(steps=[step1, step2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 233)\n",
      "(10000, 233)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train = pipeline.fit_transform(X_train)\n",
    "X_eval = pipeline.fit_transform(X_eval)\n",
    "\n",
    "print(X_test.shape)\n",
    "X_test = pipeline.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_eval = torch.from_numpy(X_eval).float().to(device)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).float().to(device)\n",
    "y_eval = torch.from_numpy(y_eval).float().to(device)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42000, 233])\n",
      "torch.Size([10000, 233])\n",
      "torch.Size([42000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data.TensorDataset(X_train, y_train)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "eval_dataset = Data.TensorDataset(X_eval, y_eval)\n",
    "eval_loader = Data.DataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(233, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(0)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optim = optim.Adam(model.parameters(), lr= lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_func(model, loader):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        optim.zero_grad()\n",
    "        pred = model(batch_x)\n",
    "        loss = criterion(batch_y, pred)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    print('training loss', np.array(train_loss).mean())\n",
    "    return model,  np.array(train_loss).mean()\n",
    "\n",
    "\n",
    "'''\n",
    "def train_func(model, loader, accumlation_steps=32):\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "    train_loss = []\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        pred = model(batch_x)\n",
    "        loss = criterion(pred, batch_y)\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        loss = loss / accumlation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if step % accumlation_steps == 0 or step == len(loader)-1:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    print('training loss', np.array(train_loss).mean())\n",
    "    return model, np.array(train_loss).mean()\n",
    "'''\n",
    "'''\n",
    "Batch update\n",
    "'''\n",
    "'''\n",
    "def train_func(model, loader, accumlation_steps=64):\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "    train_loss = []\n",
    "    loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        pred = model(batch_x)\n",
    "        current_loss = criterion(pred, batch_y)\n",
    "        loss = loss + current_loss\n",
    "        train_loss.append(current_loss.item())\n",
    "        if (step+1) % accumlation_steps == 0 or (step+1) == len(loader):\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            loss = 0\n",
    "            \n",
    "    print('training loss', np.array(train_loss).mean())\n",
    "    return model, np.array(train_loss).mean()\n",
    "'''\n",
    "\n",
    "def eval_func(model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            pred = model(batch_x)\n",
    "            loss = criterion(pred, batch_y)\n",
    "        print('testing loss', loss.item())\n",
    "    return loss\n",
    "\n",
    "def test_func(model, X, y_scaler=None):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(X)\n",
    "        pred = pred.cpu().numpy()\n",
    "        \n",
    "        if y_scaler != None:\n",
    "            pred = y_scaler.inverse_transform(pred)\n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 0\n",
      "training loss 0.891704988208684\n",
      "epochs 1\n",
      "training loss 0.8371079740650726\n",
      "epochs 2\n",
      "training loss 0.7757588924557873\n",
      "epochs 3\n",
      "training loss 0.6790706565885832\n",
      "epochs 4\n",
      "training loss 0.6570962084287947\n",
      "epochs 5\n",
      "training loss 0.5459763398450432\n",
      "epochs 6\n",
      "training loss 0.5586036241810882\n",
      "epochs 7\n",
      "training loss 0.669762240508289\n",
      "epochs 8\n",
      "training loss 0.5276855471910853\n",
      "epochs 9\n",
      "training loss 0.4288416277955879\n",
      "testing loss 57.6245002746582\n",
      "epochs 10\n",
      "training loss 0.5564338343725963\n",
      "epochs 11\n",
      "training loss 0.5772662321049156\n",
      "epochs 12\n",
      "training loss 0.31720159674684206\n",
      "epochs 13\n",
      "training loss 0.4026045934946248\n",
      "epochs 14\n",
      "training loss 0.34067599615364363\n",
      "epochs 15\n",
      "training loss 0.3632085832118085\n",
      "epochs 16\n",
      "training loss 0.3721002091399648\n",
      "epochs 17\n",
      "training loss 0.2660829225159956\n",
      "epochs 18\n",
      "training loss 0.21156891202836325\n",
      "epochs 19\n",
      "training loss 0.32462619033952556\n",
      "testing loss 5.689515113830566\n",
      "epochs 20\n",
      "training loss 0.22949241924692285\n",
      "epochs 21\n",
      "training loss 0.17414625493640248\n",
      "epochs 22\n",
      "training loss 0.20099266511247013\n",
      "epochs 23\n",
      "training loss 0.17799594197986704\n",
      "epochs 24\n",
      "training loss 0.14223547400624464\n",
      "epochs 25\n",
      "training loss 0.17641170543025841\n",
      "epochs 26\n",
      "training loss 0.14263970146350788\n",
      "epochs 27\n",
      "training loss 0.1884749035717863\n",
      "epochs 28\n",
      "training loss 0.13454521914774722\n",
      "epochs 29\n",
      "training loss 0.31237838902708254\n",
      "testing loss 2.0894227027893066\n",
      "epochs 30\n",
      "training loss 0.2953649175776677\n",
      "epochs 31\n",
      "training loss 0.2304785703957984\n",
      "epochs 32\n",
      "training loss 0.15724750770763918\n",
      "epochs 33\n",
      "training loss 0.13414259803566067\n",
      "epochs 34\n",
      "training loss 0.18384120933937304\n",
      "epochs 35\n",
      "training loss 0.1369682271936626\n",
      "epochs 36\n",
      "training loss 0.1756918532830296\n",
      "epochs 37\n",
      "training loss 0.17942658169477274\n",
      "epochs 38\n",
      "training loss 0.1869045632355141\n",
      "epochs 39\n",
      "training loss 0.15414763875305654\n",
      "testing loss 2.5795931816101074\n",
      "epochs 40\n",
      "training loss 0.11395552884222883\n",
      "epochs 41\n",
      "training loss 0.12001219576958454\n",
      "epochs 42\n",
      "training loss 0.11463296981246182\n",
      "epochs 43\n",
      "training loss 0.09342586389093688\n",
      "epochs 44\n",
      "training loss 0.13145542240729838\n",
      "epochs 45\n",
      "training loss 0.10115839990250992\n",
      "epochs 46\n",
      "training loss 0.0816679592272549\n",
      "epochs 47\n",
      "training loss 0.09374793661814747\n",
      "epochs 48\n",
      "training loss 0.08479649531796123\n",
      "epochs 49\n",
      "training loss 0.12833532230420547\n",
      "testing loss 0.8117413520812988\n",
      "epochs 50\n",
      "training loss 0.1271737725332831\n",
      "epochs 51\n",
      "training loss 0.10177538683920195\n",
      "epochs 52\n",
      "training loss 0.12886066122940093\n",
      "epochs 53\n",
      "training loss 0.11121218764420712\n",
      "epochs 54\n",
      "training loss 0.10477566230251933\n",
      "epochs 55\n",
      "training loss 0.08581318991879622\n",
      "epochs 56\n",
      "training loss 0.10214666287330064\n",
      "epochs 57\n",
      "training loss 0.19142053653909402\n",
      "epochs 58\n",
      "training loss 0.105857260412339\n",
      "epochs 59\n",
      "training loss 0.08490102550748622\n",
      "testing loss 0.0338626503944397\n",
      "epochs 60\n",
      "training loss 0.08017747670863613\n",
      "epochs 61\n",
      "training loss 0.07125849242914807\n",
      "epochs 62\n",
      "training loss 0.10141540351464892\n",
      "epochs 63\n",
      "training loss 0.07042387550075849\n",
      "epochs 64\n",
      "training loss 0.06900561196785984\n",
      "epochs 65\n",
      "training loss 0.06295655002873955\n",
      "epochs 66\n",
      "training loss 0.21475851754799033\n",
      "epochs 67\n",
      "training loss 0.11533910186227524\n",
      "epochs 68\n",
      "training loss 0.17439157198550123\n",
      "epochs 69\n",
      "training loss 0.25725311127801737\n",
      "testing loss 2.5762412548065186\n",
      "epochs 70\n",
      "training loss 0.1104283813274268\n",
      "epochs 71\n",
      "training loss 0.10737457522614435\n",
      "epochs 72\n",
      "training loss 0.10433646184251164\n",
      "epochs 73\n",
      "training loss 0.06774597840778755\n",
      "epochs 74\n",
      "training loss 0.10382832870113128\n",
      "epochs 75\n",
      "training loss 0.07413743143957673\n",
      "epochs 76\n",
      "training loss 0.06805358459665016\n",
      "epochs 77\n",
      "training loss 0.052896082011813464\n",
      "epochs 78\n",
      "training loss 0.05188504508273168\n",
      "epochs 79\n",
      "training loss 0.05110249086988695\n",
      "testing loss 0.02784864977002144\n",
      "epochs 80\n",
      "training loss 0.04923134962040367\n",
      "epochs 81\n",
      "training loss 0.06748981228606267\n",
      "epochs 82\n",
      "training loss 0.05471397835874196\n",
      "epochs 83\n",
      "training loss 0.05721736868674105\n",
      "epochs 84\n",
      "training loss 0.06780276066538962\n",
      "epochs 85\n",
      "training loss 0.0828597779526855\n",
      "epochs 86\n",
      "training loss 0.0670698749522368\n",
      "epochs 87\n",
      "training loss 0.07317603199996732\n",
      "epochs 88\n",
      "training loss 0.07830631510552132\n",
      "epochs 89\n",
      "training loss 0.04743446484208107\n",
      "testing loss 0.22197790443897247\n",
      "epochs 90\n",
      "training loss 0.04832393455347329\n",
      "epochs 91\n",
      "training loss 0.07799389002788247\n",
      "epochs 92\n",
      "training loss 0.07604805751506126\n",
      "epochs 93\n",
      "training loss 0.05398998103584304\n",
      "epochs 94\n",
      "training loss 0.0487787175359148\n",
      "epochs 95\n",
      "training loss 0.03951961969335874\n",
      "epochs 96\n",
      "training loss 0.05470779548878923\n",
      "epochs 97\n",
      "training loss 0.07540645244898218\n",
      "epochs 98\n",
      "training loss 0.2513864023102955\n",
      "epochs 99\n",
      "training loss 0.26398843003606254\n",
      "testing loss 0.24211876094341278\n",
      "epochs 100\n",
      "training loss 0.23516588568913213\n",
      "epochs 101\n",
      "training loss 0.3787354668997454\n",
      "epochs 102\n",
      "training loss 0.10704303996242356\n",
      "epochs 103\n",
      "training loss 0.05852381936415579\n",
      "epochs 104\n",
      "training loss 0.058931415516770246\n",
      "epochs 105\n",
      "training loss 0.06262208808100585\n",
      "epochs 106\n",
      "training loss 0.048547961737847686\n",
      "epochs 107\n",
      "training loss 0.10486544099156604\n",
      "epochs 108\n",
      "training loss 0.056516972477688936\n",
      "epochs 109\n",
      "training loss 0.044832906719635836\n",
      "testing loss 1.202999234199524\n",
      "epochs 110\n",
      "training loss 0.044409304388770554\n",
      "epochs 111\n",
      "training loss 0.052088992498024844\n",
      "epochs 112\n",
      "training loss 0.050468486536181335\n",
      "epochs 113\n",
      "training loss 0.05498978567281456\n",
      "epochs 114\n",
      "training loss 0.08793016953224486\n",
      "epochs 115\n",
      "training loss 0.03797431088080912\n",
      "epochs 116\n",
      "training loss 0.04605594868348403\n",
      "epochs 117\n",
      "training loss 0.07535554227052313\n",
      "epochs 118\n",
      "training loss 0.07813138322283825\n",
      "epochs 119\n",
      "training loss 0.05050290565599095\n",
      "testing loss 0.20022650063037872\n",
      "epochs 120\n",
      "training loss 0.040243477786354946\n",
      "epochs 121\n",
      "training loss 0.05926223639963251\n",
      "epochs 122\n",
      "training loss 0.04779928671478322\n",
      "epochs 123\n",
      "training loss 0.08174432786463788\n",
      "epochs 124\n",
      "training loss 0.06374233253864628\n",
      "epochs 125\n",
      "training loss 0.03587161742828109\n",
      "epochs 126\n",
      "training loss 0.03694527763308901\n",
      "epochs 127\n",
      "training loss 0.03619930958657554\n",
      "epochs 128\n",
      "training loss 0.03917398464047547\n",
      "epochs 129\n",
      "training loss 0.07265419217786102\n",
      "testing loss 0.352672278881073\n",
      "epochs 130\n",
      "training loss 0.16591429156110143\n",
      "epochs 131\n",
      "training loss 0.15061457569174694\n",
      "epochs 132\n",
      "training loss 0.06298430370110454\n",
      "epochs 133\n",
      "training loss 0.07037226661023768\n",
      "epochs 134\n",
      "training loss 0.08977228859721711\n",
      "epochs 135\n",
      "training loss 0.15880052614957094\n",
      "epochs 136\n",
      "training loss 0.0674864288983923\n",
      "epochs 137\n",
      "training loss 0.046954905642478756\n",
      "epochs 138\n",
      "training loss 0.058754966865209014\n",
      "epochs 139\n",
      "training loss 0.03611089910634539\n",
      "testing loss 0.08092614263296127\n",
      "epochs 140\n",
      "training loss 0.04034847934363466\n",
      "epochs 141\n",
      "training loss 0.09493740106170828\n",
      "epochs 142\n",
      "training loss 0.14679981833041617\n",
      "epochs 143\n",
      "training loss 0.043120329177966626\n",
      "epochs 144\n",
      "training loss 0.04212575950518702\n",
      "epochs 145\n",
      "training loss 0.03487237345314387\n",
      "epochs 146\n",
      "training loss 0.030442527173594996\n",
      "epochs 147\n",
      "training loss 0.03962232168538101\n",
      "epochs 148\n",
      "training loss 0.028042986808401164\n",
      "epochs 149\n",
      "training loss 0.04556912841331778\n",
      "testing loss 0.8699881434440613\n",
      "epochs 150\n",
      "training loss 0.03680161166033059\n",
      "epochs 151\n",
      "training loss 0.06156197352842851\n",
      "epochs 152\n",
      "training loss 0.0465894632662336\n",
      "epochs 153\n",
      "training loss 0.032026317303605153\n",
      "epochs 154\n",
      "training loss 0.03835239933069908\n",
      "epochs 155\n",
      "training loss 0.047467261490722494\n",
      "epochs 156\n",
      "training loss 0.02818798486594901\n",
      "epochs 157\n",
      "training loss 0.03351909261140408\n",
      "epochs 158\n",
      "training loss 0.05218733719013857\n",
      "epochs 159\n",
      "training loss 0.080893400102628\n",
      "testing loss 0.24362412095069885\n",
      "epochs 160\n",
      "training loss 0.05622312445757967\n",
      "epochs 161\n",
      "training loss 0.05118734661038175\n",
      "epochs 162\n",
      "training loss 0.04689435897338571\n",
      "epochs 163\n",
      "training loss 0.052331582214118856\n",
      "epochs 164\n",
      "training loss 0.05413017086684704\n",
      "epochs 165\n",
      "training loss 0.040967990999872035\n",
      "epochs 166\n",
      "training loss 0.05406484499743039\n",
      "epochs 167\n",
      "training loss 0.04705555813217705\n",
      "epochs 168\n",
      "training loss 0.031537163647061044\n",
      "epochs 169\n",
      "training loss 0.03942117810136441\n",
      "testing loss 0.05431155115365982\n",
      "epochs 170\n",
      "training loss 0.04037639338410262\n",
      "epochs 171\n",
      "training loss 0.0382199502691175\n",
      "epochs 172\n",
      "training loss 0.03460849090055986\n",
      "epochs 173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.028860440570861103\n",
      "epochs 174\n",
      "training loss 0.031496198593892834\n",
      "epochs 175\n",
      "training loss 0.04418757997001663\n",
      "epochs 176\n",
      "training loss 0.03119863441609072\n",
      "epochs 177\n",
      "training loss 0.031237115389244122\n",
      "epochs 178\n",
      "training loss 0.06553011083354553\n",
      "epochs 179\n",
      "training loss 0.06612544387691852\n",
      "testing loss 0.24294714629650116\n",
      "epochs 180\n",
      "training loss 0.055464294586669315\n",
      "epochs 181\n",
      "training loss 0.027228395714226996\n",
      "epochs 182\n",
      "training loss 0.02463845879855481\n",
      "epochs 183\n",
      "training loss 0.038591744496741076\n",
      "epochs 184\n",
      "training loss 0.050527589107778936\n",
      "epochs 185\n",
      "training loss 0.0436702904827667\n",
      "epochs 186\n",
      "training loss 0.046613188991040895\n",
      "epochs 187\n",
      "training loss 0.040471050256129466\n",
      "epochs 188\n",
      "training loss 0.026535040834410623\n",
      "epochs 189\n",
      "training loss 0.028003930746380128\n",
      "testing loss 0.23654580116271973\n",
      "epochs 190\n",
      "training loss 0.023452652708599062\n",
      "epochs 191\n",
      "training loss 0.02730532439256256\n",
      "epochs 192\n",
      "training loss 0.02332054517711654\n",
      "epochs 193\n",
      "training loss 0.05966735535837484\n",
      "epochs 194\n",
      "training loss 0.23481449173819838\n",
      "epochs 195\n",
      "training loss 0.1607190146364949\n",
      "epochs 196\n",
      "training loss 0.07337757176296278\n",
      "epochs 197\n",
      "training loss 0.16451403912382595\n",
      "epochs 198\n",
      "training loss 0.2941386308972583\n",
      "epochs 199\n",
      "training loss 0.1239987980122819\n",
      "testing loss 0.033555686473846436\n",
      "epochs 200\n",
      "training loss 0.06207651082764972\n",
      "epochs 201\n",
      "training loss 0.048893871880841974\n",
      "epochs 202\n",
      "training loss 0.050810054763022694\n",
      "epochs 203\n",
      "training loss 0.03459457512380499\n",
      "epochs 204\n",
      "training loss 0.02424676452170719\n",
      "epochs 205\n",
      "training loss 0.02453821605127869\n",
      "epochs 206\n",
      "training loss 0.023980834202445817\n",
      "epochs 207\n",
      "training loss 0.022781591873728867\n",
      "epochs 208\n",
      "training loss 0.0238605795653932\n",
      "epochs 209\n",
      "training loss 0.029070914536714555\n",
      "testing loss 0.34875863790512085\n",
      "epochs 210\n",
      "training loss 0.026488202135784157\n",
      "epochs 211\n",
      "training loss 0.0364003551644132\n",
      "epochs 212\n",
      "training loss 0.026192968835433324\n",
      "epochs 213\n",
      "training loss 0.02002486623259205\n",
      "epochs 214\n",
      "training loss 0.028409802131919248\n",
      "epochs 215\n",
      "training loss 0.028338590015967686\n",
      "epochs 216\n",
      "training loss 0.025427440531326062\n",
      "epochs 217\n",
      "training loss 0.01883941444596558\n",
      "epochs 218\n",
      "training loss 0.0351390345925183\n",
      "epochs 219\n",
      "training loss 0.02788509919279904\n",
      "testing loss 0.2794475853443146\n",
      "epochs 220\n",
      "training loss 0.051307921980818114\n",
      "epochs 221\n",
      "training loss 0.04851136922158978\n",
      "epochs 222\n",
      "training loss 0.0364019220017574\n",
      "epochs 223\n",
      "training loss 0.03791061465374448\n",
      "epochs 224\n",
      "training loss 0.025098705116772292\n",
      "epochs 225\n",
      "training loss 0.02690626343317104\n",
      "epochs 226\n",
      "training loss 0.03840808978755817\n",
      "epochs 227\n",
      "training loss 0.022720435552411912\n",
      "epochs 228\n",
      "training loss 0.026480843186039816\n",
      "epochs 229\n",
      "training loss 0.02507989700093414\n",
      "testing loss 0.18436893820762634\n",
      "epochs 230\n",
      "training loss 0.032711156308086534\n",
      "epochs 231\n",
      "training loss 0.055096684105581405\n",
      "epochs 232\n",
      "training loss 0.03593871105518757\n",
      "epochs 233\n",
      "training loss 0.027547544573969913\n",
      "epochs 234\n",
      "training loss 0.02962810882000309\n",
      "epochs 235\n",
      "training loss 0.05501151586685217\n",
      "epochs 236\n",
      "training loss 0.03058491816857096\n",
      "epochs 237\n",
      "training loss 0.022602332338239207\n",
      "epochs 238\n",
      "training loss 0.024693530326652707\n",
      "epochs 239\n",
      "training loss 0.020772849172918183\n",
      "testing loss 0.7266049981117249\n",
      "epochs 240\n",
      "training loss 0.02050664692157597\n",
      "epochs 241\n",
      "training loss 0.0351598409935832\n",
      "epochs 242\n",
      "training loss 0.032592220279867905\n",
      "epochs 243\n",
      "training loss 0.01984979094034343\n",
      "epochs 244\n",
      "training loss 0.023429434605394348\n",
      "epochs 245\n",
      "training loss 0.019037730890241536\n",
      "epochs 246\n",
      "training loss 0.027508677923205223\n",
      "epochs 247\n",
      "training loss 0.024268238037598856\n",
      "epochs 248\n",
      "training loss 0.03665544158694419\n",
      "epochs 249\n",
      "training loss 0.026633441575210203\n",
      "testing loss 0.27795323729515076\n",
      "epochs 250\n",
      "training loss 0.15438803258267317\n",
      "epochs 251\n",
      "training loss 0.027569334869357674\n",
      "epochs 252\n",
      "training loss 0.022799451945518905\n",
      "epochs 253\n",
      "training loss 0.031996111509700616\n",
      "epochs 254\n",
      "training loss 0.048106904762486614\n",
      "epochs 255\n",
      "training loss 0.01979919596659866\n",
      "epochs 256\n",
      "training loss 0.01645437969515721\n",
      "epochs 257\n",
      "training loss 0.0180253057938182\n",
      "epochs 258\n",
      "training loss 0.03453563495793126\n",
      "epochs 259\n",
      "training loss 0.037645526687529955\n",
      "testing loss 0.09764073044061661\n",
      "epochs 260\n",
      "training loss 0.01880159724334424\n",
      "epochs 261\n",
      "training loss 0.10064585485749623\n",
      "epochs 262\n",
      "training loss 0.14569108635751588\n",
      "epochs 263\n",
      "training loss 0.03006551450287754\n",
      "epochs 264\n",
      "training loss 0.0282950068062002\n",
      "epochs 265\n",
      "training loss 0.02121753842203003\n",
      "epochs 266\n",
      "training loss 0.021223960839437717\n",
      "epochs 267\n",
      "training loss 0.02707016100034569\n",
      "epochs 268\n",
      "training loss 0.03305886877192692\n",
      "epochs 269\n",
      "training loss 0.018933367029283985\n",
      "testing loss 1.25626540184021\n",
      "epochs 270\n",
      "training loss 0.021727266602895475\n",
      "epochs 271\n",
      "training loss 0.01651993008669127\n",
      "epochs 272\n",
      "training loss 0.020510802417993544\n",
      "epochs 273\n",
      "training loss 0.020161689817905427\n",
      "epochs 274\n",
      "training loss 0.01813351470469074\n",
      "epochs 275\n",
      "training loss 0.022285237374969503\n",
      "epochs 276\n",
      "training loss 0.02074235024382219\n",
      "epochs 277\n",
      "training loss 0.017718937389101044\n",
      "epochs 278\n",
      "training loss 0.018737821388199474\n",
      "epochs 279\n",
      "training loss 0.022748717338298308\n",
      "testing loss 12.54991626739502\n",
      "epochs 280\n",
      "training loss 0.017313988947055558\n",
      "epochs 281\n",
      "training loss 0.020605099686619006\n",
      "epochs 282\n",
      "training loss 0.015328677159480074\n",
      "epochs 283\n",
      "training loss 0.0148137219024427\n",
      "epochs 284\n",
      "training loss 0.017984089935480646\n",
      "epochs 285\n",
      "training loss 0.03174055831111742\n",
      "epochs 286\n",
      "training loss 0.05932775495122328\n",
      "epochs 287\n",
      "training loss 0.027427227549593557\n",
      "epochs 288\n",
      "training loss 0.04386425199777339\n",
      "epochs 289\n",
      "training loss 0.026301567891443316\n",
      "testing loss 1.9269959926605225\n",
      "epochs 290\n",
      "training loss 0.016284803577670544\n",
      "epochs 291\n",
      "training loss 0.018451439778348712\n",
      "epochs 292\n",
      "training loss 0.01717855773414626\n",
      "epochs 293\n",
      "training loss 0.017143424326610385\n",
      "epochs 294\n",
      "training loss 0.017844784386794675\n",
      "epochs 295\n",
      "training loss 0.025609988184419997\n",
      "epochs 296\n",
      "training loss 0.04468145643953573\n",
      "epochs 297\n",
      "training loss 0.022357519755535054\n",
      "epochs 298\n",
      "training loss 0.0172770109874281\n",
      "epochs 299\n",
      "training loss 0.030473268715043862\n",
      "testing loss 1.0603376626968384\n",
      "epochs 300\n",
      "training loss 0.033716364303660214\n",
      "epochs 301\n",
      "training loss 0.03331155933326844\n",
      "epochs 302\n",
      "training loss 0.017490859535720313\n",
      "epochs 303\n",
      "training loss 0.02628712553079381\n",
      "epochs 304\n",
      "training loss 0.015876599982606644\n",
      "epochs 305\n",
      "training loss 0.017192927637899463\n",
      "epochs 306\n",
      "training loss 0.016197618859058075\n",
      "epochs 307\n",
      "training loss 0.025420867138062464\n",
      "epochs 308\n",
      "training loss 0.02483558736064217\n",
      "epochs 309\n",
      "training loss 0.01511345111877855\n",
      "testing loss 2.394785165786743\n",
      "epochs 310\n",
      "training loss 0.03797709905175549\n",
      "epochs 311\n",
      "training loss 0.02307378015565601\n",
      "epochs 312\n",
      "training loss 0.016911087681849797\n",
      "epochs 313\n",
      "training loss 0.018148881371951465\n",
      "epochs 314\n",
      "training loss 0.03504653961369485\n",
      "epochs 315\n",
      "training loss 0.0446943972231538\n",
      "epochs 316\n",
      "training loss 0.040803781894007414\n",
      "epochs 317\n",
      "training loss 0.022059264615403883\n",
      "epochs 318\n",
      "training loss 0.017814532895995813\n",
      "epochs 319\n",
      "training loss 0.018053042262115262\n",
      "testing loss 5.627045154571533\n",
      "epochs 320\n",
      "training loss 0.05041590438817035\n",
      "epochs 321\n",
      "training loss 0.06004656788356828\n",
      "epochs 322\n",
      "training loss 0.018603017313800978\n",
      "epochs 323\n",
      "training loss 0.014920482073317875\n",
      "epochs 324\n",
      "training loss 0.022511420435639043\n",
      "epochs 325\n",
      "training loss 0.016615605213199602\n",
      "epochs 326\n",
      "training loss 0.014573380404688191\n",
      "epochs 327\n",
      "training loss 0.013293415200755452\n",
      "epochs 328\n",
      "training loss 0.024437107086520304\n",
      "epochs 329\n",
      "training loss 0.020870044495853963\n",
      "testing loss 1.3391510248184204\n",
      "epochs 330\n",
      "training loss 0.01980178869809165\n",
      "epochs 331\n",
      "training loss 0.01766218983314254\n",
      "epochs 332\n",
      "training loss 0.0173392103166517\n",
      "epochs 333\n",
      "training loss 0.02179253122797518\n",
      "epochs 334\n",
      "training loss 0.019443918637592685\n",
      "epochs 335\n",
      "training loss 0.023925450099914362\n",
      "epochs 336\n",
      "training loss 0.01589508273384788\n",
      "epochs 337\n",
      "training loss 0.01670941525618687\n",
      "epochs 338\n",
      "training loss 0.01718418158195687\n",
      "epochs 339\n",
      "training loss 0.030976407960847472\n",
      "testing loss 0.7050395011901855\n",
      "epochs 340\n",
      "training loss 0.016012360703087213\n",
      "epochs 341\n",
      "training loss 0.03283107379790057\n",
      "epochs 342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.02647432399010568\n",
      "epochs 343\n",
      "training loss 0.016680078919638287\n",
      "epochs 344\n",
      "training loss 0.2151447957663825\n",
      "epochs 345\n",
      "training loss 0.17220550484277986\n",
      "epochs 346\n",
      "training loss 0.09215607092236028\n",
      "epochs 347\n",
      "training loss 0.10409020850378456\n",
      "epochs 348\n",
      "training loss 0.039073485570649304\n",
      "epochs 349\n",
      "training loss 0.028145679597263083\n",
      "testing loss 0.09453406929969788\n",
      "epochs 350\n",
      "training loss 0.038363206903026865\n",
      "epochs 351\n",
      "training loss 0.04126558278151082\n",
      "epochs 352\n",
      "training loss 0.017574312312133383\n",
      "epochs 353\n",
      "training loss 0.01655456501934113\n",
      "epochs 354\n",
      "training loss 0.013159128651022912\n",
      "epochs 355\n",
      "training loss 0.012242451671398048\n",
      "epochs 356\n",
      "training loss 0.012105704688777526\n",
      "epochs 357\n",
      "training loss 0.011809551509830988\n",
      "epochs 358\n",
      "training loss 0.011228812610109646\n",
      "epochs 359\n",
      "training loss 0.019070157417180864\n",
      "testing loss 0.1609077900648117\n",
      "epochs 360\n",
      "training loss 0.016417316412270972\n",
      "epochs 361\n",
      "training loss 0.020774268897984064\n",
      "epochs 362\n",
      "training loss 0.02908536913437825\n",
      "epochs 363\n",
      "training loss 0.022745687198458295\n",
      "epochs 364\n",
      "training loss 0.02761436120296518\n",
      "epochs 365\n",
      "training loss 0.024828339941009428\n",
      "epochs 366\n",
      "training loss 0.019498922486761303\n",
      "epochs 367\n",
      "training loss 0.033939567030492154\n",
      "epochs 368\n",
      "training loss 0.014997200090720347\n",
      "epochs 369\n",
      "training loss 0.02275163588029417\n",
      "testing loss 0.13491930067539215\n",
      "epochs 370\n",
      "training loss 0.013297960650401584\n",
      "epochs 371\n",
      "training loss 0.015286485898788229\n",
      "epochs 372\n",
      "training loss 0.019961169014938853\n",
      "epochs 373\n",
      "training loss 0.026293364280101025\n",
      "epochs 374\n",
      "training loss 0.016173754285343667\n",
      "epochs 375\n",
      "training loss 0.021515085350609187\n",
      "epochs 376\n",
      "training loss 0.04005228362864617\n",
      "epochs 377\n",
      "training loss 0.021990930332774015\n",
      "epochs 378\n",
      "training loss 0.03412599992007017\n",
      "epochs 379\n",
      "training loss 0.014878443613027533\n",
      "testing loss 10.248428344726562\n",
      "epochs 380\n",
      "training loss 0.0250011756451744\n",
      "epochs 381\n",
      "training loss 0.01495602625337514\n",
      "epochs 382\n",
      "training loss 0.01681370408358899\n",
      "epochs 383\n",
      "training loss 0.01794070013036782\n",
      "epochs 384\n",
      "training loss 0.025001928729541375\n",
      "epochs 385\n",
      "training loss 0.01665012335235422\n",
      "epochs 386\n",
      "training loss 0.01988658867776394\n",
      "epochs 387\n",
      "training loss 0.013122107033533129\n",
      "epochs 388\n",
      "training loss 0.016466814214645912\n",
      "epochs 389\n",
      "training loss 0.012255626747553998\n",
      "testing loss 1.2127768993377686\n",
      "epochs 390\n",
      "training loss 0.0186917811462825\n",
      "epochs 391\n",
      "training loss 0.013790164467398867\n",
      "epochs 392\n",
      "training loss 0.015451430244314852\n",
      "epochs 393\n",
      "training loss 0.03278413973325355\n",
      "epochs 394\n",
      "training loss 0.019710474246830652\n",
      "epochs 395\n",
      "training loss 0.021325966021554038\n",
      "epochs 396\n",
      "training loss 0.015121767125233556\n",
      "epochs 397\n",
      "training loss 0.020017833395324872\n",
      "epochs 398\n",
      "training loss 0.022977963456827583\n",
      "epochs 399\n",
      "training loss 0.01477777907540175\n",
      "testing loss 0.895379900932312\n",
      "epochs 400\n",
      "training loss 0.021088789708235047\n",
      "epochs 401\n",
      "training loss 0.01128518483178182\n",
      "epochs 402\n",
      "training loss 0.01216802630065517\n",
      "epochs 403\n",
      "training loss 0.01457119272231604\n",
      "epochs 404\n",
      "training loss 0.016850951367594076\n",
      "epochs 405\n",
      "training loss 0.025478283578360624\n",
      "epochs 406\n",
      "training loss 0.017394634799072238\n",
      "epochs 407\n",
      "training loss 0.025738823055430796\n",
      "epochs 408\n",
      "training loss 0.018795162604444405\n",
      "epochs 409\n",
      "training loss 0.012352507273581895\n",
      "testing loss 0.2840319573879242\n",
      "epochs 410\n",
      "training loss 0.011487821888912356\n",
      "epochs 411\n",
      "training loss 0.01432545703948673\n",
      "epochs 412\n",
      "training loss 0.012218418016775765\n",
      "epochs 413\n",
      "training loss 0.010382989963347262\n",
      "epochs 414\n",
      "training loss 0.02461549404895667\n",
      "epochs 415\n",
      "training loss 0.027436418622506387\n",
      "epochs 416\n",
      "training loss 0.013944695452510408\n",
      "epochs 417\n",
      "training loss 0.010746152117622622\n",
      "epochs 418\n",
      "training loss 0.009039042150658188\n",
      "epochs 419\n",
      "training loss 0.0236555021637204\n",
      "testing loss 0.4619519114494324\n",
      "epochs 420\n",
      "training loss 0.01658622256875264\n",
      "epochs 421\n",
      "training loss 0.0248973126438531\n",
      "epochs 422\n",
      "training loss 0.012237948445941914\n",
      "epochs 423\n",
      "training loss 0.01180625090544874\n",
      "epochs 424\n",
      "training loss 0.018984961676213778\n",
      "epochs 425\n",
      "training loss 0.012411379862125173\n",
      "epochs 426\n",
      "training loss 0.023580560757016592\n",
      "epochs 427\n",
      "training loss 0.015862160205671733\n",
      "epochs 428\n",
      "training loss 0.013212738692704024\n",
      "epochs 429\n",
      "training loss 0.014680419293599147\n",
      "testing loss 5.833261966705322\n",
      "epochs 430\n",
      "training loss 0.012790864037180489\n",
      "epochs 431\n",
      "training loss 0.02353626104974837\n",
      "epochs 432\n",
      "training loss 0.01656532674802072\n",
      "epochs 433\n",
      "training loss 0.010560128645914973\n",
      "epochs 434\n",
      "training loss 0.013063201192540654\n",
      "epochs 435\n",
      "training loss 0.007240267690609802\n",
      "epochs 436\n",
      "training loss 0.01287443407416118\n",
      "epochs 437\n",
      "training loss 0.008701835966415026\n",
      "epochs 438\n",
      "training loss 0.025398413170919273\n",
      "epochs 439\n",
      "training loss 0.10766061989195419\n",
      "testing loss 4.204384803771973\n",
      "epochs 440\n",
      "training loss 0.10040522561775464\n",
      "epochs 441\n",
      "training loss 0.022903390146904824\n",
      "epochs 442\n",
      "training loss 0.015434366549280557\n",
      "epochs 443\n",
      "training loss 0.015384891790083864\n",
      "epochs 444\n",
      "training loss 0.017928450269568148\n",
      "epochs 445\n",
      "training loss 0.017420744941090093\n",
      "epochs 446\n",
      "training loss 0.0159511237828569\n",
      "epochs 447\n",
      "training loss 0.011788937875605894\n",
      "epochs 448\n",
      "training loss 0.012317110149359161\n",
      "epochs 449\n",
      "training loss 0.014089118189771066\n",
      "testing loss 3.921983242034912\n",
      "epochs 450\n",
      "training loss 0.02064296827990223\n",
      "epochs 451\n",
      "training loss 0.016460413089247814\n",
      "epochs 452\n",
      "training loss 0.01973400577677019\n",
      "epochs 453\n",
      "training loss 0.010736887068064376\n",
      "epochs 454\n",
      "training loss 0.010199180337120637\n",
      "epochs 455\n",
      "training loss 0.013973706420228788\n",
      "epochs 456\n",
      "training loss 0.013548584540628574\n",
      "epochs 457\n",
      "training loss 0.014724522095286486\n",
      "epochs 458\n",
      "training loss 0.012533577631763888\n",
      "epochs 459\n",
      "training loss 0.014617437537703098\n",
      "testing loss 2.290404796600342\n",
      "epochs 460\n",
      "training loss 0.03328275178474459\n",
      "epochs 461\n",
      "training loss 0.01861133523511164\n",
      "epochs 462\n",
      "training loss 0.009830563904886897\n",
      "epochs 463\n",
      "training loss 0.010811114167286591\n",
      "epochs 464\n",
      "training loss 0.01088291307572614\n",
      "epochs 465\n",
      "training loss 0.01451772093264894\n",
      "epochs 466\n",
      "training loss 0.02040832216399863\n",
      "epochs 467\n",
      "training loss 0.0190344437138375\n",
      "epochs 468\n",
      "training loss 0.01744839190223226\n",
      "epochs 469\n",
      "training loss 0.016421614111768026\n",
      "testing loss 3.757856845855713\n",
      "epochs 470\n",
      "training loss 0.020877846223161075\n",
      "epochs 471\n",
      "training loss 0.014474187548639196\n",
      "epochs 472\n",
      "training loss 0.014836899206663172\n",
      "epochs 473\n",
      "training loss 0.018913253674735175\n",
      "epochs 474\n",
      "training loss 0.011287509469371853\n",
      "epochs 475\n",
      "training loss 0.010018850846046751\n",
      "epochs 476\n",
      "training loss 0.013469223508780654\n",
      "epochs 477\n",
      "training loss 0.019166903513850586\n",
      "epochs 478\n",
      "training loss 0.010886365330467622\n",
      "epochs 479\n",
      "training loss 0.020766051686509992\n",
      "testing loss 2.947550058364868\n",
      "epochs 480\n",
      "training loss 0.01483951107683507\n",
      "epochs 481\n",
      "training loss 0.01985290657559579\n",
      "epochs 482\n",
      "training loss 0.012397523629338\n",
      "epochs 483\n",
      "training loss 0.012242465208030559\n",
      "epochs 484\n",
      "training loss 0.008501074092011108\n",
      "epochs 485\n",
      "training loss 0.021149537118264672\n",
      "epochs 486\n",
      "training loss 0.014212255429645831\n",
      "epochs 487\n",
      "training loss 0.014602989940480753\n",
      "epochs 488\n",
      "training loss 0.012677164295349609\n",
      "epochs 489\n",
      "training loss 0.019024530734697526\n",
      "testing loss 1.4265296459197998\n",
      "epochs 490\n",
      "training loss 0.009612795867195184\n",
      "epochs 491\n",
      "training loss 0.023559090438665765\n",
      "epochs 492\n",
      "training loss 0.04270981140152523\n",
      "epochs 493\n",
      "training loss 0.011463626145356984\n",
      "epochs 494\n",
      "training loss 0.012408558035156492\n",
      "epochs 495\n",
      "training loss 0.008288329163792007\n",
      "epochs 496\n",
      "training loss 0.053734019686552614\n",
      "epochs 497\n",
      "training loss 0.04679420596891732\n",
      "epochs 498\n",
      "training loss 0.03211344318520842\n",
      "epochs 499\n",
      "training loss 0.024708856930109588\n",
      "testing loss 5.678060054779053\n",
      "epochs 500\n",
      "training loss 0.01787942513597734\n",
      "epochs 501\n",
      "training loss 0.015458980860245048\n",
      "epochs 502\n",
      "training loss 0.012154642681619435\n",
      "epochs 503\n",
      "training loss 0.015646355562476497\n",
      "epochs 504\n",
      "training loss 0.01045731260074359\n",
      "epochs 505\n",
      "training loss 0.013786648075576082\n",
      "epochs 506\n",
      "training loss 0.010898139133033427\n",
      "epochs 507\n",
      "training loss 0.013290434786485452\n",
      "epochs 508\n",
      "training loss 0.017350546087166577\n",
      "epochs 509\n",
      "training loss 0.00924427251924168\n",
      "testing loss 6.129805088043213\n",
      "epochs 510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.016389336988724995\n",
      "epochs 511\n",
      "training loss 0.031612300994156885\n",
      "epochs 512\n",
      "training loss 0.009967116135257212\n",
      "epochs 513\n",
      "training loss 0.011590784157372334\n",
      "epochs 514\n",
      "training loss 0.01591555246039096\n",
      "epochs 515\n",
      "training loss 0.01308713473915828\n",
      "epochs 516\n",
      "training loss 0.010737793908144036\n",
      "epochs 517\n",
      "training loss 0.011750520267664934\n",
      "epochs 518\n",
      "training loss 0.016110320314482757\n",
      "epochs 519\n",
      "training loss 0.022067480856044725\n",
      "testing loss 2.449211835861206\n",
      "epochs 520\n",
      "training loss 0.010666084158318963\n",
      "epochs 521\n",
      "training loss 0.010654883927693874\n",
      "epochs 522\n",
      "training loss 0.008678732190111822\n",
      "epochs 523\n",
      "training loss 0.007627153525016073\n",
      "epochs 524\n",
      "training loss 0.007280558217881304\n",
      "epochs 525\n",
      "training loss 0.0066827078984880986\n",
      "epochs 526\n",
      "training loss 0.009903571358176343\n",
      "epochs 527\n",
      "training loss 0.025485502211659242\n",
      "epochs 528\n",
      "training loss 0.01437384294853969\n",
      "epochs 529\n",
      "training loss 0.01550876687450165\n",
      "testing loss 7.888591766357422\n",
      "epochs 530\n",
      "training loss 0.011822559883479368\n",
      "epochs 531\n",
      "training loss 0.00990818597827897\n",
      "epochs 532\n",
      "training loss 0.008361440707901211\n",
      "epochs 533\n",
      "training loss 0.011417838521866184\n",
      "epochs 534\n",
      "training loss 0.023791947708267605\n",
      "epochs 535\n",
      "training loss 0.02306121348149397\n",
      "epochs 536\n",
      "training loss 0.0354328347765135\n",
      "epochs 537\n",
      "training loss 0.01945310425803517\n",
      "epochs 538\n",
      "training loss 0.00762791625986045\n",
      "epochs 539\n",
      "training loss 0.007900344087939822\n",
      "testing loss 3.3788132667541504\n",
      "epochs 540\n",
      "training loss 0.010239083081427397\n",
      "epochs 541\n",
      "training loss 0.008063613308706518\n",
      "epochs 542\n",
      "training loss 0.011532947203765314\n",
      "epochs 543\n",
      "training loss 0.009909229193616545\n",
      "epochs 544\n",
      "training loss 0.010097088649248083\n",
      "epochs 545\n",
      "training loss 0.011767382678491149\n",
      "epochs 546\n",
      "training loss 0.008593021355795137\n",
      "epochs 547\n",
      "training loss 0.016592176748213894\n",
      "epochs 548\n",
      "training loss 0.0076328526533237006\n",
      "epochs 549\n",
      "training loss 0.013078630575910211\n",
      "testing loss 3.041738510131836\n",
      "epochs 550\n",
      "training loss 0.014161481236982526\n",
      "epochs 551\n",
      "training loss 0.013275538614922852\n",
      "epochs 552\n",
      "training loss 0.015524286738681522\n",
      "epochs 553\n",
      "training loss 0.011240535150423194\n",
      "epochs 554\n",
      "training loss 0.010642660475533569\n",
      "epochs 555\n",
      "training loss 0.014911057853676152\n",
      "epochs 556\n",
      "training loss 0.031558504735938075\n",
      "epochs 557\n",
      "training loss 0.3266561383550817\n",
      "epochs 558\n",
      "training loss 0.055764170736074446\n",
      "epochs 559\n",
      "training loss 0.017871593881511327\n",
      "testing loss 6.002264499664307\n",
      "epochs 560\n",
      "training loss 0.01514727928195939\n",
      "epochs 561\n",
      "training loss 0.011438485277308659\n",
      "epochs 562\n",
      "training loss 0.011767630248020093\n",
      "epochs 563\n",
      "training loss 0.012923594490822518\n",
      "epochs 564\n",
      "training loss 0.0077767622346679366\n",
      "epochs 565\n",
      "training loss 0.009328081892013099\n",
      "epochs 566\n",
      "training loss 0.00894778654628405\n",
      "epochs 567\n",
      "training loss 0.015193913110785863\n",
      "epochs 568\n",
      "training loss 0.010536179447433713\n",
      "epochs 569\n",
      "training loss 0.009815821263261817\n",
      "testing loss 9.007682800292969\n",
      "epochs 570\n",
      "training loss 0.007861133862399694\n",
      "epochs 571\n",
      "training loss 0.012150796360308021\n",
      "epochs 572\n",
      "training loss 0.011034714845432477\n",
      "epochs 573\n",
      "training loss 0.016971112216230145\n",
      "epochs 574\n",
      "training loss 0.01614089803629075\n",
      "epochs 575\n",
      "training loss 0.014441099833471305\n",
      "epochs 576\n",
      "training loss 0.016791668298626036\n",
      "epochs 577\n",
      "training loss 0.011290899758706941\n",
      "epochs 578\n",
      "training loss 0.014586812309977232\n",
      "epochs 579\n",
      "training loss 0.006885644147229014\n",
      "testing loss 8.728157997131348\n",
      "epochs 580\n",
      "training loss 0.008256790202788331\n",
      "epochs 581\n",
      "training loss 0.025389637203999994\n",
      "epochs 582\n",
      "training loss 0.01775331483504763\n",
      "epochs 583\n",
      "training loss 0.00771289527500895\n",
      "epochs 584\n",
      "training loss 0.007157398039249308\n",
      "epochs 585\n",
      "training loss 0.010738694552106388\n",
      "epochs 586\n",
      "training loss 0.008608319353538028\n",
      "epochs 587\n",
      "training loss 0.00966149209716329\n",
      "epochs 588\n",
      "training loss 0.014089451680862994\n",
      "epochs 589\n",
      "training loss 0.007417950652201067\n",
      "testing loss 18.10494041442871\n",
      "epochs 590\n",
      "training loss 0.006107009325007146\n",
      "epochs 591\n",
      "training loss 0.007243106263040593\n",
      "epochs 592\n",
      "training loss 0.006490243135979681\n",
      "epochs 593\n",
      "training loss 0.006760471482112101\n",
      "epochs 594\n",
      "training loss 0.006258568904277953\n",
      "epochs 595\n",
      "training loss 0.023693157207559455\n",
      "epochs 596\n",
      "training loss 0.01566311261673091\n",
      "epochs 597\n",
      "training loss 0.013339606534238114\n",
      "epochs 598\n",
      "training loss 0.028025334130859735\n",
      "epochs 599\n",
      "training loss 0.010023211068332647\n",
      "testing loss 25.195873260498047\n",
      "epochs 600\n",
      "training loss 0.007398216954121987\n",
      "epochs 601\n",
      "training loss 0.00929995744037583\n",
      "epochs 602\n",
      "training loss 0.00878468411127952\n",
      "epochs 603\n",
      "training loss 0.008026846217443094\n",
      "epochs 604\n",
      "training loss 0.01689524634063921\n",
      "epochs 605\n",
      "training loss 0.007569564132471428\n",
      "epochs 606\n",
      "training loss 0.007597433167481513\n",
      "epochs 607\n",
      "training loss 0.010643284741055333\n",
      "epochs 608\n",
      "training loss 0.013640694037982912\n",
      "epochs 609\n",
      "training loss 0.01571181856632007\n",
      "testing loss 20.577043533325195\n",
      "epochs 610\n",
      "training loss 0.01454441497864371\n",
      "epochs 611\n",
      "training loss 0.019745932086229778\n",
      "epochs 612\n",
      "training loss 0.01006044179737342\n",
      "epochs 613\n",
      "training loss 0.017406655008424865\n",
      "epochs 614\n",
      "training loss 0.016304307979898472\n",
      "epochs 615\n",
      "training loss 0.05893345009394441\n",
      "epochs 616\n",
      "training loss 0.037646633626498054\n",
      "epochs 617\n",
      "training loss 0.007862472837565072\n",
      "epochs 618\n",
      "training loss 0.008422534640483332\n",
      "epochs 619\n",
      "training loss 0.008968869979126435\n",
      "testing loss 4.992964744567871\n",
      "epochs 620\n",
      "training loss 0.0066810098194488975\n",
      "epochs 621\n",
      "training loss 0.009355056974472422\n",
      "epochs 622\n",
      "training loss 0.007437635659042633\n",
      "epochs 623\n",
      "training loss 0.005435014720723936\n",
      "epochs 624\n",
      "training loss 0.006545595130459829\n",
      "epochs 625\n",
      "training loss 0.006255165567960252\n",
      "epochs 626\n",
      "training loss 0.005290389650811752\n",
      "epochs 627\n",
      "training loss 0.008354291041861429\n",
      "epochs 628\n",
      "training loss 0.009019334727164471\n",
      "epochs 629\n",
      "training loss 0.008848614011411415\n",
      "testing loss 5.264016628265381\n",
      "epochs 630\n",
      "training loss 0.010131502697582949\n",
      "epochs 631\n",
      "training loss 0.014984193773037105\n",
      "epochs 632\n",
      "training loss 0.008067605733363466\n",
      "epochs 633\n",
      "training loss 0.006837141200561415\n",
      "epochs 634\n",
      "training loss 0.006812791101578059\n",
      "epochs 635\n",
      "training loss 0.012908887151967396\n",
      "epochs 636\n",
      "training loss 0.019738129183480686\n",
      "epochs 637\n",
      "training loss 0.011262084494316668\n",
      "epochs 638\n",
      "training loss 0.005320807517449739\n",
      "epochs 639\n",
      "training loss 0.005688795636436253\n",
      "testing loss 2.2490997314453125\n",
      "epochs 640\n",
      "training loss 0.007267566379441908\n",
      "epochs 641\n",
      "training loss 0.008335355494284269\n",
      "epochs 642\n",
      "training loss 0.006115928623881756\n",
      "epochs 643\n",
      "training loss 0.007895150264217094\n",
      "epochs 644\n",
      "training loss 0.011691795660634384\n",
      "epochs 645\n",
      "training loss 0.0129391744394194\n",
      "epochs 646\n",
      "training loss 0.01259404997284891\n",
      "epochs 647\n",
      "training loss 0.03599290634985223\n",
      "epochs 648\n",
      "training loss 0.021941526442991965\n",
      "epochs 649\n",
      "training loss 0.013997809883827964\n",
      "testing loss 24.52541732788086\n",
      "epochs 650\n",
      "training loss 0.0067815971673663815\n",
      "epochs 651\n",
      "training loss 0.009131524002066616\n",
      "epochs 652\n",
      "training loss 0.007351109187937144\n",
      "epochs 653\n",
      "training loss 0.026403282145320468\n",
      "epochs 654\n",
      "training loss 0.010548176579741818\n",
      "epochs 655\n",
      "training loss 0.008358625861618556\n",
      "epochs 656\n",
      "training loss 0.007794632449407469\n",
      "epochs 657\n",
      "training loss 0.014108609801835634\n",
      "epochs 658\n",
      "training loss 0.010450161307711493\n",
      "epochs 659\n",
      "training loss 0.010485832191382846\n",
      "testing loss 13.264117240905762\n",
      "epochs 660\n",
      "training loss 0.013065318701167901\n",
      "epochs 661\n",
      "training loss 0.007411003410533973\n",
      "epochs 662\n",
      "training loss 0.010041584589546829\n",
      "epochs 663\n",
      "training loss 0.010002356708388437\n",
      "epochs 664\n",
      "training loss 0.0065227549565448\n",
      "epochs 665\n",
      "training loss 0.013982948807604385\n",
      "epochs 666\n",
      "training loss 0.01022595777599649\n",
      "epochs 667\n",
      "training loss 0.013398045647155606\n",
      "epochs 668\n",
      "training loss 0.007599786663371505\n",
      "epochs 669\n",
      "training loss 0.009079127290257902\n",
      "testing loss 8.59615707397461\n",
      "epochs 670\n",
      "training loss 0.00840831225540376\n",
      "epochs 671\n",
      "training loss 0.01290987419020949\n",
      "epochs 672\n",
      "training loss 0.016929387587658835\n",
      "epochs 673\n",
      "training loss 0.016190007640839076\n",
      "epochs 674\n",
      "training loss 0.016081180894803822\n",
      "epochs 675\n",
      "training loss 0.02168831085419339\n",
      "epochs 676\n",
      "training loss 0.013787677167265705\n",
      "epochs 677\n",
      "training loss 0.006856431106500554\n",
      "epochs 678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006373259181749414\n",
      "epochs 679\n",
      "training loss 0.009758664186423023\n",
      "testing loss 19.823009490966797\n",
      "epochs 680\n",
      "training loss 0.005536799822849306\n",
      "epochs 681\n",
      "training loss 0.005700899487022649\n",
      "epochs 682\n",
      "training loss 0.006063223317632396\n",
      "epochs 683\n",
      "training loss 0.006639228737207525\n",
      "epochs 684\n",
      "training loss 0.008138028970411555\n",
      "epochs 685\n",
      "training loss 0.020363239849437817\n",
      "epochs 686\n",
      "training loss 0.00922966781777866\n",
      "epochs 687\n",
      "training loss 0.005706960537160436\n",
      "epochs 688\n",
      "training loss 0.007279398343808046\n",
      "epochs 689\n",
      "training loss 0.006669040652217738\n",
      "testing loss 9.631669998168945\n",
      "epochs 690\n",
      "training loss 0.013244209441384583\n",
      "epochs 691\n",
      "training loss 0.005647602278005445\n",
      "epochs 692\n",
      "training loss 0.010241025621353678\n",
      "epochs 693\n",
      "training loss 0.025194594369157018\n",
      "epochs 694\n",
      "training loss 0.013067529373096697\n",
      "epochs 695\n",
      "training loss 0.009484790278260003\n",
      "epochs 696\n",
      "training loss 0.00657114566190902\n",
      "epochs 697\n",
      "training loss 0.017254316729182997\n",
      "epochs 698\n",
      "training loss 0.010537458515980027\n",
      "epochs 699\n",
      "training loss 0.015216974789897602\n",
      "testing loss 18.58045768737793\n",
      "epochs 700\n",
      "training loss 0.01196860990515261\n",
      "epochs 701\n",
      "training loss 0.006302409137909611\n",
      "epochs 702\n",
      "training loss 0.006630809386401917\n",
      "epochs 703\n",
      "training loss 0.005591686102420543\n",
      "epochs 704\n",
      "training loss 0.0055660412809105985\n",
      "epochs 705\n",
      "training loss 0.005047178195761235\n",
      "epochs 706\n",
      "training loss 0.010321387175865697\n",
      "epochs 707\n",
      "training loss 0.006366765099775159\n",
      "epochs 708\n",
      "training loss 0.007119360491351197\n",
      "epochs 709\n",
      "training loss 0.011400945922077604\n",
      "testing loss 11.10302734375\n",
      "epochs 710\n",
      "training loss 0.0058345431292598894\n",
      "epochs 711\n",
      "training loss 0.0068490444005213005\n",
      "epochs 712\n",
      "training loss 0.007944945086527502\n",
      "epochs 713\n",
      "training loss 0.02272062159256276\n",
      "epochs 714\n",
      "training loss 0.01683033233169805\n",
      "epochs 715\n",
      "training loss 0.009382770650766113\n",
      "epochs 716\n",
      "training loss 0.011685366004310322\n",
      "epochs 717\n",
      "training loss 0.006246077109862006\n",
      "epochs 718\n",
      "training loss 0.00615796446235794\n",
      "epochs 719\n",
      "training loss 0.008397027412711672\n",
      "testing loss 9.664851188659668\n",
      "epochs 720\n",
      "training loss 0.010249112327724243\n",
      "epochs 721\n",
      "training loss 0.008096086671293684\n",
      "epochs 722\n",
      "training loss 0.01201538678653764\n",
      "epochs 723\n",
      "training loss 0.00951251493992679\n",
      "epochs 724\n",
      "training loss 0.008568833502376395\n",
      "epochs 725\n",
      "training loss 0.011969843127228546\n",
      "epochs 726\n",
      "training loss 0.032497856027982906\n",
      "epochs 727\n",
      "training loss 0.018492499352291678\n",
      "epochs 728\n",
      "training loss 0.016464100282808595\n",
      "epochs 729\n",
      "training loss 0.004706000288327535\n",
      "testing loss 6.018984794616699\n",
      "epochs 730\n",
      "training loss 0.005901287950462464\n",
      "epochs 731\n",
      "training loss 0.004647781215184791\n",
      "epochs 732\n",
      "training loss 0.005521599630909887\n",
      "epochs 733\n",
      "training loss 0.005894118117761206\n",
      "epochs 734\n",
      "training loss 0.010091635137514182\n",
      "epochs 735\n",
      "training loss 0.004933040986317351\n",
      "epochs 736\n",
      "training loss 0.006371753242318377\n",
      "epochs 737\n",
      "training loss 0.005327436614386512\n",
      "epochs 738\n",
      "training loss 0.004539465682693955\n",
      "epochs 739\n",
      "training loss 0.017745857628655028\n",
      "testing loss 12.421365737915039\n",
      "epochs 740\n",
      "training loss 0.030727618558783875\n",
      "epochs 741\n",
      "training loss 0.010804376361722295\n",
      "epochs 742\n",
      "training loss 0.0129509571615155\n",
      "epochs 743\n",
      "training loss 0.013720252510216652\n",
      "epochs 744\n",
      "training loss 0.010187337227222143\n",
      "epochs 745\n",
      "training loss 0.03560078313404864\n",
      "epochs 746\n",
      "training loss 0.012309340175918558\n",
      "epochs 747\n",
      "training loss 0.011125529335924622\n",
      "epochs 748\n",
      "training loss 0.01136219678869979\n",
      "epochs 749\n",
      "training loss 0.017734117060899734\n",
      "testing loss 8.78978157043457\n",
      "epochs 750\n",
      "training loss 0.012125582733389102\n",
      "epochs 751\n",
      "training loss 0.007157654926237283\n",
      "epochs 752\n",
      "training loss 0.012242652259936386\n",
      "epochs 753\n",
      "training loss 0.012902095743144552\n",
      "epochs 754\n",
      "training loss 0.006270755523335979\n",
      "epochs 755\n",
      "training loss 0.005804400680812471\n",
      "epochs 756\n",
      "training loss 0.0062456053674616145\n",
      "epochs 757\n",
      "training loss 0.013290047920026788\n",
      "epochs 758\n",
      "training loss 0.008022767797140687\n",
      "epochs 759\n",
      "training loss 0.00830792393182602\n",
      "testing loss 5.3561906814575195\n",
      "epochs 760\n",
      "training loss 0.0046592904854509416\n",
      "epochs 761\n",
      "training loss 0.004867966303065645\n",
      "epochs 762\n",
      "training loss 0.008195066217079081\n",
      "epochs 763\n",
      "training loss 0.022095854776544552\n",
      "epochs 764\n",
      "training loss 0.013488760275879141\n",
      "epochs 765\n",
      "training loss 0.0061143296107536915\n",
      "epochs 766\n",
      "training loss 0.007016337838616561\n",
      "epochs 767\n",
      "training loss 0.008279933720460217\n",
      "epochs 768\n",
      "training loss 0.006524099360191912\n",
      "epochs 769\n",
      "training loss 0.007683311727349505\n",
      "testing loss 25.590280532836914\n",
      "epochs 770\n",
      "training loss 0.010532496096284101\n",
      "epochs 771\n",
      "training loss 0.007813564594835043\n",
      "epochs 772\n",
      "training loss 0.01278252280035028\n",
      "epochs 773\n",
      "training loss 0.010869472767367508\n",
      "epochs 774\n",
      "training loss 0.00609242941859658\n",
      "epochs 775\n",
      "training loss 0.007121440248958992\n",
      "epochs 776\n",
      "training loss 0.006972262874302087\n",
      "epochs 777\n",
      "training loss 0.009450878653054436\n",
      "epochs 778\n",
      "training loss 0.011937143762285512\n",
      "epochs 779\n",
      "training loss 0.006329751693446076\n",
      "testing loss 32.82730484008789\n",
      "epochs 780\n",
      "training loss 0.006245684098790992\n",
      "epochs 781\n",
      "training loss 0.007615939758464017\n",
      "epochs 782\n",
      "training loss 0.011021306215446782\n",
      "epochs 783\n",
      "training loss 0.006952955108135939\n",
      "epochs 784\n",
      "training loss 0.018293136277828703\n",
      "epochs 785\n",
      "training loss 0.008028075941414995\n",
      "epochs 786\n",
      "training loss 0.011412834656904593\n",
      "epochs 787\n",
      "training loss 0.009342254421701937\n",
      "epochs 788\n",
      "training loss 0.007510736624174045\n",
      "epochs 789\n",
      "training loss 0.010416361338204958\n",
      "testing loss 31.114940643310547\n",
      "epochs 790\n",
      "training loss 0.005844589644535022\n",
      "epochs 791\n",
      "training loss 0.00870124942580746\n",
      "epochs 792\n",
      "training loss 0.008058698466894302\n",
      "epochs 793\n",
      "training loss 0.00881896554210195\n",
      "epochs 794\n",
      "training loss 0.005905852862633764\n",
      "epochs 795\n",
      "training loss 0.01389160015704957\n",
      "epochs 796\n",
      "training loss 0.0056900474525085004\n",
      "epochs 797\n",
      "training loss 0.005349064583071705\n",
      "epochs 798\n",
      "training loss 0.0041613809129392556\n",
      "epochs 799\n",
      "training loss 0.004254721737974747\n",
      "testing loss 21.868297576904297\n",
      "epochs 800\n",
      "training loss 0.0073561324378136886\n",
      "epochs 801\n",
      "training loss 0.0064419463192197405\n",
      "epochs 802\n",
      "training loss 0.004271951053912441\n",
      "epochs 803\n",
      "training loss 0.0074863904042903225\n",
      "epochs 804\n",
      "training loss 0.022156908921897412\n",
      "epochs 805\n",
      "training loss 0.024859961226695416\n",
      "epochs 806\n",
      "training loss 0.01297206952180149\n",
      "epochs 807\n",
      "training loss 0.010003007022720395\n",
      "epochs 808\n",
      "training loss 0.007209689333809144\n",
      "epochs 809\n",
      "training loss 0.004435414962018981\n",
      "testing loss 37.988426208496094\n",
      "epochs 810\n",
      "training loss 0.005552424024084978\n",
      "epochs 811\n",
      "training loss 0.007682466048352194\n",
      "epochs 812\n",
      "training loss 0.00669690085730205\n",
      "epochs 813\n",
      "training loss 0.007606407127230231\n",
      "epochs 814\n",
      "training loss 0.011674136076489407\n",
      "epochs 815\n",
      "training loss 0.004093485268425535\n",
      "epochs 816\n",
      "training loss 0.003861309930145966\n",
      "epochs 817\n",
      "training loss 0.004723192347834508\n",
      "epochs 818\n",
      "training loss 0.007349321319524086\n",
      "epochs 819\n",
      "training loss 0.01014777881968202\n",
      "testing loss 32.10493850708008\n",
      "epochs 820\n",
      "training loss 0.013279482581172928\n",
      "epochs 821\n",
      "training loss 0.006744300924721315\n",
      "epochs 822\n",
      "training loss 0.00521808043827839\n",
      "epochs 823\n",
      "training loss 0.020672490291128106\n",
      "epochs 824\n",
      "training loss 0.006663812473275219\n",
      "epochs 825\n",
      "training loss 0.007734600111672824\n",
      "epochs 826\n",
      "training loss 0.008015594062762279\n",
      "epochs 827\n",
      "training loss 0.004354781510703491\n",
      "epochs 828\n",
      "training loss 0.005041248404251581\n",
      "epochs 829\n",
      "training loss 0.005416953977140965\n",
      "testing loss 66.79618835449219\n",
      "epochs 830\n",
      "training loss 0.011240578479500432\n",
      "epochs 831\n",
      "training loss 0.014928425506999096\n",
      "epochs 832\n",
      "training loss 0.013597747253141168\n",
      "epochs 833\n",
      "training loss 0.01187636566870479\n",
      "epochs 834\n",
      "training loss 0.005457747096167595\n",
      "epochs 835\n",
      "training loss 0.004222166655359395\n",
      "epochs 836\n",
      "training loss 0.0047817318354535735\n",
      "epochs 837\n",
      "training loss 0.008576925734594239\n",
      "epochs 838\n",
      "training loss 0.007669485869789214\n",
      "epochs 839\n",
      "training loss 0.011939387652797229\n",
      "testing loss 37.8348388671875\n",
      "epochs 840\n",
      "training loss 0.010783904731612315\n",
      "epochs 841\n",
      "training loss 0.010445696064694362\n",
      "epochs 842\n",
      "training loss 0.005730003318890477\n",
      "epochs 843\n",
      "training loss 0.005139549545746184\n",
      "epochs 844\n",
      "training loss 0.004698794826187871\n",
      "epochs 845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.005584405764062522\n",
      "epochs 846\n",
      "training loss 0.006281777143224396\n",
      "epochs 847\n",
      "training loss 0.004927224005487832\n",
      "epochs 848\n",
      "training loss 0.0047610784097659314\n",
      "epochs 849\n",
      "training loss 0.005218989996832203\n",
      "testing loss 56.801734924316406\n",
      "epochs 850\n",
      "training loss 0.009162647363899106\n",
      "epochs 851\n",
      "training loss 0.02213123999394928\n",
      "epochs 852\n",
      "training loss 0.03868753788287215\n",
      "epochs 853\n",
      "training loss 0.01476904655609167\n",
      "epochs 854\n",
      "training loss 0.013152241668748585\n",
      "epochs 855\n",
      "training loss 0.007184910430629371\n",
      "epochs 856\n",
      "training loss 0.005947617722223654\n",
      "epochs 857\n",
      "training loss 0.006951162068060401\n",
      "epochs 858\n",
      "training loss 0.004726746293123473\n",
      "epochs 859\n",
      "training loss 0.005120386737822132\n",
      "testing loss 22.354782104492188\n",
      "epochs 860\n",
      "training loss 0.007062457211202744\n",
      "epochs 861\n",
      "training loss 0.006606382854762628\n",
      "epochs 862\n",
      "training loss 0.004491049112667414\n",
      "epochs 863\n",
      "training loss 0.004278903969591766\n",
      "epochs 864\n",
      "training loss 0.003787844721227884\n",
      "epochs 865\n",
      "training loss 0.004691540202210573\n",
      "epochs 866\n",
      "training loss 0.003973563828957126\n",
      "epochs 867\n",
      "training loss 0.004345295593057844\n",
      "epochs 868\n",
      "training loss 0.004333998824497967\n",
      "epochs 869\n",
      "training loss 0.011281168316914277\n",
      "testing loss 64.97769927978516\n",
      "epochs 870\n",
      "training loss 0.004864393735558472\n",
      "epochs 871\n",
      "training loss 0.003844413962074076\n",
      "epochs 872\n",
      "training loss 0.0037565030863113474\n",
      "epochs 873\n",
      "training loss 0.004395280558982808\n",
      "epochs 874\n",
      "training loss 0.013100201695818792\n",
      "epochs 875\n",
      "training loss 0.006561703213744543\n",
      "epochs 876\n",
      "training loss 0.013267855622777432\n",
      "epochs 877\n",
      "training loss 0.014230759793215177\n",
      "epochs 878\n",
      "training loss 0.01094943317161365\n",
      "epochs 879\n",
      "training loss 0.018167467195202004\n",
      "testing loss 24.655439376831055\n",
      "epochs 880\n",
      "training loss 0.004891629052828207\n",
      "epochs 881\n",
      "training loss 0.004980816616620304\n",
      "epochs 882\n",
      "training loss 0.003553570369540742\n",
      "epochs 883\n",
      "training loss 0.0069698692285314655\n",
      "epochs 884\n",
      "training loss 0.006519630097671214\n",
      "epochs 885\n",
      "training loss 0.016509882353641318\n",
      "epochs 886\n",
      "training loss 0.004381760127955314\n",
      "epochs 887\n",
      "training loss 0.004612500746170003\n",
      "epochs 888\n",
      "training loss 0.005381887841433512\n",
      "epochs 889\n",
      "training loss 0.006332749692074051\n",
      "testing loss 47.721195220947266\n",
      "epochs 890\n",
      "training loss 0.004590663296226976\n",
      "epochs 891\n",
      "training loss 0.010033035147088495\n",
      "epochs 892\n",
      "training loss 0.014133991329987166\n",
      "epochs 893\n",
      "training loss 0.01609132683158598\n",
      "epochs 894\n",
      "training loss 0.011647478922862898\n",
      "epochs 895\n",
      "training loss 0.011929731574756177\n",
      "epochs 896\n",
      "training loss 0.005374964135416755\n",
      "epochs 897\n",
      "training loss 0.005041061633153621\n",
      "epochs 898\n",
      "training loss 0.00849438260916169\n",
      "epochs 899\n",
      "training loss 0.004155832023190504\n",
      "testing loss 30.892431259155273\n",
      "epochs 900\n",
      "training loss 0.00954507744348975\n",
      "epochs 901\n",
      "training loss 0.01321850530662094\n",
      "epochs 902\n",
      "training loss 0.008227647802877155\n",
      "epochs 903\n",
      "training loss 0.0047682194780050355\n",
      "epochs 904\n",
      "training loss 0.005086401870445997\n",
      "epochs 905\n",
      "training loss 0.02909366763337995\n",
      "epochs 906\n",
      "training loss 0.0062087371522052725\n",
      "epochs 907\n",
      "training loss 0.010986008651018368\n",
      "epochs 908\n",
      "training loss 0.014832693523953132\n",
      "epochs 909\n",
      "training loss 0.00849929905962199\n",
      "testing loss 40.341976165771484\n",
      "epochs 910\n",
      "training loss 0.005432950782194508\n",
      "epochs 911\n",
      "training loss 0.003999610543674366\n",
      "epochs 912\n",
      "training loss 0.004460335727498838\n",
      "epochs 913\n",
      "training loss 0.004517644618383863\n",
      "epochs 914\n",
      "training loss 0.006840491673739796\n",
      "epochs 915\n",
      "training loss 0.011584223985361556\n",
      "epochs 916\n",
      "training loss 0.006845339894238295\n",
      "epochs 917\n",
      "training loss 0.0069961119619565025\n",
      "epochs 918\n",
      "training loss 0.003810600361391676\n",
      "epochs 919\n",
      "training loss 0.004214272022981084\n",
      "testing loss 32.230560302734375\n",
      "epochs 920\n",
      "training loss 0.004236713664211107\n",
      "epochs 921\n",
      "training loss 0.007613239713210726\n",
      "epochs 922\n",
      "training loss 0.005532937051700146\n",
      "epochs 923\n",
      "training loss 0.0044322450687600806\n",
      "epochs 924\n",
      "training loss 0.0053443933632507015\n",
      "epochs 925\n",
      "training loss 0.009964576538539294\n",
      "epochs 926\n",
      "training loss 0.009792130744564488\n",
      "epochs 927\n",
      "training loss 0.012375962872509704\n",
      "epochs 928\n",
      "training loss 0.004781695882874457\n",
      "epochs 929\n",
      "training loss 0.0043828111600525905\n",
      "testing loss 57.18214797973633\n",
      "epochs 930\n",
      "training loss 0.004297458822133415\n",
      "epochs 931\n",
      "training loss 0.004099332556043836\n",
      "epochs 932\n",
      "training loss 0.0035110206703062763\n",
      "epochs 933\n",
      "training loss 0.0039960474509632945\n",
      "epochs 934\n",
      "training loss 0.008040809887461364\n",
      "epochs 935\n",
      "training loss 0.009628405951132828\n",
      "epochs 936\n",
      "training loss 0.013149662788562251\n",
      "epochs 937\n",
      "training loss 0.00894910505563585\n",
      "epochs 938\n",
      "training loss 0.008478298954750326\n",
      "epochs 939\n",
      "training loss 0.0038058479372976403\n",
      "testing loss 46.12000274658203\n",
      "epochs 940\n",
      "training loss 0.0040659749922765925\n",
      "epochs 941\n",
      "training loss 0.004566465630760473\n",
      "epochs 942\n",
      "training loss 0.007063010545927241\n",
      "epochs 943\n",
      "training loss 0.005488351060811318\n",
      "epochs 944\n",
      "training loss 0.00436417274035965\n",
      "epochs 945\n",
      "training loss 0.004271748161761824\n",
      "epochs 946\n",
      "training loss 0.008028628025909491\n",
      "epochs 947\n",
      "training loss 0.008895886946949318\n",
      "epochs 948\n",
      "training loss 0.018748865025400213\n",
      "epochs 949\n",
      "training loss 0.011611961293260031\n",
      "testing loss 78.40483093261719\n",
      "epochs 950\n",
      "training loss 0.007841294682866921\n",
      "epochs 951\n",
      "training loss 0.008370224282032613\n",
      "epochs 952\n",
      "training loss 0.006023765856992792\n",
      "epochs 953\n",
      "training loss 0.005058078973017859\n",
      "epochs 954\n",
      "training loss 0.005165129599415444\n",
      "epochs 955\n",
      "training loss 0.011330254119115346\n",
      "epochs 956\n",
      "training loss 0.005034758429974318\n",
      "epochs 957\n",
      "training loss 0.012231314332563092\n",
      "epochs 958\n",
      "training loss 0.0075782235161486\n",
      "epochs 959\n",
      "training loss 0.008440439372013013\n",
      "testing loss 117.3755111694336\n",
      "epochs 960\n",
      "training loss 0.00577158192313756\n",
      "epochs 961\n",
      "training loss 0.005974313443893511\n",
      "epochs 962\n",
      "training loss 0.007963008611378344\n",
      "epochs 963\n",
      "training loss 0.004748059686442668\n",
      "epochs 964\n",
      "training loss 0.010840946421817396\n",
      "epochs 965\n",
      "training loss 0.009257892901642304\n",
      "epochs 966\n",
      "training loss 0.00626132620752535\n",
      "epochs 967\n",
      "training loss 0.0062788840884229905\n",
      "epochs 968\n",
      "training loss 0.005734228531858235\n",
      "epochs 969\n",
      "training loss 0.00986575318683842\n",
      "testing loss 69.9521713256836\n",
      "epochs 970\n",
      "training loss 0.007011944610117511\n",
      "epochs 971\n",
      "training loss 0.003969123721771845\n",
      "epochs 972\n",
      "training loss 0.0040896535194902255\n",
      "epochs 973\n",
      "training loss 0.004358704495384838\n",
      "epochs 974\n",
      "training loss 0.0037683358571181697\n",
      "epochs 975\n",
      "training loss 0.006110239874882003\n",
      "epochs 976\n",
      "training loss 0.007038154750074627\n",
      "epochs 977\n",
      "training loss 0.012023436783982272\n",
      "epochs 978\n",
      "training loss 0.007780561640604653\n",
      "epochs 979\n",
      "training loss 0.01248045107138089\n",
      "testing loss 54.08324432373047\n",
      "epochs 980\n",
      "training loss 0.0037874694179856417\n",
      "epochs 981\n",
      "training loss 0.005266455416990952\n",
      "epochs 982\n",
      "training loss 0.0042376543832660625\n",
      "epochs 983\n",
      "training loss 0.006391917143694379\n",
      "epochs 984\n",
      "training loss 0.008763219869102943\n",
      "epochs 985\n",
      "training loss 0.010267608790573748\n",
      "epochs 986\n",
      "training loss 0.00732445963235064\n",
      "epochs 987\n",
      "training loss 0.003897689911536872\n",
      "epochs 988\n",
      "training loss 0.003427848961401844\n",
      "epochs 989\n",
      "training loss 0.0035991335442910593\n",
      "testing loss 55.832481384277344\n",
      "epochs 990\n",
      "training loss 0.006034003289162435\n",
      "epochs 991\n",
      "training loss 0.013350323300498228\n",
      "epochs 992\n",
      "training loss 0.005803536366191552\n",
      "epochs 993\n",
      "training loss 0.008706672294206465\n",
      "epochs 994\n",
      "training loss 0.007618331624138536\n",
      "epochs 995\n",
      "training loss 0.004375310233709487\n",
      "epochs 996\n",
      "training loss 0.008166880238180359\n",
      "epochs 997\n",
      "training loss 0.008401318688199601\n",
      "epochs 998\n",
      "training loss 0.015903742665029835\n",
      "epochs 999\n",
      "training loss 0.004278719057611218\n",
      "testing loss 41.386295318603516\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "for t in range(1000):\n",
    "    print('epochs', t)\n",
    "    model, train_loss = train_func(model, train_loader)\n",
    "    if (t+1) % 10 == 0:\n",
    "        eval_loss = eval_func(model, eval_loader)\n",
    "        eval_losses.append(eval_loss)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8222768b70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8222aec048>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmcZFV5+P/Pqa2runrft2HWno1hWBwGGAggMAFEAyQacQv6NS+MSxSjP8WvJvklXzUaEzUmQr6oIDFGJICiSECWQWQbZthm34eZ6el1uqeXquraz/ePe291dXd1d3VXL1V3nvfrxau7bt2qujXVPP30c55zjtJaI4QQwr4cC30BQggh5pYEeiGEsDkJ9EIIYXMS6IUQwuYk0AshhM1JoBdCCJuTQC+EEDYngV4IIWxOAr0QQtica6EvAKCmpkYvWbJkoS9DCCEKyquvvnpKa1071Xl5EeiXLFnC9u3bF/oyhBCioCiljmVz3pSlG6XUPUqpbqXUrrRj31JK7VNK7VBK/UIpVZF235eUUoeUUvuVUtfO7PKFEELMlmxq9D8Grhtz7ElgndZ6PXAA+BKAUmotcAtwtvmYO5VSzlm7WiGEENM2ZaDXWj8H9I059lutddy8+TLQYn5/I3C/1jqitT4KHAI2zuL1CiGEmKbZ6Lr5X8D/mN83AyfS7mszjwkhhFggOQV6pdSXgTjwU+tQhtMyLnivlLpNKbVdKbW9p6cnl8sQQggxiRkHeqXUrcA7gQ/okd1L2oBFaae1AO2ZHq+1vltrvUFrvaG2dsruICGEEDM0o0CvlLoO+CLwR1rrUNpdvwJuUUoVKaWWAq3AK7lfphBCiJnKpr3yZ8BLwCqlVJtS6qPAvwGlwJNKqTeUUv8OoLXeDTwA7AEeBz6ptU7M2dULIUSeiCeSPLDtBIlk/m3POuWEKa31+zIc/tEk538N+FouFyWEEIXmpSO9fOGhHSyt9XPhkqqFvpxRZK0bIYSYBUNho+M8GIlPceb8k0AvhBCzwArw4Vj+Vasl0AshxCwIRY0APyyBXggh7CkYNTL64Whyga9kPAn0QggxC0IRyeiFEMLWrIxeavRCCGFTVkYvgV4IIWxqpEYvgV4IIWxJum6EEMLmrD56CfRCCGFTVkYvNXohhLApqdELIYTNSR+9EELY3EgfvcyMFUII29FaS41eCCHsLBJPpjYckdKNEELYUChtAFYGY4UQwoasHvqSIpdk9EIIYUdWRl9d4pEavRBC2JHVcVPt9xBLaOKJ/Oq8kUAvhBA5snroq/xFAITjEuiFEMJWrIy+psQD5N+ArAR6IYTIUcgq3ZiBPt/q9BLohRAiR0GzdFNtlm7yrfNGAr0QQuRobEZfcKUbpdQ9SqlupdSutGNVSqknlVIHza+V5nGllPqeUuqQUmqHUuqCubx4IYTIB8HUYKwZ6Aswo/8xcN2YY3cAT2utW4GnzdsA1wOt5n+3AXfNzmUKIUT+CkXj+NxO/EUuoABr9Frr54C+MYdvBO4zv78PuCnt+H9ow8tAhVKqcbYuVggh8lEwmsBf5MTrcgIFGOgnUK+17gAwv9aZx5uBE2nntZnHhBDCtkKROMUeFz6PEegLsXQzHSrDMZ3xRKVuU0ptV0pt7+npmeXLEEKI+ROMJij2OPG5zUAftceEqS6rJGN+7TaPtwGL0s5rAdozPYHW+m6t9Qat9Yba2toZXoYQQiy8UDSOv8g1EuhtktH/CrjV/P5W4JG0439mdt9cDAxYJR4hhLCrYMTI6L0eI6TmW43eNdUJSqmfAVcCNUqpNuBvgW8ADyilPgocB95jnv4Y8A7gEBACPjIH1yyEEHklFI3TUObF43TgUPnXRz9loNdav2+Cu67OcK4GPpnrRQkhRCEJRhIUFzlRSuF1O/Muo5eZsUIIkaNQNI7fY+TNPrfTNjV6IYQQpmDUyOgBvBLohRDCXmKJJNF4ciSj90jpRgghbMXaRrDYnCzlczvzbjBWAr0QQuTAWrnSWudGavRCCGEz1sqVVkbv9TgZjtljZqwQQgjSMnqzRu91OYhIRi+EEPaRyujNrhufR0o3QghhK2MzehmMFUIImwmaQd0vffRCCGFPoYiR0RdLH70QQthTKqNPK93EEppYIn86byTQCyFEDobNGr0vbcIU5NdSxRLohRAiB8FoAo/TgcdlhFOv21qTXjJ6IYSwhVAknmqtBGMwFiSjF0II2whGE6n6PJCXG4RLoBdCiByEovHU8gdA2gbhEuiFEMIWjN2l0jL6PNwgXAK9EELkwNhdKq1GL6UbIYSwl2AkkZosBeB1mYOxUroRQgh7CEXjqeUPYGQwNhyXQC+EELYQjI7O6EcGY6WPXgghbCEUGV2jl8FYIYSwkWRSE4olRrVXej3WzFgJ9EIIUfDC8QRaM6q90uN04FA26qNXSn1WKbVbKbVLKfUzpZRXKbVUKbVVKXVQKfVzpZRnti5WCCHyibW7VHrpRimVdxuEzzjQK6WagU8DG7TW6wAncAvwTeA7WutW4DTw0dm4UCGEyDfW7lLpg7GQf5uP5Fq6cQE+pZQLKAY6gKuAB8377wNuyvE1hBAiL6Uy+rT2SjACvS1q9Frrk8A/AccxAvwA8CrQr7WOm6e1Ac2ZHq+Uuk0ptV0ptb2np2emlyGEEAtmoow+33aZyqV0UwncCCwFmgA/cH2GU3Wmx2ut79Zab9Bab6itrZ3pZQghxIIZu1+sJd82CM+ldHMNcFRr3aO1jgEPA5uACrOUA9ACtOd4jUIIkZes/WJ97jEZvY1q9MeBi5VSxUopBVwN7AG2AO82z7kVeCS3SxRCiPw0ZAb6Uu+YwViPk2E77DCltd6KMej6GrDTfK67gS8Cf6WUOgRUAz+ahesUQoi8MxSeINC7HHm1qJlr6lMmprX+W+Bvxxw+AmzM5XmFEKIQBMxA7y8aPxhrl9KNEEKc0YbCMXxuJ27n6FDqs0t7pRBCnOkCkTgl3vGFEbtNmBJCiDPWUCQ+rj4PNuqjF0KIM91QOE5pUYZA73YSS2hiifzovJFAL4QQMxQIxyj1uscdt9akz5esXgK9EELM0FA4TkmGjN7rNkJrvtTpJdALIcQMBSao0XutjD5PthOUQC+EEDM0FM7cdZNvG4RLoBdCiBlIJrWZ0U9co8+Xhc0k0AshxAwEzCWKJ+q6AanRCyFEQQtMsM4NGIuagQR6IYQoaNaCZplq9GVmOac/FJ3Xa5qIBHohhJiBQCQGkLFG31juBaC9Pzyv1zQRCfRCCDEDg1ZGn6FG7y9yUe5z0zEwPN+XlZEEeiGEmAGrRl+WoXQD0FThk4xeCCGma0/7IFpn3IZ63k1WowdoKvfS3i8ZvRBCZG3XyQHe8b3f89rx0wt9KcBIjT5T6QasjF4CvRBCZK17yCiDdA5EFvhKDEPhOEqB35M50DdWeBkMxwmY+8ouJAn0QoiCEIgYPemD4VjWj+kZiszZCpJD4TglHhcOh8p4f3OFD4COPMjqJdALIQpC0MyMB4azD/Q33/kCdz17eE6uZyiceUEzS2O5EejbBxZ+QFYCvRCiIFhdLoPTCPSdA2E65yjQBiKxCQdiYaSXXjJ6IYTIklXrzrZ0E40niSc1oTks3WSaLGVpKPeiFHkxICuBXghREFKBfji7wU1r5cjQHA2GBiKZNx2xuJ0O6kqLpHQjhBDZCk4zow/FjPNDc7RUcGCKGj3kT4tlToFeKVWhlHpQKbVPKbVXKXWJUqpKKfWkUuqg+bVyti5WCHHmGopMr0ZvBfhQdG4y+sFsAn25jw4bZPT/AjyutV4NnAvsBe4AntZatwJPm7eFECInIxn99Eo3wbnK6COZNwZP11RhzI5d6Nm8Mw70Sqky4HLgRwBa66jWuh+4EbjPPO0+4KZcL1IIIabbdWNl9HOxy1MskSQcS05aowejxTIST9IXXNjlinPJ6JcBPcC9SqnXlVI/VEr5gXqtdQeA+bVuFq5TCHGGC0yzj94q2QTnoHQz2aYj6ZqsSVMLXL7JJdC7gAuAu7TW5wNBplGmUUrdppTarpTa3tPTk8NlCCHOBFagj8STWc12TXXdzEFGPzTJEsXpmiqMXvqTCzwgm0ugbwPatNZbzdsPYgT+LqVUI4D5tTvTg7XWd2utN2itN9TW1uZwGUKIM0EwEsdpLjcwlEWd3grw0XiSWCI5q9cyNMmmI+ma8mQZhBkHeq11J3BCKbXKPHQ1sAf4FXCreexW4JGcrlAIITAy+oYyI0POpsUyfaLUbGf1Q1mWbqr9Hjwux4L30ufadfOXwE+VUjuA84CvA98ANiulDgKbzdtz4pl9XVz+j1s41hucq5cQQuSBSDxBLKFTpZBsBmSH02rzuQ7Iaq1H/VWQbY1eKUVjHqxLn1Og11q/YZZf1mutb9Jan9Za92qtr9Zat5pf+2brYseKxjXH+0J5sQyoEGLuWIHVKoWMbbF8YnfnuCw/PYvPdUD259tOcOk3nkkF+6Ep1qJP11S+8JOmCnpmrNdtXH44Nrv1NyFEfgmaSxRbK0KmZ/Rdg2E+9pNXeeT1k6Mek57F55rRH+gK0D0USVUPAlPsLpWuscJb0F03C87rdgIQmaNFi4QQ+cHKoFOlm7Ts3QqiY9suR2X0Of7V3x8y+uAPdgXM17f2i518MBaMdem7BsPEE0k6B8Lces8rvHykN6frmS5bBPpwXAK9EHZmZfRNZkafHtR7howdp6yNSSzpgT7Xwdh+8/UOmIE+EInjciiKXFOH0MZyH0kNrx47zXv+74v87kAPW/ZlbEacM1P/3ZHHpHQjxJnB2p+1usSDx+kYtYKltcWgdY5lOGZs9ad17oH+tJXRdw8BMBSOUep1oVTm3aXSWX+FfPjebXhcDqr8HtpOz2/NvrAzepeZ0UvpRghbs7L1Uq+LMp9rVOmme9DI6IMZMvrKYo9xX46Dsf0h4/UOdZsZfTieVX0eRgaQiz1O7r/tYs5uKqPtdCin65muAs/orUAvGb0QdmYNfvqLXJT53KMGY3sCRqAfO4kqFE1Q7ffQF4zmPBhrZfRHeoLEE0lj05GiqevzACtqS7j9mlbedW4Ty2tLaKks5on2zpyuZ7oKO6NPlW4koxfCzqzB1JIiF2Ve96j2SiujH1e6iSaoKSkyHp9DRp9MagaGYzRX+IgmkhzvCzEUyT6jdzgUt1+zkuW1JQC0VProC0ZzHiCejgIP9DIYK8SZwJor4/dMnNGPL93EqfS7cSgIRWYeIwbDMbSGDUuMrTUOdAUYCscpyzLQj9VSaZRy5nP9m4IO9NaIt5RuhLC3QCSO3+PE4VCUeUfX6HsGw6lz0g1HE/jcLvweV06DsafN+vyGJVUAHOoeMjYGz2KyVCaLqooBONE3f3X6gg70ShntTdJHL4S9BSNx/GZgNTJ6I6hrrVMZ/dhAH4olKPY48XmcOe0yZfXQt1T4aK7wcbA7YG4jmF2Nfiwro5/PzpuCDvRglG+kRi+EvaXXxMu8RulGa01/KEYsofE4HakBW0soagR6f1FuGb3VcVNR7Ka1viRVusm2Rj9WbUkRRS7HvHbe2CDQO6R0I4TNBSPxVKmkzOcimkgSiSfpNidLLa4uZjiWIJE0tuxLJDXReBKfx4nPnVtGb3XcVBR7aK0r4WDXEPGknnJBs4kopWip9HGiTzL6rHndThmMFcLmAuG0QG+WTAaHY6nJUktr/MZ5Zvlm2Pwr38joneMGaqfDyugri9201pUSN3+ZlM6wRg/QUllMW79k9FnzuqR0I4TdBdJq9OU+M9CHY6nlD5bWjg70Vgbv87go9rhGrU0/Xf2hKEoZv2Ba60tSx2daowdYVOWTGv10SOlGCPsLROKpDLrMDPQDw/FU6WZ5jRGArd50a4JUsdtJscdJaMxAbTiWyDpBPB2KUe5z43AoVtSNBPqZdt2AkdH3h2IMZbGBymwo+EBfJIOxQtjeqK4bszZuZfTFHid1ZcbEKGt2rDX4WuxxGhn9mMHYv/jPV7njoR1ZvfbpUDS1lEKp101judf8PpdAP7+dNwUf6I0avWT0QthZIL3rxpdeo49QW1qUCrrByOhA7/OYGf2YwdjDPQH2dQ5l9doDwzEqikfKNFZWP9OuG4BFlUYvvQT6LHmlj14IW7O2ERw3GBuO0z0Ypq60KJXtB8aWbjwuioucBMdk9KeDMToHs9sMJD2jB1hZXwqQ9Vo3mVgZ/XxNmir8QC+lGyFsLbWbkxnMrex9cDhGT8DI6EvGBHorgy/2OCl2u4jGk8TNbQAj8QSBSJz+UCyrxc5OB2NU+EaC+mWtNSyq8lFbWjTj91Tl9+BzOyWjz5YMxgphb1ZrpJW1e91OilwOI9APRqgr9Y4E+vDo9kqf2V4JpDpvrHZJIKus3ijdjGT0b19Vx++/cBU+j3PG70kpZXbeSEafFemjF8LeMm3EXeZz0zUYZigSpzZD6WbsYCyMlHP6gtHU83ROsZdrNJ4kEIlTWTzzMs1EWiqLOSEZfXakdCOEvVkZfXqgL/e5OdxjbNRdW1qE2+mgyOUYNxhb7HZRbGbe1n2jAv3g5IG2f9icFev3THreTLRUSkafNa/LKN1orRf6UoQQc8BaZ94qwYDRYnm4x9jtqc6slZd6XQylBmOtCVPOVKAPZcjoO6bI6FPr3PhmP6NfVFnMUDg+blPzuVDwgb7IXJM+Ii2WQthS+jaCljKfOxW460qNvvaSIteojN7lUHhcjlTpxjrfWrvGoaYu3YwsfzA3GT3MT+dNwQd6a/ORiAzICmFL6dsIWsrSlh+wul/8Ra7UuaFoIjVYWmz+JWDtMmVl9Etq/FMG+pEFzeYgo6+av176nAO9UsqplHpdKfWoeXupUmqrUuqgUurnSqnZ/1WYJrWdoAzICmFL6dsIWsp8xvdOh6LKrJ+XFLlG9dFbJRt/hsHYcp+bRZXFU3bd9M9hoB+ZHVsYGf1ngL1pt78JfEdr3QqcBj46C68xIa/L2iBcAr0QdjSUto2gxcroq/0enA4FjA70xqYjxvmZBmOr/B4ay71Z1+jnonRT7nNTWuTiVCA69ck5yinQK6VagBuAH5q3FXAV8KB5yn3ATbm8xlRS+8ZK6UYIWwqmbSNosZZBsNa4AWNJgmDaYKzPjA1jB2ONma5u6su8nApEiCVGYscTuzu569nDqdunQzE8TkfqOWaTUoptX7mGO65fPevPPVauGf13gS8A1r9UNdCvtbYWlmgDmnN8jUmlSjeS0QthS+kLmlmspYqtgVgwa/Rpg7FWcB47GNsXjFHlL6Kx3IvWpFbABLjn+aN856kDRM3mjv5QlIpiN0YOO/usRHWuzTjQK6XeCXRrrV9NP5zh1Ix9j0qp25RS25VS23t6emZ6GWkZvQR6IewofRtBi1W6qS0ZyehLi1yjVq+0BmO9bgdKjSyL0BeMUOV302CuQtk5YAyGJpOaPe2DRONJdrcPAEb2Pxf1+fmWS0Z/KfBHSqm3gPsxSjbfBSqUUtan0gK0Z3qw1vpurfUGrfWG2traGV/EyGCslG6EsKP0bQQt1mBseunGX+QiEk8SSyRHDcYqpfCbSxVrrTkdjFHp99BYbgyGWnX6E6dDqfGAV4+dBowafcUc1Ofn24wDvdb6S1rrFq31EuAW4Bmt9QeALcC7zdNuBR7J+SonUSSDsULYyqHuAJ+5//XU/9Pp2whaUhl92sJi1jnBSJxQLJ4q2YAxcSoUjROMJogmklQVe2goszJ6I9DvOjkIgMuheP14P2AE+rlY/mC+zUUf/ReBv1JKHcKo2f9oDl4jRUo3QtjLln3dPPJGO8/uN0q6gQw1+iXVfpbV+LngrMrUMau8E4jEGU4r3QD4PU5C0QSnzR76Kr+HMp8Ln9uZCvS72wdwORTXrKln+7E+I/sPRanwFX5GP/OV89NorZ8FnjW/PwJsnI3nzYZVupEJU0LYQ7tZM//tnk6uW9cwahtBS3mxm2c+f+WoY+lLFYeiCYrTBjp9HhfBSILetECvlDJaLAetQD9Ia30pFy+r4vHdnbQPhOkfjlHhl4x+waUyepkwJYQtdPQbgfeZfd3EE8mMXTeZpC9VPBzLlNHHUxl9pTnJqqHcS+dAGK01u9sHOLupjLctrgLghYOniMaTc9JDP9/sE+ildCOELXQMDONxOegPxdh+7PSobQQnY/0yOBWIoDWjAn1xkTEYay1/UGUG74YyI9B3D0U4FYiyrqmM1Y2l+NxOntrbBSA1+nzgdVl99FK6EcIO2gfC/OHaejwuB4/uaB+1jeBkrEXPesy++PTSTbHbzOjNJQ2qSkYy+q7BMDvajHbKs5vLcTsdrG8p5/lDpwAolxr9wnM5HbgcSjJ6IWwgGk9yKhBhRV0JwUicR3d0AGQV6K2M3poAld51U1xkDMb2BqO4HCpV828s9xJPap470INSsKaxDIC3La5k69E+QDL6vGFsPiIZvRCFrmswjNbQVO7jD89uSK01M50avZXRjyrdpHXdVJoDsQANZi/903u7WFrtTz1HejdP5RxsOjLfbBLoHTIYK0QBSCb1pJsEtfcbHTeNFV6uXlOHtfJANhl9ybiMPn0w1lgHpy8YpTotcFu99O0DYdY2laWOX7B4JNDPxaYj880Wgb7IJdsJClEIvvvUAS775hb2dw5lvN+apdpY7qOu1Mv5iyqA7AK906HwuZ0ZM3qfx0nELAuld9FYyyAArGsuT31f5fewtMYPcGbPjM0nXrdD+uiFKACP7uzgZP8w7737Jd440T/ufquHvqnCCMCb1zYAo7cRnEyJ10X3kPHLIr1Gby1xfLJ/OLV+PRjLHLudxp8NZ6dl9AAbFldS7nPjcRV+mCz8d4BsEC5EIegeDHOkJ8gHLz6LUq+LD/zgZV463DvqnI7+MOU+dypIv2/jIj719hWjsu3JlKSt7148qr3S+L57KEJl2gQoh0NRb5Zvzm4a/Rr/33WruOfDF07zXeYn+wR6qdELkddeOmIE9T/dsIgH/2ITTRU+PvaT7cTT1oPvGBimMa2cUlHs4fPXrsLtzC5UlRS5SCSNMQCfe/RgLIDWUOUvGvWYhjIvTeXeUZk+GEsgvy2tVl/IbBLoHdJ1I0See/lIH6VFLtY2llFf5uUTb1/OYDjO4Z5g6pz2/jBNFb4Zv0Z6iWdURp9Wxqka0y75sSuW84Xr5n7zj4VU8H30YGwnaLVhCSHy08tHetm4tAqXmZ2vM0slO08OsKqhFDAy+vPPqpjxa5QUjQTxUX30aUF/bLvk5rX1M369QmGTjF5q9ELks86BMEdPBbl4WXXq2LLaEnxuJ7tOGrNSh6MJTodiOWX01uxYpUYWPIQxGb0N+uKnyxaBvkhKN0LktZfN+vwly0cCvdOhWNtUlgr0HWbHTXqNfrqs0o3P7Ry1/V96SUcCfYHyup1EZDBWiLz18pFeyryu1BIDlnOay9nTMUgiqUf10M+UVboZu5l3sVsy+oLndckSCELks5eO9LJxaTVOx+htpdc1lxOKJjh6KpCaFWv10M9EiZXRjw30aRm9HZYdni57BHq3Q2r0QuSp9v5hjvWGRpVtLOuajQx/18nBVEbfkEPpxppBm57Bw0iG7/c4U0ubn0ns0XXjdhJPauKJZGpEXwiRH6z6/MXLqsbdt6K2hCKXg50nBwhF49SUeFL7QM+EtfjZ2Ize63KilD0WKJsJW0RFa3Q9HJfyjSh83YNhW/2FuvVIH+U+N2saysbd53I6WNNoDMi294dzqs/DSNfN2Bq9w6EodjvPyPo82CbQyy5Twh5iiSTX/cvv+epv9szo8Vrr1MzQfHG8L0RrXQmOMfV5yznN5exuH6S9fzinsg1MPBgLxr6xZ2J9HuwS6F0S6IU97GgboC8Y5eHXThKIxKf9+Lt+d5jrvvvcHFzZzJ0KRKgpKZrw/nXNZQQicQ71BGjKMdCn2is946vSLZU+lteW5PT8hcoWgb7ILdsJCnuw6tmhaIJfv9k+7cfv6xjiYHcgtQl2PugJRKgpnTiTthYs0xoac5gsBWmlmwwDrj/984v44vWrcnr+QmWLQC+lG2EXLx4+xeqGUlbWl3D/K8en/Xhr8+tDPYHZvrQZiSWS9Idik2b0rXWleMwmilwmS8HEg7HWfbkM9BYyWwV6mTQlClk4lmD7W6e5dEUN79t4Fm+2DbCnfXBaz9FrBvqDXfkR6HvNJYMnC/Qel4PVjcZaN7ksfwBp7ZUZAv2ZzB6B3iWlG1H4Xj/eTySeZNPyam4+vxmPy8H926aX1fcFjd2VDnXnR6A/FTCuZ7JADyNrweec0XtcLK3xs7K+NKfnsZsZB3ql1CKl1Bal1F6l1G6l1GfM41VKqSeVUgfNr3O+oLOUboQdvHT4FE6HYuPSKiqKPbxjXQO/eP0kw9Hsfq611qnSzcHuzFv1zbceM9DXlk4e6N91biNXra5L7eE6Uw6HYsvnr+Sm85tzeh67ySWjjwOf01qvAS4GPqmUWgvcATyttW4FnjZvz6mRQC8ZvShcLx7u5Zzmckq9RovgLRvPYigc5zc7O7J6/FAkTixhtFYezpeM3ty/tXaKjH7T8hru+fCFMuFxjsz4X1Vr3aG1fs38fgjYCzQDNwL3mafdB9yU60VOJTVhSjJ6UaCCkThvnOgftUzARUuraCr38uz+7qyeo8+shy+v9dM+EJ6wPVNrPW9dOda2fpN13Yi5Nyu/PpVSS4Dzga1Avda6A4xfBkDdBI+5TSm1XSm1vaenJ6fXT2X0MhgrCtS2t/qIJzWb0gK9Uoqltf7UYl9TsQZiNy41nmOirH7L/m42fv0p2k6HcrzqqZ0KRCj2OEetBy/mX86BXilVAjwE3K61zrpFQGt9t9Z6g9Z6Q21tbU7XMDJhSko3ojC9dLgXt1OxYfHo9WCayn2094ezeg6rPm+tKXNwgkD/+vF+YgnNjraBcfclZ3lW7VSTpcT8yCnQK6XcGEH+p1rrh83DXUqpRvP+RiC7vztzUCSlG1HgXjzcy/lnVY7r/26q8NE1FCaWmDqJsTpuzltUgdupJuy8sVov93WMzsuO9QZZ8zeP8+qx0zN5CxkZgV7KNgstl64bBfwI2Ku1/nbaXb8CbjW/vxV4ZOaXl50ilwOlICKBXhTzB7IGAAAYMElEQVSgQCTOrvaBUdvsWZorfGhtbMU3Fat0U1fqZWmNn0MTdN5YHTl7O0ff//KRXiLxJE/v7ZruW5jQqaGoZPR5IJeM/lLgQ8BVSqk3zP/eAXwD2KyUOghsNm/PKaUURS6HrF4pCtLejkG0hvMWlY+7z5pAlE2dvi8Qxed24vM4aa0rzZjRR+NJjvUatfl9naMzequUs/Vo37Tfw0SM5Q8k0C+0GY+QaK2fBzIvRwdXz/R5Z0o2CBeFytozdV1TpkBv9JW3D2QR6IPR1DK8y+tK+J9dHYRjiVEbbRzrDRJPapbX+jncEyQQiadmk1qBfkdbP8PRRMZlBKYjnkhyOiQZfT6wTdOqsZ2gBHpReHadHKSmpIi6DJOFRjL6qUs3faGRQN9aV0JSw9FTwVHnWFn+u85tAmC/Wb6JxBPs6xxkdUMpsYTm9eO51+n7glG0hlqp0S84+wR6t0O6bkRB2t0+kNpSbyyv20m138PJbEo3aRn9ijpjOd6xnTfW7RvOaQRGyjf7O4eIJTQfuXQJDjU75ZtsZ8WKuWejQC8ZvSg84ViCg92BjGUbS1OFL6safW8gSrUZ6JfW+HGo8WveHOwO0FLpY0VdCaVFLvZ1GBm9VbbZtLyGtU1lbD3aO9O3lHIqiwXNxPywTaAvcjtlMFYUnH2dQySSesKMHow6fVaDsWkZvdft5Kyq4nGdN4e6A7TWlaCUYnVjaSqj39HWT2Wxm5ZKHxctrTYXWMstcbKWP5BAv/BsE+i9Lodk9KLg7G43Mumzp8joT54eRuuJJzMNRxMMxxJUpdXDV4zpvEkkNYd7AqmyzqqGUvZ1DqG1MXnqnJYKlDIWVYvEkxknVE1HauVKKd0sOPsEerdT+uhFwdl1cpByn5FJT6S5wkcwmmAwPPHWgr3mZKnqtM2vV9aXcPRUkKFwDIC20yGi8SStdcYSvqsbyhgKxzncE+Rgd4BzW4xfNhuXGDNrX8mxTn8qEMHrduCXteEXnI0CvQzG5uJA1xA3fO/3qWn0Yn7sbh/g7KYyjPmHmWXTS299blX+kez5D89uIJbQ/PpNY/VLa0bsinojo19jbvbxi9fbSCQ155hb+lX6PayqL01taziZf3vmIFv2ZZ78fipgtFZO9t7E/LBRoHfKomY5eO5AD7vbB3ltFqe/i8nFEkn2dQyl9kydSDaBvjcV6Ecy+nNbylndUJravMTaXtAq3Vibczz4ahsA61sqUo/duLSKV4+dJj7J0gu9gQj//OQBvvTwzoxlU1nnJn/YJ9BLH31OrFru/q782LDiTHCwK0A0keTspokHYiFt0tRkGb3Z4ZJeulFK8d4LF7GjbYDd7QMc7ApQX1ZEmbnefanXzaIqH12DEWpLi6gvGwnKFy2rIhRN8GZb/4Sv+fyhU8byDINhfr7txLj7e4Yk0OcL+wR6Kd3kxOqvPiCBft7sMgdip8roa/xFeJwOTk4yaSpVuhkzOcnakvCBbSc41D2UyuYtqxuMXzLntpSPKrFcsqya0iIXH/vJaxPW6p/d30OV38OFSyq589lD4xKtU4EotbIOfV6wUaCXjH6mtNYjGX3n/Af6u549zLv+9fl5f92Ftqd9EL/HydJq/6TnORyKxilaLHuDUdxORWnR6FVNKoo9XHe2sSWh0Vo5ei/VNQ3G7XOaK0Ydry4p4qFPbKLU6+L9P3iZe184OqrrJ5nUPHegh8tba/js5pV0DUa4/5WR/W0TSU1fMDLlzlJiftgm0Be5nUTiyUlb0ERmpwJRBoZjlHpdHOkJZrUk7mz6/cEedp4coHswu3XX7WLXyQHWNpXhcEw9WNlYPnmg7wtGqPJ7Mg583nLhIgbDcYLRxLiMfk2jkdGvbxn/V8XK+lIe+dSlXLmqjr/79R7ue/Gt1H07Tw7QG4xy5ao6Ni2v4aKlVdz57OFUstUXjJLU0lqZL2wT6K3tBCMyaWrarGVrN6+tJ5pIcqw3OMUjZo/Wmr3muug7T+bWt53vjvQEuOQfnmbzt3/Hx36ynV3tA5P2z6ebanasMVkqc1C9eFk1i6uLAcYF+mvW1vOtd6/n8pWZN/8p87q5+0Nv45Jl1fzblkOpjcqf3d+DUvAHrTUA3H7NSrqHIvzXViOrT/XQS0afF+wT6FO7TEn5ZrqsLefeud5Y/2R/5/xtLN0zFOF0yOjzznWCznzTWpPIckcmrTV/88huAuE4S2r8HOoOoFBcvrImq8c3V/joHAxP2AXTG4yOGohN53AoPnDRWXicDlbVjy7duJ0O3rNhEc5J/qpwOBSf3bySU4EoP916DIBnD3SzvrmcajOQX7K8mkuWVXPns4cIRuIS6POMfQK9W7YTnKmD3QFKilxsWl6DQ81v5421+YXToQouo79/2wk2fu0pBoZjU577m50dPH/oFJ+/dhU/+LMNPP25K9nz99dy1er6rF6rqcJHUkOXuazAWOnLH2Ty55ct45nPX0HlJOdMZuPSKi5dUc2//+4w7f3DvHGinytWjd4O+gvXreJUIMo9zx9NC/QyGJsPbBToZTvBmTrUHWB5XQlet5MlNX4OzOOArLWd3dtX1bKjbaCgxlju33aC3mCUX7zWNul5gUic//PoHs5uKuODFy9OHZ/ORKKpeun7ApMHeodD0VJZnPXrZXL7NUZW/6n/eg2t4cpVo8s9559VybVn13P3c0dSk7OkRp8fbBTozYxeJk1Nm7XQFcCq+tJ5bbHc2zFIU7mXy1bUcCoQoWswc8a6kKLx5LgE4kRfiDdP9KMU/HTr8Ul/QX33yQN0D0X46k3rJi2RTKZ5kl76SDzBUCQ+Yelmtly4pIrLVtTw2nFjAbRzWyrGnfP5P1xFMBrnnheO4nE5xnUBiYVho0BvZfRSupmOgeEY3UORUbMl3+oNZvWX0dFTwZxXONzXOcTqxjLOMbs+dkwyQSeTcCzBvz59kLbToZyuY6zOgTD/+vRBPvjDraz/uye49rvPjXqvj+00lhX41NtXcLA7wLa3Ms8o3tc5yL0vvsUtF57F+WdVzvh6GsuNjD7TuvSng0bpaGwP/Vz4zDWtAPxBa23GX1qt9aX8yQUthGNJamX5g7xhm0BfbXYcWKsBiuxY/fMrakcCfVKPX8d8rINdQ2z+9u/49pMHZvza0XiSQ90BVjeUsraxHIeafufNr95s55+fPMAf3/kie9oHp35AFrTW3PaT7Xz7qQOcCkTYvLaBY72h1FIBYAT69S3lfPzK5ZR6XfyXOUiZLpnUfPkXuyj3ufnCtatyuiZ/kYuKYjdbj/Txo+eP8pVf7uSB7cZs1EwLms2VC5dU8fWbz+HTV6+Y8JzbN6/E43RIfT6P2CbQr28p57xFFdy55TBRabHMmrVeeWu9tXSt8XWq8s1Xf7OXeFLzs63HCUUnXlVxMod7AsSTmjWNZfg8TlbWl0470D+6o4P6siIcSvHe//sSLx46NaNrSffa8dPsaBvg729cx+O3X873bjlv1M/Wib4Qb7YNcMM5jRR7XPzx+c08trNz3IJwD2w/wavHTvO/37FmxoOg6ZZU+/ndgR7+z6N7eGBbG196eCf7O4cyLmg2l95/0VmsGDPxKl1zhY+v3byOj/7Bsnm5HjE12wR6pRR/tXklJ/uHU5mOmNqh7gAelyM1ULe42o/H6Zi08+bZ/d387kAPN5zTyGA4zi9fb5/Ra1ubXlirKK5rLmfnNAZk+4JRXjh0ipvPb+HhT2yiscLLrfe+wi13v8SXf7GTe184yukMq3EORxOTLtZ17wtvUeZ18ScXNAPGz9ZnrmnlZP8wD73Wxm/Mss07zO343n/RYqKJJA++OvJz1xuI8I3H97FxaVXqeXJ15wcu4MG/uITX/nozL//vqyn1uvjrR3bRGxi/oNlCe8+GRfyRuS+tWHi2CfRgTN542+JKvr9l/LobIrOD3QGW15ak6q1up4NltRN33sQTSb72m70sqS7mO+89jzWNZdz34lsz6pbZ2zGEx+VgibkEwPqWcnqDUdoHspsh+/iuThJJzTvXN9JU4eO/P7aJ9208i2g8yaM7Ovi7X+/hj+96kRN9I/X73x/s4ZJvPM0V33qWu587PK41smNgmP/Z1cktG8+i2DMykHjlylrObSnn+1sO8cgb7Zy7qIJFVcYvx1UNpWxYXMlPXj7Glv3ddA2G+fpj+wiE43ztpnWzVqduqvCxYUkVVX4PVX4PX7xuNa8c7ePH5ozV+SjdiMJkq0BvZfUdA6NX08umtBCOJQqqtW+2HOoOjJstuaqhlANdmWv0P9t2goPdAe64fg0el4MPb1rM/q4hXj4y/U0q9nYMsrK+BJfT+DG01kPfmeXEqd/sbGdpjT+1+mN5sZu/v3EdD3/iUt74m8088LFL6AtGufnOF9l1coC7nzvMrfe8Qn2pl0VVPr7+2D42/cPTfH/LodRn/9OXj5PUmg+ltUHCSFbfdnqYvR2DvNPM5i0fv3I57f1hPnLvNi76+tM89Fobt12+jNb6iUscuXrvhkWct6iCN07043Qoyn3uOXstUdhsFegBNi2vZuPSKr6/5RBf+eVOrvzWFtb+zRN8+Rc7SU4wi7EvGOXqf/4dH//P186oYB+KxjnZP5waiLWsrC/lZP9wamciy66TA3z7t/u5aGkV155tTPS58bxmKordo9ZByda+zqHU6olgrLvicih2npy686ZnKMJLh3t55/rGjBmztSXeQx+/hCKXgxu//wJff2wf161r4OFPbOL+2y7h0b+8jMtaa/jWE/v59P1vMBCK8V+vHOeaNfWpbD3d21fVpdaEuf6chlH3Xb2mntf+ejM/v+1i/vZda/nU21fwl1e1TvefZFocDsVXb1qHUlBZ7M5qzRxxZpqzQK+Uuk4ptV8pdUgpdcdcvU6G1+Vzm411N37x2kmW15Zw03lN/HTrcb7w0I5xU9a11nzhwR2c7B/m8d2d/OfW4xM8s/0c6Qmi9chArMWqmX/xoR3sN/cUvef5o/zxnS/icTnM4GIEFa/byXsvXMRv93RmbP2byKlAhJ6hSGpRLeu5WutLs1oK4fFdHSQ1vHP95HXgFXWlPPTxTVy6ooYvXrea77//Avxmb/e65nL+/YNv447rV/Pojnau/vbv6AtG+cimJRmfSynFN/9kPV+/+ZyMk4/KfW4uWlbNRy5dyuevXYVvHrbQW9dczqevauXtY2apCpFuTmYzKKWcwPeBzUAbsE0p9Sut9Z65eL2xLlpWzQt3XEVtSREelwOtNUtrSvjOUweIxpP885+ei9ssF/zn1uM8tbeLr9ywht8fPMVXH93DRUurUrvvZHIqEOGuZw/TORjm41csn3I9cTB2EzrYFaC1viT12gspEk/wM3NZ2bGlmytW1vGxK5bxk5eO8djOTpbV+jnSE+SaNXX847vPHTfo96GLF/OD547w/h+8zM3nN/Ouc5tYPuavhLGs5ZCtZXIt65vLeeTNk3zugTdZ1VDCuqZyLlxaNe7f7Nc7OmitK2FVw9SlkYZyL//xvzZmvE8pxV9csZzltSV85v7XWd1QyiXLqyd8rjWNZaN+OeWDz25eudCXIPKcmotShVLqEuD/11pfa97+EoDW+h8ynb9hwwa9ffv2Wb+Ose569jDffHwfzRU+PnTJYt62uJIP/nArFy+r5t4PX0hvMMr1//IcNSVF/PKTl+J0KHqGIgQicRJJYwGrJ3Z38qPnjxKJJ/F7nAyG49x8fjMfuXQJkXiS3kCESDzJ4mo/S2v8JJKan71ynJ+8dIzOwTCVxW6uW9fIDec00lpfQrXfg8tp/DLqC0bpCUSIJ4zPxKEUlX43tSVFuJwOEknN8b4Q+zuHCMcSeN1OY/PlIhd+j4tSr4vjfSGe2tvFM/u66Q/F2LC4kouWVXFuSwWN5T7qyorY0TbAHQ/v4EhPkPe8rYV/fPf6jOWP08Eo9730Fk/s7uJPN7Tw4U1LJhxYfGJ3J/e+cJStR/vQGtY2lnHDeuN9Lqnxk0xqwvEEiaTG5XDwHy+9xT/8zz5e/co1qYWxAN480c8//XY/+zuH6DbXdakodnPt2gYuX1lLMBKnczDMd546wO1Xr0xN4JkNHQPDOB2KulLvrD2nEHNJKfWq1nrDlOfNUaB/N3Cd1vrPzdsfAi7SWn8q0/nzFegBntnXxd3PHUkNHlb7PTx+++XUmmtybNnXzUd+vI1Sr4tAJE6mf54b1jfyV5tXUltaxF3PHuYeM/Bn4lCQ1HDZihrecU4jW4/28uSeLkLmcq8OBZXFHobCcaITtPw5HYqaEg/9oVhWyzB7XA4uXV5NXamXbW/1ceTU+GWHWyp9fO3mc7higuVpZ6pzIMyjO9p5bGcHrx3vT11PprkNdaVFvPLlayZ8rtPBKNve6uOxnR08uaeLYHSkk6q2tIiHP74pYy1diDPFQgf69wDXjgn0G7XWf5l2zm3AbQBnnXXW244dGz+zcC7t6xzkv7e3ce3ZDWxcWjXqvge2nWD7sT4ayn00lHkp87lwKoXDoVhW4x/XSdHeP8z2Y6ep8Lmp8nvwuBwc6w1x9FSAweE4N57XNOoxw9EELx/p5WT/MF2DYU4FopT73NSXFVFX6sXtVGiM8YPeYJTOgTAdA2EqfG5WNpSysr6UMq+L4ViCcCxBKJogEI4TiMSp8nu4ZHn1qNbA7sEwezoG6R4y6uIep4MPXDy6fXAunOwf5vFdnXQPhfG5nXjdTpxKkTCX9z23pYLLWrNbpjccS3Cga4jKYg+1pUWptY2EOJMtdKDPy9KNEELYSbaBfq5GBbcBrUqppUopD3AL8Ks5ei0hhBCTmJO/3bXWcaXUp4AnACdwj9Z691y8lhBCiMnNWZFWa/0Y8NhcPb8QQojsLHxDtxBCiDklgV4IIWxOAr0QQticBHohhLA5CfRCCGFzczJhatoXoVQPMNOpsTVA7vvHFZ4z8X2fie8Zzsz3fSa+Z5j++16stZ5yHZO8CPS5UEptz2ZmmN2cie/7THzPcGa+7zPxPcPcvW8p3QghhM1JoBdCCJuzQ6C/e6EvYIGcie/7THzPcGa+7zPxPcMcve+Cr9ELIYSYnB0yeiGEEJMo6EC/UBuQzyel1CKl1Bal1F6l1G6l1GfM41VKqSeVUgfNr5ULfa1zQSnlVEq9rpR61Ly9VCm11XzfPzeXwbYNpVSFUupBpdQ+8zO/5Ez4rJVSnzV/vncppX6mlPLa8bNWSt2jlOpWSu1KO5bx81WG75nxbYdS6oKZvm7BBvq0DcivB9YC71NKrV3Yq5oTceBzWus1wMXAJ833eQfwtNa6FXjavG1HnwH2pt3+JvAd832fBj66IFc1d/4FeFxrvRo4F+O92/qzVko1A58GNmit12EsbX4L9vysfwxcN+bYRJ/v9UCr+d9twF0zfdGCDfTARuCQ1vqI1joK3A/cuMDXNOu01h1a69fM74cw/sdvxniv95mn3QfctDBXOHeUUi3ADcAPzdsKuAp40DzFVu9bKVUGXA78CEBrHdVa93MGfNYYS6b7lFIuoBjowIaftdb6OaBvzOGJPt8bgf/QhpeBCqVU40xet5ADfTNwIu12m3nMtpRSS4Dzga1Avda6A4xfBkDdwl3ZnPku8AXA2lm8GujXWsfN23b7zJcBPcC9Zrnqh0opPzb/rLXWJ4F/Ao5jBPgB4FXs/Vmnm+jznbUYV8iBXmU4ZtsWIqVUCfAQcLvWenChr2euKaXeCXRrrV9NP5zhVDt95i7gAuAurfX5QBCblWkyMWvSNwJLgSbAj1G2GMtOn3U2Zu3nvZADfRuwKO12C9C+QNcyp5RSbowg/1Ot9cPm4S7rzzjza/dCXd8cuRT4I6XUWxhluaswMvwK8897sN9n3ga0aa23mrcfxAj8dv+srwGOaq17tNYx4GFgE/b+rNNN9PnOWowr5EB/RmxAbtalfwTs1Vp/O+2uXwG3mt/fCjwy39c2l7TWX9Jat2itl2B8ts9orT8AbAHebZ5mq/ette4ETiilVpmHrgb2YPPPGqNkc7FSqtj8ebfet20/6zEm+nx/BfyZ2X1zMTBglXimTWtdsP8B7wAOAIeBLy/09czRe7wM48+1HcAb5n/vwKhXPw0cNL9WLfS1zuG/wZXAo+b3y4BXgEPAfwNFC319s/xezwO2m5/3L4HKM+GzBv4O2AfsAn4CFNnxswZ+hjEOEcPI2D860eeLUbr5vhnfdmJ0Jc3odWVmrBBC2Fwhl26EEEJkQQK9EELYnAR6IYSwOQn0QghhcxLohRDC5iTQCyGEzUmgF0IIm5NAL4QQNvf/ALdM9b4JgoXEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(eval_losses, label='Validation loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vaildation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected 2D or 3D input (got 1D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-20aaa26f569b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-4f8f8fcee564>\u001b[0m in \u001b[0;36meval_func\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testing loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-48cf73af39a6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_input_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36m_check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise ValueError('expected 2D or 3D input (got {}D input)'\n\u001b[0;32m--> 169\u001b[0;31m                              .format(input.dim()))\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected 2D or 3D input (got 1D input)"
     ]
    }
   ],
   "source": [
    "eval_func(model, eval_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1044760e+08],\n",
       "       [1.1426060e+07],\n",
       "       [7.4501824e+07],\n",
       "       ...,\n",
       "       [7.9217030e+06],\n",
       "       [1.1804740e+07],\n",
       "       [5.2489572e+07]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = test_func(model, X_test, y_scaler)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission = pd.read_csv('./dataset-0510/submit_test.csv')\n",
    "submission['total_price'] = pred\n",
    "submission.to_csv('submission/DNN2_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size use 128 or 32 , learning rate use 0.003 which find loss will stock in 0.6\n",
    "\n",
    "Result 1 DNN 233->256->128->1, lr=0.001, batch_size=128, predict score : 13\n",
    "change: \n",
    "- replacing Standard to MinMax \n",
    "- adding DropOut 0.3 layer\n",
    "- batch size change to 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
