{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler,scale, MaxAbsScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import sklearn.metrics as metrics\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "use_gpu = True\n",
    "y_scale = True\n",
    "lr = 0.0005\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# Batch size and learning rate is hyperparameters in deep learning\n",
    "# suggest batch_size is reduced, lr is also reduced which will reduce concussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('./dataset-0510/train.csv')\n",
    "X_test = pd.read_csv('./dataset-0510/test.csv')\n",
    "\n",
    "\n",
    "columns = X.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_id', 'building_material', 'city', 'txn_dt', 'total_floor',\n",
       "       'building_type', 'building_use', 'building_complete_dt', 'parking_way',\n",
       "       'parking_area',\n",
       "       ...\n",
       "       'XIV_500', 'XIV_index_500', 'XIV_1000', 'XIV_index_1000', 'XIV_5000',\n",
       "       'XIV_index_5000', 'XIV_10000', 'XIV_index_10000', 'XIV_MIN',\n",
       "       'total_price'],\n",
       "      dtype='object', length=235)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputer, Scaler, Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/islab/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# step1. Imputation transformer for completing missing values.\n",
    "step1 = ('Imputer', Imputer())\n",
    "# step2. MinMaxScaler\n",
    "step2 = ('MinMaxScaler', MinMaxScaler())\n",
    "# step3. feature selection\n",
    "#step3 = ('FeatureSelection', SelectFromModel(RandomForestRegressor()))\n",
    "step3 = ('FeatureSelection', VarianceThreshold())\n",
    "\n",
    "pipeline = Pipeline(steps=[step1, step2, step3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['total_price']\n",
    "X = X.drop(columns=['building_id', 'total_price'], axis=1)\n",
    "X_test = X_test.drop(columns=['building_id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X sacle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 211)\n",
      "(10000, 211)\n"
     ]
    }
   ],
   "source": [
    "X = pipeline.fit_transform(X)\n",
    "print(X.shape)\n",
    "\n",
    "X_test = pipeline.transform(X_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5088052947.245064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    6.000000e+04\n",
       "mean     1.293727e+07\n",
       "std      5.522463e+07\n",
       "min      2.261495e+05\n",
       "25%      2.433114e+06\n",
       "50%      5.240482e+06\n",
       "75%      1.123932e+07\n",
       "max      5.088279e+09\n",
       "Name: total_price, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran = y.max() - y.min()\n",
    "print(ran)\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_scaler = StandardScaler()\n",
    "#y_scaler = MaxAbsScaler() # sparse\n",
    "y_scaler = MinMaxScaler(feature_range=[0, 1])\n",
    "if y_scale:\n",
    "    y = y_scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.28321267e-05],\n",
       "       [6.08347143e-04],\n",
       "       [1.83660349e-03],\n",
       "       ...,\n",
       "       [2.27773819e-03],\n",
       "       [3.50995685e-03],\n",
       "       [1.62815648e-03]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_eval = torch.from_numpy(X_eval).float().to(device)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).float().to(device)\n",
    "y_eval = torch.from_numpy(y_eval).float().to(device)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42000, 211])\n",
      "torch.Size([10000, 211])\n",
      "torch.Size([42000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data.TensorDataset(X_train, y_train)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "eval_dataset = Data.TensorDataset(X_eval, y_eval)\n",
    "eval_loader = Data.DataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # apply a uniform distribution to the weights and a bias=0\n",
    "        m.weight.data.uniform_(0.0, 1.0)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(211, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=512)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=512)\n",
    "        \n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=256)\n",
    "        \n",
    "        self.fc5 = nn.Linear(256, 128)\n",
    "        self.bn5 = nn.BatchNorm1d(num_features=128)\n",
    "        \n",
    "        self.fc6 = nn.Linear(128, 64)\n",
    "        self.bn6 = nn.BatchNorm1d(num_features=64)\n",
    "        \n",
    "        self.fc7 = nn.Linear(64, 32)\n",
    "        self.bn7 = nn.BatchNorm1d(num_features=32)\n",
    "        \n",
    "        self.fc8 = nn.Linear(32, 1)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(0)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = F.relu(self.bn5(self.fc5(x)))\n",
    "        x = F.relu(self.bn6(self.fc6(x)))\n",
    "        x = F.relu(self.bn7(self.fc7(x)))\n",
    "        x = self.fc8(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN().to(device)\n",
    "model.apply(weights_init_uniform)\n",
    "criterion = nn.MSELoss()\n",
    "#optim = optim.Adam(model.parameters(), lr= lr)\n",
    "optim = optim.SGD(model.parameters(), lr= lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_func(model, loader):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        optim.zero_grad()\n",
    "        pred = model(batch_x)\n",
    "        loss = torch.sqrt(criterion(pred, batch_y))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    print('training loss', np.array(train_loss).mean())\n",
    "    return np.array(train_loss).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_func(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = []\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            pred = model(batch_x)\n",
    "            loss = torch.sqrt(criterion(pred, batch_y))\n",
    "            \n",
    "            eval_loss.append(loss.item())\n",
    "        print('testing loss', np.array(eval_loss).mean())\n",
    "    return np.array(eval_loss).mean()\n",
    "\n",
    "def test_func(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X)\n",
    "        \n",
    "        pred = pred.cpu().numpy()\n",
    "        if y_scale:\n",
    "            pred = y_scaler.inverse_transform(pred)            \n",
    "    return pred\n",
    "\n",
    "\n",
    "def accuracy(model, pct_close=0.5):\n",
    "    #pred, y_eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_eval)\n",
    "        \n",
    "    n_correct = torch.sum((torch.abs(pred - y_eval) < torch.abs(pct_close * y_eval)))\n",
    "    result = (n_correct.item()/len(y_eval))  # scalar\n",
    "    return result \n",
    "\n",
    "def plot(label, pred):\n",
    "    plt.plot(label, label='actual')\n",
    "    plt.plot(pred, label='pred')\n",
    "    plt.legend(frameon=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 0\n",
      "training loss 7.757733353727857\n",
      "epochs 1\n",
      "training loss 4.869616896910508\n",
      "epochs 2\n",
      "training loss 2.656036224771053\n",
      "epochs 3\n",
      "training loss 0.987369618154949\n",
      "epochs 4\n",
      "training loss 0.1594524697420445\n",
      "epochs 5\n",
      "training loss 0.08364216624376984\n",
      "epochs 6\n",
      "training loss 0.06658315844267697\n",
      "epochs 7\n",
      "training loss 0.05595757138538868\n",
      "epochs 8\n",
      "training loss 0.0434831399357337\n",
      "epochs 9\n",
      "training loss 0.03101975721587345\n",
      "testing loss 0.027255225761509533\n",
      "epochs 10\n",
      "training loss 0.021338753427710033\n",
      "epochs 11\n",
      "training loss 0.013311693132755922\n",
      "epochs 12\n",
      "training loss 0.009517531914177908\n",
      "epochs 13\n",
      "training loss 0.008789948289676096\n",
      "epochs 14\n",
      "training loss 0.008574305208029062\n",
      "epochs 15\n",
      "training loss 0.008201966389141817\n",
      "epochs 16\n",
      "training loss 0.007970483688422476\n",
      "epochs 17\n",
      "training loss 0.0078529944122517\n",
      "epochs 18\n",
      "training loss 0.007594428021647602\n",
      "epochs 19\n",
      "training loss 0.007398072979766819\n",
      "testing loss 0.008392489594506456\n",
      "epochs 20\n",
      "training loss 0.0072353095390146034\n",
      "epochs 21\n",
      "training loss 0.007126530127792328\n",
      "epochs 22\n",
      "training loss 0.007014908630041388\n",
      "epochs 23\n",
      "training loss 0.006947178709940662\n",
      "epochs 24\n",
      "training loss 0.006809184401675402\n",
      "epochs 25\n",
      "training loss 0.006785075896062908\n",
      "epochs 26\n",
      "training loss 0.006767140586413216\n",
      "epochs 27\n",
      "training loss 0.00667527327943865\n",
      "epochs 28\n",
      "training loss 0.006613500408140382\n",
      "epochs 29\n",
      "training loss 0.006689302653348849\n",
      "testing loss 0.007841635904941998\n",
      "epochs 30\n",
      "training loss 0.006708278404945072\n",
      "epochs 31\n",
      "training loss 0.006639863115644767\n",
      "epochs 32\n",
      "training loss 0.006661494117439654\n",
      "epochs 33\n",
      "training loss 0.006660270117192124\n",
      "epochs 34\n",
      "training loss 0.006701067592849598\n",
      "epochs 35\n",
      "training loss 0.006667135817304503\n",
      "epochs 36\n",
      "training loss 0.0066182938525396594\n",
      "epochs 37\n",
      "training loss 0.0065158445669032386\n",
      "epochs 38\n",
      "training loss 0.00665827133936247\n",
      "epochs 39\n",
      "training loss 0.006581889310466708\n",
      "testing loss 0.007522138889139214\n",
      "epochs 40\n",
      "training loss 0.006537811749255267\n",
      "epochs 41\n",
      "training loss 0.006558523745499665\n",
      "epochs 42\n",
      "training loss 0.006605120593721562\n",
      "epochs 43\n",
      "training loss 0.006567183793890418\n",
      "epochs 44\n",
      "training loss 0.006562775244375855\n",
      "epochs 45\n",
      "training loss 0.006568690965031373\n",
      "epochs 46\n",
      "training loss 0.006517195294959341\n",
      "epochs 47\n",
      "training loss 0.00658829014332212\n",
      "epochs 48\n",
      "training loss 0.006624310439463852\n",
      "epochs 49\n",
      "training loss 0.006569154216273827\n",
      "testing loss 0.007531426429206915\n",
      "epochs 50\n",
      "training loss 0.006583313105591094\n",
      "epochs 51\n",
      "training loss 0.006575307405785881\n",
      "epochs 52\n",
      "training loss 0.006501702869545929\n",
      "epochs 53\n",
      "training loss 0.00665301970017955\n",
      "epochs 54\n",
      "training loss 0.006474663694898702\n",
      "epochs 55\n",
      "training loss 0.006501483721417972\n",
      "epochs 56\n",
      "training loss 0.006529237367679849\n",
      "epochs 57\n",
      "training loss 0.006575045993588412\n",
      "epochs 58\n",
      "training loss 0.0065055336540644155\n",
      "epochs 59\n",
      "training loss 0.006573987351220928\n",
      "testing loss 0.007950189977501196\n",
      "epochs 60\n",
      "training loss 0.006571971911138722\n",
      "epochs 61\n",
      "training loss 0.0066233733325692795\n",
      "epochs 62\n",
      "training loss 0.006510764636781553\n",
      "epochs 63\n",
      "training loss 0.006601887921757407\n",
      "epochs 64\n",
      "training loss 0.006450876077682369\n",
      "epochs 65\n",
      "training loss 0.0066088818365390945\n",
      "epochs 66\n",
      "training loss 0.0065532149435629595\n",
      "epochs 67\n",
      "training loss 0.006681750770545024\n",
      "epochs 68\n",
      "training loss 0.006562679560889\n",
      "epochs 69\n",
      "training loss 0.0065858492024078695\n",
      "testing loss 0.007920631209177012\n",
      "epochs 70\n",
      "training loss 0.006663502561633325\n",
      "epochs 71\n",
      "training loss 0.0066391278725643965\n",
      "epochs 72\n",
      "training loss 0.006575665872977158\n",
      "epochs 73\n",
      "training loss 0.00660775981399041\n",
      "epochs 74\n",
      "training loss 0.006544586831185752\n",
      "epochs 75\n",
      "training loss 0.006624331181206511\n",
      "epochs 76\n",
      "training loss 0.006488773058165759\n",
      "epochs 77\n",
      "training loss 0.006563629727884221\n",
      "epochs 78\n",
      "training loss 0.00651465854741984\n",
      "epochs 79\n",
      "training loss 0.006610644533296079\n",
      "testing loss 0.007396296386653227\n",
      "epochs 80\n",
      "training loss 0.006517781637889251\n",
      "epochs 81\n",
      "training loss 0.006602056754758655\n",
      "epochs 82\n",
      "training loss 0.006594629550995277\n",
      "epochs 83\n",
      "training loss 0.006566192172071401\n",
      "epochs 84\n",
      "training loss 0.006622570930940515\n",
      "epochs 85\n",
      "training loss 0.006484538612895756\n",
      "epochs 86\n",
      "training loss 0.00654690427419738\n",
      "epochs 87\n",
      "training loss 0.006476519460574721\n",
      "epochs 88\n",
      "training loss 0.006601313005929026\n",
      "epochs 89\n",
      "training loss 0.00656289263454331\n",
      "testing loss 0.007507987998274042\n",
      "epochs 90\n",
      "training loss 0.006590372266029899\n",
      "epochs 91\n",
      "training loss 0.006528950990941998\n",
      "epochs 92\n",
      "training loss 0.006593721580131144\n",
      "epochs 93\n",
      "training loss 0.006571637058919324\n",
      "epochs 94\n",
      "training loss 0.006662281578984168\n",
      "epochs 95\n",
      "training loss 0.006569574369077987\n",
      "epochs 96\n",
      "training loss 0.006597720811251087\n",
      "epochs 97\n",
      "training loss 0.006447689940000808\n",
      "epochs 98\n",
      "training loss 0.006588867472026123\n",
      "epochs 99\n",
      "training loss 0.006604615161917243\n",
      "testing loss 0.007480587387549962\n",
      "epochs 100\n",
      "training loss 0.006897542404865784\n",
      "epochs 101\n",
      "training loss 0.006619785606612007\n",
      "epochs 102\n",
      "training loss 0.0066220649531518756\n",
      "epochs 103\n",
      "training loss 0.00658695307548942\n",
      "epochs 104\n",
      "training loss 0.00653155503396828\n",
      "epochs 105\n",
      "training loss 0.006581487480868051\n",
      "epochs 106\n",
      "training loss 0.006622289809522907\n",
      "epochs 107\n",
      "training loss 0.006617353206123919\n",
      "epochs 108\n",
      "training loss 0.006574626915060595\n",
      "epochs 109\n",
      "training loss 0.006592640899175546\n",
      "testing loss 0.008148881005691298\n",
      "epochs 110\n",
      "training loss 0.006550589741255782\n",
      "epochs 111\n",
      "training loss 0.006554072707223403\n",
      "epochs 112\n",
      "training loss 0.006525568549427852\n",
      "epochs 113\n",
      "training loss 0.006681914694641227\n",
      "epochs 114\n",
      "training loss 0.006587261584230718\n",
      "epochs 115\n",
      "training loss 0.00638296486283327\n",
      "epochs 116\n",
      "training loss 0.006538486477952892\n",
      "epochs 117\n",
      "training loss 0.006537506551096799\n",
      "epochs 118\n",
      "training loss 0.006603466774305855\n",
      "epochs 119\n",
      "training loss 0.006472113375597291\n",
      "testing loss 0.00813088192379908\n",
      "epochs 120\n",
      "training loss 0.006512341044608243\n",
      "epochs 121\n",
      "training loss 0.006650252957717943\n",
      "epochs 122\n",
      "training loss 0.006512290986019857\n",
      "epochs 123\n",
      "training loss 0.0066096196230642475\n",
      "epochs 124\n",
      "training loss 0.006601653790022147\n",
      "epochs 125\n",
      "training loss 0.00654184631154077\n",
      "epochs 126\n",
      "training loss 0.006587772222170691\n",
      "epochs 127\n",
      "training loss 0.006709728308593837\n",
      "epochs 128\n",
      "training loss 0.0066375633911158905\n",
      "epochs 129\n",
      "training loss 0.006621657051556492\n",
      "testing loss 0.007291368405328047\n",
      "epochs 130\n",
      "training loss 0.006648750399666808\n",
      "epochs 131\n",
      "training loss 0.006667429852330531\n",
      "epochs 132\n",
      "training loss 0.006533233868170496\n",
      "epochs 133\n",
      "training loss 0.006569347532938792\n",
      "epochs 134\n",
      "training loss 0.006568568954697179\n",
      "epochs 135\n",
      "training loss 0.006519083040879649\n",
      "epochs 136\n",
      "training loss 0.006509930794523593\n",
      "epochs 137\n",
      "training loss 0.0065751125311371405\n",
      "epochs 138\n",
      "training loss 0.00656217293450071\n",
      "epochs 139\n",
      "training loss 0.006614980489188256\n",
      "testing loss 0.007307298796033447\n",
      "epochs 140\n",
      "training loss 0.0065333082660534465\n",
      "epochs 141\n",
      "training loss 0.00656807216699589\n",
      "epochs 142\n",
      "training loss 0.006590380627410005\n",
      "epochs 143\n",
      "training loss 0.006526640600080412\n",
      "epochs 144\n",
      "training loss 0.006545368177564333\n",
      "epochs 145\n",
      "training loss 0.0066281246254220605\n",
      "epochs 146\n",
      "training loss 0.006573991564287886\n",
      "epochs 147\n",
      "training loss 0.006581261457513032\n",
      "epochs 148\n",
      "training loss 0.00653661674456133\n",
      "epochs 149\n",
      "training loss 0.00662349629687219\n",
      "testing loss 0.00734595834370042\n",
      "epochs 150\n",
      "training loss 0.006541592024739458\n",
      "epochs 151\n",
      "training loss 0.006567878516300335\n",
      "epochs 152\n",
      "training loss 0.0065406056713329845\n",
      "epochs 153\n",
      "training loss 0.006592669880221454\n",
      "epochs 154\n",
      "training loss 0.00657749661395898\n",
      "epochs 155\n",
      "training loss 0.006557016604435616\n",
      "epochs 156\n",
      "training loss 0.006499878370053047\n",
      "epochs 157\n",
      "training loss 0.006599291863972972\n",
      "epochs 158\n",
      "training loss 0.006600616933950605\n",
      "epochs 159\n",
      "training loss 0.006577502363955879\n",
      "testing loss 0.007688193054414017\n",
      "epochs 160\n",
      "training loss 0.006530545379451298\n",
      "epochs 161\n",
      "training loss 0.006683890478144822\n",
      "epochs 162\n",
      "training loss 0.006501876284498552\n",
      "epochs 163\n",
      "training loss 0.006551501351425645\n",
      "epochs 164\n",
      "training loss 0.006535266446428073\n",
      "epochs 165\n",
      "training loss 0.006472735986323025\n",
      "epochs 166\n",
      "training loss 0.006610013368344189\n",
      "epochs 167\n",
      "training loss 0.0065827506040944785\n",
      "epochs 168\n",
      "training loss 0.006483972101970533\n",
      "epochs 169\n",
      "training loss 0.006591868284299947\n",
      "testing loss 0.007497182297553365\n",
      "epochs 170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0065788087020359235\n",
      "epochs 171\n",
      "training loss 0.006599082731454998\n",
      "epochs 172\n",
      "training loss 0.006449920552751576\n",
      "epochs 173\n",
      "training loss 0.006638270587303695\n",
      "epochs 174\n",
      "training loss 0.006560761761061825\n",
      "epochs 175\n",
      "training loss 0.006590016605190955\n",
      "epochs 176\n",
      "training loss 0.006614301734785111\n",
      "epochs 177\n",
      "training loss 0.006587463702814014\n",
      "epochs 178\n",
      "training loss 0.0065802633748205785\n",
      "epochs 179\n",
      "training loss 0.0064861448092448405\n",
      "testing loss 0.007192077915223831\n",
      "epochs 180\n",
      "training loss 0.006646287191933797\n",
      "epochs 181\n",
      "training loss 0.006489354950421147\n",
      "epochs 182\n",
      "training loss 0.006588032854491658\n",
      "epochs 183\n",
      "training loss 0.006575350394531926\n",
      "epochs 184\n",
      "training loss 0.006641523599395341\n",
      "epochs 185\n",
      "training loss 0.006570450745743776\n",
      "epochs 186\n",
      "training loss 0.006598848884211893\n",
      "epochs 187\n",
      "training loss 0.006617256917206622\n",
      "epochs 188\n",
      "training loss 0.006548602416358413\n",
      "epochs 189\n",
      "training loss 0.00658730280976233\n",
      "testing loss 0.00801090740744051\n",
      "epochs 190\n",
      "training loss 0.006557579265162915\n",
      "epochs 191\n",
      "training loss 0.006638927014026576\n",
      "epochs 192\n",
      "training loss 0.006506630959411047\n",
      "epochs 193\n",
      "training loss 0.00661930394547179\n",
      "epochs 194\n",
      "training loss 0.006662847417248215\n",
      "epochs 195\n",
      "training loss 0.00660631232109419\n",
      "epochs 196\n",
      "training loss 0.00643363278776713\n",
      "epochs 197\n",
      "training loss 0.006619103859379707\n",
      "epochs 198\n",
      "training loss 0.006541367114937686\n",
      "epochs 199\n",
      "training loss 0.006583683915283406\n",
      "testing loss 0.00732139529865103\n",
      "epochs 200\n",
      "training loss 0.006564669589523373\n",
      "epochs 201\n",
      "training loss 0.0065375252585790165\n",
      "epochs 202\n",
      "training loss 0.0065733759404276145\n",
      "epochs 203\n",
      "training loss 0.006646426396704871\n",
      "epochs 204\n",
      "training loss 0.0066030656286914435\n",
      "epochs 205\n",
      "training loss 0.0065803759983290655\n",
      "epochs 206\n",
      "training loss 0.006559682851812964\n",
      "epochs 207\n",
      "training loss 0.006472017795433457\n",
      "epochs 208\n",
      "training loss 0.006480277251271038\n",
      "epochs 209\n",
      "training loss 0.006586417148532317\n",
      "testing loss 0.007471503186244385\n",
      "epochs 210\n",
      "training loss 0.006558248914047895\n",
      "epochs 211\n",
      "training loss 0.006459377648373981\n",
      "epochs 212\n",
      "training loss 0.006572968574201173\n",
      "epochs 213\n",
      "training loss 0.0066150683877100784\n",
      "epochs 214\n",
      "training loss 0.0065833131200987785\n",
      "epochs 215\n",
      "training loss 0.006589596664694373\n",
      "epochs 216\n",
      "training loss 0.006553404169060901\n",
      "epochs 217\n",
      "training loss 0.006557145164458585\n",
      "epochs 218\n",
      "training loss 0.0066087539010542505\n",
      "epochs 219\n",
      "training loss 0.006608301806221164\n",
      "testing loss 0.007362155862159181\n",
      "epochs 220\n",
      "training loss 0.006515078575670221\n",
      "epochs 221\n",
      "training loss 0.006600601309174417\n",
      "epochs 222\n",
      "training loss 0.0065040056994269775\n",
      "epochs 223\n",
      "training loss 0.006575812138390577\n",
      "epochs 224\n",
      "training loss 0.006624677874173369\n",
      "epochs 225\n",
      "training loss 0.006569798468572504\n",
      "epochs 226\n",
      "training loss 0.006594190724324265\n",
      "epochs 227\n",
      "training loss 0.006536936377158168\n",
      "epochs 228\n",
      "training loss 0.006637170701712302\n",
      "epochs 229\n",
      "training loss 0.006605481632311452\n",
      "testing loss 0.0073112507238455695\n",
      "epochs 230\n",
      "training loss 0.0066048662428741306\n",
      "epochs 231\n",
      "training loss 0.006580283914517192\n",
      "epochs 232\n",
      "training loss 0.006587397991141763\n",
      "epochs 233\n",
      "training loss 0.0065647732366674715\n",
      "epochs 234\n",
      "training loss 0.006622390182176464\n",
      "epochs 235\n",
      "training loss 0.006576146950979227\n",
      "epochs 236\n",
      "training loss 0.0066352130170758685\n",
      "epochs 237\n",
      "training loss 0.006484928494177781\n",
      "epochs 238\n",
      "training loss 0.006559028128150703\n",
      "epochs 239\n",
      "training loss 0.006583333170780272\n",
      "testing loss 0.007930370784159882\n",
      "epochs 240\n",
      "training loss 0.00657175921893036\n",
      "epochs 241\n",
      "training loss 0.006549917335203059\n",
      "epochs 242\n",
      "training loss 0.006582814252490919\n",
      "epochs 243\n",
      "training loss 0.00669351826445978\n",
      "epochs 244\n",
      "training loss 0.006545227437455626\n",
      "epochs 245\n",
      "training loss 0.0065279094478268304\n",
      "epochs 246\n",
      "training loss 0.006610060797149378\n",
      "epochs 247\n",
      "training loss 0.006592382840022422\n",
      "epochs 248\n",
      "training loss 0.006546250705134498\n",
      "epochs 249\n",
      "training loss 0.006594119999008565\n",
      "testing loss 0.0077354328488891426\n",
      "epochs 250\n",
      "training loss 0.006584867986137087\n",
      "epochs 251\n",
      "training loss 0.006607274522744459\n",
      "epochs 252\n",
      "training loss 0.006592447178064644\n",
      "epochs 253\n",
      "training loss 0.006615128642369889\n",
      "epochs 254\n",
      "training loss 0.006521849063618568\n",
      "epochs 255\n",
      "training loss 0.006567448417947693\n",
      "epochs 256\n",
      "training loss 0.006571387261007898\n",
      "epochs 257\n",
      "training loss 0.006594522668991385\n",
      "epochs 258\n",
      "training loss 0.006486679401141124\n",
      "epochs 259\n",
      "training loss 0.006540443902158697\n",
      "testing loss 0.00725465667341565\n",
      "epochs 260\n",
      "training loss 0.006618147596680081\n",
      "epochs 261\n",
      "training loss 0.0066509205419117284\n",
      "epochs 262\n",
      "training loss 0.006474837818250943\n",
      "epochs 263\n",
      "training loss 0.006598386726707311\n",
      "epochs 264\n",
      "training loss 0.006592360411849903\n",
      "epochs 265\n",
      "training loss 0.006533761153147644\n",
      "epochs 266\n",
      "training loss 0.006583699636305697\n",
      "epochs 267\n",
      "training loss 0.006595548064346032\n",
      "epochs 268\n",
      "training loss 0.006545684062224731\n",
      "epochs 269\n",
      "training loss 0.006576337776922459\n",
      "testing loss 0.0072314040883979264\n",
      "epochs 270\n",
      "training loss 0.006594367118659896\n",
      "epochs 271\n",
      "training loss 0.006526650923536383\n",
      "epochs 272\n",
      "training loss 0.006634663357710789\n",
      "epochs 273\n",
      "training loss 0.006570100558548033\n",
      "epochs 274\n",
      "training loss 0.006617463642156663\n",
      "epochs 275\n",
      "training loss 0.006567694679522564\n",
      "epochs 276\n",
      "training loss 0.006606586149392656\n",
      "epochs 277\n",
      "training loss 0.006553533567698802\n",
      "epochs 278\n",
      "training loss 0.006621813111072321\n",
      "epochs 279\n",
      "training loss 0.006584813957396997\n",
      "testing loss 0.0073578227815013515\n",
      "epochs 280\n",
      "training loss 0.006560761829354096\n",
      "epochs 281\n",
      "training loss 0.006648129619386239\n",
      "epochs 282\n",
      "training loss 0.006627773041596302\n",
      "epochs 283\n",
      "training loss 0.006497122946062258\n",
      "epochs 284\n",
      "training loss 0.006614245614815379\n",
      "epochs 285\n",
      "training loss 0.006540305674710172\n",
      "epochs 286\n",
      "training loss 0.006475422745797002\n",
      "epochs 287\n",
      "training loss 0.006621126175750511\n",
      "epochs 288\n",
      "training loss 0.006523426062721861\n",
      "epochs 289\n",
      "training loss 0.006626793752016788\n",
      "testing loss 0.0076321400569897176\n",
      "epochs 290\n",
      "training loss 0.006557547051734136\n",
      "epochs 291\n",
      "training loss 0.006621932812207761\n",
      "epochs 292\n",
      "training loss 0.006609240731883986\n",
      "epochs 293\n",
      "training loss 0.006531134086426087\n",
      "epochs 294\n",
      "training loss 0.006574597934014687\n",
      "epochs 295\n",
      "training loss 0.006561043571417523\n",
      "epochs 296\n",
      "training loss 0.0065636612187595445\n",
      "epochs 297\n",
      "training loss 0.006550376415309301\n",
      "epochs 298\n",
      "training loss 0.006656754185519207\n",
      "epochs 299\n",
      "training loss 0.00660213434185527\n",
      "testing loss 0.007718087644613487\n",
      "epochs 300\n",
      "training loss 0.0065951747151012435\n",
      "epochs 301\n",
      "training loss 0.006405430217131995\n",
      "epochs 302\n",
      "training loss 0.006571058140739255\n",
      "epochs 303\n",
      "training loss 0.006630792419106266\n",
      "epochs 304\n",
      "training loss 0.006598607403449828\n",
      "epochs 305\n",
      "training loss 0.006575566927738216\n",
      "epochs 306\n",
      "training loss 0.006588816568099732\n",
      "epochs 307\n",
      "training loss 0.006522737386831759\n",
      "epochs 308\n",
      "training loss 0.006536146374291947\n",
      "epochs 309\n",
      "training loss 0.006581237061603113\n",
      "testing loss 0.007936755837608419\n",
      "epochs 310\n",
      "training loss 0.006578075100093129\n",
      "epochs 311\n",
      "training loss 0.0064780329719276416\n",
      "epochs 312\n",
      "training loss 0.006540152745669578\n",
      "epochs 313\n",
      "training loss 0.006586144835756112\n",
      "epochs 314\n",
      "training loss 0.00656684035099873\n",
      "epochs 315\n",
      "training loss 0.006594937485444577\n",
      "epochs 316\n",
      "training loss 0.00655966619239118\n",
      "epochs 317\n",
      "training loss 0.006587155218841932\n",
      "epochs 318\n",
      "training loss 0.00657620658747014\n",
      "epochs 319\n",
      "training loss 0.006523248185417799\n",
      "testing loss 0.00735531041666152\n",
      "epochs 320\n",
      "training loss 0.006563339677163737\n",
      "epochs 321\n",
      "training loss 0.006543946530284943\n",
      "epochs 322\n",
      "training loss 0.006559940246089523\n",
      "epochs 323\n",
      "training loss 0.006569590737638389\n",
      "epochs 324\n",
      "training loss 0.0065936920276240055\n",
      "epochs 325\n",
      "training loss 0.006573572124306418\n",
      "epochs 326\n",
      "training loss 0.006554691861935616\n",
      "epochs 327\n",
      "training loss 0.0065582137849283795\n",
      "epochs 328\n",
      "training loss 0.006613700297140265\n",
      "epochs 329\n",
      "training loss 0.00657789445535979\n",
      "testing loss 0.007502294787047233\n",
      "epochs 330\n",
      "training loss 0.006616963596949439\n",
      "epochs 331\n",
      "training loss 0.006651877781226212\n",
      "epochs 332\n",
      "training loss 0.0065842617291102246\n",
      "epochs 333\n",
      "training loss 0.006552395915420086\n",
      "epochs 334\n",
      "training loss 0.0066151738433608935\n",
      "epochs 335\n",
      "training loss 0.006529640019616525\n",
      "epochs 336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006573342899706973\n",
      "epochs 337\n",
      "training loss 0.006476160321076114\n",
      "epochs 338\n",
      "training loss 0.006644068989976335\n",
      "epochs 339\n",
      "training loss 0.0066472104284204655\n",
      "testing loss 0.007452625276005649\n",
      "epochs 340\n",
      "training loss 0.006643609057455168\n",
      "epochs 341\n",
      "training loss 0.006431681619204105\n",
      "epochs 342\n",
      "training loss 0.006625445785020736\n",
      "epochs 343\n",
      "training loss 0.006610166868492169\n",
      "epochs 344\n",
      "training loss 0.006597515072469744\n",
      "epochs 345\n",
      "training loss 0.0066628186545252665\n",
      "epochs 346\n",
      "training loss 0.00661892944551993\n",
      "epochs 347\n",
      "training loss 0.006477860277405399\n",
      "epochs 348\n",
      "training loss 0.0065154865462655675\n",
      "epochs 349\n",
      "training loss 0.006516161088132523\n",
      "testing loss 0.007526349471155422\n",
      "epochs 350\n",
      "training loss 0.006484328083040074\n",
      "epochs 351\n",
      "training loss 0.006532548982324503\n",
      "epochs 352\n",
      "training loss 0.006505370165906439\n",
      "epochs 353\n",
      "training loss 0.006461290829095956\n",
      "epochs 354\n",
      "training loss 0.006541187178603965\n",
      "epochs 355\n",
      "training loss 0.006519784313332187\n",
      "epochs 356\n",
      "training loss 0.006566428880512964\n",
      "epochs 357\n",
      "training loss 0.0065866018964634435\n",
      "epochs 358\n",
      "training loss 0.006554799739308202\n",
      "epochs 359\n",
      "training loss 0.006577979965775208\n",
      "testing loss 0.007693881961576482\n",
      "epochs 360\n",
      "training loss 0.006546423750318089\n",
      "epochs 361\n",
      "training loss 0.00663030635612227\n",
      "epochs 362\n",
      "training loss 0.006646066098360735\n",
      "epochs 363\n",
      "training loss 0.006636446709164325\n",
      "epochs 364\n",
      "training loss 0.006578868409649874\n",
      "epochs 365\n",
      "training loss 0.006580454556379003\n",
      "epochs 366\n",
      "training loss 0.00656684075332159\n",
      "epochs 367\n",
      "training loss 0.006565462561157223\n",
      "epochs 368\n",
      "training loss 0.006531972244897104\n",
      "epochs 369\n",
      "training loss 0.006514507660778143\n",
      "testing loss 0.007481126118028629\n",
      "epochs 370\n",
      "training loss 0.006560650083383596\n",
      "epochs 371\n",
      "training loss 0.006535069438795868\n",
      "epochs 372\n",
      "training loss 0.006563198443439911\n",
      "epochs 373\n",
      "training loss 0.006562198733409873\n",
      "epochs 374\n",
      "training loss 0.006553364506113063\n",
      "epochs 375\n",
      "training loss 0.0066577929512974116\n",
      "epochs 376\n",
      "training loss 0.006519791230666918\n",
      "epochs 377\n",
      "training loss 0.006602691769554622\n",
      "epochs 378\n",
      "training loss 0.006575642288082186\n",
      "epochs 379\n",
      "training loss 0.006512688880497561\n",
      "testing loss 0.007926072533975573\n",
      "epochs 380\n",
      "training loss 0.006597409677680435\n",
      "epochs 381\n",
      "training loss 0.00653917400242823\n",
      "epochs 382\n",
      "training loss 0.006548387187074496\n",
      "epochs 383\n",
      "training loss 0.006603213228104143\n",
      "epochs 384\n",
      "training loss 0.006515917505525438\n",
      "epochs 385\n",
      "training loss 0.0065728143695458815\n",
      "epochs 386\n",
      "training loss 0.006578860899623155\n",
      "epochs 387\n",
      "training loss 0.006593957595034619\n",
      "epochs 388\n",
      "training loss 0.006556328169514615\n",
      "epochs 389\n",
      "training loss 0.006601366943023012\n",
      "testing loss 0.007566039623851155\n",
      "epochs 390\n",
      "training loss 0.00664407877134029\n",
      "epochs 391\n",
      "training loss 0.006585123846491397\n",
      "epochs 392\n",
      "training loss 0.006576775497470991\n",
      "epochs 393\n",
      "training loss 0.0064129545498429865\n",
      "epochs 394\n",
      "training loss 0.006536833945121107\n",
      "epochs 395\n",
      "training loss 0.006600830230527871\n",
      "epochs 396\n",
      "training loss 0.006698496992159126\n",
      "epochs 397\n",
      "training loss 0.006586461881031268\n",
      "epochs 398\n",
      "training loss 0.0065150150528890975\n",
      "epochs 399\n",
      "training loss 0.0066574589964313756\n",
      "testing loss 0.0074932683417771725\n",
      "epochs 400\n",
      "training loss 0.00654246280639288\n",
      "epochs 401\n",
      "training loss 0.006494641209893087\n",
      "epochs 402\n",
      "training loss 0.006524536755830502\n",
      "epochs 403\n",
      "training loss 0.006593033040825583\n",
      "epochs 404\n",
      "training loss 0.0066091337040955584\n",
      "epochs 405\n",
      "training loss 0.006643354880346104\n",
      "epochs 406\n",
      "training loss 0.006524058660396055\n",
      "epochs 407\n",
      "training loss 0.00659891030656338\n",
      "epochs 408\n",
      "training loss 0.006584207087155081\n",
      "epochs 409\n",
      "training loss 0.006611269429512888\n",
      "testing loss 0.007399852352930193\n",
      "epochs 410\n",
      "training loss 0.006677083843415971\n",
      "epochs 411\n",
      "training loss 0.006651738508870559\n",
      "epochs 412\n",
      "training loss 0.006581040541570645\n",
      "epochs 413\n",
      "training loss 0.006655659515263462\n",
      "epochs 414\n",
      "training loss 0.006541305489832462\n",
      "epochs 415\n",
      "training loss 0.006603220156054253\n",
      "epochs 416\n",
      "training loss 0.006575417209849736\n",
      "epochs 417\n",
      "training loss 0.006547067747847673\n",
      "epochs 418\n",
      "training loss 0.006617356463276008\n",
      "epochs 419\n",
      "training loss 0.006501274426130855\n",
      "testing loss 0.007814240025964083\n",
      "epochs 420\n",
      "training loss 0.006612781914004824\n",
      "epochs 421\n",
      "training loss 0.006561440100757501\n",
      "epochs 422\n",
      "training loss 0.006587476098391944\n",
      "epochs 423\n",
      "training loss 0.006515563907608457\n",
      "epochs 424\n",
      "training loss 0.00652984471172151\n",
      "epochs 425\n",
      "training loss 0.00653959299214071\n",
      "epochs 426\n",
      "training loss 0.006636410629968399\n",
      "epochs 427\n",
      "training loss 0.006545941973116396\n",
      "epochs 428\n",
      "training loss 0.006588365653696779\n",
      "epochs 429\n",
      "training loss 0.006611302105064496\n",
      "testing loss 0.007342191366809057\n",
      "epochs 430\n",
      "training loss 0.006560300697295114\n",
      "epochs 431\n",
      "training loss 0.006466168362157573\n",
      "epochs 432\n",
      "training loss 0.006413565691005677\n",
      "epochs 433\n",
      "training loss 0.00659913213648989\n",
      "epochs 434\n",
      "training loss 0.006590204504474228\n",
      "epochs 435\n",
      "training loss 0.006510390193445048\n",
      "epochs 436\n",
      "training loss 0.006599505371088586\n",
      "epochs 437\n",
      "training loss 0.0065675134870356745\n",
      "epochs 438\n",
      "training loss 0.006616335502672816\n",
      "epochs 439\n",
      "training loss 0.006611310220521657\n",
      "testing loss 0.007554964718891057\n",
      "epochs 440\n",
      "training loss 0.006541491021886241\n",
      "epochs 441\n",
      "training loss 0.006491704201707928\n",
      "epochs 442\n",
      "training loss 0.006548661840895593\n",
      "epochs 443\n",
      "training loss 0.006623107301620455\n",
      "epochs 444\n",
      "training loss 0.0065681890723943755\n",
      "epochs 445\n",
      "training loss 0.006619993058007811\n",
      "epochs 446\n",
      "training loss 0.006570092724044567\n",
      "epochs 447\n",
      "training loss 0.006615649472488979\n",
      "epochs 448\n",
      "training loss 0.006477905626657864\n",
      "epochs 449\n",
      "training loss 0.006612442211264965\n",
      "testing loss 0.008475189894794467\n",
      "epochs 450\n",
      "training loss 0.006595434725007161\n",
      "epochs 451\n",
      "training loss 0.006561541060795356\n",
      "epochs 452\n",
      "training loss 0.00653680464490614\n",
      "epochs 453\n",
      "training loss 0.0065250433057880645\n",
      "epochs 454\n",
      "training loss 0.006630787465262784\n",
      "epochs 455\n",
      "training loss 0.006588106737528732\n",
      "epochs 456\n",
      "training loss 0.006551140412620317\n",
      "epochs 457\n",
      "training loss 0.00647975005227846\n",
      "epochs 458\n",
      "training loss 0.0065062173711571926\n",
      "epochs 459\n",
      "training loss 0.006584710186052965\n",
      "testing loss 0.007782216423276997\n",
      "epochs 460\n",
      "training loss 0.006520046940282848\n",
      "epochs 461\n",
      "training loss 0.006586739583589182\n",
      "epochs 462\n",
      "training loss 0.0066326791109194505\n",
      "epochs 463\n",
      "training loss 0.006593617676094877\n",
      "epochs 464\n",
      "training loss 0.006608853202969923\n",
      "epochs 465\n",
      "training loss 0.006607144574229931\n",
      "epochs 466\n",
      "training loss 0.006499457564403085\n",
      "epochs 467\n",
      "training loss 0.006642923911886239\n",
      "epochs 468\n",
      "training loss 0.006572883976354493\n",
      "epochs 469\n",
      "training loss 0.006496722381109071\n",
      "testing loss 0.007266650409834024\n",
      "epochs 470\n",
      "training loss 0.006569811112196454\n",
      "epochs 471\n",
      "training loss 0.006464823459821405\n",
      "epochs 472\n",
      "training loss 0.006614707625827733\n",
      "epochs 473\n",
      "training loss 0.006570831895522818\n",
      "epochs 474\n",
      "training loss 0.006556366398324766\n",
      "epochs 475\n",
      "training loss 0.006620248596008448\n",
      "epochs 476\n",
      "training loss 0.0065051352776485105\n",
      "epochs 477\n",
      "training loss 0.006537931138652414\n",
      "epochs 478\n",
      "training loss 0.006520763031096625\n",
      "epochs 479\n",
      "training loss 0.006603926524596351\n",
      "testing loss 0.007282055080505022\n",
      "epochs 480\n",
      "training loss 0.0065923218153937976\n",
      "epochs 481\n",
      "training loss 0.006547294455186936\n",
      "epochs 482\n",
      "training loss 0.0065079200421927046\n",
      "epochs 483\n",
      "training loss 0.006618332405826558\n",
      "epochs 484\n",
      "training loss 0.006638070229811758\n",
      "epochs 485\n",
      "training loss 0.006537203484506412\n",
      "epochs 486\n",
      "training loss 0.006571532367929635\n",
      "epochs 487\n",
      "training loss 0.006522958460943292\n",
      "epochs 488\n",
      "training loss 0.0066134565485794225\n",
      "epochs 489\n",
      "training loss 0.0065904080214567685\n",
      "testing loss 0.007388058120999089\n",
      "epochs 490\n",
      "training loss 0.006671883784712923\n",
      "epochs 491\n",
      "training loss 0.006591061499227393\n",
      "epochs 492\n",
      "training loss 0.006626612861360505\n",
      "epochs 493\n",
      "training loss 0.006521795127586119\n",
      "epochs 494\n",
      "training loss 0.006627831243949526\n",
      "epochs 495\n",
      "training loss 0.006639064489198583\n",
      "epochs 496\n",
      "training loss 0.006547950787079098\n",
      "epochs 497\n",
      "training loss 0.006598969380439211\n",
      "epochs 498\n",
      "training loss 0.006583290643803209\n",
      "epochs 499\n",
      "training loss 0.0065657840726761245\n",
      "testing loss 0.007446193966869239\n",
      "epochs 500\n",
      "training loss 0.0065801242535571515\n",
      "epochs 501\n",
      "training loss 0.006641046969944719\n",
      "epochs 502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006554855537631954\n",
      "epochs 503\n",
      "training loss 0.006510293226912731\n",
      "epochs 504\n",
      "training loss 0.007146237280055609\n",
      "epochs 505\n",
      "training loss 0.006518146347631007\n",
      "epochs 506\n",
      "training loss 0.006558205031840793\n",
      "epochs 507\n",
      "training loss 0.0065793750728381445\n",
      "epochs 508\n",
      "training loss 0.00665103023663718\n",
      "epochs 509\n",
      "training loss 0.006570431062000546\n",
      "testing loss 0.00728666788901711\n",
      "epochs 510\n",
      "training loss 0.006588077022606297\n",
      "epochs 511\n",
      "training loss 0.006523149781917027\n",
      "epochs 512\n",
      "training loss 0.006550826336466637\n",
      "epochs 513\n",
      "training loss 0.006595251206690757\n",
      "epochs 514\n",
      "training loss 0.006534411213827803\n",
      "epochs 515\n",
      "training loss 0.006507090323655597\n",
      "epochs 516\n",
      "training loss 0.006617390828442089\n",
      "epochs 517\n",
      "training loss 0.006596533445737645\n",
      "epochs 518\n",
      "training loss 0.006595994977458836\n",
      "epochs 519\n",
      "training loss 0.006605922981550334\n",
      "testing loss 0.007095547452410485\n",
      "epochs 520\n",
      "training loss 0.006624949820365157\n",
      "epochs 521\n",
      "training loss 0.0065947593374483535\n",
      "epochs 522\n",
      "training loss 0.006580608804557348\n",
      "epochs 523\n",
      "training loss 0.006564280446010181\n",
      "epochs 524\n",
      "training loss 0.006623684844401266\n",
      "epochs 525\n",
      "training loss 0.006617361889857697\n",
      "epochs 526\n",
      "training loss 0.0065578490008789715\n",
      "epochs 527\n",
      "training loss 0.006579485351055589\n",
      "epochs 528\n",
      "training loss 0.006499873180902\n",
      "epochs 529\n",
      "training loss 0.006583114220806055\n",
      "testing loss 0.008266446925369455\n",
      "epochs 530\n",
      "training loss 0.006605630090862151\n",
      "epochs 531\n",
      "training loss 0.006552017202224344\n",
      "epochs 532\n",
      "training loss 0.0066170558286689244\n",
      "epochs 533\n",
      "training loss 0.0065656852629601865\n",
      "epochs 534\n",
      "training loss 0.006627039880899424\n",
      "epochs 535\n",
      "training loss 0.006664209537375096\n",
      "epochs 536\n",
      "training loss 0.006630782627480778\n",
      "epochs 537\n",
      "training loss 0.006483483167880393\n",
      "epochs 538\n",
      "training loss 0.006548766839023577\n",
      "epochs 539\n",
      "training loss 0.006536349957089837\n",
      "testing loss 0.008248238228195419\n",
      "epochs 540\n",
      "training loss 0.00659648969622278\n",
      "epochs 541\n",
      "training loss 0.006569428893448756\n",
      "epochs 542\n",
      "training loss 0.006593917312502499\n",
      "epochs 543\n",
      "training loss 0.0065776877661481905\n",
      "epochs 544\n",
      "training loss 0.006534472145748786\n",
      "epochs 545\n",
      "training loss 0.006558561354725534\n",
      "epochs 546\n",
      "training loss 0.006553846593283819\n",
      "epochs 547\n",
      "training loss 0.006571029301231733\n",
      "epochs 548\n",
      "training loss 0.0065823768702191286\n",
      "epochs 549\n",
      "training loss 0.006617861674634292\n",
      "testing loss 0.00750604222267734\n",
      "epochs 550\n",
      "training loss 0.006511540417163137\n",
      "epochs 551\n",
      "training loss 0.006478317294943523\n",
      "epochs 552\n",
      "training loss 0.00651182870891534\n",
      "epochs 553\n",
      "training loss 0.006639328956502096\n",
      "epochs 554\n",
      "training loss 0.006539394059586466\n",
      "epochs 555\n",
      "training loss 0.006605272472193493\n",
      "epochs 556\n",
      "training loss 0.006603339353666887\n",
      "epochs 557\n",
      "training loss 0.006548422066732607\n",
      "epochs 558\n",
      "training loss 0.0065324834436829285\n",
      "epochs 559\n",
      "training loss 0.006645788295310568\n",
      "testing loss 0.00715110638073203\n",
      "epochs 560\n",
      "training loss 0.00653233885054553\n",
      "epochs 561\n",
      "training loss 0.006572805394596876\n",
      "epochs 562\n",
      "training loss 0.0065895019868377445\n",
      "epochs 563\n",
      "training loss 0.006605087070001182\n",
      "epochs 564\n",
      "training loss 0.006599815870213427\n",
      "epochs 565\n",
      "training loss 0.006592171324705734\n",
      "epochs 566\n",
      "training loss 0.006553500549624303\n",
      "epochs 567\n",
      "training loss 0.006609515009566825\n",
      "epochs 568\n",
      "training loss 0.006653437106785192\n",
      "epochs 569\n",
      "training loss 0.006615395577395148\n",
      "testing loss 0.007545333415936969\n",
      "epochs 570\n",
      "training loss 0.006553453854341796\n",
      "epochs 571\n",
      "training loss 0.006546750176049724\n",
      "epochs 572\n",
      "training loss 0.0065569814247161285\n",
      "epochs 573\n",
      "training loss 0.00658241959110378\n",
      "epochs 574\n",
      "training loss 0.00658991469330746\n",
      "epochs 575\n",
      "training loss 0.0066052620305529705\n",
      "epochs 576\n",
      "training loss 0.006590662385750347\n",
      "epochs 577\n",
      "training loss 0.006543464309360413\n",
      "epochs 578\n",
      "training loss 0.006626590734664747\n",
      "epochs 579\n",
      "training loss 0.006472144517226649\n",
      "testing loss 0.007654034705165483\n",
      "epochs 580\n",
      "training loss 0.006527266811896512\n",
      "epochs 581\n",
      "training loss 0.006571150952058428\n",
      "epochs 582\n",
      "training loss 0.006562662332129062\n",
      "epochs 583\n",
      "training loss 0.0066104826707064565\n",
      "epochs 584\n",
      "training loss 0.006541963796538944\n",
      "epochs 585\n",
      "training loss 0.006567679383469275\n",
      "epochs 586\n",
      "training loss 0.006588515117170012\n",
      "epochs 587\n",
      "training loss 0.006580087187838681\n",
      "epochs 588\n",
      "training loss 0.006477350196065127\n",
      "epochs 589\n",
      "training loss 0.006522687145658857\n",
      "testing loss 0.007462527607864839\n",
      "epochs 590\n",
      "training loss 0.0064959096043628265\n",
      "epochs 591\n",
      "training loss 0.006621002142832792\n",
      "epochs 592\n",
      "training loss 0.006560908946120902\n",
      "epochs 593\n",
      "training loss 0.006605142157094547\n",
      "epochs 594\n",
      "training loss 0.00656447466781312\n",
      "epochs 595\n",
      "training loss 0.006499393284745447\n",
      "epochs 596\n",
      "training loss 0.006610495163945872\n",
      "epochs 597\n",
      "training loss 0.006575856244582174\n",
      "epochs 598\n",
      "training loss 0.006569971924925104\n",
      "epochs 599\n",
      "training loss 0.006611308317184221\n",
      "testing loss 0.007642884491051131\n",
      "epochs 600\n",
      "training loss 0.006603025220544007\n",
      "epochs 601\n",
      "training loss 0.006553526966348517\n",
      "epochs 602\n",
      "training loss 0.006548240061107695\n",
      "epochs 603\n",
      "training loss 0.006660106220695907\n",
      "epochs 604\n",
      "training loss 0.006602691737708485\n",
      "epochs 605\n",
      "training loss 0.0066305971722115725\n",
      "epochs 606\n",
      "training loss 0.006583561350826435\n",
      "epochs 607\n",
      "training loss 0.006542362438339321\n",
      "epochs 608\n",
      "training loss 0.006605046107377121\n",
      "epochs 609\n",
      "training loss 0.006525374254378799\n",
      "testing loss 0.0075703339951760165\n",
      "epochs 610\n",
      "training loss 0.006566653269453478\n",
      "epochs 611\n",
      "training loss 0.006612367243690014\n",
      "epochs 612\n",
      "training loss 0.006619942433973576\n",
      "epochs 613\n",
      "training loss 0.006627626775829037\n",
      "epochs 614\n",
      "training loss 0.006535668597672712\n",
      "epochs 615\n",
      "training loss 0.006579747399821945\n",
      "epochs 616\n",
      "training loss 0.006495275718335481\n",
      "epochs 617\n",
      "training loss 0.0065912101782241266\n",
      "epochs 618\n",
      "training loss 0.006552340956772805\n",
      "epochs 619\n",
      "training loss 0.006480382574229618\n",
      "testing loss 0.007464508676293788\n",
      "epochs 620\n",
      "training loss 0.0065656555625454366\n",
      "epochs 621\n",
      "training loss 0.00656937788478396\n",
      "epochs 622\n",
      "training loss 0.00665174099286922\n",
      "epochs 623\n",
      "training loss 0.006607947654535253\n",
      "epochs 624\n",
      "training loss 0.006535250519467433\n",
      "epochs 625\n",
      "training loss 0.006608949705256336\n",
      "epochs 626\n",
      "training loss 0.0065633146867926\n",
      "epochs 627\n",
      "training loss 0.006493959463368479\n",
      "epochs 628\n",
      "training loss 0.006644092156271532\n",
      "epochs 629\n",
      "training loss 0.006575909934691677\n",
      "testing loss 0.007449826400674192\n",
      "epochs 630\n",
      "training loss 0.006422369842811015\n",
      "epochs 631\n",
      "training loss 0.006642658371721765\n",
      "epochs 632\n",
      "training loss 0.006639157299456217\n",
      "epochs 633\n",
      "training loss 0.006535267691612017\n",
      "epochs 634\n",
      "training loss 0.0065823553889384065\n",
      "epochs 635\n",
      "training loss 0.006588032671553295\n",
      "epochs 636\n",
      "training loss 0.00656292335084982\n",
      "epochs 637\n",
      "training loss 0.006586093540476086\n",
      "epochs 638\n",
      "training loss 0.00661240993520545\n",
      "epochs 639\n",
      "training loss 0.0066132076887136446\n",
      "testing loss 0.007251013784524027\n",
      "epochs 640\n",
      "training loss 0.006541420580001157\n",
      "epochs 641\n",
      "training loss 0.006488788742388069\n",
      "epochs 642\n",
      "training loss 0.00663484779744208\n",
      "epochs 643\n",
      "training loss 0.006555914254660936\n",
      "epochs 644\n",
      "training loss 0.00658595831788748\n",
      "epochs 645\n",
      "training loss 0.006751533888922976\n",
      "epochs 646\n",
      "training loss 0.006639509327712506\n",
      "epochs 647\n",
      "training loss 0.006595668964311594\n",
      "epochs 648\n",
      "training loss 0.006592897297729565\n",
      "epochs 649\n",
      "training loss 0.006545622295227276\n",
      "testing loss 0.007315821986123655\n",
      "epochs 650\n",
      "training loss 0.006596107323198242\n",
      "epochs 651\n",
      "training loss 0.006602897494889818\n",
      "epochs 652\n",
      "training loss 0.006489275990402196\n",
      "epochs 653\n",
      "training loss 0.006513886771866865\n",
      "epochs 654\n",
      "training loss 0.006539119886642034\n",
      "epochs 655\n",
      "training loss 0.006606148652474772\n",
      "epochs 656\n",
      "training loss 0.006566421237794009\n",
      "epochs 657\n",
      "training loss 0.006633478798549679\n",
      "epochs 658\n",
      "training loss 0.006490297207543711\n",
      "epochs 659\n",
      "training loss 0.006511926944693126\n",
      "testing loss 0.007744265231468999\n",
      "epochs 660\n",
      "training loss 0.006499736063468262\n",
      "epochs 661\n",
      "training loss 0.006589863612104241\n",
      "epochs 662\n",
      "training loss 0.006584742301466414\n",
      "epochs 663\n",
      "training loss 0.0066091453562431195\n",
      "epochs 664\n",
      "training loss 0.006515559270103267\n",
      "epochs 665\n",
      "training loss 0.006544163236875755\n",
      "epochs 666\n",
      "training loss 0.0065406157283429455\n",
      "epochs 667\n",
      "training loss 0.00653529050689202\n",
      "epochs 668\n",
      "training loss 0.0066118532194441755\n",
      "epochs 669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006609882565645507\n",
      "testing loss 0.007345991361892868\n",
      "epochs 670\n",
      "training loss 0.006598087347191858\n",
      "epochs 671\n",
      "training loss 0.006562367818349446\n",
      "epochs 672\n",
      "training loss 0.006546816706167687\n",
      "epochs 673\n",
      "training loss 0.006597120375698008\n",
      "epochs 674\n",
      "training loss 0.0065391126338613294\n",
      "epochs 675\n",
      "training loss 0.006566629828573813\n",
      "epochs 676\n",
      "training loss 0.00658190844185639\n",
      "epochs 677\n",
      "training loss 0.00659012279188753\n",
      "epochs 678\n",
      "training loss 0.006597082065857132\n",
      "epochs 679\n",
      "training loss 0.006641759377929713\n",
      "testing loss 0.00850573293072112\n",
      "epochs 680\n",
      "training loss 0.006569565101144524\n",
      "epochs 681\n",
      "training loss 0.006543656774318146\n",
      "epochs 682\n",
      "training loss 0.0064993786677225605\n",
      "epochs 683\n",
      "training loss 0.00657626271982448\n",
      "epochs 684\n",
      "training loss 0.006570228683694314\n",
      "epochs 685\n",
      "training loss 0.006529515697606655\n",
      "epochs 686\n",
      "training loss 0.006525928418202988\n",
      "epochs 687\n",
      "training loss 0.00661168382878169\n",
      "epochs 688\n",
      "training loss 0.0066011653205317565\n",
      "epochs 689\n",
      "training loss 0.006556801037936497\n",
      "testing loss 0.007311442619229251\n",
      "epochs 690\n",
      "training loss 0.006578720544145009\n",
      "epochs 691\n",
      "training loss 0.006648472245955291\n",
      "epochs 692\n",
      "training loss 0.006535273089886029\n",
      "epochs 693\n",
      "training loss 0.006544721002144156\n",
      "epochs 694\n",
      "training loss 0.006607387272575955\n",
      "epochs 695\n",
      "training loss 0.006618651175393092\n",
      "epochs 696\n",
      "training loss 0.00656679568856128\n",
      "epochs 697\n",
      "training loss 0.006562460860376187\n",
      "epochs 698\n",
      "training loss 0.006520425103955809\n",
      "epochs 699\n",
      "training loss 0.006614300176801336\n",
      "testing loss 0.007731738726522589\n",
      "epochs 700\n",
      "training loss 0.0065484665511855375\n",
      "epochs 701\n",
      "training loss 0.006548340833961035\n",
      "epochs 702\n",
      "training loss 0.006556250915033205\n",
      "epochs 703\n",
      "training loss 0.006596591627213957\n",
      "epochs 704\n",
      "training loss 0.006604489235569777\n",
      "epochs 705\n",
      "training loss 0.006521907148141086\n",
      "epochs 706\n",
      "training loss 0.006608388901512651\n",
      "epochs 707\n",
      "training loss 0.006609609236623695\n",
      "epochs 708\n",
      "training loss 0.006511929670368579\n",
      "epochs 709\n",
      "training loss 0.006634242221922543\n",
      "testing loss 0.008169770615624515\n",
      "epochs 710\n",
      "training loss 0.006661136017186641\n",
      "epochs 711\n",
      "training loss 0.006562690697129148\n",
      "epochs 712\n",
      "training loss 0.006550753388290596\n",
      "epochs 713\n",
      "training loss 0.006535768505726519\n",
      "epochs 714\n",
      "training loss 0.006652440243799772\n",
      "epochs 715\n",
      "training loss 0.006588396841679957\n",
      "epochs 716\n",
      "training loss 0.006626838823146326\n",
      "epochs 717\n",
      "training loss 0.006575424613722665\n",
      "epochs 718\n",
      "training loss 0.00655953052395666\n",
      "epochs 719\n",
      "training loss 0.006608987275205303\n",
      "testing loss 0.007657927645911985\n",
      "epochs 720\n",
      "training loss 0.006563421927950144\n",
      "epochs 721\n",
      "training loss 0.006595150594129638\n",
      "epochs 722\n",
      "training loss 0.006458200508655023\n",
      "epochs 723\n",
      "training loss 0.006584067048369072\n",
      "epochs 724\n",
      "training loss 0.006558266733376745\n",
      "epochs 725\n",
      "training loss 0.006559570165673741\n",
      "epochs 726\n",
      "training loss 0.006459312376178416\n",
      "epochs 727\n",
      "training loss 0.006653955682905498\n",
      "epochs 728\n",
      "training loss 0.00652548944043975\n",
      "epochs 729\n",
      "training loss 0.006536076082083705\n",
      "testing loss 0.007502244670664007\n",
      "epochs 730\n",
      "training loss 0.006607881244017224\n",
      "epochs 731\n",
      "training loss 0.0065581402493680415\n",
      "epochs 732\n",
      "training loss 0.0066355067244405356\n",
      "epochs 733\n",
      "training loss 0.006524811125867136\n",
      "epochs 734\n",
      "training loss 0.006641529304453803\n",
      "epochs 735\n",
      "training loss 0.006641019626144082\n",
      "epochs 736\n",
      "training loss 0.006651298277077228\n",
      "epochs 737\n",
      "training loss 0.006591638611377667\n",
      "epochs 738\n",
      "training loss 0.006611836945360645\n",
      "epochs 739\n",
      "training loss 0.006501596529280207\n",
      "testing loss 0.007376259084665448\n",
      "epochs 740\n",
      "training loss 0.006598398438300994\n",
      "epochs 741\n",
      "training loss 0.006530685040329818\n",
      "epochs 742\n",
      "training loss 0.006571239403641491\n",
      "epochs 743\n",
      "training loss 0.0065328061405858055\n",
      "epochs 744\n",
      "training loss 0.0065482508226249675\n",
      "epochs 745\n",
      "training loss 0.006549022801639371\n",
      "epochs 746\n",
      "training loss 0.006588962858990062\n",
      "epochs 747\n",
      "training loss 0.006627127625498253\n",
      "epochs 748\n",
      "training loss 0.006521793269894814\n",
      "epochs 749\n",
      "training loss 0.006590600370706872\n",
      "testing loss 0.007782325590300169\n",
      "epochs 750\n",
      "training loss 0.00663191726322266\n",
      "epochs 751\n",
      "training loss 0.006569920617968161\n",
      "epochs 752\n",
      "training loss 0.006537839897701624\n",
      "epochs 753\n",
      "training loss 0.006564225234363036\n",
      "epochs 754\n",
      "training loss 0.006527386006678378\n",
      "epochs 755\n",
      "training loss 0.006526420825645101\n",
      "epochs 756\n",
      "training loss 0.006535547395522698\n",
      "epochs 757\n",
      "training loss 0.006627519681517567\n",
      "epochs 758\n",
      "training loss 0.006582712405007389\n",
      "epochs 759\n",
      "training loss 0.006500058465263605\n",
      "testing loss 0.007275930284654577\n",
      "epochs 760\n",
      "training loss 0.006611308054984363\n",
      "epochs 761\n",
      "training loss 0.006565535102764252\n",
      "epochs 762\n",
      "training loss 0.006536670759147582\n",
      "epochs 763\n",
      "training loss 0.00659994261819122\n",
      "epochs 764\n",
      "training loss 0.006584238460199698\n",
      "epochs 765\n",
      "training loss 0.006543501212663587\n",
      "epochs 766\n",
      "training loss 0.006507732253663219\n",
      "epochs 767\n",
      "training loss 0.006560891301945803\n",
      "epochs 768\n",
      "training loss 0.006602203579248696\n",
      "epochs 769\n",
      "training loss 0.006607746618366757\n",
      "testing loss 0.007882365887548695\n",
      "epochs 770\n",
      "training loss 0.006598835571819073\n",
      "epochs 771\n",
      "training loss 0.006606913437093056\n",
      "epochs 772\n",
      "training loss 0.006615253951609995\n",
      "epochs 773\n",
      "training loss 0.006579312907410134\n",
      "epochs 774\n",
      "training loss 0.006602513346620085\n",
      "epochs 775\n",
      "training loss 0.006505600579782165\n",
      "epochs 776\n",
      "training loss 0.006593462110194165\n",
      "epochs 777\n",
      "training loss 0.006573822026956249\n",
      "epochs 778\n",
      "training loss 0.006511295657310533\n",
      "epochs 779\n",
      "training loss 0.006600343805913301\n",
      "testing loss 0.0080466254875196\n",
      "epochs 780\n",
      "training loss 0.006608383559146302\n",
      "epochs 781\n",
      "training loss 0.0065068191141711045\n",
      "epochs 782\n",
      "training loss 0.006468343199938422\n",
      "epochs 783\n",
      "training loss 0.006612117856239906\n",
      "epochs 784\n",
      "training loss 0.0065118803456567205\n",
      "epochs 785\n",
      "training loss 0.006545553194772237\n",
      "epochs 786\n",
      "training loss 0.006561341810487438\n",
      "epochs 787\n",
      "training loss 0.00654512815252456\n",
      "epochs 788\n",
      "training loss 0.00656918806057865\n",
      "epochs 789\n",
      "training loss 0.0065459359467658\n",
      "testing loss 0.008047280876058424\n",
      "epochs 790\n",
      "training loss 0.006538387681329255\n",
      "epochs 791\n",
      "training loss 0.006556172723213839\n",
      "epochs 792\n",
      "training loss 0.006567643399104068\n",
      "epochs 793\n",
      "training loss 0.006637275021517576\n",
      "epochs 794\n",
      "training loss 0.006642513782830518\n",
      "epochs 795\n",
      "training loss 0.006554759903329688\n",
      "epochs 796\n",
      "training loss 0.006581480116979715\n",
      "epochs 797\n",
      "training loss 0.006621521761029461\n",
      "epochs 798\n",
      "training loss 0.0065348997261335555\n",
      "epochs 799\n",
      "training loss 0.00663604335413572\n",
      "testing loss 0.007828938084529013\n",
      "epochs 800\n",
      "training loss 0.006627424148061401\n",
      "epochs 801\n",
      "training loss 0.00656162569862663\n",
      "epochs 802\n",
      "training loss 0.0065682476303627925\n",
      "epochs 803\n",
      "training loss 0.006613240386911393\n",
      "epochs 804\n",
      "training loss 0.00660553892280363\n",
      "epochs 805\n",
      "training loss 0.006578703963630876\n",
      "epochs 806\n",
      "training loss 0.006567559882610651\n",
      "epochs 807\n",
      "training loss 0.006454266717099786\n",
      "epochs 808\n",
      "training loss 0.00661753920879283\n",
      "epochs 809\n",
      "training loss 0.006560588848570469\n",
      "testing loss 0.007267592249418351\n",
      "epochs 810\n",
      "training loss 0.006597324877080018\n",
      "epochs 811\n",
      "training loss 0.006469339453954941\n",
      "epochs 812\n",
      "training loss 0.006610783647128741\n",
      "epochs 813\n",
      "training loss 0.006582418635011988\n",
      "epochs 814\n",
      "training loss 0.006598902307167694\n",
      "epochs 815\n",
      "training loss 0.006643763567479526\n",
      "epochs 816\n",
      "training loss 0.0065668849893746545\n",
      "epochs 817\n",
      "training loss 0.006642181751824983\n",
      "epochs 818\n",
      "training loss 0.006533041337397414\n",
      "epochs 819\n",
      "training loss 0.0065633138948853345\n",
      "testing loss 0.007371071766540144\n",
      "epochs 820\n",
      "training loss 0.0066088662375936615\n",
      "epochs 821\n",
      "training loss 0.006569038257767138\n",
      "epochs 822\n",
      "training loss 0.006561392043875729\n",
      "epochs 823\n",
      "training loss 0.00654888793785057\n",
      "epochs 824\n",
      "training loss 0.006457531943246383\n",
      "epochs 825\n",
      "training loss 0.006577051226983815\n",
      "epochs 826\n",
      "training loss 0.006611478204646439\n",
      "epochs 827\n",
      "training loss 0.006619513567004885\n",
      "epochs 828\n",
      "training loss 0.006619187968565111\n",
      "epochs 829\n",
      "training loss 0.00652734125117944\n",
      "testing loss 0.007350730537172698\n",
      "epochs 830\n",
      "training loss 0.006597415268092804\n",
      "epochs 831\n",
      "training loss 0.006523230607411895\n",
      "epochs 832\n",
      "training loss 0.006512293151557151\n",
      "epochs 833\n",
      "training loss 0.006608130519298163\n",
      "epochs 834\n",
      "training loss 0.0066391411849572165\n",
      "epochs 835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006554403138845185\n",
      "epochs 836\n",
      "training loss 0.00649603900724688\n",
      "epochs 837\n",
      "training loss 0.006613963036260095\n",
      "epochs 838\n",
      "training loss 0.006544177803652514\n",
      "epochs 839\n",
      "training loss 0.0066009625455642\n",
      "testing loss 0.00760994087789102\n",
      "epochs 840\n",
      "training loss 0.006644012295714605\n",
      "epochs 841\n",
      "training loss 0.006523576961040474\n",
      "epochs 842\n",
      "training loss 0.006636129950858245\n",
      "epochs 843\n",
      "training loss 0.006616554224770233\n",
      "epochs 844\n",
      "training loss 0.006608624230662649\n",
      "epochs 845\n",
      "training loss 0.006506728987835098\n",
      "epochs 846\n",
      "training loss 0.006557488937842402\n",
      "epochs 847\n",
      "training loss 0.006579241226356488\n",
      "epochs 848\n",
      "training loss 0.0065429002824338505\n",
      "epochs 849\n",
      "training loss 0.006588052415096492\n",
      "testing loss 0.007404079659121996\n",
      "epochs 850\n",
      "training loss 0.006569503896054458\n",
      "epochs 851\n",
      "training loss 0.006637012955766613\n",
      "epochs 852\n",
      "training loss 0.006405854719410732\n",
      "epochs 853\n",
      "training loss 0.006926432701668456\n",
      "epochs 854\n",
      "training loss 0.00657482826756738\n",
      "epochs 855\n",
      "training loss 0.006552106556468459\n",
      "epochs 856\n",
      "training loss 0.006548405286649349\n",
      "epochs 857\n",
      "training loss 0.006613817236154116\n",
      "epochs 858\n",
      "training loss 0.0065094634698064134\n",
      "epochs 859\n",
      "training loss 0.006568396001867384\n",
      "testing loss 0.007507705612682757\n",
      "epochs 860\n",
      "training loss 0.006504278307310716\n",
      "epochs 861\n",
      "training loss 0.006559809577144615\n",
      "epochs 862\n",
      "training loss 0.006575405407317639\n",
      "epochs 863\n",
      "training loss 0.006594513271550299\n",
      "epochs 864\n",
      "training loss 0.0065210020345831034\n",
      "epochs 865\n",
      "training loss 0.006616056449131016\n",
      "epochs 866\n",
      "training loss 0.0065356864513246215\n",
      "epochs 867\n",
      "training loss 0.0065332811274834675\n",
      "epochs 868\n",
      "training loss 0.006581938447994052\n",
      "epochs 869\n",
      "training loss 0.006608861466688543\n",
      "testing loss 0.007641283374160845\n",
      "epochs 870\n",
      "training loss 0.006574400614290343\n",
      "epochs 871\n",
      "training loss 0.006560110926166697\n",
      "epochs 872\n",
      "training loss 0.006670778632478217\n",
      "epochs 873\n",
      "training loss 0.006553550679335727\n",
      "epochs 874\n",
      "training loss 0.006618737903038623\n",
      "epochs 875\n",
      "training loss 0.0066001051881189226\n",
      "epochs 876\n",
      "training loss 0.0065725134624774065\n",
      "epochs 877\n",
      "training loss 0.006448864480721167\n",
      "epochs 878\n",
      "training loss 0.006537583040917083\n",
      "epochs 879\n",
      "training loss 0.006596347578591738\n",
      "testing loss 0.007514678408286092\n",
      "epochs 880\n",
      "training loss 0.006570326693010683\n",
      "epochs 881\n",
      "training loss 0.006571384747640023\n",
      "epochs 882\n",
      "training loss 0.006615911362024653\n",
      "epochs 883\n",
      "training loss 0.006551534198592545\n",
      "epochs 884\n",
      "training loss 0.0065963929819826345\n",
      "epochs 885\n",
      "training loss 0.006656918005230851\n",
      "epochs 886\n",
      "training loss 0.006578115468255972\n",
      "epochs 887\n",
      "training loss 0.006559517093733134\n",
      "epochs 888\n",
      "training loss 0.006593848129955197\n",
      "epochs 889\n",
      "training loss 0.006538209778902129\n",
      "testing loss 0.007950758470671502\n",
      "epochs 890\n",
      "training loss 0.006569502128240027\n",
      "epochs 891\n",
      "training loss 0.006570495262750535\n",
      "epochs 892\n",
      "training loss 0.006570645561300241\n",
      "epochs 893\n",
      "training loss 0.0065470658438025455\n",
      "epochs 894\n",
      "training loss 0.006628369700197572\n",
      "epochs 895\n",
      "training loss 0.006566843828243008\n",
      "epochs 896\n",
      "training loss 0.006591945574519799\n",
      "epochs 897\n",
      "training loss 0.006568323443983441\n",
      "epochs 898\n",
      "training loss 0.0064833895954385535\n",
      "epochs 899\n",
      "training loss 0.006623149745797563\n",
      "testing loss 0.007414123140017878\n",
      "epochs 900\n",
      "training loss 0.0065709988230635554\n",
      "epochs 901\n",
      "training loss 0.006576624970690639\n",
      "epochs 902\n",
      "training loss 0.006590908765155924\n",
      "epochs 903\n",
      "training loss 0.006614783270663857\n",
      "epochs 904\n",
      "training loss 0.006586782260596933\n",
      "epochs 905\n",
      "training loss 0.00657906043690015\n",
      "epochs 906\n",
      "training loss 0.006615267310356636\n",
      "epochs 907\n",
      "training loss 0.006536411088226097\n",
      "epochs 908\n",
      "training loss 0.006633103652828936\n",
      "epochs 909\n",
      "training loss 0.00664786222506892\n",
      "testing loss 0.00737446278469069\n",
      "epochs 910\n",
      "training loss 0.006484246793299302\n",
      "epochs 911\n",
      "training loss 0.006485152030703371\n",
      "epochs 912\n",
      "training loss 0.006621410799535462\n",
      "epochs 913\n",
      "training loss 0.006603315632187374\n",
      "epochs 914\n",
      "training loss 0.006493397952994405\n",
      "epochs 915\n",
      "training loss 0.006594649067723212\n",
      "epochs 916\n",
      "training loss 0.006592702012973354\n",
      "epochs 917\n",
      "training loss 0.0065323266127829\n",
      "epochs 918\n",
      "training loss 0.006491744812255255\n",
      "epochs 919\n",
      "training loss 0.006479299096829374\n",
      "testing loss 0.00751425966154784\n",
      "epochs 920\n",
      "training loss 0.006571432889798673\n",
      "epochs 921\n",
      "training loss 0.006568976767831071\n",
      "epochs 922\n",
      "training loss 0.006595504180723547\n",
      "epochs 923\n",
      "training loss 0.006563338011964635\n",
      "epochs 924\n",
      "training loss 0.006541138858106949\n",
      "epochs 925\n",
      "training loss 0.006517033311354398\n",
      "epochs 926\n",
      "training loss 0.006533240095859445\n",
      "epochs 927\n",
      "training loss 0.0066152328202829305\n",
      "epochs 928\n",
      "training loss 0.006560931173662759\n",
      "epochs 929\n",
      "training loss 0.0066424624737505\n",
      "testing loss 0.00748400793060766\n",
      "epochs 930\n",
      "training loss 0.0065776112692509875\n",
      "epochs 931\n",
      "training loss 0.006634695610770316\n",
      "epochs 932\n",
      "training loss 0.0065932425685894976\n",
      "epochs 933\n",
      "training loss 0.006512463021327071\n",
      "epochs 934\n",
      "training loss 0.006653960211072287\n",
      "epochs 935\n",
      "training loss 0.006550740216020778\n",
      "epochs 936\n",
      "training loss 0.006659007635842273\n",
      "epochs 937\n",
      "training loss 0.006610373083196331\n",
      "epochs 938\n",
      "training loss 0.006615023665826507\n",
      "epochs 939\n",
      "training loss 0.006585737767821818\n",
      "testing loss 0.007828281168555114\n",
      "epochs 940\n",
      "training loss 0.006474488075131885\n",
      "epochs 941\n",
      "training loss 0.006497615318473621\n",
      "epochs 942\n",
      "training loss 0.006608177960134114\n",
      "epochs 943\n",
      "training loss 0.0066085217646563756\n",
      "epochs 944\n",
      "training loss 0.006571940684232487\n",
      "epochs 945\n",
      "training loss 0.006610331659510981\n",
      "epochs 946\n",
      "training loss 0.006452862217195762\n",
      "epochs 947\n",
      "training loss 0.006506020914816997\n",
      "epochs 948\n",
      "training loss 0.006652327640460504\n",
      "epochs 949\n",
      "training loss 0.006567377493416715\n",
      "testing loss 0.007617662343731586\n",
      "epochs 950\n",
      "training loss 0.006564551002295025\n",
      "epochs 951\n",
      "training loss 0.006491847526661395\n",
      "epochs 952\n",
      "training loss 0.006593867551144722\n",
      "epochs 953\n",
      "training loss 0.006621607527983202\n",
      "epochs 954\n",
      "training loss 0.006628169235136462\n",
      "epochs 955\n",
      "training loss 0.0064800451141654725\n",
      "epochs 956\n",
      "training loss 0.00663404617993597\n",
      "epochs 957\n",
      "training loss 0.006599198249069617\n",
      "epochs 958\n",
      "training loss 0.006540742416343847\n",
      "epochs 959\n",
      "training loss 0.006536395580572923\n",
      "testing loss 0.00779163727457536\n",
      "epochs 960\n",
      "training loss 0.006573250734156683\n",
      "epochs 961\n",
      "training loss 0.00665176549033293\n",
      "epochs 962\n",
      "training loss 0.0066226687721800525\n",
      "epochs 963\n",
      "training loss 0.006623363673636027\n",
      "epochs 964\n",
      "training loss 0.0065649454726053455\n",
      "epochs 965\n",
      "training loss 0.006496019996872517\n",
      "epochs 966\n",
      "training loss 0.006557930054958332\n",
      "epochs 967\n",
      "training loss 0.006556704271327149\n",
      "epochs 968\n",
      "training loss 0.006510578903743267\n",
      "epochs 969\n",
      "training loss 0.006526345823393246\n",
      "testing loss 0.007175359738733064\n",
      "epochs 970\n",
      "training loss 0.006508879620429004\n",
      "epochs 971\n",
      "training loss 0.006538252553217521\n",
      "epochs 972\n",
      "training loss 0.006606934103820371\n",
      "epochs 973\n",
      "training loss 0.006553677154498282\n",
      "epochs 974\n",
      "training loss 0.00658174858274261\n",
      "epochs 975\n",
      "training loss 0.006563989723336212\n",
      "epochs 976\n",
      "training loss 0.006672963778499661\n",
      "epochs 977\n",
      "training loss 0.006509776892052754\n",
      "epochs 978\n",
      "training loss 0.006641266920595319\n",
      "epochs 979\n",
      "training loss 0.006476753811694478\n",
      "testing loss 0.007403412910594427\n",
      "epochs 980\n",
      "training loss 0.006592614609835879\n",
      "epochs 981\n",
      "training loss 0.006565394174763339\n",
      "epochs 982\n",
      "training loss 0.006605096827657458\n",
      "epochs 983\n",
      "training loss 0.006550246225825672\n",
      "epochs 984\n",
      "training loss 0.006600501773012717\n",
      "epochs 985\n",
      "training loss 0.006587515952770448\n",
      "epochs 986\n",
      "training loss 0.006621331273009123\n",
      "epochs 987\n",
      "training loss 0.00665818654513128\n",
      "epochs 988\n",
      "training loss 0.006548329958505362\n",
      "epochs 989\n",
      "training loss 0.006608237076117614\n",
      "testing loss 0.007645164571821373\n",
      "epochs 990\n",
      "training loss 0.006592757271682011\n",
      "epochs 991\n",
      "training loss 0.00652189059664232\n",
      "epochs 992\n",
      "training loss 0.006524039356329544\n",
      "epochs 993\n",
      "training loss 0.006605338965865322\n",
      "epochs 994\n",
      "training loss 0.006553627575017331\n",
      "epochs 995\n",
      "training loss 0.006562243642831804\n",
      "epochs 996\n",
      "training loss 0.006619643404411781\n",
      "epochs 997\n",
      "training loss 0.0066451630686016295\n",
      "epochs 998\n",
      "training loss 0.0065615920588447734\n",
      "epochs 999\n",
      "training loss 0.006599164369380198\n",
      "testing loss 0.007441436564747958\n",
      "epochs 1000\n",
      "training loss 0.006548661520664996\n",
      "epochs 1001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006574706879648236\n",
      "epochs 1002\n",
      "training loss 0.006616026761100874\n",
      "epochs 1003\n",
      "training loss 0.006553025085388436\n",
      "epochs 1004\n",
      "training loss 0.006536053294757534\n",
      "epochs 1005\n",
      "training loss 0.006582265805402108\n",
      "epochs 1006\n",
      "training loss 0.0065443283647559245\n",
      "epochs 1007\n",
      "training loss 0.0065693501825373635\n",
      "epochs 1008\n",
      "training loss 0.0065936746831564376\n",
      "epochs 1009\n",
      "training loss 0.006530618960657765\n",
      "testing loss 0.007211524528345582\n",
      "epochs 1010\n",
      "training loss 0.006543023043275332\n",
      "epochs 1011\n",
      "training loss 0.006499366415452246\n",
      "epochs 1012\n",
      "training loss 0.006635553010323263\n",
      "epochs 1013\n",
      "training loss 0.006535095853397005\n",
      "epochs 1014\n",
      "training loss 0.006591149099103223\n",
      "epochs 1015\n",
      "training loss 0.006580571720438888\n",
      "epochs 1016\n",
      "training loss 0.006625869122792409\n",
      "epochs 1017\n",
      "training loss 0.006568143865741834\n",
      "epochs 1018\n",
      "training loss 0.00651962846400086\n",
      "epochs 1019\n",
      "training loss 0.006580530141061313\n",
      "testing loss 0.007232893833804691\n",
      "epochs 1020\n",
      "training loss 0.006504837145793922\n",
      "epochs 1021\n",
      "training loss 0.006757256137221304\n",
      "epochs 1022\n",
      "training loss 0.006677450782033448\n",
      "epochs 1023\n",
      "training loss 0.006539842449298474\n",
      "epochs 1024\n",
      "training loss 0.006584512865620929\n",
      "epochs 1025\n",
      "training loss 0.006640735733758738\n",
      "epochs 1026\n",
      "training loss 0.006509102429801139\n",
      "epochs 1027\n",
      "training loss 0.00657134501675376\n",
      "epochs 1028\n",
      "training loss 0.006655973478894126\n",
      "epochs 1029\n",
      "training loss 0.0065436379730667476\n",
      "testing loss 0.00767985626837199\n",
      "epochs 1030\n",
      "training loss 0.006525932501231555\n",
      "epochs 1031\n",
      "training loss 0.006603488833770881\n",
      "epochs 1032\n",
      "training loss 0.006528267018726283\n",
      "epochs 1033\n",
      "training loss 0.006552270155026376\n",
      "epochs 1034\n",
      "training loss 0.006630964968551664\n",
      "epochs 1035\n",
      "training loss 0.006551088711125125\n",
      "epochs 1036\n",
      "training loss 0.00653919795213839\n",
      "epochs 1037\n",
      "training loss 0.006516960031624691\n",
      "epochs 1038\n",
      "training loss 0.0066091982547992045\n",
      "epochs 1039\n",
      "training loss 0.006568892932876288\n",
      "testing loss 0.007543479601839385\n",
      "epochs 1040\n",
      "training loss 0.006606183208717673\n",
      "epochs 1041\n",
      "training loss 0.00658057072011635\n",
      "epochs 1042\n",
      "training loss 0.006627636500223791\n",
      "epochs 1043\n",
      "training loss 0.006600431569973658\n",
      "epochs 1044\n",
      "training loss 0.006649701606261467\n",
      "epochs 1045\n",
      "training loss 0.006601493044878076\n",
      "epochs 1046\n",
      "training loss 0.006619049353655258\n",
      "epochs 1047\n",
      "training loss 0.006581882580316493\n",
      "epochs 1048\n",
      "training loss 0.00660177739478625\n",
      "epochs 1049\n",
      "training loss 0.006570332044577028\n",
      "testing loss 0.0073060268554374155\n",
      "epochs 1050\n",
      "training loss 0.006561639609372973\n",
      "epochs 1051\n",
      "training loss 0.0065218613523350165\n",
      "epochs 1052\n",
      "training loss 0.006677510726724194\n",
      "epochs 1053\n",
      "training loss 0.006576256435166331\n",
      "epochs 1054\n",
      "training loss 0.006544842962939915\n",
      "epochs 1055\n",
      "training loss 0.006534886291310033\n",
      "epochs 1056\n",
      "training loss 0.006590220215234985\n",
      "epochs 1057\n",
      "training loss 0.006555865672671754\n",
      "epochs 1058\n",
      "training loss 0.006560015762479563\n",
      "epochs 1059\n",
      "training loss 0.006570518924076235\n",
      "testing loss 0.007185668881870277\n",
      "epochs 1060\n",
      "training loss 0.006581367931178683\n",
      "epochs 1061\n",
      "training loss 0.006593272866296233\n",
      "epochs 1062\n",
      "training loss 0.006469182931608591\n",
      "epochs 1063\n",
      "training loss 0.006566301663335424\n",
      "epochs 1064\n",
      "training loss 0.006569624264189538\n",
      "epochs 1065\n",
      "training loss 0.006585423800652693\n",
      "epochs 1066\n",
      "training loss 0.006637015026827034\n",
      "epochs 1067\n",
      "training loss 0.006616589197489832\n",
      "epochs 1068\n",
      "training loss 0.006552537692297465\n",
      "epochs 1069\n",
      "training loss 0.006562796341379861\n",
      "testing loss 0.007605331302587445\n",
      "epochs 1070\n",
      "training loss 0.006561216898262501\n",
      "epochs 1071\n",
      "training loss 0.006592369710921811\n",
      "epochs 1072\n",
      "training loss 0.006580971417761773\n",
      "epochs 1073\n",
      "training loss 0.006594764479537888\n",
      "epochs 1074\n",
      "training loss 0.006601306740024713\n",
      "epochs 1075\n",
      "training loss 0.006645261957932909\n",
      "epochs 1076\n",
      "training loss 0.0066545634674853825\n",
      "epochs 1077\n",
      "training loss 0.006690761235069858\n",
      "epochs 1078\n",
      "training loss 0.006482549254091791\n",
      "epochs 1079\n",
      "training loss 0.006652695353646793\n",
      "testing loss 0.007130473929767807\n",
      "epochs 1080\n",
      "training loss 0.0065631910625669684\n",
      "epochs 1081\n",
      "training loss 0.006576439529221425\n",
      "epochs 1082\n",
      "training loss 0.006578842979802051\n",
      "epochs 1083\n",
      "training loss 0.006519964967265522\n",
      "epochs 1084\n",
      "training loss 0.00655667691903421\n",
      "epochs 1085\n",
      "training loss 0.006519644946145817\n",
      "epochs 1086\n",
      "training loss 0.00659019688298603\n",
      "epochs 1087\n",
      "training loss 0.006565976169634055\n",
      "epochs 1088\n",
      "training loss 0.006526467855665889\n",
      "epochs 1089\n",
      "training loss 0.006631666841834648\n",
      "testing loss 0.007785535751055952\n",
      "epochs 1090\n",
      "training loss 0.006531163415302577\n",
      "epochs 1091\n",
      "training loss 0.006514111893268523\n",
      "epochs 1092\n",
      "training loss 0.006649621487750833\n",
      "epochs 1093\n",
      "training loss 0.00654095571982111\n",
      "epochs 1094\n",
      "training loss 0.006588616356038485\n",
      "epochs 1095\n",
      "training loss 0.006577693588683512\n",
      "epochs 1096\n",
      "training loss 0.006539444657789945\n",
      "epochs 1097\n",
      "training loss 0.006507008079946109\n",
      "epochs 1098\n",
      "training loss 0.006545875876459738\n",
      "epochs 1099\n",
      "training loss 0.006593289151702835\n",
      "testing loss 0.007425865131192543\n",
      "epochs 1100\n",
      "training loss 0.006580916267683826\n",
      "epochs 1101\n",
      "training loss 0.0065859116399434245\n",
      "epochs 1102\n",
      "training loss 0.006609544706796961\n",
      "epochs 1103\n",
      "training loss 0.00649495642790769\n",
      "epochs 1104\n",
      "training loss 0.006745683320155396\n",
      "epochs 1105\n",
      "training loss 0.00664010038245526\n",
      "epochs 1106\n",
      "training loss 0.006594176595254963\n",
      "epochs 1107\n",
      "training loss 0.006626167940754319\n",
      "epochs 1108\n",
      "training loss 0.006565552824077771\n",
      "epochs 1109\n",
      "training loss 0.006649544672746109\n",
      "testing loss 0.00807897732692196\n",
      "epochs 1110\n",
      "training loss 0.006490667800328448\n",
      "epochs 1111\n",
      "training loss 0.006594771846964683\n",
      "epochs 1112\n",
      "training loss 0.006544539445195925\n",
      "epochs 1113\n",
      "training loss 0.006571938789387355\n",
      "epochs 1114\n",
      "training loss 0.00659682091939798\n",
      "epochs 1115\n",
      "training loss 0.006542458295210698\n",
      "epochs 1116\n",
      "training loss 0.006497029031805219\n",
      "epochs 1117\n",
      "training loss 0.006542659318286892\n",
      "epochs 1118\n",
      "training loss 0.00662017035005065\n",
      "epochs 1119\n",
      "training loss 0.006604216519409173\n",
      "testing loss 0.007583844557186549\n",
      "epochs 1120\n",
      "training loss 0.006572170229416374\n",
      "epochs 1121\n",
      "training loss 0.006609107065509925\n",
      "epochs 1122\n",
      "training loss 0.0065963615717841905\n",
      "epochs 1123\n",
      "training loss 0.006623653201726025\n",
      "epochs 1124\n",
      "training loss 0.006630913772348507\n",
      "epochs 1125\n",
      "training loss 0.006580814161538897\n",
      "epochs 1126\n",
      "training loss 0.006591011565546633\n",
      "epochs 1127\n",
      "training loss 0.006559845460663722\n",
      "epochs 1128\n",
      "training loss 0.006585985581718929\n",
      "epochs 1129\n",
      "training loss 0.006474729380386352\n",
      "testing loss 0.007859198351767151\n",
      "epochs 1130\n",
      "training loss 0.006518127964625537\n",
      "epochs 1131\n",
      "training loss 0.006517823142959173\n",
      "epochs 1132\n",
      "training loss 0.006593063333578475\n",
      "epochs 1133\n",
      "training loss 0.006591122643102108\n",
      "epochs 1134\n",
      "training loss 0.006557025842999864\n",
      "epochs 1135\n",
      "training loss 0.00651582398722203\n",
      "epochs 1136\n",
      "training loss 0.006584488307295713\n",
      "epochs 1137\n",
      "training loss 0.006645870598466178\n",
      "epochs 1138\n",
      "training loss 0.006547154069631884\n",
      "epochs 1139\n",
      "training loss 0.006535908625189407\n",
      "testing loss 0.0073197002296421865\n",
      "epochs 1140\n",
      "training loss 0.006637355589412583\n",
      "epochs 1141\n",
      "training loss 0.006494566040979783\n",
      "epochs 1142\n",
      "training loss 0.006576591742785483\n",
      "epochs 1143\n",
      "training loss 0.006573621344633717\n",
      "epochs 1144\n",
      "training loss 0.006588124907165086\n",
      "epochs 1145\n",
      "training loss 0.006586314376803132\n",
      "epochs 1146\n",
      "training loss 0.006575011780222249\n",
      "epochs 1147\n",
      "training loss 0.006481150238800194\n",
      "epochs 1148\n",
      "training loss 0.0065937472024711705\n",
      "epochs 1149\n",
      "training loss 0.006540122926008737\n",
      "testing loss 0.0074598754818901315\n",
      "epochs 1150\n",
      "training loss 0.006493994100995951\n",
      "epochs 1151\n",
      "training loss 0.0065310600406198694\n",
      "epochs 1152\n",
      "training loss 0.006602605968631668\n",
      "epochs 1153\n",
      "training loss 0.006548351150693932\n",
      "epochs 1154\n",
      "training loss 0.006609604789487632\n",
      "epochs 1155\n",
      "training loss 0.006566178244340451\n",
      "epochs 1156\n",
      "training loss 0.006618097595768587\n",
      "epochs 1157\n",
      "training loss 0.006505379505316786\n",
      "epochs 1158\n",
      "training loss 0.006659537673741013\n",
      "epochs 1159\n",
      "training loss 0.006598863347665631\n",
      "testing loss 0.008120668658999897\n",
      "epochs 1160\n",
      "training loss 0.006575970834412677\n",
      "epochs 1161\n",
      "training loss 0.006581354506970539\n",
      "epochs 1162\n",
      "training loss 0.006628313655950876\n",
      "epochs 1163\n",
      "training loss 0.006609242715190609\n",
      "epochs 1164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0064676527095262224\n",
      "epochs 1165\n",
      "training loss 0.006551559062994519\n",
      "epochs 1166\n",
      "training loss 0.006479694385585406\n",
      "epochs 1167\n",
      "training loss 0.006558312960874889\n",
      "epochs 1168\n",
      "training loss 0.00653636945824855\n",
      "epochs 1169\n",
      "training loss 0.0066011970371608044\n",
      "testing loss 0.007450181055411115\n",
      "epochs 1170\n",
      "training loss 0.00662347754834384\n",
      "epochs 1171\n",
      "training loss 0.006600927967029004\n",
      "epochs 1172\n",
      "training loss 0.006588646588283719\n",
      "epochs 1173\n",
      "training loss 0.00656347027817022\n",
      "epochs 1174\n",
      "training loss 0.0065618623932682\n",
      "epochs 1175\n",
      "training loss 0.006555767117371064\n",
      "epochs 1176\n",
      "training loss 0.0066267875689124305\n",
      "epochs 1177\n",
      "training loss 0.006556029080506697\n",
      "epochs 1178\n",
      "training loss 0.006635015134028751\n",
      "epochs 1179\n",
      "training loss 0.006567702198395432\n",
      "testing loss 0.00718080697264126\n",
      "epochs 1180\n",
      "training loss 0.006553921204182039\n",
      "epochs 1181\n",
      "training loss 0.006496485907621229\n",
      "epochs 1182\n",
      "training loss 0.006626775598303503\n",
      "epochs 1183\n",
      "training loss 0.006598317698083003\n",
      "epochs 1184\n",
      "training loss 0.006510082928118463\n",
      "epochs 1185\n",
      "training loss 0.006606870981592877\n",
      "epochs 1186\n",
      "training loss 0.00664724104706549\n",
      "epochs 1187\n",
      "training loss 0.0065939773205316715\n",
      "epochs 1188\n",
      "training loss 0.006544760568492047\n",
      "epochs 1189\n",
      "training loss 0.006508448944245904\n",
      "testing loss 0.007242664946606449\n",
      "epochs 1190\n",
      "training loss 0.006570992046913364\n",
      "epochs 1191\n",
      "training loss 0.0065644813639941235\n",
      "epochs 1192\n",
      "training loss 0.006571059306307857\n",
      "epochs 1193\n",
      "training loss 0.006587883305387701\n",
      "epochs 1194\n",
      "training loss 0.006625453897647129\n",
      "epochs 1195\n",
      "training loss 0.006559011838497951\n",
      "epochs 1196\n",
      "training loss 0.00650085969495454\n",
      "epochs 1197\n",
      "training loss 0.006597471386293306\n",
      "epochs 1198\n",
      "training loss 0.006523851831061454\n",
      "epochs 1199\n",
      "training loss 0.0065137815968158985\n",
      "testing loss 0.008006296039671235\n",
      "epochs 1200\n",
      "training loss 0.006622164663051943\n",
      "epochs 1201\n",
      "training loss 0.0065384292967991175\n",
      "epochs 1202\n",
      "training loss 0.00656929410998299\n",
      "epochs 1203\n",
      "training loss 0.006553514126340057\n",
      "epochs 1204\n",
      "training loss 0.006618297432753113\n",
      "epochs 1205\n",
      "training loss 0.006462617285442071\n",
      "epochs 1206\n",
      "training loss 0.00656293478573596\n",
      "epochs 1207\n",
      "training loss 0.006499389467809044\n",
      "epochs 1208\n",
      "training loss 0.006471092774501597\n",
      "epochs 1209\n",
      "training loss 0.006670266648154355\n",
      "testing loss 0.00777690814220155\n",
      "epochs 1210\n",
      "training loss 0.006588190351329789\n",
      "epochs 1211\n",
      "training loss 0.00660443979762721\n",
      "epochs 1212\n",
      "training loss 0.006589211508317491\n",
      "epochs 1213\n",
      "training loss 0.006581669049847044\n",
      "epochs 1214\n",
      "training loss 0.006586964733652363\n",
      "epochs 1215\n",
      "training loss 0.006537290377398007\n",
      "epochs 1216\n",
      "training loss 0.006493288425638094\n",
      "epochs 1217\n",
      "training loss 0.006437822940722561\n",
      "epochs 1218\n",
      "training loss 0.006511610652402179\n",
      "epochs 1219\n",
      "training loss 0.006538996161216457\n",
      "testing loss 0.007956684841146918\n",
      "epochs 1220\n",
      "training loss 0.00654025164349316\n",
      "epochs 1221\n",
      "training loss 0.006569088808201412\n",
      "epochs 1222\n",
      "training loss 0.006546806133958004\n",
      "epochs 1223\n",
      "training loss 0.006621285607418368\n",
      "epochs 1224\n",
      "training loss 0.006589380677472403\n",
      "epochs 1225\n",
      "training loss 0.0066195668855761275\n",
      "epochs 1226\n",
      "training loss 0.006578553741865743\n",
      "epochs 1227\n",
      "training loss 0.0066127950958284835\n",
      "epochs 1228\n",
      "training loss 0.006575419756832977\n",
      "epochs 1229\n",
      "training loss 0.006551355274965969\n",
      "testing loss 0.007701542978967599\n",
      "epochs 1230\n",
      "training loss 0.006603821372191526\n",
      "epochs 1231\n",
      "training loss 0.006574564039109892\n",
      "epochs 1232\n",
      "training loss 0.006615747057897885\n",
      "epochs 1233\n",
      "training loss 0.006647497452342582\n",
      "epochs 1234\n",
      "training loss 0.006601897712321358\n",
      "epochs 1235\n",
      "training loss 0.0066074406307779455\n",
      "epochs 1236\n",
      "training loss 0.006581324280386842\n",
      "epochs 1237\n",
      "training loss 0.006553050260821012\n",
      "epochs 1238\n",
      "training loss 0.006548494590647338\n",
      "epochs 1239\n",
      "training loss 0.006588553923102928\n",
      "testing loss 0.007231197779806635\n",
      "epochs 1240\n",
      "training loss 0.006583721530524657\n",
      "epochs 1241\n",
      "training loss 0.006550236556630887\n",
      "epochs 1242\n",
      "training loss 0.00652875068263465\n",
      "epochs 1243\n",
      "training loss 0.006683727227417486\n",
      "epochs 1244\n",
      "training loss 0.006492470168171728\n",
      "epochs 1245\n",
      "training loss 0.0065806160279688365\n",
      "epochs 1246\n",
      "training loss 0.006529331257167671\n",
      "epochs 1247\n",
      "training loss 0.006576349613069922\n",
      "epochs 1248\n",
      "training loss 0.006609027305799098\n",
      "epochs 1249\n",
      "training loss 0.0065197424232778575\n",
      "testing loss 0.007746091831627415\n",
      "epochs 1250\n",
      "training loss 0.0066308529349043965\n",
      "epochs 1251\n",
      "training loss 0.00660356616185225\n",
      "epochs 1252\n",
      "training loss 0.006513182573815904\n",
      "epochs 1253\n",
      "training loss 0.006552502411378032\n",
      "epochs 1254\n",
      "training loss 0.006639097802734461\n",
      "epochs 1255\n",
      "training loss 0.006577595785305493\n",
      "epochs 1256\n",
      "training loss 0.006612703982969933\n",
      "epochs 1257\n",
      "training loss 0.0065787507516210245\n",
      "epochs 1258\n",
      "training loss 0.006460158408783648\n",
      "epochs 1259\n",
      "training loss 0.00649319976988591\n",
      "testing loss 0.007497748057027711\n",
      "epochs 1260\n",
      "training loss 0.006545115447331412\n",
      "epochs 1261\n",
      "training loss 0.0066299697743038045\n",
      "epochs 1262\n",
      "training loss 0.006634367069747514\n",
      "epochs 1263\n",
      "training loss 0.006547204146974106\n",
      "epochs 1264\n",
      "training loss 0.006532519267755914\n",
      "epochs 1265\n",
      "training loss 0.0065841798916159025\n",
      "epochs 1266\n",
      "training loss 0.006607141317431687\n",
      "epochs 1267\n",
      "training loss 0.0065641406786240255\n",
      "epochs 1268\n",
      "training loss 0.006559518247270974\n",
      "epochs 1269\n",
      "training loss 0.006639430999308598\n",
      "testing loss 0.007275115808386171\n",
      "epochs 1270\n",
      "training loss 0.006558470352266923\n",
      "epochs 1271\n",
      "training loss 0.006556918707996109\n",
      "epochs 1272\n",
      "training loss 0.006519025002710952\n",
      "epochs 1273\n",
      "training loss 0.006606265871734627\n",
      "epochs 1274\n",
      "training loss 0.006542618672354968\n",
      "epochs 1275\n",
      "training loss 0.0066041268435190785\n",
      "epochs 1276\n",
      "training loss 0.00656708653118903\n",
      "epochs 1277\n",
      "training loss 0.006605521880874359\n",
      "epochs 1278\n",
      "training loss 0.006548746857342047\n",
      "epochs 1279\n",
      "training loss 0.006574997092430171\n",
      "testing loss 0.007303715334273875\n",
      "epochs 1280\n",
      "training loss 0.0065993381498557005\n",
      "epochs 1281\n",
      "training loss 0.006522364212740722\n",
      "epochs 1282\n",
      "training loss 0.006547543743206329\n",
      "epochs 1283\n",
      "training loss 0.006570146993399912\n",
      "epochs 1284\n",
      "training loss 0.006556297902592477\n",
      "epochs 1285\n",
      "training loss 0.006535574311169719\n",
      "epochs 1286\n",
      "training loss 0.006600235724017749\n",
      "epochs 1287\n",
      "training loss 0.006524356425666225\n",
      "epochs 1288\n",
      "training loss 0.006593061598317873\n",
      "epochs 1289\n",
      "training loss 0.006642585752976317\n",
      "testing loss 0.007330266348456184\n",
      "epochs 1290\n",
      "training loss 0.006604952226735448\n",
      "epochs 1291\n",
      "training loss 0.006567020573239988\n",
      "epochs 1292\n",
      "training loss 0.006551588103840393\n",
      "epochs 1293\n",
      "training loss 0.006499516975140273\n",
      "epochs 1294\n",
      "training loss 0.006521807825702347\n",
      "epochs 1295\n",
      "training loss 0.006559260788594449\n",
      "epochs 1296\n",
      "training loss 0.006591718257857787\n",
      "epochs 1297\n",
      "training loss 0.006566264072863391\n",
      "epochs 1298\n",
      "training loss 0.006586622541606154\n",
      "epochs 1299\n",
      "training loss 0.00655771269799864\n",
      "testing loss 0.007150688148367859\n",
      "epochs 1300\n",
      "training loss 0.006572251397080289\n",
      "epochs 1301\n",
      "training loss 0.006512644018844834\n",
      "epochs 1302\n",
      "training loss 0.0067309803194239015\n",
      "epochs 1303\n",
      "training loss 0.006469132418681989\n",
      "epochs 1304\n",
      "training loss 0.0065829458916814585\n",
      "epochs 1305\n",
      "training loss 0.006465010718289638\n",
      "epochs 1306\n",
      "training loss 0.0065778917445458674\n",
      "epochs 1307\n",
      "training loss 0.006537823680587294\n",
      "epochs 1308\n",
      "training loss 0.006477764194426452\n",
      "epochs 1309\n",
      "training loss 0.006559880993167635\n",
      "testing loss 0.007354274545697139\n",
      "epochs 1310\n",
      "training loss 0.006535547240892012\n",
      "epochs 1311\n",
      "training loss 0.006600336076502085\n",
      "epochs 1312\n",
      "training loss 0.006571238165534466\n",
      "epochs 1313\n",
      "training loss 0.006515254140237069\n",
      "epochs 1314\n",
      "training loss 0.006624825990201175\n",
      "epochs 1315\n",
      "training loss 0.006633601524714256\n",
      "epochs 1316\n",
      "training loss 0.006543676260615328\n",
      "epochs 1317\n",
      "training loss 0.006616356219292412\n",
      "epochs 1318\n",
      "training loss 0.0065619890694152624\n",
      "epochs 1319\n",
      "training loss 0.006562697872417585\n",
      "testing loss 0.007275501245480849\n",
      "epochs 1320\n",
      "training loss 0.006556242037391839\n",
      "epochs 1321\n",
      "training loss 0.006574282431861776\n",
      "epochs 1322\n",
      "training loss 0.006627163107048346\n",
      "epochs 1323\n",
      "training loss 0.006600347172757639\n",
      "epochs 1324\n",
      "training loss 0.006594197901028087\n",
      "epochs 1325\n",
      "training loss 0.006596351856589431\n",
      "epochs 1326\n",
      "training loss 0.006595066824990203\n",
      "epochs 1327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006520974986597692\n",
      "epochs 1328\n",
      "training loss 0.006628098292205494\n",
      "epochs 1329\n",
      "training loss 0.006600810218415588\n",
      "testing loss 0.00756456283852458\n",
      "epochs 1330\n",
      "training loss 0.00661307680993808\n",
      "epochs 1331\n",
      "training loss 0.00653733281237512\n",
      "epochs 1332\n",
      "training loss 0.006544821864166679\n",
      "epochs 1333\n",
      "training loss 0.006597834436497496\n",
      "epochs 1334\n",
      "training loss 0.006587124302258608\n",
      "epochs 1335\n",
      "training loss 0.006538342083323079\n",
      "epochs 1336\n",
      "training loss 0.006623632114629706\n",
      "epochs 1337\n",
      "training loss 0.0065856334869396\n",
      "epochs 1338\n",
      "training loss 0.006548426160730399\n",
      "epochs 1339\n",
      "training loss 0.006595331795816522\n",
      "testing loss 0.008294169475648103\n",
      "epochs 1340\n",
      "training loss 0.006564284247023515\n",
      "epochs 1341\n",
      "training loss 0.006599237322156236\n",
      "epochs 1342\n",
      "training loss 0.006673609971643445\n",
      "epochs 1343\n",
      "training loss 0.006511178883896592\n",
      "epochs 1344\n",
      "training loss 0.006570287153908932\n",
      "epochs 1345\n",
      "training loss 0.006560658405132954\n",
      "epochs 1346\n",
      "training loss 0.006535265916012975\n",
      "epochs 1347\n",
      "training loss 0.006585806620938528\n",
      "epochs 1348\n",
      "training loss 0.006474110688150812\n",
      "epochs 1349\n",
      "training loss 0.006554723168811039\n",
      "testing loss 0.008388854051339076\n",
      "epochs 1350\n",
      "training loss 0.006586939889419609\n",
      "epochs 1351\n",
      "training loss 0.006595430620747836\n",
      "epochs 1352\n",
      "training loss 0.006597645134922672\n",
      "epochs 1353\n",
      "training loss 0.006600788053858312\n",
      "epochs 1354\n",
      "training loss 0.006589181659287436\n",
      "epochs 1355\n",
      "training loss 0.006519919674628411\n",
      "epochs 1356\n",
      "training loss 0.006611609112437373\n",
      "epochs 1357\n",
      "training loss 0.006568614374718835\n",
      "epochs 1358\n",
      "training loss 0.006559111148905926\n",
      "epochs 1359\n",
      "training loss 0.0065797484765752106\n",
      "testing loss 0.007183338091906874\n",
      "epochs 1360\n",
      "training loss 0.006558545668733994\n",
      "epochs 1361\n",
      "training loss 0.006483364338621406\n",
      "epochs 1362\n",
      "training loss 0.006601465527692917\n",
      "epochs 1363\n",
      "training loss 0.006587944677493063\n",
      "epochs 1364\n",
      "training loss 0.006565329325767546\n",
      "epochs 1365\n",
      "training loss 0.006636077770609465\n",
      "epochs 1366\n",
      "training loss 0.0065161733595105195\n",
      "epochs 1367\n",
      "training loss 0.006588918795259981\n",
      "epochs 1368\n",
      "training loss 0.006496463416110283\n",
      "epochs 1369\n",
      "training loss 0.0065265574162022\n",
      "testing loss 0.007650393268575641\n",
      "epochs 1370\n",
      "training loss 0.006573968686907917\n",
      "epochs 1371\n",
      "training loss 0.006578369278780618\n",
      "epochs 1372\n",
      "training loss 0.00662835058897711\n",
      "epochs 1373\n",
      "training loss 0.006613493968851547\n",
      "epochs 1374\n",
      "training loss 0.006647479240598559\n",
      "epochs 1375\n",
      "training loss 0.006591799290706389\n",
      "epochs 1376\n",
      "training loss 0.006480427776989852\n",
      "epochs 1377\n",
      "training loss 0.006599944360174895\n",
      "epochs 1378\n",
      "training loss 0.006609052635154667\n",
      "epochs 1379\n",
      "training loss 0.006554661193044466\n",
      "testing loss 0.007989163229783587\n",
      "epochs 1380\n",
      "training loss 0.006621466419167002\n",
      "epochs 1381\n",
      "training loss 0.006491798403995582\n",
      "epochs 1382\n",
      "training loss 0.006573967876954508\n",
      "epochs 1383\n",
      "training loss 0.006566818800010353\n",
      "epochs 1384\n",
      "training loss 0.006636516195311463\n",
      "epochs 1385\n",
      "training loss 0.0065644477135968875\n",
      "epochs 1386\n",
      "training loss 0.006608911334553954\n",
      "epochs 1387\n",
      "training loss 0.006561581025219956\n",
      "epochs 1388\n",
      "training loss 0.006515647929747756\n",
      "epochs 1389\n",
      "training loss 0.006545025480933782\n",
      "testing loss 0.007899564403565006\n",
      "epochs 1390\n",
      "training loss 0.006524959565310153\n",
      "epochs 1391\n",
      "training loss 0.006548841176045023\n",
      "epochs 1392\n",
      "training loss 0.006573898252099752\n",
      "epochs 1393\n",
      "training loss 0.006577195207260005\n",
      "epochs 1394\n",
      "training loss 0.006498259523003093\n",
      "epochs 1395\n",
      "training loss 0.006518182504319198\n",
      "epochs 1396\n",
      "training loss 0.006613426970949221\n",
      "epochs 1397\n",
      "training loss 0.006526559732477874\n",
      "epochs 1398\n",
      "training loss 0.006642730031190808\n",
      "epochs 1399\n",
      "training loss 0.006499473663686709\n",
      "testing loss 0.007431547402681664\n",
      "epochs 1400\n",
      "training loss 0.006599897160308045\n",
      "epochs 1401\n",
      "training loss 0.006570561190622669\n",
      "epochs 1402\n",
      "training loss 0.006596751753127593\n",
      "epochs 1403\n",
      "training loss 0.006656669835010855\n",
      "epochs 1404\n",
      "training loss 0.006449620408928843\n",
      "epochs 1405\n",
      "training loss 0.0065415255806060635\n",
      "epochs 1406\n",
      "training loss 0.006541295490853239\n",
      "epochs 1407\n",
      "training loss 0.006656244288532681\n",
      "epochs 1408\n",
      "training loss 0.0066616322631497605\n",
      "epochs 1409\n",
      "training loss 0.006546833824173839\n",
      "testing loss 0.007551599612644484\n",
      "epochs 1410\n",
      "training loss 0.006606774529552591\n",
      "epochs 1411\n",
      "training loss 0.0066314415668638415\n",
      "epochs 1412\n",
      "training loss 0.006529943976837202\n",
      "epochs 1413\n",
      "training loss 0.0065830376871088715\n",
      "epochs 1414\n",
      "training loss 0.006561053226812315\n",
      "epochs 1415\n",
      "training loss 0.006538953306931878\n",
      "epochs 1416\n",
      "training loss 0.00659342344969194\n",
      "epochs 1417\n",
      "training loss 0.0066349126351148025\n",
      "epochs 1418\n",
      "training loss 0.0066092957542235415\n",
      "epochs 1419\n",
      "training loss 0.006568817686116872\n",
      "testing loss 0.00742820123085052\n",
      "epochs 1420\n",
      "training loss 0.006636928049012407\n",
      "epochs 1421\n",
      "training loss 0.006481779538983858\n",
      "epochs 1422\n",
      "training loss 0.006556644116098055\n",
      "epochs 1423\n",
      "training loss 0.006621717817523718\n",
      "epochs 1424\n",
      "training loss 0.006541207357377697\n",
      "epochs 1425\n",
      "training loss 0.00658802552315715\n",
      "epochs 1426\n",
      "training loss 0.006552608200705591\n",
      "epochs 1427\n",
      "training loss 0.006592281334354092\n",
      "epochs 1428\n",
      "training loss 0.006597285013855364\n",
      "epochs 1429\n",
      "training loss 0.006675954145240657\n",
      "testing loss 0.0074751367792487144\n",
      "epochs 1430\n",
      "training loss 0.006605657507555055\n",
      "epochs 1431\n",
      "training loss 0.00662940047719547\n",
      "epochs 1432\n",
      "training loss 0.0065909454359823\n",
      "epochs 1433\n",
      "training loss 0.00660440382670815\n",
      "epochs 1434\n",
      "training loss 0.006522051185386476\n",
      "epochs 1435\n",
      "training loss 0.006550115859772908\n",
      "epochs 1436\n",
      "training loss 0.006658949555565906\n",
      "epochs 1437\n",
      "training loss 0.00654501857421443\n",
      "epochs 1438\n",
      "training loss 0.0066285058105857\n",
      "epochs 1439\n",
      "training loss 0.006629326240312433\n",
      "testing loss 0.007908698868217831\n",
      "epochs 1440\n",
      "training loss 0.006611211008129013\n",
      "epochs 1441\n",
      "training loss 0.006595294339805955\n",
      "epochs 1442\n",
      "training loss 0.006530476738996011\n",
      "epochs 1443\n",
      "training loss 0.006581833073020117\n",
      "epochs 1444\n",
      "training loss 0.006528697244817317\n",
      "epochs 1445\n",
      "training loss 0.0066194982819208215\n",
      "epochs 1446\n",
      "training loss 0.0065008544017727795\n",
      "epochs 1447\n",
      "training loss 0.006565866149016472\n",
      "epochs 1448\n",
      "training loss 0.006532809663476213\n",
      "epochs 1449\n",
      "training loss 0.006593275001402775\n",
      "testing loss 0.0072790577401677875\n",
      "epochs 1450\n",
      "training loss 0.0065584515117386236\n",
      "epochs 1451\n",
      "training loss 0.0066524097401546856\n",
      "epochs 1452\n",
      "training loss 0.006632208307188763\n",
      "epochs 1453\n",
      "training loss 0.006554016534176777\n",
      "epochs 1454\n",
      "training loss 0.0066040490342071986\n",
      "epochs 1455\n",
      "training loss 0.006565735454240808\n",
      "epochs 1456\n",
      "training loss 0.006628257916011709\n",
      "epochs 1457\n",
      "training loss 0.006500299685948888\n",
      "epochs 1458\n",
      "training loss 0.006571663099859125\n",
      "epochs 1459\n",
      "training loss 0.006623792235235653\n",
      "testing loss 0.007945161903977182\n",
      "epochs 1460\n",
      "training loss 0.006519631523353056\n",
      "epochs 1461\n",
      "training loss 0.006461681439654511\n",
      "epochs 1462\n",
      "training loss 0.006547631657651221\n",
      "epochs 1463\n",
      "training loss 0.006645899447173695\n",
      "epochs 1464\n",
      "training loss 0.00657494127747566\n",
      "epochs 1465\n",
      "training loss 0.006498196913852246\n",
      "epochs 1466\n",
      "training loss 0.006678605317364597\n",
      "epochs 1467\n",
      "training loss 0.006579218511214893\n",
      "epochs 1468\n",
      "training loss 0.006602364447885009\n",
      "epochs 1469\n",
      "training loss 0.006529643696076081\n",
      "testing loss 0.007747757468200851\n",
      "epochs 1470\n",
      "training loss 0.006468587643806582\n",
      "epochs 1471\n",
      "training loss 0.006545707871104197\n",
      "epochs 1472\n",
      "training loss 0.006519316815938178\n",
      "epochs 1473\n",
      "training loss 0.006536029843616336\n",
      "epochs 1474\n",
      "training loss 0.006549843760719664\n",
      "epochs 1475\n",
      "training loss 0.006661271394405933\n",
      "epochs 1476\n",
      "training loss 0.0065767510477764855\n",
      "epochs 1477\n",
      "training loss 0.006587890085076353\n",
      "epochs 1478\n",
      "training loss 0.006668395125009912\n",
      "epochs 1479\n",
      "training loss 0.006491780054251633\n",
      "testing loss 0.007642376466145647\n",
      "epochs 1480\n",
      "training loss 0.006537484639539386\n",
      "epochs 1481\n",
      "training loss 0.006567219085425229\n",
      "epochs 1482\n",
      "training loss 0.006667456314700873\n",
      "epochs 1483\n",
      "training loss 0.006532411154368071\n",
      "epochs 1484\n",
      "training loss 0.006561667664757842\n",
      "epochs 1485\n",
      "training loss 0.0065186709607172886\n",
      "epochs 1486\n",
      "training loss 0.00655954024092065\n",
      "epochs 1487\n",
      "training loss 0.0065262487659225165\n",
      "epochs 1488\n",
      "training loss 0.006661661030826552\n",
      "epochs 1489\n",
      "training loss 0.006603091857523451\n",
      "testing loss 0.0075152637040678495\n",
      "epochs 1490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006543996664596365\n",
      "epochs 1491\n",
      "training loss 0.00660822181651046\n",
      "epochs 1492\n",
      "training loss 0.006614684763132371\n",
      "epochs 1493\n",
      "training loss 0.006530931714489082\n",
      "epochs 1494\n",
      "training loss 0.0065965426772249735\n",
      "epochs 1495\n",
      "training loss 0.006633741401792659\n",
      "epochs 1496\n",
      "training loss 0.006601051947577519\n",
      "epochs 1497\n",
      "training loss 0.0065546980015169206\n",
      "epochs 1498\n",
      "training loss 0.006471297225283635\n",
      "epochs 1499\n",
      "training loss 0.006610603309533698\n",
      "testing loss 0.007254006966109659\n",
      "epochs 1500\n",
      "training loss 0.006513760312627378\n",
      "epochs 1501\n",
      "training loss 0.006549962976378443\n",
      "epochs 1502\n",
      "training loss 0.006541745804071902\n",
      "epochs 1503\n",
      "training loss 0.006541033651563693\n",
      "epochs 1504\n",
      "training loss 0.006547519550403993\n",
      "epochs 1505\n",
      "training loss 0.006540972574919712\n",
      "epochs 1506\n",
      "training loss 0.006521745836843473\n",
      "epochs 1507\n",
      "training loss 0.006608766334847544\n",
      "epochs 1508\n",
      "training loss 0.0064571105622428845\n",
      "epochs 1509\n",
      "training loss 0.006573340808123486\n",
      "testing loss 0.007304022764846524\n",
      "epochs 1510\n",
      "training loss 0.006589555319562826\n",
      "epochs 1511\n",
      "training loss 0.006592118707811036\n",
      "epochs 1512\n",
      "training loss 0.006598562434227928\n",
      "epochs 1513\n",
      "training loss 0.006668480955302081\n",
      "epochs 1514\n",
      "training loss 0.0065915033250967875\n",
      "epochs 1515\n",
      "training loss 0.0065812606178365625\n",
      "epochs 1516\n",
      "training loss 0.006632280790765053\n",
      "epochs 1517\n",
      "training loss 0.006545253366226257\n",
      "epochs 1518\n",
      "training loss 0.006620648521208134\n",
      "epochs 1519\n",
      "training loss 0.006516302407487072\n",
      "testing loss 0.007805146407402048\n",
      "epochs 1520\n",
      "training loss 0.0066071894493288045\n",
      "epochs 1521\n",
      "training loss 0.006580355289140235\n",
      "epochs 1522\n",
      "training loss 0.006359342182420285\n",
      "epochs 1523\n",
      "training loss 0.0065907688682621465\n",
      "epochs 1524\n",
      "training loss 0.006426859053989765\n",
      "epochs 1525\n",
      "training loss 0.00658224036989275\n",
      "epochs 1526\n",
      "training loss 0.006572560549113549\n",
      "epochs 1527\n",
      "training loss 0.006598453486825149\n",
      "epochs 1528\n",
      "training loss 0.00653700444473684\n",
      "epochs 1529\n",
      "training loss 0.006566911247929723\n",
      "testing loss 0.007702051334954957\n",
      "epochs 1530\n",
      "training loss 0.006538180365456455\n",
      "epochs 1531\n",
      "training loss 0.006669201400898125\n",
      "epochs 1532\n",
      "training loss 0.006576002511057131\n",
      "epochs 1533\n",
      "training loss 0.0065245389341062505\n",
      "epochs 1534\n",
      "training loss 0.006527533825121729\n",
      "epochs 1535\n",
      "training loss 0.006458151563619883\n",
      "epochs 1536\n",
      "training loss 0.006514909131576995\n",
      "epochs 1537\n",
      "training loss 0.006541151596207772\n",
      "epochs 1538\n",
      "training loss 0.006521968489108001\n",
      "epochs 1539\n",
      "training loss 0.0065021736297535635\n",
      "testing loss 0.0073278610353483585\n",
      "epochs 1540\n",
      "training loss 0.00656573987873073\n",
      "epochs 1541\n",
      "training loss 0.006587762378529847\n",
      "epochs 1542\n",
      "training loss 0.00662331441544721\n",
      "epochs 1543\n",
      "training loss 0.006621486185356341\n",
      "epochs 1544\n",
      "training loss 0.006606992549973463\n",
      "epochs 1545\n",
      "training loss 0.006600450322040467\n",
      "epochs 1546\n",
      "training loss 0.006607544406014282\n",
      "epochs 1547\n",
      "training loss 0.006512273622798453\n",
      "epochs 1548\n",
      "training loss 0.006591595306647177\n",
      "epochs 1549\n",
      "training loss 0.006598185834554123\n",
      "testing loss 0.007416366232090121\n",
      "epochs 1550\n",
      "training loss 0.006531558705120146\n",
      "epochs 1551\n",
      "training loss 0.006592638657915216\n",
      "epochs 1552\n",
      "training loss 0.006600322099232701\n",
      "epochs 1553\n",
      "training loss 0.00654243123377914\n",
      "epochs 1554\n",
      "training loss 0.00663604565059602\n",
      "epochs 1555\n",
      "training loss 0.006501592065867229\n",
      "epochs 1556\n",
      "training loss 0.006572565018541907\n",
      "epochs 1557\n",
      "training loss 0.006493429796300308\n",
      "epochs 1558\n",
      "training loss 0.006540179313486018\n",
      "epochs 1559\n",
      "training loss 0.006544899143771152\n",
      "testing loss 0.007542755667793941\n",
      "epochs 1560\n",
      "training loss 0.006524892911699949\n",
      "epochs 1561\n",
      "training loss 0.0066507715143767215\n",
      "epochs 1562\n",
      "training loss 0.006583565835116324\n",
      "epochs 1563\n",
      "training loss 0.0065557737059828945\n",
      "epochs 1564\n",
      "training loss 0.006627427152213626\n",
      "epochs 1565\n",
      "training loss 0.00655848029569233\n",
      "epochs 1566\n",
      "training loss 0.006630680516735852\n",
      "epochs 1567\n",
      "training loss 0.006559173373072365\n",
      "epochs 1568\n",
      "training loss 0.0065499976546982\n",
      "epochs 1569\n",
      "training loss 0.006614936167858314\n",
      "testing loss 0.008272345392495816\n",
      "epochs 1570\n",
      "training loss 0.006535279736174752\n",
      "epochs 1571\n",
      "training loss 0.0066277313925110735\n",
      "epochs 1572\n",
      "training loss 0.006595789345185128\n",
      "epochs 1573\n",
      "training loss 0.006635552081123764\n",
      "epochs 1574\n",
      "training loss 0.006465793664051973\n",
      "epochs 1575\n",
      "training loss 0.006652894187478014\n",
      "epochs 1576\n",
      "training loss 0.00664216438152666\n",
      "epochs 1577\n",
      "training loss 0.006549599023867048\n",
      "epochs 1578\n",
      "training loss 0.006520848110527661\n",
      "epochs 1579\n",
      "training loss 0.006609782131776599\n",
      "testing loss 0.0073781585443025155\n",
      "epochs 1580\n",
      "training loss 0.0065771744786096474\n",
      "epochs 1581\n",
      "training loss 0.006523993893138679\n",
      "epochs 1582\n",
      "training loss 0.00661941589384218\n",
      "epochs 1583\n",
      "training loss 0.006652741904206302\n",
      "epochs 1584\n",
      "training loss 0.006617985114860023\n",
      "epochs 1585\n",
      "training loss 0.006585552249214185\n",
      "epochs 1586\n",
      "training loss 0.006619551570061311\n",
      "epochs 1587\n",
      "training loss 0.006533251480145612\n",
      "epochs 1588\n",
      "training loss 0.00663217207619292\n",
      "epochs 1589\n",
      "training loss 0.006487577044303135\n",
      "testing loss 0.007371231650187244\n",
      "epochs 1590\n",
      "training loss 0.006577809613713241\n",
      "epochs 1591\n",
      "training loss 0.006568687192679561\n",
      "epochs 1592\n",
      "training loss 0.006523906393401257\n",
      "epochs 1593\n",
      "training loss 0.006530630510543844\n",
      "epochs 1594\n",
      "training loss 0.0065863668627748236\n",
      "epochs 1595\n",
      "training loss 0.006581844508967796\n",
      "epochs 1596\n",
      "training loss 0.006600669363660789\n",
      "epochs 1597\n",
      "training loss 0.006520452288171916\n",
      "epochs 1598\n",
      "training loss 0.006607134850542868\n",
      "epochs 1599\n",
      "training loss 0.006563954261601492\n",
      "testing loss 0.007308396647010554\n",
      "epochs 1600\n",
      "training loss 0.006562271935293468\n",
      "epochs 1601\n",
      "training loss 0.006506313282874693\n",
      "epochs 1602\n",
      "training loss 0.006560348093723397\n",
      "epochs 1603\n",
      "training loss 0.006503420009696715\n",
      "epochs 1604\n",
      "training loss 0.006576819990416224\n",
      "epochs 1605\n",
      "training loss 0.0065538483632213256\n",
      "epochs 1606\n",
      "training loss 0.006590544167937184\n",
      "epochs 1607\n",
      "training loss 0.006565820695025603\n",
      "epochs 1608\n",
      "training loss 0.006589200490261895\n",
      "epochs 1609\n",
      "training loss 0.006510478941551027\n",
      "testing loss 0.007644101105471875\n",
      "epochs 1610\n",
      "training loss 0.006607549362688532\n",
      "epochs 1611\n",
      "training loss 0.006561912505641172\n",
      "epochs 1612\n",
      "training loss 0.0065777283257417\n",
      "epochs 1613\n",
      "training loss 0.006615266529772442\n",
      "epochs 1614\n",
      "training loss 0.006517086766510183\n",
      "epochs 1615\n",
      "training loss 0.006590121286272958\n",
      "epochs 1616\n",
      "training loss 0.006583467044861914\n",
      "epochs 1617\n",
      "training loss 0.006556087860690379\n",
      "epochs 1618\n",
      "training loss 0.006591450167879304\n",
      "epochs 1619\n",
      "training loss 0.006518366179389364\n",
      "testing loss 0.007192804072719358\n",
      "epochs 1620\n",
      "training loss 0.006622279257482444\n",
      "epochs 1621\n",
      "training loss 0.006540436296593568\n",
      "epochs 1622\n",
      "training loss 0.006567960623779128\n",
      "epochs 1623\n",
      "training loss 0.006502146220137578\n",
      "epochs 1624\n",
      "training loss 0.006649964611119616\n",
      "epochs 1625\n",
      "training loss 0.006604633781645662\n",
      "epochs 1626\n",
      "training loss 0.006590282680016679\n",
      "epochs 1627\n",
      "training loss 0.006622599201109883\n",
      "epochs 1628\n",
      "training loss 0.0065825415844533676\n",
      "epochs 1629\n",
      "training loss 0.00657234470555304\n",
      "testing loss 0.007604117582036936\n",
      "epochs 1630\n",
      "training loss 0.006848255940925296\n",
      "epochs 1631\n",
      "training loss 0.006588647544021664\n",
      "epochs 1632\n",
      "training loss 0.006608121371672327\n",
      "epochs 1633\n",
      "training loss 0.006682428292856526\n",
      "epochs 1634\n",
      "training loss 0.006562714864100728\n",
      "epochs 1635\n",
      "training loss 0.006618348598171671\n",
      "epochs 1636\n",
      "training loss 0.006647564382306484\n",
      "epochs 1637\n",
      "training loss 0.0065405437642125286\n",
      "epochs 1638\n",
      "training loss 0.006538499734084204\n",
      "epochs 1639\n",
      "training loss 0.006476458605084466\n",
      "testing loss 0.008308192804217972\n",
      "epochs 1640\n",
      "training loss 0.006589297541363397\n",
      "epochs 1641\n",
      "training loss 0.006571778096966332\n",
      "epochs 1642\n",
      "training loss 0.006563005783513052\n",
      "epochs 1643\n",
      "training loss 0.006583316834419852\n",
      "epochs 1644\n",
      "training loss 0.006465112659542349\n",
      "epochs 1645\n",
      "training loss 0.006406540125764137\n",
      "epochs 1646\n",
      "training loss 0.006547578507510657\n",
      "epochs 1647\n",
      "training loss 0.006580121050189648\n",
      "epochs 1648\n",
      "training loss 0.006521070457423123\n",
      "epochs 1649\n",
      "training loss 0.006614522593758298\n",
      "testing loss 0.00752577589203914\n",
      "epochs 1650\n",
      "training loss 0.006578858557516726\n",
      "epochs 1651\n",
      "training loss 0.006547630285436576\n",
      "epochs 1652\n",
      "training loss 0.006548293908324806\n",
      "epochs 1653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006618677948086345\n",
      "epochs 1654\n",
      "training loss 0.006584315243004439\n",
      "epochs 1655\n",
      "training loss 0.00654536460867395\n",
      "epochs 1656\n",
      "training loss 0.0065956958505894005\n",
      "epochs 1657\n",
      "training loss 0.006549905439255629\n",
      "epochs 1658\n",
      "training loss 0.006591392465864271\n",
      "epochs 1659\n",
      "training loss 0.006582111824023619\n",
      "testing loss 0.007661902147563214\n",
      "epochs 1660\n",
      "training loss 0.0066324486078745415\n",
      "epochs 1661\n",
      "training loss 0.006554750114534967\n",
      "epochs 1662\n",
      "training loss 0.006617032044204829\n",
      "epochs 1663\n",
      "training loss 0.0065617260128956025\n",
      "epochs 1664\n",
      "training loss 0.006567781521814187\n",
      "epochs 1665\n",
      "training loss 0.00645157763745783\n",
      "epochs 1666\n",
      "training loss 0.006559961823616347\n",
      "epochs 1667\n",
      "training loss 0.006581018920520768\n",
      "epochs 1668\n",
      "training loss 0.006558351447992593\n",
      "epochs 1669\n",
      "training loss 0.0065757763734098675\n",
      "testing loss 0.007599136417282494\n",
      "epochs 1670\n",
      "training loss 0.006485850070773038\n",
      "epochs 1671\n",
      "training loss 0.006573943691582937\n",
      "epochs 1672\n",
      "training loss 0.006479344133635853\n",
      "epochs 1673\n",
      "training loss 0.0065103990130556755\n",
      "epochs 1674\n",
      "training loss 0.006554560753867868\n",
      "epochs 1675\n",
      "training loss 0.0066253749287820286\n",
      "epochs 1676\n",
      "training loss 0.0065659675800233035\n",
      "epochs 1677\n",
      "training loss 0.006588572271785339\n",
      "epochs 1678\n",
      "training loss 0.00664333791767833\n",
      "epochs 1679\n",
      "training loss 0.0065622558805944345\n",
      "testing loss 0.007396488554498299\n",
      "epochs 1680\n",
      "training loss 0.006553753145041912\n",
      "epochs 1681\n",
      "training loss 0.006658777779273938\n",
      "epochs 1682\n",
      "training loss 0.006515151035892678\n",
      "epochs 1683\n",
      "training loss 0.006553475010791923\n",
      "epochs 1684\n",
      "training loss 0.0065562984496383364\n",
      "epochs 1685\n",
      "training loss 0.006576793886845688\n",
      "epochs 1686\n",
      "training loss 0.006984037606153173\n",
      "epochs 1687\n",
      "training loss 0.006622452561327434\n",
      "epochs 1688\n",
      "training loss 0.006483619012176357\n",
      "epochs 1689\n",
      "training loss 0.006614901139231053\n",
      "testing loss 0.00829566694822506\n",
      "epochs 1690\n",
      "training loss 0.0066439029785427955\n",
      "epochs 1691\n",
      "training loss 0.0065891248096873285\n",
      "epochs 1692\n",
      "training loss 0.006556588391374264\n",
      "epochs 1693\n",
      "training loss 0.00680624269830816\n",
      "epochs 1694\n",
      "training loss 0.006582043079183774\n",
      "epochs 1695\n",
      "training loss 0.00654811711378939\n",
      "epochs 1696\n",
      "training loss 0.006563591333828006\n",
      "epochs 1697\n",
      "training loss 0.0066371575690732315\n",
      "epochs 1698\n",
      "training loss 0.006498374936940845\n",
      "epochs 1699\n",
      "training loss 0.006618955555105848\n",
      "testing loss 0.007326817334008713\n",
      "epochs 1700\n",
      "training loss 0.006599159618998145\n",
      "epochs 1701\n",
      "training loss 0.006555013650162061\n",
      "epochs 1702\n",
      "training loss 0.006503307781434168\n",
      "epochs 1703\n",
      "training loss 0.006532131646134209\n",
      "epochs 1704\n",
      "training loss 0.006561828781086329\n",
      "epochs 1705\n",
      "training loss 0.006661194231570628\n",
      "epochs 1706\n",
      "training loss 0.006591977180395061\n",
      "epochs 1707\n",
      "training loss 0.006532389662118125\n",
      "epochs 1708\n",
      "training loss 0.006615913107192942\n",
      "epochs 1709\n",
      "training loss 0.006645764720677129\n",
      "testing loss 0.007666647361578248\n",
      "epochs 1710\n",
      "training loss 0.0065865486553844665\n",
      "epochs 1711\n",
      "training loss 0.006516669096643145\n",
      "epochs 1712\n",
      "training loss 0.006582454548254156\n",
      "epochs 1713\n",
      "training loss 0.006471161259972351\n",
      "epochs 1714\n",
      "training loss 0.006591157027022025\n",
      "epochs 1715\n",
      "training loss 0.006606769864801261\n",
      "epochs 1716\n",
      "training loss 0.006538122005287932\n",
      "epochs 1717\n",
      "training loss 0.006582076645719516\n",
      "epochs 1718\n",
      "training loss 0.006594221793415201\n",
      "epochs 1719\n",
      "training loss 0.00663217985337334\n",
      "testing loss 0.007513095156606683\n",
      "epochs 1720\n",
      "training loss 0.006537034227243854\n",
      "epochs 1721\n",
      "training loss 0.0064512267960009015\n",
      "epochs 1722\n",
      "training loss 0.006664071664834072\n",
      "epochs 1723\n",
      "training loss 0.006585363973792654\n",
      "epochs 1724\n",
      "training loss 0.006654897448889866\n",
      "epochs 1725\n",
      "training loss 0.006436948143271305\n",
      "epochs 1726\n",
      "training loss 0.006644052597000561\n",
      "epochs 1727\n",
      "training loss 0.006586001936833184\n",
      "epochs 1728\n",
      "training loss 0.006596218700461248\n",
      "epochs 1729\n",
      "training loss 0.0065598360837457025\n",
      "testing loss 0.007686809365842359\n",
      "epochs 1730\n",
      "training loss 0.0065958219432444695\n",
      "epochs 1731\n",
      "training loss 0.006578544193332432\n",
      "epochs 1732\n",
      "training loss 0.006572889301736236\n",
      "epochs 1733\n",
      "training loss 0.006609336995678221\n",
      "epochs 1734\n",
      "training loss 0.006628981023944778\n",
      "epochs 1735\n",
      "training loss 0.006635066571554854\n",
      "epochs 1736\n",
      "training loss 0.006572754376378239\n",
      "epochs 1737\n",
      "training loss 0.0065544370539192005\n",
      "epochs 1738\n",
      "training loss 0.006593698000190015\n",
      "epochs 1739\n",
      "training loss 0.006606277281497704\n",
      "testing loss 0.007445664629703454\n",
      "epochs 1740\n",
      "training loss 0.006557615013512865\n",
      "epochs 1741\n",
      "training loss 0.006578413696002815\n",
      "epochs 1742\n",
      "training loss 0.006545259706084376\n",
      "epochs 1743\n",
      "training loss 0.0065140343383718905\n",
      "epochs 1744\n",
      "training loss 0.006578663737006418\n",
      "epochs 1745\n",
      "training loss 0.006624199580585182\n",
      "epochs 1746\n",
      "training loss 0.006629205232070006\n",
      "epochs 1747\n",
      "training loss 0.006618212055383777\n",
      "epochs 1748\n",
      "training loss 0.0065245225046843425\n",
      "epochs 1749\n",
      "training loss 0.006625995162370584\n",
      "testing loss 0.007410212434788651\n",
      "epochs 1750\n",
      "training loss 0.006539463317856804\n",
      "epochs 1751\n",
      "training loss 0.0065490930050322756\n",
      "epochs 1752\n",
      "training loss 0.0065512618897086445\n",
      "epochs 1753\n",
      "training loss 0.006615476258167018\n",
      "epochs 1754\n",
      "training loss 0.006456310537043608\n",
      "epochs 1755\n",
      "training loss 0.006521024193478844\n",
      "epochs 1756\n",
      "training loss 0.006579736250843344\n",
      "epochs 1757\n",
      "training loss 0.006606398412170834\n",
      "epochs 1758\n",
      "training loss 0.00664658310128189\n",
      "epochs 1759\n",
      "training loss 0.006582269280523311\n",
      "testing loss 0.007788986565399889\n",
      "epochs 1760\n",
      "training loss 0.006615493256573234\n",
      "epochs 1761\n",
      "training loss 0.006515974130079512\n",
      "epochs 1762\n",
      "training loss 0.006627107292447681\n",
      "epochs 1763\n",
      "training loss 0.0065776600125939284\n",
      "epochs 1764\n",
      "training loss 0.0065389577466371755\n",
      "epochs 1765\n",
      "training loss 0.006508117553701562\n",
      "epochs 1766\n",
      "training loss 0.006537823978879441\n",
      "epochs 1767\n",
      "training loss 0.006511239645971512\n",
      "epochs 1768\n",
      "training loss 0.006571858101892413\n",
      "epochs 1769\n",
      "training loss 0.006587231060416412\n",
      "testing loss 0.007476455875480852\n",
      "epochs 1770\n",
      "training loss 0.006570014127071573\n",
      "epochs 1771\n",
      "training loss 0.006517957226163778\n",
      "epochs 1772\n",
      "training loss 0.006550696570536626\n",
      "epochs 1773\n",
      "training loss 0.006651825988792608\n",
      "epochs 1774\n",
      "training loss 0.006531552041846817\n",
      "epochs 1775\n",
      "training loss 0.006641620014989494\n",
      "epochs 1776\n",
      "training loss 0.00659014296995357\n",
      "epochs 1777\n",
      "training loss 0.006543294625359624\n",
      "epochs 1778\n",
      "training loss 0.006531170161375861\n",
      "epochs 1779\n",
      "training loss 0.0066329305853530494\n",
      "testing loss 0.008112035714856065\n",
      "epochs 1780\n",
      "training loss 0.006623104372483574\n",
      "epochs 1781\n",
      "training loss 0.006521187306206254\n",
      "epochs 1782\n",
      "training loss 0.006615011449294635\n",
      "epochs 1783\n",
      "training loss 0.0066278480017404875\n",
      "epochs 1784\n",
      "training loss 0.006435357176452467\n",
      "epochs 1785\n",
      "training loss 0.006623911745648118\n",
      "epochs 1786\n",
      "training loss 0.00658416029668801\n",
      "epochs 1787\n",
      "training loss 0.006612375113578078\n",
      "epochs 1788\n",
      "training loss 0.006532985729796636\n",
      "epochs 1789\n",
      "training loss 0.006522468592699809\n",
      "testing loss 0.007253030551174748\n",
      "epochs 1790\n",
      "training loss 0.006565280001055687\n",
      "epochs 1791\n",
      "training loss 0.006628653492798355\n",
      "epochs 1792\n",
      "training loss 0.006549173850696364\n",
      "epochs 1793\n",
      "training loss 0.006607246569267228\n",
      "epochs 1794\n",
      "training loss 0.0064989558621352155\n",
      "epochs 1795\n",
      "training loss 0.0065973001609395026\n",
      "epochs 1796\n",
      "training loss 0.006638611942504201\n",
      "epochs 1797\n",
      "training loss 0.00658830715022064\n",
      "epochs 1798\n",
      "training loss 0.0065773228954065215\n",
      "epochs 1799\n",
      "training loss 0.006652719161818569\n",
      "testing loss 0.007555727992233232\n",
      "epochs 1800\n",
      "training loss 0.006562938474226279\n",
      "epochs 1801\n",
      "training loss 0.006570934367544474\n",
      "epochs 1802\n",
      "training loss 0.0066229125564793365\n",
      "epochs 1803\n",
      "training loss 0.0065339546120586865\n",
      "epochs 1804\n",
      "training loss 0.006526882927242021\n",
      "epochs 1805\n",
      "training loss 0.006578529526771112\n",
      "epochs 1806\n",
      "training loss 0.006552291064492022\n",
      "epochs 1807\n",
      "training loss 0.006616208970187215\n",
      "epochs 1808\n",
      "training loss 0.006560347157270056\n",
      "epochs 1809\n",
      "training loss 0.006568917885739753\n",
      "testing loss 0.008054035811533107\n",
      "epochs 1810\n",
      "training loss 0.006470763254387016\n",
      "epochs 1811\n",
      "training loss 0.006474661124561627\n",
      "epochs 1812\n",
      "training loss 0.0065901234652563975\n",
      "epochs 1813\n",
      "training loss 0.00661547893395019\n",
      "epochs 1814\n",
      "training loss 0.006582205672819156\n",
      "epochs 1815\n",
      "training loss 0.006579420778059647\n",
      "epochs 1816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006534101676810136\n",
      "epochs 1817\n",
      "training loss 0.006629117453148118\n",
      "epochs 1818\n",
      "training loss 0.006536915171877297\n",
      "epochs 1819\n",
      "training loss 0.006611265305792036\n",
      "testing loss 0.007756431535850709\n",
      "epochs 1820\n",
      "training loss 0.006530882726638581\n",
      "epochs 1821\n",
      "training loss 0.006586377191892329\n",
      "epochs 1822\n",
      "training loss 0.006548551842924151\n",
      "epochs 1823\n",
      "training loss 0.006600601097574531\n",
      "epochs 1824\n",
      "training loss 0.006514042921259569\n",
      "epochs 1825\n",
      "training loss 0.006518465962535546\n",
      "epochs 1826\n",
      "training loss 0.006652255196514962\n",
      "epochs 1827\n",
      "training loss 0.006555868185331938\n",
      "epochs 1828\n",
      "training loss 0.006576600408119271\n",
      "epochs 1829\n",
      "training loss 0.006585900062457361\n",
      "testing loss 0.007632337002816828\n",
      "epochs 1830\n",
      "training loss 0.006571760295683625\n",
      "epochs 1831\n",
      "training loss 0.006625210819978233\n",
      "epochs 1832\n",
      "training loss 0.006506519557024978\n",
      "epochs 1833\n",
      "training loss 0.006519966932172154\n",
      "epochs 1834\n",
      "training loss 0.006548678911840231\n",
      "epochs 1835\n",
      "training loss 0.006600062128603685\n",
      "epochs 1836\n",
      "training loss 0.006488235246978904\n",
      "epochs 1837\n",
      "training loss 0.006603330687271561\n",
      "epochs 1838\n",
      "training loss 0.006529605679927354\n",
      "epochs 1839\n",
      "training loss 0.006496869245937553\n",
      "testing loss 0.007330603856289535\n",
      "epochs 1840\n",
      "training loss 0.00652609099697684\n",
      "epochs 1841\n",
      "training loss 0.006503516596552313\n",
      "epochs 1842\n",
      "training loss 0.006599718812388852\n",
      "epochs 1843\n",
      "training loss 0.006575761690217787\n",
      "epochs 1844\n",
      "training loss 0.006573138355863447\n",
      "epochs 1845\n",
      "training loss 0.006543012047865878\n",
      "epochs 1846\n",
      "training loss 0.006436974898272259\n",
      "epochs 1847\n",
      "training loss 0.006606150637550625\n",
      "epochs 1848\n",
      "training loss 0.006496187930751173\n",
      "epochs 1849\n",
      "training loss 0.006461139822854323\n",
      "testing loss 0.00801451017102557\n",
      "epochs 1850\n",
      "training loss 0.006582860716358167\n",
      "epochs 1851\n",
      "training loss 0.00659103551808756\n",
      "epochs 1852\n",
      "training loss 0.006533668840751279\n",
      "epochs 1853\n",
      "training loss 0.006562793072550855\n",
      "epochs 1854\n",
      "training loss 0.006530265778863638\n",
      "epochs 1855\n",
      "training loss 0.006618410119246182\n",
      "epochs 1856\n",
      "training loss 0.006629618955493018\n",
      "epochs 1857\n",
      "training loss 0.0065831982638222646\n",
      "epochs 1858\n",
      "training loss 0.0066029251444133675\n",
      "epochs 1859\n",
      "training loss 0.006599212196262094\n",
      "testing loss 0.007367747406481851\n",
      "epochs 1860\n",
      "training loss 0.0065543869528693\n",
      "epochs 1861\n",
      "training loss 0.006456031421578764\n",
      "epochs 1862\n",
      "training loss 0.006555245919252171\n",
      "epochs 1863\n",
      "training loss 0.006609764041755587\n",
      "epochs 1864\n",
      "training loss 0.006597107921027802\n",
      "epochs 1865\n",
      "training loss 0.006557979659916193\n",
      "epochs 1866\n",
      "training loss 0.006574416835650824\n",
      "epochs 1867\n",
      "training loss 0.0066155245383255935\n",
      "epochs 1868\n",
      "training loss 0.006654453278083275\n",
      "epochs 1869\n",
      "training loss 0.0065541081611735115\n",
      "testing loss 0.007390052734061759\n",
      "epochs 1870\n",
      "training loss 0.006513116369590072\n",
      "epochs 1871\n",
      "training loss 0.006598045005630079\n",
      "epochs 1872\n",
      "training loss 0.006533136175546263\n",
      "epochs 1873\n",
      "training loss 0.006588615002577677\n",
      "epochs 1874\n",
      "training loss 0.006453483469152999\n",
      "epochs 1875\n",
      "training loss 0.006570658938093026\n",
      "epochs 1876\n",
      "training loss 0.0066422907657508025\n",
      "epochs 1877\n",
      "training loss 0.006577267274713444\n",
      "epochs 1878\n",
      "training loss 0.006577506048907738\n",
      "epochs 1879\n",
      "training loss 0.00656705267343806\n",
      "testing loss 0.007483714086776401\n",
      "epochs 1880\n",
      "training loss 0.006556660526766128\n",
      "epochs 1881\n",
      "training loss 0.006524369137936291\n",
      "epochs 1882\n",
      "training loss 0.006595219363384855\n",
      "epochs 1883\n",
      "training loss 0.006571666933426288\n",
      "epochs 1884\n",
      "training loss 0.006537987491452789\n",
      "epochs 1885\n",
      "training loss 0.006515182872829353\n",
      "epochs 1886\n",
      "training loss 0.006462987245550226\n",
      "epochs 1887\n",
      "training loss 0.006478240969307766\n",
      "epochs 1888\n",
      "training loss 0.006580844221461145\n",
      "epochs 1889\n",
      "training loss 0.00648681089950097\n",
      "testing loss 0.007343444974056003\n",
      "epochs 1890\n",
      "training loss 0.006576031376041563\n",
      "epochs 1891\n",
      "training loss 0.006557299387867713\n",
      "epochs 1892\n",
      "training loss 0.006591460040181672\n",
      "epochs 1893\n",
      "training loss 0.006532858113127005\n",
      "epochs 1894\n",
      "training loss 0.006602242516104619\n",
      "epochs 1895\n",
      "training loss 0.006577420715415301\n",
      "epochs 1896\n",
      "training loss 0.006577998209365369\n",
      "epochs 1897\n",
      "training loss 0.006602383102644179\n",
      "epochs 1898\n",
      "training loss 0.006622692932074714\n",
      "epochs 1899\n",
      "training loss 0.006631678458597613\n",
      "testing loss 0.0075203994824055665\n",
      "epochs 1900\n",
      "training loss 0.006568541118696807\n",
      "epochs 1901\n",
      "training loss 0.006553772983415828\n",
      "epochs 1902\n",
      "training loss 0.006636723463768904\n",
      "epochs 1903\n",
      "training loss 0.006573195072417473\n",
      "epochs 1904\n",
      "training loss 0.006548841598537102\n",
      "epochs 1905\n",
      "training loss 0.006575719742486565\n",
      "epochs 1906\n",
      "training loss 0.006549906053532221\n",
      "epochs 1907\n",
      "training loss 0.00649976842162004\n",
      "epochs 1908\n",
      "training loss 0.006607560792266983\n",
      "epochs 1909\n",
      "training loss 0.0066020352430471805\n",
      "testing loss 0.007947452622300661\n",
      "epochs 1910\n",
      "training loss 0.006556158741698303\n",
      "epochs 1911\n",
      "training loss 0.006619939068190776\n",
      "epochs 1912\n",
      "training loss 0.006603180630044802\n",
      "epochs 1913\n",
      "training loss 0.006625583087515913\n",
      "epochs 1914\n",
      "training loss 0.006539195599416583\n",
      "epochs 1915\n",
      "training loss 0.006567751874830177\n",
      "epochs 1916\n",
      "training loss 0.006557822803185409\n",
      "epochs 1917\n",
      "training loss 0.006635822641654761\n",
      "epochs 1918\n",
      "training loss 0.006476933168428511\n",
      "epochs 1919\n",
      "training loss 0.006551522766537172\n",
      "testing loss 0.008461040269636304\n",
      "epochs 1920\n",
      "training loss 0.006549849447731982\n",
      "epochs 1921\n",
      "training loss 0.006530365290256121\n",
      "epochs 1922\n",
      "training loss 0.006465791951791358\n",
      "epochs 1923\n",
      "training loss 0.006572352768640999\n",
      "epochs 1924\n",
      "training loss 0.006541786946449712\n",
      "epochs 1925\n",
      "training loss 0.006597445616753359\n",
      "epochs 1926\n",
      "training loss 0.006607588755298055\n",
      "epochs 1927\n",
      "training loss 0.006598362165551327\n",
      "epochs 1928\n",
      "training loss 0.006623159157392487\n",
      "epochs 1929\n",
      "training loss 0.006543927125018487\n",
      "testing loss 0.007326354117443164\n",
      "epochs 1930\n",
      "training loss 0.006592807848654732\n",
      "epochs 1931\n",
      "training loss 0.00659797189963874\n",
      "epochs 1932\n",
      "training loss 0.006550200421173454\n",
      "epochs 1933\n",
      "training loss 0.006470722664716599\n",
      "epochs 1934\n",
      "training loss 0.006564301852275558\n",
      "epochs 1935\n",
      "training loss 0.006581480445348768\n",
      "epochs 1936\n",
      "training loss 0.006655323564352349\n",
      "epochs 1937\n",
      "training loss 0.00658473946362179\n",
      "epochs 1938\n",
      "training loss 0.006556721227625697\n",
      "epochs 1939\n",
      "training loss 0.006522504830772572\n",
      "testing loss 0.00803589104536049\n",
      "epochs 1940\n",
      "training loss 0.006611629518026367\n",
      "epochs 1941\n",
      "training loss 0.0066393410651109505\n",
      "epochs 1942\n",
      "training loss 0.006586914506987145\n",
      "epochs 1943\n",
      "training loss 0.006524026217321246\n",
      "epochs 1944\n",
      "training loss 0.006576763805692681\n",
      "epochs 1945\n",
      "training loss 0.006556530820636085\n",
      "epochs 1946\n",
      "training loss 0.006589021005789469\n",
      "epochs 1947\n",
      "training loss 0.006572382243302025\n",
      "epochs 1948\n",
      "training loss 0.00653876242012714\n",
      "epochs 1949\n",
      "training loss 0.006574458248720796\n",
      "testing loss 0.00777592431549115\n",
      "epochs 1950\n",
      "training loss 0.00658851905370635\n",
      "epochs 1951\n",
      "training loss 0.006549610477153178\n",
      "epochs 1952\n",
      "training loss 0.006676830555167966\n",
      "epochs 1953\n",
      "training loss 0.006530554484969464\n",
      "epochs 1954\n",
      "training loss 0.006524894538683687\n",
      "epochs 1955\n",
      "training loss 0.006588349757874583\n",
      "epochs 1956\n",
      "training loss 0.006645615775234386\n",
      "epochs 1957\n",
      "training loss 0.006573852865693462\n",
      "epochs 1958\n",
      "training loss 0.006598509153164357\n",
      "epochs 1959\n",
      "training loss 0.006621620157099468\n",
      "testing loss 0.007353415611160404\n",
      "epochs 1960\n",
      "training loss 0.006572810360117275\n",
      "epochs 1961\n",
      "training loss 0.006641551490064914\n",
      "epochs 1962\n",
      "training loss 0.006562477664167124\n",
      "epochs 1963\n",
      "training loss 0.006530491519849578\n",
      "epochs 1964\n",
      "training loss 0.006593865956360967\n",
      "epochs 1965\n",
      "training loss 0.006624560492144371\n",
      "epochs 1966\n",
      "training loss 0.0065405698377061575\n",
      "epochs 1967\n",
      "training loss 0.006542172713564726\n",
      "epochs 1968\n",
      "training loss 0.006640728477439574\n",
      "epochs 1969\n",
      "training loss 0.006546386315538279\n",
      "testing loss 0.007428495282743206\n",
      "epochs 1970\n",
      "training loss 0.006552130984932207\n",
      "epochs 1971\n",
      "training loss 0.006610396383245303\n",
      "epochs 1972\n",
      "training loss 0.006592962836017294\n",
      "epochs 1973\n",
      "training loss 0.0065327005476427565\n",
      "epochs 1974\n",
      "training loss 0.006628373001934253\n",
      "epochs 1975\n",
      "training loss 0.00656017173884159\n",
      "epochs 1976\n",
      "training loss 0.006586281684620765\n",
      "epochs 1977\n",
      "training loss 0.006607859599613514\n",
      "epochs 1978\n",
      "training loss 0.006589542887184916\n",
      "epochs 1979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0065153523558456345\n",
      "testing loss 0.007498615735098191\n",
      "epochs 1980\n",
      "training loss 0.006650850143549698\n",
      "epochs 1981\n",
      "training loss 0.006473048650277467\n",
      "epochs 1982\n",
      "training loss 0.006721713028545279\n",
      "epochs 1983\n",
      "training loss 0.006537042846223821\n",
      "epochs 1984\n",
      "training loss 0.006613035922683209\n",
      "epochs 1985\n",
      "training loss 0.006601641686367137\n",
      "epochs 1986\n",
      "training loss 0.006655980077059798\n",
      "epochs 1987\n",
      "training loss 0.006522570492552542\n",
      "epochs 1988\n",
      "training loss 0.0065683105477134735\n",
      "epochs 1989\n",
      "training loss 0.006591425441123409\n",
      "testing loss 0.007949800478254861\n",
      "epochs 1990\n",
      "training loss 0.006563264355035131\n",
      "epochs 1991\n",
      "training loss 0.006604998007679987\n",
      "epochs 1992\n",
      "training loss 0.007105242167273961\n",
      "epochs 1993\n",
      "training loss 0.006526423837935784\n",
      "epochs 1994\n",
      "training loss 0.006602572277895993\n",
      "epochs 1995\n",
      "training loss 0.006559564907522731\n",
      "epochs 1996\n",
      "training loss 0.006522876303926065\n",
      "epochs 1997\n",
      "training loss 0.006534120037523311\n",
      "epochs 1998\n",
      "training loss 0.006660700865913647\n",
      "epochs 1999\n",
      "training loss 0.006557698826529199\n",
      "testing loss 0.007610702546187908\n",
      "epochs 2000\n",
      "training loss 0.006604224742789354\n",
      "epochs 2001\n",
      "training loss 0.006646599430211545\n",
      "epochs 2002\n",
      "training loss 0.006669725164046404\n",
      "epochs 2003\n",
      "training loss 0.006440044305001394\n",
      "epochs 2004\n",
      "training loss 0.006617829079051873\n",
      "epochs 2005\n",
      "training loss 0.006464650229930221\n",
      "epochs 2006\n",
      "training loss 0.006507568673151446\n",
      "epochs 2007\n",
      "training loss 0.006509820822382906\n",
      "epochs 2008\n",
      "training loss 0.0066353319861040105\n",
      "epochs 2009\n",
      "training loss 0.006588462291506196\n",
      "testing loss 0.0072592003477393845\n",
      "epochs 2010\n",
      "training loss 0.006624837367764269\n",
      "epochs 2011\n",
      "training loss 0.0066302715812025645\n",
      "epochs 2012\n",
      "training loss 0.006573835062995372\n",
      "epochs 2013\n",
      "training loss 0.0065422894738863665\n",
      "epochs 2014\n",
      "training loss 0.0065804735893995065\n",
      "epochs 2015\n",
      "training loss 0.006566372472158772\n",
      "epochs 2016\n",
      "training loss 0.0064540944164081\n",
      "epochs 2017\n",
      "training loss 0.006628934384569934\n",
      "epochs 2018\n",
      "training loss 0.006608224799431928\n",
      "epochs 2019\n",
      "training loss 0.006561941027041174\n",
      "testing loss 0.007811833475221028\n",
      "epochs 2020\n",
      "training loss 0.006572272679853426\n",
      "epochs 2021\n",
      "training loss 0.00656785473006701\n",
      "epochs 2022\n",
      "training loss 0.006562371415547506\n",
      "epochs 2023\n",
      "training loss 0.006560199251780598\n",
      "epochs 2024\n",
      "training loss 0.00653522322874369\n",
      "epochs 2025\n",
      "training loss 0.006572990790066114\n",
      "epochs 2026\n",
      "training loss 0.006545153095834184\n",
      "epochs 2027\n",
      "training loss 0.006582304034212259\n",
      "epochs 2028\n",
      "training loss 0.0065779303031404555\n",
      "epochs 2029\n",
      "training loss 0.006513016789905319\n",
      "testing loss 0.007894237720612304\n",
      "epochs 2030\n",
      "training loss 0.006608224822785762\n",
      "epochs 2031\n",
      "training loss 0.006509830061301\n",
      "epochs 2032\n",
      "training loss 0.006589916323475812\n",
      "epochs 2033\n",
      "training loss 0.006572073645391544\n",
      "epochs 2034\n",
      "training loss 0.006541938924352358\n",
      "epochs 2035\n",
      "training loss 0.006531036196001961\n",
      "epochs 2036\n",
      "training loss 0.006606099935316433\n",
      "epochs 2037\n",
      "training loss 0.00666120476167264\n",
      "epochs 2038\n",
      "training loss 0.006570886278108872\n",
      "epochs 2039\n",
      "training loss 0.006669512918391647\n",
      "testing loss 0.007638490050436652\n",
      "epochs 2040\n",
      "training loss 0.0066272365803317955\n",
      "epochs 2041\n",
      "training loss 0.006537685880938539\n",
      "epochs 2042\n",
      "training loss 0.0065435746717931965\n",
      "epochs 2043\n",
      "training loss 0.006457558599170467\n",
      "epochs 2044\n",
      "training loss 0.00652371043067618\n",
      "epochs 2045\n",
      "training loss 0.006549743618065984\n",
      "epochs 2046\n",
      "training loss 0.006622863723259521\n",
      "epochs 2047\n",
      "training loss 0.006636900744488672\n",
      "epochs 2048\n",
      "training loss 0.00658357190003613\n",
      "epochs 2049\n",
      "training loss 0.006490934342230843\n",
      "testing loss 0.00734742957411717\n",
      "epochs 2050\n",
      "training loss 0.006528575614867534\n",
      "epochs 2051\n",
      "training loss 0.006590433732612132\n",
      "epochs 2052\n",
      "training loss 0.006607908189033461\n",
      "epochs 2053\n",
      "training loss 0.006656411447842524\n",
      "epochs 2054\n",
      "training loss 0.0066562943998441145\n",
      "epochs 2055\n",
      "training loss 0.006416588365298478\n",
      "epochs 2056\n",
      "training loss 0.006565166853147482\n",
      "epochs 2057\n",
      "training loss 0.006465922885059201\n",
      "epochs 2058\n",
      "training loss 0.006585277939331364\n",
      "epochs 2059\n",
      "training loss 0.006518934560390501\n",
      "testing loss 0.007657322388617916\n",
      "epochs 2060\n",
      "training loss 0.006544730865600375\n",
      "epochs 2061\n",
      "training loss 0.006493006435528647\n",
      "epochs 2062\n",
      "training loss 0.006516600149757254\n",
      "epochs 2063\n",
      "training loss 0.006549412331552355\n",
      "epochs 2064\n",
      "training loss 0.006531761248672443\n",
      "epochs 2065\n",
      "training loss 0.006491865734866959\n",
      "epochs 2066\n",
      "training loss 0.006613767223565706\n",
      "epochs 2067\n",
      "training loss 0.006548140095730842\n",
      "epochs 2068\n",
      "training loss 0.006600767203838788\n",
      "epochs 2069\n",
      "training loss 0.0065789436486246786\n",
      "testing loss 0.007552793315539123\n",
      "epochs 2070\n",
      "training loss 0.00652147285954455\n",
      "epochs 2071\n",
      "training loss 0.006448261254738824\n",
      "epochs 2072\n",
      "training loss 0.006611523353268242\n",
      "epochs 2073\n",
      "training loss 0.006579351190712564\n",
      "epochs 2074\n",
      "training loss 0.00657489412927032\n",
      "epochs 2075\n",
      "training loss 0.0066123046472776215\n",
      "epochs 2076\n",
      "training loss 0.0064909139677802935\n",
      "epochs 2077\n",
      "training loss 0.006583715113881964\n",
      "epochs 2078\n",
      "training loss 0.00657001654914719\n",
      "epochs 2079\n",
      "training loss 0.006532754468459828\n",
      "testing loss 0.007298016452547559\n",
      "epochs 2080\n",
      "training loss 0.006537613227870032\n",
      "epochs 2081\n",
      "training loss 0.006620007544815382\n",
      "epochs 2082\n",
      "training loss 0.0065266926759446325\n",
      "epochs 2083\n",
      "training loss 0.006578802149162535\n",
      "epochs 2084\n",
      "training loss 0.0065520094391977625\n",
      "epochs 2085\n",
      "training loss 0.006547446140812817\n",
      "epochs 2086\n",
      "training loss 0.006550526558166783\n",
      "epochs 2087\n",
      "training loss 0.006515080169392438\n",
      "epochs 2088\n",
      "training loss 0.006573603232674256\n",
      "epochs 2089\n",
      "training loss 0.006436556318313404\n",
      "testing loss 0.007489106814370405\n",
      "epochs 2090\n",
      "training loss 0.006537248093513121\n",
      "epochs 2091\n",
      "training loss 0.006572873110806507\n",
      "epochs 2092\n",
      "training loss 0.006588919383351971\n",
      "epochs 2093\n",
      "training loss 0.006558423612045979\n",
      "epochs 2094\n",
      "training loss 0.006547905417657414\n",
      "epochs 2095\n",
      "training loss 0.00661716512425613\n",
      "epochs 2096\n",
      "training loss 0.006508310174351518\n",
      "epochs 2097\n",
      "training loss 0.0066195022740109756\n",
      "epochs 2098\n",
      "training loss 0.006582343672744726\n",
      "epochs 2099\n",
      "training loss 0.006583541512452519\n",
      "testing loss 0.008099510595334548\n",
      "epochs 2100\n",
      "training loss 0.00659217212262838\n",
      "epochs 2101\n",
      "training loss 0.006594381319913775\n",
      "epochs 2102\n",
      "training loss 0.006615113314470463\n",
      "epochs 2103\n",
      "training loss 0.00655184333257966\n",
      "epochs 2104\n",
      "training loss 0.006501715054231665\n",
      "epochs 2105\n",
      "training loss 0.006562562484582915\n",
      "epochs 2106\n",
      "training loss 0.006558796554014064\n",
      "epochs 2107\n",
      "training loss 0.006637369335266707\n",
      "epochs 2108\n",
      "training loss 0.006612891623591806\n",
      "epochs 2109\n",
      "training loss 0.006598178466419635\n",
      "testing loss 0.007537103000473469\n",
      "epochs 2110\n",
      "training loss 0.006492851224181591\n",
      "epochs 2111\n",
      "training loss 0.006610943194150381\n",
      "epochs 2112\n",
      "training loss 0.0065505907401629355\n",
      "epochs 2113\n",
      "training loss 0.006529444944690969\n",
      "epochs 2114\n",
      "training loss 0.006641666859594998\n",
      "epochs 2115\n",
      "training loss 0.006550930345595155\n",
      "epochs 2116\n",
      "training loss 0.006632474099645408\n",
      "epochs 2117\n",
      "training loss 0.00660358843220947\n",
      "epochs 2118\n",
      "training loss 0.00655345344812663\n",
      "epochs 2119\n",
      "training loss 0.006585167749929256\n",
      "testing loss 0.00736694424726888\n",
      "epochs 2120\n",
      "training loss 0.006511944641237427\n",
      "epochs 2121\n",
      "training loss 0.006516312402927835\n",
      "epochs 2122\n",
      "training loss 0.006611521968315143\n",
      "epochs 2123\n",
      "training loss 0.006582149513572522\n",
      "epochs 2124\n",
      "training loss 0.006626098916730008\n",
      "epochs 2125\n",
      "training loss 0.0065889405996020665\n",
      "epochs 2126\n",
      "training loss 0.006647202523147806\n",
      "epochs 2127\n",
      "training loss 0.00658396721957676\n",
      "epochs 2128\n",
      "training loss 0.006485072204115656\n",
      "epochs 2129\n",
      "training loss 0.006537879628234042\n",
      "testing loss 0.007322573396938701\n",
      "epochs 2130\n",
      "training loss 0.0065902545096318725\n",
      "epochs 2131\n",
      "training loss 0.006547412648230882\n",
      "epochs 2132\n",
      "training loss 0.006555448313127626\n",
      "epochs 2133\n",
      "training loss 0.006544248337537548\n",
      "epochs 2134\n",
      "training loss 0.006575456249674833\n",
      "epochs 2135\n",
      "training loss 0.006516506929745994\n",
      "epochs 2136\n",
      "training loss 0.006538388638128739\n",
      "epochs 2137\n",
      "training loss 0.006597866940079964\n",
      "epochs 2138\n",
      "training loss 0.0065973436213622155\n",
      "epochs 2139\n",
      "training loss 0.00655326671971965\n",
      "testing loss 0.007598501562424902\n",
      "epochs 2140\n",
      "training loss 0.006655260308371081\n",
      "epochs 2141\n",
      "training loss 0.006622072147901841\n",
      "epochs 2142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006593114908750659\n",
      "epochs 2143\n",
      "training loss 0.006473070949296209\n",
      "epochs 2144\n",
      "training loss 0.006633079067611427\n",
      "epochs 2145\n",
      "training loss 0.006608958082913356\n",
      "epochs 2146\n",
      "training loss 0.006623123713350065\n",
      "epochs 2147\n",
      "training loss 0.0065934632333012515\n",
      "epochs 2148\n",
      "training loss 0.006559236708315129\n",
      "epochs 2149\n",
      "training loss 0.0065649735428517444\n",
      "testing loss 0.007321690527081542\n",
      "epochs 2150\n",
      "training loss 0.006515179317738963\n",
      "epochs 2151\n",
      "training loss 0.006599185383584248\n",
      "epochs 2152\n",
      "training loss 0.00659903531928057\n",
      "epochs 2153\n",
      "training loss 0.006660433270257974\n",
      "epochs 2154\n",
      "training loss 0.006605347275583917\n",
      "epochs 2155\n",
      "training loss 0.006602594519945535\n",
      "epochs 2156\n",
      "training loss 0.006515201355619385\n",
      "epochs 2157\n",
      "training loss 0.006498573923633522\n",
      "epochs 2158\n",
      "training loss 0.006626216178805225\n",
      "epochs 2159\n",
      "training loss 0.0065542602140915025\n",
      "testing loss 0.00735806207938767\n",
      "epochs 2160\n",
      "training loss 0.006575050932216517\n",
      "epochs 2161\n",
      "training loss 0.006570844792854324\n",
      "epochs 2162\n",
      "training loss 0.0066060248177107934\n",
      "epochs 2163\n",
      "training loss 0.006541281968275917\n",
      "epochs 2164\n",
      "training loss 0.0065688501865147275\n",
      "epochs 2165\n",
      "training loss 0.006641529971099598\n",
      "epochs 2166\n",
      "training loss 0.006568941559450061\n",
      "epochs 2167\n",
      "training loss 0.006594603869562975\n",
      "epochs 2168\n",
      "training loss 0.006516049388869691\n",
      "epochs 2169\n",
      "training loss 0.006473022334045507\n",
      "testing loss 0.007304223434312652\n",
      "epochs 2170\n",
      "training loss 0.006586078187099751\n",
      "epochs 2171\n",
      "training loss 0.006644962757817359\n",
      "epochs 2172\n",
      "training loss 0.006561883384118417\n",
      "epochs 2173\n",
      "training loss 0.006534058674617945\n",
      "epochs 2174\n",
      "training loss 0.006611062890331977\n",
      "epochs 2175\n",
      "training loss 0.006495734681672556\n",
      "epochs 2176\n",
      "training loss 0.00657816309167644\n",
      "epochs 2177\n",
      "training loss 0.00653919211544923\n",
      "epochs 2178\n",
      "training loss 0.006539803346842641\n",
      "epochs 2179\n",
      "training loss 0.006520095884964142\n",
      "testing loss 0.007746121068386004\n",
      "epochs 2180\n",
      "training loss 0.006622637211597075\n",
      "epochs 2181\n",
      "training loss 0.0065623418464096075\n",
      "epochs 2182\n",
      "training loss 0.00659026857076275\n",
      "epochs 2183\n",
      "training loss 0.00664822230119779\n",
      "epochs 2184\n",
      "training loss 0.006551706962114749\n",
      "epochs 2185\n",
      "training loss 0.006586748598168935\n",
      "epochs 2186\n",
      "training loss 0.006518088531323728\n",
      "epochs 2187\n",
      "training loss 0.006586290918231169\n",
      "epochs 2188\n",
      "training loss 0.006598699855615348\n",
      "epochs 2189\n",
      "training loss 0.00661773643333261\n",
      "testing loss 0.007397543814806069\n",
      "epochs 2190\n",
      "training loss 0.006623665338642556\n",
      "epochs 2191\n",
      "training loss 0.0066425998923071525\n",
      "epochs 2192\n",
      "training loss 0.006536478696866555\n",
      "epochs 2193\n",
      "training loss 0.006621696649750519\n",
      "epochs 2194\n",
      "training loss 0.006670384081844863\n",
      "epochs 2195\n",
      "training loss 0.00655267786200648\n",
      "epochs 2196\n",
      "training loss 0.0065874633210142195\n",
      "epochs 2197\n",
      "training loss 0.0066065447646303605\n",
      "epochs 2198\n",
      "training loss 0.0065166147830570545\n",
      "epochs 2199\n",
      "training loss 0.006578424788366064\n",
      "testing loss 0.007464808075034872\n",
      "epochs 2200\n",
      "training loss 0.006540012978283423\n",
      "epochs 2201\n",
      "training loss 0.00660630780531201\n",
      "epochs 2202\n",
      "training loss 0.006540821391047406\n",
      "epochs 2203\n",
      "training loss 0.006614506463336229\n",
      "epochs 2204\n",
      "training loss 0.006576914528503697\n",
      "epochs 2205\n",
      "training loss 0.006566263505648312\n",
      "epochs 2206\n",
      "training loss 0.006533318791909308\n",
      "epochs 2207\n",
      "training loss 0.006526013960110697\n",
      "epochs 2208\n",
      "training loss 0.006532417824010628\n",
      "epochs 2209\n",
      "training loss 0.006612004672947105\n",
      "testing loss 0.008397150036700546\n",
      "epochs 2210\n",
      "training loss 0.006623694429734557\n",
      "epochs 2211\n",
      "training loss 0.006483655046079999\n",
      "epochs 2212\n",
      "training loss 0.006598414178784812\n",
      "epochs 2213\n",
      "training loss 0.006464525881735749\n",
      "epochs 2214\n",
      "training loss 0.006590365374525924\n",
      "epochs 2215\n",
      "training loss 0.006606637281903538\n",
      "epochs 2216\n",
      "training loss 0.006585585628211566\n",
      "epochs 2217\n",
      "training loss 0.006503937168663939\n",
      "epochs 2218\n",
      "training loss 0.006647194641936673\n",
      "epochs 2219\n",
      "training loss 0.006554983854562746\n",
      "testing loss 0.00741237196708032\n",
      "epochs 2220\n",
      "training loss 0.006586371156695585\n",
      "epochs 2221\n",
      "training loss 0.006634577053972721\n",
      "epochs 2222\n",
      "training loss 0.006612809607405272\n",
      "epochs 2223\n",
      "training loss 0.006568266702660198\n",
      "epochs 2224\n",
      "training loss 0.006487906716571481\n",
      "epochs 2225\n",
      "training loss 0.006611014217758228\n",
      "epochs 2226\n",
      "training loss 0.006567334850732022\n",
      "epochs 2227\n",
      "training loss 0.006603096743428508\n",
      "epochs 2228\n",
      "training loss 0.0066141630218599255\n",
      "epochs 2229\n",
      "training loss 0.006509544103870616\n",
      "testing loss 0.007486540612810892\n",
      "epochs 2230\n",
      "training loss 0.006606158115377361\n",
      "epochs 2231\n",
      "training loss 0.006526781149466146\n",
      "epochs 2232\n",
      "training loss 0.006565655536006989\n",
      "epochs 2233\n",
      "training loss 0.006582066436909637\n",
      "epochs 2234\n",
      "training loss 0.006655672741671712\n",
      "epochs 2235\n",
      "training loss 0.006564638989632185\n",
      "epochs 2236\n",
      "training loss 0.006633356804846245\n",
      "epochs 2237\n",
      "training loss 0.00657985228578076\n",
      "epochs 2238\n",
      "training loss 0.006550592973638654\n",
      "epochs 2239\n",
      "training loss 0.0066103324492951705\n",
      "testing loss 0.00746564858953026\n",
      "epochs 2240\n",
      "training loss 0.006581606754203721\n",
      "epochs 2241\n",
      "training loss 0.006601732275533663\n",
      "epochs 2242\n",
      "training loss 0.0065860455536558135\n",
      "epochs 2243\n",
      "training loss 0.006616629439329666\n",
      "epochs 2244\n",
      "training loss 0.006593364875446608\n",
      "epochs 2245\n",
      "training loss 0.006581629192283926\n",
      "epochs 2246\n",
      "training loss 0.006613195794535444\n",
      "epochs 2247\n",
      "training loss 0.0065946471803088455\n",
      "epochs 2248\n",
      "training loss 0.006560047103678043\n",
      "epochs 2249\n",
      "training loss 0.006544287653362497\n",
      "testing loss 0.007903945450637991\n",
      "epochs 2250\n",
      "training loss 0.006526723654451002\n",
      "epochs 2251\n",
      "training loss 0.006598121964296264\n",
      "epochs 2252\n",
      "training loss 0.006592889981610435\n",
      "epochs 2253\n",
      "training loss 0.0066333159384682865\n",
      "epochs 2254\n",
      "training loss 0.006589452617895141\n",
      "epochs 2255\n",
      "training loss 0.006531144180589874\n",
      "epochs 2256\n",
      "training loss 0.006654950902276421\n",
      "epochs 2257\n",
      "training loss 0.006525506683353527\n",
      "epochs 2258\n",
      "training loss 0.0064921935773978295\n",
      "epochs 2259\n",
      "training loss 0.006631895067173093\n",
      "testing loss 0.007193884912197947\n",
      "epochs 2260\n",
      "training loss 0.0066286647047615384\n",
      "epochs 2261\n",
      "training loss 0.006625234360996306\n",
      "epochs 2262\n",
      "training loss 0.0065298954390787655\n",
      "epochs 2263\n",
      "training loss 0.006542304584878217\n",
      "epochs 2264\n",
      "training loss 0.006599060060897995\n",
      "epochs 2265\n",
      "training loss 0.006569179741306213\n",
      "epochs 2266\n",
      "training loss 0.006589934655881309\n",
      "epochs 2267\n",
      "training loss 0.006611681065952411\n",
      "epochs 2268\n",
      "training loss 0.006588199809278534\n",
      "epochs 2269\n",
      "training loss 0.006626871942066925\n",
      "testing loss 0.007657906706015586\n",
      "epochs 2270\n",
      "training loss 0.006551191720284952\n",
      "epochs 2271\n",
      "training loss 0.006542551439145076\n",
      "epochs 2272\n",
      "training loss 0.006509477493783465\n",
      "epochs 2273\n",
      "training loss 0.006590137353710445\n",
      "epochs 2274\n",
      "training loss 0.006591012634161441\n",
      "epochs 2275\n",
      "training loss 0.006553796598033861\n",
      "epochs 2276\n",
      "training loss 0.006576326119113363\n",
      "epochs 2277\n",
      "training loss 0.0066039551617039826\n",
      "epochs 2278\n",
      "training loss 0.006586414653564432\n",
      "epochs 2279\n",
      "training loss 0.006602881119960189\n",
      "testing loss 0.007414637512448145\n",
      "epochs 2280\n",
      "training loss 0.00657045177968168\n",
      "epochs 2281\n",
      "training loss 0.006628882229444217\n",
      "epochs 2282\n",
      "training loss 0.006483559167977864\n",
      "epochs 2283\n",
      "training loss 0.006538316815536707\n",
      "epochs 2284\n",
      "training loss 0.006609217401758108\n",
      "epochs 2285\n",
      "training loss 0.006529758518737229\n",
      "epochs 2286\n",
      "training loss 0.006535730022854967\n",
      "epochs 2287\n",
      "training loss 0.006507263935700421\n",
      "epochs 2288\n",
      "training loss 0.006546646187444272\n",
      "epochs 2289\n",
      "training loss 0.0065885759970756375\n",
      "testing loss 0.007791241386522215\n",
      "epochs 2290\n",
      "training loss 0.006595592715814257\n",
      "epochs 2291\n",
      "training loss 0.0064931329910142355\n",
      "epochs 2292\n",
      "training loss 0.006546430048068538\n",
      "epochs 2293\n",
      "training loss 0.00661415607161752\n",
      "epochs 2294\n",
      "training loss 0.006631010162819596\n",
      "epochs 2295\n",
      "training loss 0.006565056087684147\n",
      "epochs 2296\n",
      "training loss 0.006561513048225849\n",
      "epochs 2297\n",
      "training loss 0.006634569788807409\n",
      "epochs 2298\n",
      "training loss 0.0065485700628066315\n",
      "epochs 2299\n",
      "training loss 0.006527543528993419\n",
      "testing loss 0.007487837513347308\n",
      "epochs 2300\n",
      "training loss 0.006553243729639742\n",
      "epochs 2301\n",
      "training loss 0.006518204990876301\n",
      "epochs 2302\n",
      "training loss 0.006627425937460435\n",
      "epochs 2303\n",
      "training loss 0.006612564825260372\n",
      "epochs 2304\n",
      "training loss 0.006586211347827931\n",
      "epochs 2305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006544331220292846\n",
      "epochs 2306\n",
      "training loss 0.006570670130948528\n",
      "epochs 2307\n",
      "training loss 0.006518573640339009\n",
      "epochs 2308\n",
      "training loss 0.006496676977010484\n",
      "epochs 2309\n",
      "training loss 0.006596613949940378\n",
      "testing loss 0.008040167478818102\n",
      "epochs 2310\n",
      "training loss 0.006582076929503978\n",
      "epochs 2311\n",
      "training loss 0.00651763128449342\n",
      "epochs 2312\n",
      "training loss 0.006543222675383046\n",
      "epochs 2313\n",
      "training loss 0.006594665145776079\n",
      "epochs 2314\n",
      "training loss 0.006647245380263153\n",
      "epochs 2315\n",
      "training loss 0.006646745746578783\n",
      "epochs 2316\n",
      "training loss 0.006581970722284338\n",
      "epochs 2317\n",
      "training loss 0.006638756405426285\n",
      "epochs 2318\n",
      "training loss 0.006524686508395887\n",
      "epochs 2319\n",
      "training loss 0.0065036894361514\n",
      "testing loss 0.008458868585566574\n",
      "epochs 2320\n",
      "training loss 0.006596680670781316\n",
      "epochs 2321\n",
      "training loss 0.006564613487599786\n",
      "epochs 2322\n",
      "training loss 0.006509892076698322\n",
      "epochs 2323\n",
      "training loss 0.0065850583821574745\n",
      "epochs 2324\n",
      "training loss 0.006541220177570782\n",
      "epochs 2325\n",
      "training loss 0.006618128738459483\n",
      "epochs 2326\n",
      "training loss 0.006589229305707892\n",
      "epochs 2327\n",
      "training loss 0.006608296051978114\n",
      "epochs 2328\n",
      "training loss 0.006569620844975997\n",
      "epochs 2329\n",
      "training loss 0.006635274665888773\n",
      "testing loss 0.007628597773228448\n",
      "epochs 2330\n",
      "training loss 0.006566027312760318\n",
      "epochs 2331\n",
      "training loss 0.006553486869585528\n",
      "epochs 2332\n",
      "training loss 0.006584571976650586\n",
      "epochs 2333\n",
      "training loss 0.006580644413491988\n",
      "epochs 2334\n",
      "training loss 0.00654355905480162\n",
      "epochs 2335\n",
      "training loss 0.006502825858447939\n",
      "epochs 2336\n",
      "training loss 0.006564313445330844\n",
      "epochs 2337\n",
      "training loss 0.0064062863884856045\n",
      "epochs 2338\n",
      "training loss 0.006555907575110692\n",
      "epochs 2339\n",
      "training loss 0.006505554073453402\n",
      "testing loss 0.007372063785256382\n",
      "epochs 2340\n",
      "training loss 0.006608342891275928\n",
      "epochs 2341\n",
      "training loss 0.006566148195033581\n",
      "epochs 2342\n",
      "training loss 0.006546774979944038\n",
      "epochs 2343\n",
      "training loss 0.006594455128643198\n",
      "epochs 2344\n",
      "training loss 0.006607361122297752\n",
      "epochs 2345\n",
      "training loss 0.006485359321099261\n",
      "epochs 2346\n",
      "training loss 0.006547855957422553\n",
      "epochs 2347\n",
      "training loss 0.006635106050856638\n",
      "epochs 2348\n",
      "training loss 0.006476209973449335\n",
      "epochs 2349\n",
      "training loss 0.006544321933251701\n",
      "testing loss 0.0075917059113106405\n",
      "epochs 2350\n",
      "training loss 0.006480033311987222\n",
      "epochs 2351\n",
      "training loss 0.006510687064546469\n",
      "epochs 2352\n",
      "training loss 0.006539804970995611\n",
      "epochs 2353\n",
      "training loss 0.006578854032888397\n",
      "epochs 2354\n",
      "training loss 0.006528351427973064\n",
      "epochs 2355\n",
      "training loss 0.006522847575526175\n",
      "epochs 2356\n",
      "training loss 0.006607171630707646\n",
      "epochs 2357\n",
      "training loss 0.006572676652697082\n",
      "epochs 2358\n",
      "training loss 0.006476898023032082\n",
      "epochs 2359\n",
      "training loss 0.006576057451304422\n",
      "testing loss 0.007367673795670271\n",
      "epochs 2360\n",
      "training loss 0.006616668411924031\n",
      "epochs 2361\n",
      "training loss 0.006641968598555332\n",
      "epochs 2362\n",
      "training loss 0.006594599534949928\n",
      "epochs 2363\n",
      "training loss 0.006598157163831125\n",
      "epochs 2364\n",
      "training loss 0.006654604981047608\n",
      "epochs 2365\n",
      "training loss 0.006597685825085341\n",
      "epochs 2366\n",
      "training loss 0.006588383889148482\n",
      "epochs 2367\n",
      "training loss 0.006567247237410046\n",
      "epochs 2368\n",
      "training loss 0.0065863245824283965\n",
      "epochs 2369\n",
      "training loss 0.00647691595134549\n",
      "testing loss 0.007541707133979979\n",
      "epochs 2370\n",
      "training loss 0.006585355321197321\n",
      "epochs 2371\n",
      "training loss 0.006518689840876336\n",
      "epochs 2372\n",
      "training loss 0.006631137852735342\n",
      "epochs 2373\n",
      "training loss 0.006596028188733231\n",
      "epochs 2374\n",
      "training loss 0.0066216509600982385\n",
      "epochs 2375\n",
      "training loss 0.0066147307804460135\n",
      "epochs 2376\n",
      "training loss 0.006542799870149546\n",
      "epochs 2377\n",
      "training loss 0.0065906888785514425\n",
      "epochs 2378\n",
      "training loss 0.006597589623567997\n",
      "epochs 2379\n",
      "training loss 0.006576904346232266\n",
      "testing loss 0.007283115847009822\n",
      "epochs 2380\n",
      "training loss 0.0066554840614504\n",
      "epochs 2381\n",
      "training loss 0.0066056542337722055\n",
      "epochs 2382\n",
      "training loss 0.006638709411497785\n",
      "epochs 2383\n",
      "training loss 0.006623835738119882\n",
      "epochs 2384\n",
      "training loss 0.006589064335289176\n",
      "epochs 2385\n",
      "training loss 0.006555324550194377\n",
      "epochs 2386\n",
      "training loss 0.006601780732261374\n",
      "epochs 2387\n",
      "training loss 0.006527172218609069\n",
      "epochs 2388\n",
      "training loss 0.006572873544975504\n",
      "epochs 2389\n",
      "training loss 0.006580795767210356\n",
      "testing loss 0.007459764825411352\n",
      "epochs 2390\n",
      "training loss 0.006585028645650436\n",
      "epochs 2391\n",
      "training loss 0.006608564946602316\n",
      "epochs 2392\n",
      "training loss 0.006510693091958603\n",
      "epochs 2393\n",
      "training loss 0.006570215405624552\n",
      "epochs 2394\n",
      "training loss 0.006509424869811847\n",
      "epochs 2395\n",
      "training loss 0.006605298502871761\n",
      "epochs 2396\n",
      "training loss 0.006600343615544173\n",
      "epochs 2397\n",
      "training loss 0.0065706849772635975\n",
      "epochs 2398\n",
      "training loss 0.006517788610423952\n",
      "epochs 2399\n",
      "training loss 0.006558102001804055\n",
      "testing loss 0.007392186636818533\n",
      "epochs 2400\n",
      "training loss 0.006572794702433412\n",
      "epochs 2401\n",
      "training loss 0.006626428442859971\n",
      "epochs 2402\n",
      "training loss 0.0065735049335580425\n",
      "epochs 2403\n",
      "training loss 0.0065860868856950595\n",
      "epochs 2404\n",
      "training loss 0.0065133345871031464\n",
      "epochs 2405\n",
      "training loss 0.006546408119172673\n",
      "epochs 2406\n",
      "training loss 0.006554114186816415\n",
      "epochs 2407\n",
      "training loss 0.006610618278987162\n",
      "epochs 2408\n",
      "training loss 0.006484809602288059\n",
      "epochs 2409\n",
      "training loss 0.006584668609152313\n",
      "testing loss 0.007284872986851854\n",
      "epochs 2410\n",
      "training loss 0.006534116756663541\n",
      "epochs 2411\n",
      "training loss 0.006485240699547855\n",
      "epochs 2412\n",
      "training loss 0.006544840230541389\n",
      "epochs 2413\n",
      "training loss 0.006521879324171477\n",
      "epochs 2414\n",
      "training loss 0.006626513801121625\n",
      "epochs 2415\n",
      "training loss 0.0065105186239603945\n",
      "epochs 2416\n",
      "training loss 0.006518244104301359\n",
      "epochs 2417\n",
      "training loss 0.006496933538333646\n",
      "epochs 2418\n",
      "training loss 0.006612388381740152\n",
      "epochs 2419\n",
      "training loss 0.006586899707379743\n",
      "testing loss 0.007352359039735392\n",
      "epochs 2420\n",
      "training loss 0.006582010440432147\n",
      "epochs 2421\n",
      "training loss 0.006576997776781874\n",
      "epochs 2422\n",
      "training loss 0.006521972361244375\n",
      "epochs 2423\n",
      "training loss 0.006605566621157921\n",
      "epochs 2424\n",
      "training loss 0.00653324155299712\n",
      "epochs 2425\n",
      "training loss 0.006646561885385641\n",
      "epochs 2426\n",
      "training loss 0.006531350470663226\n",
      "epochs 2427\n",
      "training loss 0.006602783001659262\n",
      "epochs 2428\n",
      "training loss 0.006544928286878511\n",
      "epochs 2429\n",
      "training loss 0.006579588044584817\n",
      "testing loss 0.0074119875810261\n",
      "epochs 2430\n",
      "training loss 0.006612836840805969\n",
      "epochs 2431\n",
      "training loss 0.006575726377806063\n",
      "epochs 2432\n",
      "training loss 0.006661551034978186\n",
      "epochs 2433\n",
      "training loss 0.006594878372999537\n",
      "epochs 2434\n",
      "training loss 0.00652427397566454\n",
      "epochs 2435\n",
      "training loss 0.006563795997625313\n",
      "epochs 2436\n",
      "training loss 0.006492753382234362\n",
      "epochs 2437\n",
      "training loss 0.0065469025173521735\n",
      "epochs 2438\n",
      "training loss 0.006560080881813671\n",
      "epochs 2439\n",
      "training loss 0.006541776350178504\n",
      "testing loss 0.007287694616837704\n",
      "epochs 2440\n",
      "training loss 0.006472243348173344\n",
      "epochs 2441\n",
      "training loss 0.006551328300226672\n",
      "epochs 2442\n",
      "training loss 0.006575984251190057\n",
      "epochs 2443\n",
      "training loss 0.006527914982331537\n",
      "epochs 2444\n",
      "training loss 0.006542845664894078\n",
      "epochs 2445\n",
      "training loss 0.006566781887153339\n",
      "epochs 2446\n",
      "training loss 0.006536242621809267\n",
      "epochs 2447\n",
      "training loss 0.0065379391557403976\n",
      "epochs 2448\n",
      "training loss 0.006602143502927252\n",
      "epochs 2449\n",
      "training loss 0.0065051616876496505\n",
      "testing loss 0.007745767889068799\n",
      "epochs 2450\n",
      "training loss 0.00664265141971013\n",
      "epochs 2451\n",
      "training loss 0.006620633857123735\n",
      "epochs 2452\n",
      "training loss 0.006694548149412005\n",
      "epochs 2453\n",
      "training loss 0.006586063118569417\n",
      "epochs 2454\n",
      "training loss 0.006502901449853322\n",
      "epochs 2455\n",
      "training loss 0.0065906156844524584\n",
      "epochs 2456\n",
      "training loss 0.006529363210519668\n",
      "epochs 2457\n",
      "training loss 0.006572047657528638\n",
      "epochs 2458\n",
      "training loss 0.006586015822102618\n",
      "epochs 2459\n",
      "training loss 0.0066088221342328335\n",
      "testing loss 0.007778806251916268\n",
      "epochs 2460\n",
      "training loss 0.006543182797296861\n",
      "epochs 2461\n",
      "training loss 0.006610247247079586\n",
      "epochs 2462\n",
      "training loss 0.006559434873731325\n",
      "epochs 2463\n",
      "training loss 0.006585114617127144\n",
      "epochs 2464\n",
      "training loss 0.006574894315747142\n",
      "epochs 2465\n",
      "training loss 0.00661549008399033\n",
      "epochs 2466\n",
      "training loss 0.006589494275826519\n",
      "epochs 2467\n",
      "training loss 0.0064874217062792245\n",
      "epochs 2468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.00650646604171538\n",
      "epochs 2469\n",
      "training loss 0.0066436696514532555\n",
      "testing loss 0.007398105971184923\n",
      "epochs 2470\n",
      "training loss 0.006555121904380597\n",
      "epochs 2471\n",
      "training loss 0.0065728180778515735\n",
      "epochs 2472\n",
      "training loss 0.006546690485066533\n",
      "epochs 2473\n",
      "training loss 0.006550706527408179\n",
      "epochs 2474\n",
      "training loss 0.006488078775586373\n",
      "epochs 2475\n",
      "training loss 0.006607553895101472\n",
      "epochs 2476\n",
      "training loss 0.006555712309816009\n",
      "epochs 2477\n",
      "training loss 0.006510327182679032\n",
      "epochs 2478\n",
      "training loss 0.0065757112717680566\n",
      "epochs 2479\n",
      "training loss 0.006599405582988561\n",
      "testing loss 0.0072827006934593755\n",
      "epochs 2480\n",
      "training loss 0.006627495127438502\n",
      "epochs 2481\n",
      "training loss 0.006559160300940955\n",
      "epochs 2482\n",
      "training loss 0.006520279541988164\n",
      "epochs 2483\n",
      "training loss 0.006546256534039046\n",
      "epochs 2484\n",
      "training loss 0.006489825084675273\n",
      "epochs 2485\n",
      "training loss 0.0066044500913601195\n",
      "epochs 2486\n",
      "training loss 0.006536537351788766\n",
      "epochs 2487\n",
      "training loss 0.006600617317519629\n",
      "epochs 2488\n",
      "training loss 0.006562131275153948\n",
      "epochs 2489\n",
      "training loss 0.0066230510524969465\n",
      "testing loss 0.007286832857784544\n",
      "epochs 2490\n",
      "training loss 0.006567448697132158\n",
      "epochs 2491\n",
      "training loss 0.0065717349061742425\n",
      "epochs 2492\n",
      "training loss 0.006525823693951779\n",
      "epochs 2493\n",
      "training loss 0.006477075074813513\n",
      "epochs 2494\n",
      "training loss 0.00664705606028834\n",
      "epochs 2495\n",
      "training loss 0.006499648641576951\n",
      "epochs 2496\n",
      "training loss 0.006594226299643539\n",
      "epochs 2497\n",
      "training loss 0.006582591975303112\n",
      "epochs 2498\n",
      "training loss 0.006554187272638535\n",
      "epochs 2499\n",
      "training loss 0.006564148965696479\n",
      "testing loss 0.007324820650239468\n",
      "epochs 2500\n",
      "training loss 0.006535460883015512\n",
      "epochs 2501\n",
      "training loss 0.006502047373267814\n",
      "epochs 2502\n",
      "training loss 0.006538016540437122\n",
      "epochs 2503\n",
      "training loss 0.006529784075261906\n",
      "epochs 2504\n",
      "training loss 0.0066084898251043715\n",
      "epochs 2505\n",
      "training loss 0.006538238486425108\n",
      "epochs 2506\n",
      "training loss 0.006559815441079913\n",
      "epochs 2507\n",
      "training loss 0.006594535176738485\n",
      "epochs 2508\n",
      "training loss 0.006456491971914982\n",
      "epochs 2509\n",
      "training loss 0.0064936285211469715\n",
      "testing loss 0.00813352884658685\n",
      "epochs 2510\n",
      "training loss 0.0065682390085520575\n",
      "epochs 2511\n",
      "training loss 0.006520091931443197\n",
      "epochs 2512\n",
      "training loss 0.006574710056477292\n",
      "epochs 2513\n",
      "training loss 0.0065537100168805585\n",
      "epochs 2514\n",
      "training loss 0.006523208953454343\n",
      "epochs 2515\n",
      "training loss 0.006569816896516411\n",
      "epochs 2516\n",
      "training loss 0.006593359357926508\n",
      "epochs 2517\n",
      "training loss 0.006595787710416779\n",
      "epochs 2518\n",
      "training loss 0.006582926044461394\n",
      "epochs 2519\n",
      "training loss 0.006581456299607945\n",
      "testing loss 0.008070469279079995\n",
      "epochs 2520\n",
      "training loss 0.006587930373623854\n",
      "epochs 2521\n",
      "training loss 0.0065898703833005895\n",
      "epochs 2522\n",
      "training loss 0.0065875000594251745\n",
      "epochs 2523\n",
      "training loss 0.006487059819166715\n",
      "epochs 2524\n",
      "training loss 0.006646929904294843\n",
      "epochs 2525\n",
      "training loss 0.00662916761116722\n",
      "epochs 2526\n",
      "training loss 0.006590685000753534\n",
      "epochs 2527\n",
      "training loss 0.006469531652820509\n",
      "epochs 2528\n",
      "training loss 0.006582361872104141\n",
      "epochs 2529\n",
      "training loss 0.006596203309577242\n",
      "testing loss 0.007248576507566774\n",
      "epochs 2530\n",
      "training loss 0.006619247641856004\n",
      "epochs 2531\n",
      "training loss 0.00659159216414118\n",
      "epochs 2532\n",
      "training loss 0.006549277164871411\n",
      "epochs 2533\n",
      "training loss 0.006601370262098145\n",
      "epochs 2534\n",
      "training loss 0.006600576758987658\n",
      "epochs 2535\n",
      "training loss 0.006557838496607714\n",
      "epochs 2536\n",
      "training loss 0.006551669078304413\n",
      "epochs 2537\n",
      "training loss 0.006632572328700121\n",
      "epochs 2538\n",
      "training loss 0.0066077720659068785\n",
      "epochs 2539\n",
      "training loss 0.006493813534108724\n",
      "testing loss 0.007317094976108864\n",
      "epochs 2540\n",
      "training loss 0.006573952027486133\n",
      "epochs 2541\n",
      "training loss 0.0065498370751540394\n",
      "epochs 2542\n",
      "training loss 0.00646126464131008\n",
      "epochs 2543\n",
      "training loss 0.006623054363079777\n",
      "epochs 2544\n",
      "training loss 0.006539592117079644\n",
      "epochs 2545\n",
      "training loss 0.0066166871445293126\n",
      "epochs 2546\n",
      "training loss 0.006611399035150678\n",
      "epochs 2547\n",
      "training loss 0.006743791491914212\n",
      "epochs 2548\n",
      "training loss 0.006600378160464003\n",
      "epochs 2549\n",
      "training loss 0.006595162173738777\n",
      "testing loss 0.007367290393779622\n",
      "epochs 2550\n",
      "training loss 0.006548912476714259\n",
      "epochs 2551\n",
      "training loss 0.006545424246580247\n",
      "epochs 2552\n",
      "training loss 0.006577935857106691\n",
      "epochs 2553\n",
      "training loss 0.00652799108044434\n",
      "epochs 2554\n",
      "training loss 0.006565024184932123\n",
      "epochs 2555\n",
      "training loss 0.006521730899589991\n",
      "epochs 2556\n",
      "training loss 0.006639694443289587\n",
      "epochs 2557\n",
      "training loss 0.006479948402756095\n",
      "epochs 2558\n",
      "training loss 0.006583288044450765\n",
      "epochs 2559\n",
      "training loss 0.006492561611876254\n",
      "testing loss 0.008138761910154148\n",
      "epochs 2560\n",
      "training loss 0.006583689817080223\n",
      "epochs 2561\n",
      "training loss 0.006538151748164197\n",
      "epochs 2562\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "#accs = []\n",
    "for t in range(5000):\n",
    "    print('epochs', t)\n",
    "    train_loss = train_func(model, train_loader)\n",
    "    if (t+1) % 10 == 0:\n",
    "        eval_loss = eval_func(model, eval_loader)\n",
    "        #acc = accuracy(model)\n",
    "        \n",
    "        eval_losses.append(eval_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        #accs.append(acc)\n",
    "        #print('accuracy: ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa1d6916d68>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(eval_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa1d6936a58>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(eval_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "label = y_train.cpu()\n",
    "label = y_scaler.inverse_transform(label)\n",
    "with torch.no_grad():\n",
    "    pred = model(X_train)\n",
    "    pred = pred.cpu().numpy()\n",
    "    pred = y_scaler.inverse_transform(pred)\n",
    "    plot(label, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test_func(model, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>total_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X5gsdTWGS3W7JJQB</td>\n",
       "      <td>1.787341e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BTshNOJyKHnT2YIT</td>\n",
       "      <td>4.164898e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dhdymr0lV8N5kZOT</td>\n",
       "      <td>1.366612e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VEwyGGMcD56w5BOc</td>\n",
       "      <td>1.343023e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wmUeMoJZfsqaSX9b</td>\n",
       "      <td>1.932745e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EtBjGAHmHCe9t7TZ</td>\n",
       "      <td>5.281519e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hPNH34vmaZtvBtqc</td>\n",
       "      <td>1.480271e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wXjeI38bYDMJJwZC</td>\n",
       "      <td>1.103791e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fxZSGX6aPAFKU8W4</td>\n",
       "      <td>2.926602e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ewr0Fx6ign87OwaV</td>\n",
       "      <td>6.620901e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gHKurnEP4AowzsLg</td>\n",
       "      <td>1.666348e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PmLfTgY2FElLrTl0</td>\n",
       "      <td>8.056260e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>eM2NppIOwzW0o8iy</td>\n",
       "      <td>9.510944e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dxxwNun97NH4WTrZ</td>\n",
       "      <td>4.979172e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jykBfhh3vdeFUi3H</td>\n",
       "      <td>6.267576e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NlXbvdFfmJZf3L18</td>\n",
       "      <td>2.568831e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D7jaFWHCzSqLBwdt</td>\n",
       "      <td>1.908631e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>L10dBBdqGmemweSl</td>\n",
       "      <td>6.590401e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>OgB0AdiPKlElakKN</td>\n",
       "      <td>2.311245e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>StiWNN1GQrpPBOYt</td>\n",
       "      <td>3.890137e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>a016eMAVQKnfwMnt</td>\n",
       "      <td>3.130959e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gsCFcQHnOH3AKMcZ</td>\n",
       "      <td>7.562794e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>IbNsDXfsPwSuFpow</td>\n",
       "      <td>8.608911e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>EgAVWOVxD1Jy5YkE</td>\n",
       "      <td>2.583701e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BrKghvR76XdbQPnx</td>\n",
       "      <td>8.499508e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>a7fxkXTnUGWHUmKG</td>\n",
       "      <td>3.249687e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>WgzXa170DfpzpURE</td>\n",
       "      <td>1.055089e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>JPWqZbLq0VNC0yKI</td>\n",
       "      <td>1.426529e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>JQgTtbVstqFZwEK1</td>\n",
       "      <td>2.501520e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bCSDbEthlS3nSIor</td>\n",
       "      <td>4.044996e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>QL412tWF5RDIX7IO</td>\n",
       "      <td>2.855370e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>d3c2ceGtckONZzsr</td>\n",
       "      <td>1.074882e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>P1j8YRbxDAovumaI</td>\n",
       "      <td>1.087405e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>IxcBhEoFLcrI9TPr</td>\n",
       "      <td>3.645594e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>rKiV0KDbAl2myBQI</td>\n",
       "      <td>2.926602e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>GSdIXmKr0g5jQQcF</td>\n",
       "      <td>3.384842e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>Am6Wcg3TO64qvzd8</td>\n",
       "      <td>3.890967e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>RZqACAhkL4Tgw4Jr</td>\n",
       "      <td>9.674688e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>u7NKZfWoMUlZy9rJ</td>\n",
       "      <td>5.230691e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>C1BqV4MWH15rjAgz</td>\n",
       "      <td>5.239147e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>wz8A2UbwsgR0lXGJ</td>\n",
       "      <td>4.983484e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>MGJ8ABBTmC2yIaSm</td>\n",
       "      <td>9.950168e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>MjHL2HP1PGIp8aBt</td>\n",
       "      <td>2.409051e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>FMz7nnURFn85LaGt</td>\n",
       "      <td>5.450432e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>kydULx0r0G7OklRD</td>\n",
       "      <td>7.823972e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>nVNYRuk2fRbtlV00</td>\n",
       "      <td>2.945555e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>F8SGEOGPxrPfiRv2</td>\n",
       "      <td>2.375709e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>w7VMfiMvRb765ejK</td>\n",
       "      <td>3.244109e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>lgZWdUKliWt2y5sM</td>\n",
       "      <td>1.530564e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>TER8YrP9mw7UwWwr</td>\n",
       "      <td>6.472142e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>TXHk3oUpVsm5Cmag</td>\n",
       "      <td>5.305676e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>JtgDm9aQcGE9zELB</td>\n",
       "      <td>2.370131e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>wTQmcqbN0OCuSF1t</td>\n",
       "      <td>1.513394e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>WgsI1cBtzSfiWA1j</td>\n",
       "      <td>2.509481e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>qNgt1ajb5uVMKbqm</td>\n",
       "      <td>4.627620e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>UEeCDaAJzPwdKKKA</td>\n",
       "      <td>2.926602e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>i0fgbPaQsDWs7Q87</td>\n",
       "      <td>4.304960e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>YunNwAhcqkf6YclI</td>\n",
       "      <td>2.926602e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>A2NotxtRY9MYoWMl</td>\n",
       "      <td>5.816262e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>kKvgBXiA50gRmQhP</td>\n",
       "      <td>6.059150e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           building_id   total_price\n",
       "0     X5gsdTWGS3W7JJQB  1.787341e+07\n",
       "1     BTshNOJyKHnT2YIT  4.164898e+06\n",
       "2     dhdymr0lV8N5kZOT  1.366612e+07\n",
       "3     VEwyGGMcD56w5BOc  1.343023e+07\n",
       "4     wmUeMoJZfsqaSX9b  1.932745e+06\n",
       "5     EtBjGAHmHCe9t7TZ  5.281519e+06\n",
       "6     hPNH34vmaZtvBtqc  1.480271e+07\n",
       "7     wXjeI38bYDMJJwZC  1.103791e+07\n",
       "8     fxZSGX6aPAFKU8W4  2.926602e+06\n",
       "9     ewr0Fx6ign87OwaV  6.620901e+06\n",
       "10    gHKurnEP4AowzsLg  1.666348e+06\n",
       "11    PmLfTgY2FElLrTl0  8.056260e+06\n",
       "12    eM2NppIOwzW0o8iy  9.510944e+06\n",
       "13    dxxwNun97NH4WTrZ  4.979172e+06\n",
       "14    jykBfhh3vdeFUi3H  6.267576e+06\n",
       "15    NlXbvdFfmJZf3L18  2.568831e+07\n",
       "16    D7jaFWHCzSqLBwdt  1.908631e+06\n",
       "17    L10dBBdqGmemweSl  6.590401e+06\n",
       "18    OgB0AdiPKlElakKN  2.311245e+06\n",
       "19    StiWNN1GQrpPBOYt  3.890137e+06\n",
       "20    a016eMAVQKnfwMnt  3.130959e+07\n",
       "21    gsCFcQHnOH3AKMcZ  7.562794e+06\n",
       "22    IbNsDXfsPwSuFpow  8.608911e+06\n",
       "23    EgAVWOVxD1Jy5YkE  2.583701e+06\n",
       "24    BrKghvR76XdbQPnx  8.499508e+06\n",
       "25    a7fxkXTnUGWHUmKG  3.249687e+07\n",
       "26    WgzXa170DfpzpURE  1.055089e+07\n",
       "27    JPWqZbLq0VNC0yKI  1.426529e+07\n",
       "28    JQgTtbVstqFZwEK1  2.501520e+06\n",
       "29    bCSDbEthlS3nSIor  4.044996e+06\n",
       "...                ...           ...\n",
       "9970  QL412tWF5RDIX7IO  2.855370e+06\n",
       "9971  d3c2ceGtckONZzsr  1.074882e+07\n",
       "9972  P1j8YRbxDAovumaI  1.087405e+07\n",
       "9973  IxcBhEoFLcrI9TPr  3.645594e+06\n",
       "9974  rKiV0KDbAl2myBQI  2.926602e+06\n",
       "9975  GSdIXmKr0g5jQQcF  3.384842e+06\n",
       "9976  Am6Wcg3TO64qvzd8  3.890967e+07\n",
       "9977  RZqACAhkL4Tgw4Jr  9.674688e+07\n",
       "9978  u7NKZfWoMUlZy9rJ  5.230691e+06\n",
       "9979  C1BqV4MWH15rjAgz  5.239147e+06\n",
       "9980  wz8A2UbwsgR0lXGJ  4.983484e+06\n",
       "9981  MGJ8ABBTmC2yIaSm  9.950168e+06\n",
       "9982  MjHL2HP1PGIp8aBt  2.409051e+06\n",
       "9983  FMz7nnURFn85LaGt  5.450432e+07\n",
       "9984  kydULx0r0G7OklRD  7.823972e+06\n",
       "9985  nVNYRuk2fRbtlV00  2.945555e+07\n",
       "9986  F8SGEOGPxrPfiRv2  2.375709e+06\n",
       "9987  w7VMfiMvRb765ejK  3.244109e+07\n",
       "9988  lgZWdUKliWt2y5sM  1.530564e+07\n",
       "9989  TER8YrP9mw7UwWwr  6.472142e+06\n",
       "9990  TXHk3oUpVsm5Cmag  5.305676e+06\n",
       "9991  JtgDm9aQcGE9zELB  2.370131e+06\n",
       "9992  wTQmcqbN0OCuSF1t  1.513394e+07\n",
       "9993  WgsI1cBtzSfiWA1j  2.509481e+07\n",
       "9994  qNgt1ajb5uVMKbqm  4.627620e+06\n",
       "9995  UEeCDaAJzPwdKKKA  2.926602e+06\n",
       "9996  i0fgbPaQsDWs7Q87  4.304960e+07\n",
       "9997  YunNwAhcqkf6YclI  2.926602e+06\n",
       "9998  A2NotxtRY9MYoWMl  5.816262e+06\n",
       "9999  kKvgBXiA50gRmQhP  6.059150e+06\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('./dataset-0510/submit_test.csv')\n",
    "submission['total_price'] = pred\n",
    "submission.to_csv('submission/DNN2_result.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size use 128 or 32 , learning rate use 0.003 which find loss will stock in 0.6\n",
    "\n",
    "Result 1 DNN 233->256->128->1, lr=0.001, batch_size=128, predict score : 13\n",
    "change: \n",
    "- replacing Standard to MinMax \n",
    "- adding DropOut 0.3 layer\n",
    "- batch size change to 512\n",
    "\n",
    "Result 2 lr=0.001, batch_size=64, DNN 233->256->128->64->1\n",
    "after 1k loss : 0.00011785521522113447, can't decrease...\n",
    "- x_scale false\n",
    "- y_scale true\n",
    "\n",
    "Result 3 lr=0.001 batch_size=128 DNN 211->256->512->512->256->128->1\n",
    "after 1w loss : 0.0003, test loss : 0.0007 score: 1670\n",
    "\n",
    "Result 4 lr=0.001 batch_size=128 DNN 211->256->512->512->256->128->64->32->1\n",
    "train_loss: 0.0004, test loss: 0.0002 score: 1600\n",
    "\n",
    "Result 5 lr=0.001 batch_size=128 DNN 211->256->512->512->256->128->64->32->1 + batch_noram\n",
    "train_loss: 0.0005, test loss: 0.003 score: 1000\n",
    "\n",
    "Result 6 lr=0.001 batch_size=128 DNN 211->256->512->512->256->128->64->1 + batch_noram + weight_decay\n",
    "after 2k\n",
    "train_loss: 0.004, test loss 0.006 ,look like L2 regular not work which will increase loss\n",
    "\n",
    "Result 6 lr=0.0015 batch_size=128 DNN 211->256->512->512->256->128->64->32->1 + batch_noram\n",
    "after 3k\n",
    "train_loss 0.0004 test loss 0.002 score : 1400\n",
    "I think we should reduce lr or add L2 regularzation\n",
    "\n",
    "Result 7 lr=0.0015 batch_size= 128 DNN 211->256->512->512->256->128->64->32->1 + batch_noram + weight_decay(0.0005)\n",
    "after 3.5k \n",
    "train loss 0.0007, test loss 0.002, score: 2217\n",
    "regularzation is work, but test loss is not reduce, next tuning lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why output is negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
