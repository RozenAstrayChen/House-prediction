{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler,scale, MaxAbsScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import sklearn.metrics as metrics\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "use_gpu = True\n",
    "y_scale = True\n",
    "lr = 0.0005\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# Batch size and learning rate is hyperparameters in deep learning\n",
    "# suggest batch_size is reduced, lr is also reduced which will reduce concussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('./dataset-0510/train.csv')\n",
    "X_test = pd.read_csv('./dataset-0510/test.csv')\n",
    "\n",
    "\n",
    "columns = X.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_id', 'building_material', 'city', 'txn_dt', 'total_floor',\n",
       "       'building_type', 'building_use', 'building_complete_dt', 'parking_way',\n",
       "       'parking_area',\n",
       "       ...\n",
       "       'XIV_500', 'XIV_index_500', 'XIV_1000', 'XIV_index_1000', 'XIV_5000',\n",
       "       'XIV_index_5000', 'XIV_10000', 'XIV_index_10000', 'XIV_MIN',\n",
       "       'total_price'],\n",
       "      dtype='object', length=235)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputer, Scaler, Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/islab/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# step1. Imputation transformer for completing missing values.\n",
    "step1 = ('Imputer', Imputer())\n",
    "# step2. MinMaxScaler\n",
    "step2 = ('MinMaxScaler', MinMaxScaler())\n",
    "# step3. feature selection\n",
    "#step3 = ('FeatureSelection', SelectFromModel(RandomForestRegressor()))\n",
    "step3 = ('FeatureSelection', VarianceThreshold())\n",
    "\n",
    "pipeline = Pipeline(steps=[step1, step2, step3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['total_price']\n",
    "X = X.drop(columns=['building_id', 'total_price'], axis=1)\n",
    "X_test = X_test.drop(columns=['building_id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X sacle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 211)\n",
      "(10000, 211)\n"
     ]
    }
   ],
   "source": [
    "X = pipeline.fit_transform(X)\n",
    "print(X.shape)\n",
    "\n",
    "X_test = pipeline.transform(X_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5088052947.245064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    6.000000e+04\n",
       "mean     1.293727e+07\n",
       "std      5.522463e+07\n",
       "min      2.261495e+05\n",
       "25%      2.433114e+06\n",
       "50%      5.240482e+06\n",
       "75%      1.123932e+07\n",
       "max      5.088279e+09\n",
       "Name: total_price, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran = y.max() - y.min()\n",
    "print(ran)\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_scaler = StandardScaler()\n",
    "#y_scaler = MaxAbsScaler() # sparse\n",
    "y_scaler = MinMaxScaler(feature_range=[0, 1])\n",
    "if y_scale:\n",
    "    y = y_scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.28321267e-05],\n",
       "       [6.08347143e-04],\n",
       "       [1.83660349e-03],\n",
       "       ...,\n",
       "       [2.27773819e-03],\n",
       "       [3.50995685e-03],\n",
       "       [1.62815648e-03]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_eval = torch.from_numpy(X_eval).float().to(device)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).float().to(device)\n",
    "y_eval = torch.from_numpy(y_eval).float().to(device)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42000, 211])\n",
      "torch.Size([10000, 211])\n",
      "torch.Size([42000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data.TensorDataset(X_train, y_train)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "eval_dataset = Data.TensorDataset(X_eval, y_eval)\n",
    "eval_loader = Data.DataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # apply a uniform distribution to the weights and a bias=0\n",
    "        m.weight.data.uniform_(0.0, 1.0)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(211, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=512)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=512)\n",
    "        \n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=256)\n",
    "        \n",
    "        self.fc5 = nn.Linear(256, 128)\n",
    "        self.bn5 = nn.BatchNorm1d(num_features=128)\n",
    "        \n",
    "        self.fc6 = nn.Linear(128, 64)\n",
    "        self.bn6 = nn.BatchNorm1d(num_features=64)\n",
    "        \n",
    "        self.fc7 = nn.Linear(64, 32)\n",
    "        self.bn7 = nn.BatchNorm1d(num_features=32)\n",
    "        \n",
    "        self.fc8 = nn.Linear(32, 1)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(0)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = F.relu(self.bn5(self.fc5(x)))\n",
    "        x = F.relu(self.bn6(self.fc6(x)))\n",
    "        x = F.relu(self.bn7(self.fc7(x)))\n",
    "        x = self.fc8(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN().to(device)\n",
    "model.apply(weights_init_uniform)\n",
    "criterion = nn.MSELoss()\n",
    "#optim = optim.Adam(model.parameters(), lr= lr)\n",
    "optim = optim.SGD(model.parameters(), lr= lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_func(model, loader):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        optim.zero_grad()\n",
    "        pred = model(batch_x)\n",
    "        loss = torch.sqrt(criterion(pred, batch_y))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    print('training loss', np.array(train_loss).mean())\n",
    "    return np.array(train_loss).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_func(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = []\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            pred = model(batch_x)\n",
    "            loss = torch.sqrt(criterion(pred, batch_y))\n",
    "            \n",
    "            eval_loss.append(loss.item())\n",
    "        print('testing loss', np.array(eval_loss).mean())\n",
    "    return np.array(eval_loss).mean()\n",
    "\n",
    "def test_func(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X)\n",
    "        \n",
    "        pred = pred.cpu().numpy()\n",
    "        if y_scale:\n",
    "            pred = y_scaler.inverse_transform(pred)            \n",
    "    return pred\n",
    "\n",
    "\n",
    "def accuracy(model, pct_close=0.5):\n",
    "    #pred, y_eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_eval)\n",
    "        \n",
    "    n_correct = torch.sum((torch.abs(pred - y_eval) < torch.abs(pct_close * y_eval)))\n",
    "    result = (n_correct.item()/len(y_eval))  # scalar\n",
    "    return result \n",
    "\n",
    "def plot(label, pred):\n",
    "    plt.plot(label, label='actual')\n",
    "    plt.plot(pred, label='pred')\n",
    "    plt.legend(frameon=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 0\n",
      "training loss 7.757733353727857\n",
      "epochs 1\n",
      "training loss 4.869616896910508\n",
      "epochs 2\n",
      "training loss 2.656036224771053\n",
      "epochs 3\n",
      "training loss 0.987369618154949\n",
      "epochs 4\n",
      "training loss 0.1594524697420445\n",
      "epochs 5\n",
      "training loss 0.08364216624376984\n",
      "epochs 6\n",
      "training loss 0.06658315844267697\n",
      "epochs 7\n",
      "training loss 0.05595757138538868\n",
      "epochs 8\n",
      "training loss 0.0434831399357337\n",
      "epochs 9\n",
      "training loss 0.03101975721587345\n",
      "testing loss 0.027255225761509533\n",
      "epochs 10\n",
      "training loss 0.021338753427710033\n",
      "epochs 11\n",
      "training loss 0.013311693132755922\n",
      "epochs 12\n",
      "training loss 0.009517531914177908\n",
      "epochs 13\n",
      "training loss 0.008789948289676096\n",
      "epochs 14\n",
      "training loss 0.008574305208029062\n",
      "epochs 15\n",
      "training loss 0.008201966389141817\n",
      "epochs 16\n",
      "training loss 0.007970483688422476\n",
      "epochs 17\n",
      "training loss 0.0078529944122517\n",
      "epochs 18\n",
      "training loss 0.007594428021647602\n",
      "epochs 19\n",
      "training loss 0.007398072979766819\n",
      "testing loss 0.008392489594506456\n",
      "epochs 20\n",
      "training loss 0.0072353095390146034\n",
      "epochs 21\n",
      "training loss 0.007126530127792328\n",
      "epochs 22\n",
      "training loss 0.007014908630041388\n",
      "epochs 23\n",
      "training loss 0.006947178709940662\n",
      "epochs 24\n",
      "training loss 0.006809184401675402\n",
      "epochs 25\n",
      "training loss 0.006785075896062908\n",
      "epochs 26\n",
      "training loss 0.006767140586413216\n",
      "epochs 27\n",
      "training loss 0.00667527327943865\n",
      "epochs 28\n",
      "training loss 0.006613500408140382\n",
      "epochs 29\n",
      "training loss 0.006689302653348849\n",
      "testing loss 0.007841635904941998\n",
      "epochs 30\n",
      "training loss 0.006708278404945072\n",
      "epochs 31\n",
      "training loss 0.006639863115644767\n",
      "epochs 32\n",
      "training loss 0.006661494117439654\n",
      "epochs 33\n",
      "training loss 0.006660270117192124\n",
      "epochs 34\n",
      "training loss 0.006701067592849598\n",
      "epochs 35\n",
      "training loss 0.006667135817304503\n",
      "epochs 36\n",
      "training loss 0.0066182938525396594\n",
      "epochs 37\n",
      "training loss 0.0065158445669032386\n",
      "epochs 38\n",
      "training loss 0.00665827133936247\n",
      "epochs 39\n",
      "training loss 0.006581889310466708\n",
      "testing loss 0.007522138889139214\n",
      "epochs 40\n",
      "training loss 0.006537811749255267\n",
      "epochs 41\n",
      "training loss 0.006558523745499665\n",
      "epochs 42\n",
      "training loss 0.006605120593721562\n",
      "epochs 43\n",
      "training loss 0.006567183793890418\n",
      "epochs 44\n",
      "training loss 0.006562775244375855\n",
      "epochs 45\n",
      "training loss 0.006568690965031373\n",
      "epochs 46\n",
      "training loss 0.006517195294959341\n",
      "epochs 47\n",
      "training loss 0.00658829014332212\n",
      "epochs 48\n",
      "training loss 0.006624310439463852\n",
      "epochs 49\n",
      "training loss 0.006569154216273827\n",
      "testing loss 0.007531426429206915\n",
      "epochs 50\n",
      "training loss 0.006583313105591094\n",
      "epochs 51\n",
      "training loss 0.006575307405785881\n",
      "epochs 52\n",
      "training loss 0.006501702869545929\n",
      "epochs 53\n",
      "training loss 0.00665301970017955\n",
      "epochs 54\n",
      "training loss 0.006474663694898702\n",
      "epochs 55\n",
      "training loss 0.006501483721417972\n",
      "epochs 56\n",
      "training loss 0.006529237367679849\n",
      "epochs 57\n",
      "training loss 0.006575045993588412\n",
      "epochs 58\n",
      "training loss 0.0065055336540644155\n",
      "epochs 59\n",
      "training loss 0.006573987351220928\n",
      "testing loss 0.007950189977501196\n",
      "epochs 60\n",
      "training loss 0.006571971911138722\n",
      "epochs 61\n",
      "training loss 0.0066233733325692795\n",
      "epochs 62\n",
      "training loss 0.006510764636781553\n",
      "epochs 63\n",
      "training loss 0.006601887921757407\n",
      "epochs 64\n",
      "training loss 0.006450876077682369\n",
      "epochs 65\n",
      "training loss 0.0066088818365390945\n",
      "epochs 66\n",
      "training loss 0.0065532149435629595\n",
      "epochs 67\n",
      "training loss 0.006681750770545024\n",
      "epochs 68\n",
      "training loss 0.006562679560889\n",
      "epochs 69\n",
      "training loss 0.0065858492024078695\n",
      "testing loss 0.007920631209177012\n",
      "epochs 70\n",
      "training loss 0.006663502561633325\n",
      "epochs 71\n",
      "training loss 0.0066391278725643965\n",
      "epochs 72\n",
      "training loss 0.006575665872977158\n",
      "epochs 73\n",
      "training loss 0.00660775981399041\n",
      "epochs 74\n",
      "training loss 0.006544586831185752\n",
      "epochs 75\n",
      "training loss 0.006624331181206511\n",
      "epochs 76\n",
      "training loss 0.006488773058165759\n",
      "epochs 77\n",
      "training loss 0.006563629727884221\n",
      "epochs 78\n",
      "training loss 0.00651465854741984\n",
      "epochs 79\n",
      "training loss 0.006610644533296079\n",
      "testing loss 0.007396296386653227\n",
      "epochs 80\n",
      "training loss 0.006517781637889251\n",
      "epochs 81\n",
      "training loss 0.006602056754758655\n",
      "epochs 82\n",
      "training loss 0.006594629550995277\n",
      "epochs 83\n",
      "training loss 0.006566192172071401\n",
      "epochs 84\n",
      "training loss 0.006622570930940515\n",
      "epochs 85\n",
      "training loss 0.006484538612895756\n",
      "epochs 86\n",
      "training loss 0.00654690427419738\n",
      "epochs 87\n",
      "training loss 0.006476519460574721\n",
      "epochs 88\n",
      "training loss 0.006601313005929026\n",
      "epochs 89\n",
      "training loss 0.00656289263454331\n",
      "testing loss 0.007507987998274042\n",
      "epochs 90\n",
      "training loss 0.006590372266029899\n",
      "epochs 91\n",
      "training loss 0.006528950990941998\n",
      "epochs 92\n",
      "training loss 0.006593721580131144\n",
      "epochs 93\n",
      "training loss 0.006571637058919324\n",
      "epochs 94\n",
      "training loss 0.006662281578984168\n",
      "epochs 95\n",
      "training loss 0.006569574369077987\n",
      "epochs 96\n",
      "training loss 0.006597720811251087\n",
      "epochs 97\n",
      "training loss 0.006447689940000808\n",
      "epochs 98\n",
      "training loss 0.006588867472026123\n",
      "epochs 99\n",
      "training loss 0.006604615161917243\n",
      "testing loss 0.007480587387549962\n",
      "epochs 100\n",
      "training loss 0.006897542404865784\n",
      "epochs 101\n",
      "training loss 0.006619785606612007\n",
      "epochs 102\n",
      "training loss 0.0066220649531518756\n",
      "epochs 103\n",
      "training loss 0.00658695307548942\n",
      "epochs 104\n",
      "training loss 0.00653155503396828\n",
      "epochs 105\n",
      "training loss 0.006581487480868051\n",
      "epochs 106\n",
      "training loss 0.006622289809522907\n",
      "epochs 107\n",
      "training loss 0.006617353206123919\n",
      "epochs 108\n",
      "training loss 0.006574626915060595\n",
      "epochs 109\n",
      "training loss 0.006592640899175546\n",
      "testing loss 0.008148881005691298\n",
      "epochs 110\n",
      "training loss 0.006550589741255782\n",
      "epochs 111\n",
      "training loss 0.006554072707223403\n",
      "epochs 112\n",
      "training loss 0.006525568549427852\n",
      "epochs 113\n",
      "training loss 0.006681914694641227\n",
      "epochs 114\n",
      "training loss 0.006587261584230718\n",
      "epochs 115\n",
      "training loss 0.00638296486283327\n",
      "epochs 116\n",
      "training loss 0.006538486477952892\n",
      "epochs 117\n",
      "training loss 0.006537506551096799\n",
      "epochs 118\n",
      "training loss 0.006603466774305855\n",
      "epochs 119\n",
      "training loss 0.006472113375597291\n",
      "testing loss 0.00813088192379908\n",
      "epochs 120\n",
      "training loss 0.006512341044608243\n",
      "epochs 121\n",
      "training loss 0.006650252957717943\n",
      "epochs 122\n",
      "training loss 0.006512290986019857\n",
      "epochs 123\n",
      "training loss 0.0066096196230642475\n",
      "epochs 124\n",
      "training loss 0.006601653790022147\n",
      "epochs 125\n",
      "training loss 0.00654184631154077\n",
      "epochs 126\n",
      "training loss 0.006587772222170691\n",
      "epochs 127\n",
      "training loss 0.006709728308593837\n",
      "epochs 128\n",
      "training loss 0.0066375633911158905\n",
      "epochs 129\n",
      "training loss 0.006621657051556492\n",
      "testing loss 0.007291368405328047\n",
      "epochs 130\n",
      "training loss 0.006648750399666808\n",
      "epochs 131\n",
      "training loss 0.006667429852330531\n",
      "epochs 132\n",
      "training loss 0.006533233868170496\n",
      "epochs 133\n",
      "training loss 0.006569347532938792\n",
      "epochs 134\n",
      "training loss 0.006568568954697179\n",
      "epochs 135\n",
      "training loss 0.006519083040879649\n",
      "epochs 136\n",
      "training loss 0.006509930794523593\n",
      "epochs 137\n",
      "training loss 0.0065751125311371405\n",
      "epochs 138\n",
      "training loss 0.00656217293450071\n",
      "epochs 139\n",
      "training loss 0.006614980489188256\n",
      "testing loss 0.007307298796033447\n",
      "epochs 140\n",
      "training loss 0.0065333082660534465\n",
      "epochs 141\n",
      "training loss 0.00656807216699589\n",
      "epochs 142\n",
      "training loss 0.006590380627410005\n",
      "epochs 143\n",
      "training loss 0.006526640600080412\n",
      "epochs 144\n",
      "training loss 0.006545368177564333\n",
      "epochs 145\n",
      "training loss 0.0066281246254220605\n",
      "epochs 146\n",
      "training loss 0.006573991564287886\n",
      "epochs 147\n",
      "training loss 0.006581261457513032\n",
      "epochs 148\n",
      "training loss 0.00653661674456133\n",
      "epochs 149\n",
      "training loss 0.00662349629687219\n",
      "testing loss 0.00734595834370042\n",
      "epochs 150\n",
      "training loss 0.006541592024739458\n",
      "epochs 151\n",
      "training loss 0.006567878516300335\n",
      "epochs 152\n",
      "training loss 0.0065406056713329845\n",
      "epochs 153\n",
      "training loss 0.006592669880221454\n",
      "epochs 154\n",
      "training loss 0.00657749661395898\n",
      "epochs 155\n",
      "training loss 0.006557016604435616\n",
      "epochs 156\n",
      "training loss 0.006499878370053047\n",
      "epochs 157\n",
      "training loss 0.006599291863972972\n",
      "epochs 158\n",
      "training loss 0.006600616933950605\n",
      "epochs 159\n",
      "training loss 0.006577502363955879\n",
      "testing loss 0.007688193054414017\n",
      "epochs 160\n",
      "training loss 0.006530545379451298\n",
      "epochs 161\n",
      "training loss 0.006683890478144822\n",
      "epochs 162\n",
      "training loss 0.006501876284498552\n",
      "epochs 163\n",
      "training loss 0.006551501351425645\n",
      "epochs 164\n",
      "training loss 0.006535266446428073\n",
      "epochs 165\n",
      "training loss 0.006472735986323025\n",
      "epochs 166\n",
      "training loss 0.006610013368344189\n",
      "epochs 167\n",
      "training loss 0.0065827506040944785\n",
      "epochs 168\n",
      "training loss 0.006483972101970533\n",
      "epochs 169\n",
      "training loss 0.006591868284299947\n",
      "testing loss 0.007497182297553365\n",
      "epochs 170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0065788087020359235\n",
      "epochs 171\n",
      "training loss 0.006599082731454998\n",
      "epochs 172\n",
      "training loss 0.006449920552751576\n",
      "epochs 173\n",
      "training loss 0.006638270587303695\n",
      "epochs 174\n",
      "training loss 0.006560761761061825\n",
      "epochs 175\n",
      "training loss 0.006590016605190955\n",
      "epochs 176\n",
      "training loss 0.006614301734785111\n",
      "epochs 177\n",
      "training loss 0.006587463702814014\n",
      "epochs 178\n",
      "training loss 0.0065802633748205785\n",
      "epochs 179\n",
      "training loss 0.0064861448092448405\n",
      "testing loss 0.007192077915223831\n",
      "epochs 180\n",
      "training loss 0.006646287191933797\n",
      "epochs 181\n",
      "training loss 0.006489354950421147\n",
      "epochs 182\n",
      "training loss 0.006588032854491658\n",
      "epochs 183\n",
      "training loss 0.006575350394531926\n",
      "epochs 184\n",
      "training loss 0.006641523599395341\n",
      "epochs 185\n",
      "training loss 0.006570450745743776\n",
      "epochs 186\n",
      "training loss 0.006598848884211893\n",
      "epochs 187\n",
      "training loss 0.006617256917206622\n",
      "epochs 188\n",
      "training loss 0.006548602416358413\n",
      "epochs 189\n",
      "training loss 0.00658730280976233\n",
      "testing loss 0.00801090740744051\n",
      "epochs 190\n",
      "training loss 0.006557579265162915\n",
      "epochs 191\n",
      "training loss 0.006638927014026576\n",
      "epochs 192\n",
      "training loss 0.006506630959411047\n",
      "epochs 193\n",
      "training loss 0.00661930394547179\n",
      "epochs 194\n",
      "training loss 0.006662847417248215\n",
      "epochs 195\n",
      "training loss 0.00660631232109419\n",
      "epochs 196\n",
      "training loss 0.00643363278776713\n",
      "epochs 197\n",
      "training loss 0.006619103859379707\n",
      "epochs 198\n",
      "training loss 0.006541367114937686\n",
      "epochs 199\n",
      "training loss 0.006583683915283406\n",
      "testing loss 0.00732139529865103\n",
      "epochs 200\n",
      "training loss 0.006564669589523373\n",
      "epochs 201\n",
      "training loss 0.0065375252585790165\n",
      "epochs 202\n",
      "training loss 0.0065733759404276145\n",
      "epochs 203\n",
      "training loss 0.006646426396704871\n",
      "epochs 204\n",
      "training loss 0.0066030656286914435\n",
      "epochs 205\n",
      "training loss 0.0065803759983290655\n",
      "epochs 206\n",
      "training loss 0.006559682851812964\n",
      "epochs 207\n",
      "training loss 0.006472017795433457\n",
      "epochs 208\n",
      "training loss 0.006480277251271038\n",
      "epochs 209\n",
      "training loss 0.006586417148532317\n",
      "testing loss 0.007471503186244385\n",
      "epochs 210\n",
      "training loss 0.006558248914047895\n",
      "epochs 211\n",
      "training loss 0.006459377648373981\n",
      "epochs 212\n",
      "training loss 0.006572968574201173\n",
      "epochs 213\n",
      "training loss 0.0066150683877100784\n",
      "epochs 214\n",
      "training loss 0.0065833131200987785\n",
      "epochs 215\n",
      "training loss 0.006589596664694373\n",
      "epochs 216\n",
      "training loss 0.006553404169060901\n",
      "epochs 217\n",
      "training loss 0.006557145164458585\n",
      "epochs 218\n",
      "training loss 0.0066087539010542505\n",
      "epochs 219\n",
      "training loss 0.006608301806221164\n",
      "testing loss 0.007362155862159181\n",
      "epochs 220\n",
      "training loss 0.006515078575670221\n",
      "epochs 221\n",
      "training loss 0.006600601309174417\n",
      "epochs 222\n",
      "training loss 0.0065040056994269775\n",
      "epochs 223\n",
      "training loss 0.006575812138390577\n",
      "epochs 224\n",
      "training loss 0.006624677874173369\n",
      "epochs 225\n",
      "training loss 0.006569798468572504\n",
      "epochs 226\n",
      "training loss 0.006594190724324265\n",
      "epochs 227\n",
      "training loss 0.006536936377158168\n",
      "epochs 228\n",
      "training loss 0.006637170701712302\n",
      "epochs 229\n",
      "training loss 0.006605481632311452\n",
      "testing loss 0.0073112507238455695\n",
      "epochs 230\n",
      "training loss 0.0066048662428741306\n",
      "epochs 231\n",
      "training loss 0.006580283914517192\n",
      "epochs 232\n",
      "training loss 0.006587397991141763\n",
      "epochs 233\n",
      "training loss 0.0065647732366674715\n",
      "epochs 234\n",
      "training loss 0.006622390182176464\n",
      "epochs 235\n",
      "training loss 0.006576146950979227\n",
      "epochs 236\n",
      "training loss 0.0066352130170758685\n",
      "epochs 237\n",
      "training loss 0.006484928494177781\n",
      "epochs 238\n",
      "training loss 0.006559028128150703\n",
      "epochs 239\n",
      "training loss 0.006583333170780272\n",
      "testing loss 0.007930370784159882\n",
      "epochs 240\n",
      "training loss 0.00657175921893036\n",
      "epochs 241\n",
      "training loss 0.006549917335203059\n",
      "epochs 242\n",
      "training loss 0.006582814252490919\n",
      "epochs 243\n",
      "training loss 0.00669351826445978\n",
      "epochs 244\n",
      "training loss 0.006545227437455626\n",
      "epochs 245\n",
      "training loss 0.0065279094478268304\n",
      "epochs 246\n",
      "training loss 0.006610060797149378\n",
      "epochs 247\n",
      "training loss 0.006592382840022422\n",
      "epochs 248\n",
      "training loss 0.006546250705134498\n",
      "epochs 249\n",
      "training loss 0.006594119999008565\n",
      "testing loss 0.0077354328488891426\n",
      "epochs 250\n",
      "training loss 0.006584867986137087\n",
      "epochs 251\n",
      "training loss 0.006607274522744459\n",
      "epochs 252\n",
      "training loss 0.006592447178064644\n",
      "epochs 253\n",
      "training loss 0.006615128642369889\n",
      "epochs 254\n",
      "training loss 0.006521849063618568\n",
      "epochs 255\n",
      "training loss 0.006567448417947693\n",
      "epochs 256\n",
      "training loss 0.006571387261007898\n",
      "epochs 257\n",
      "training loss 0.006594522668991385\n",
      "epochs 258\n",
      "training loss 0.006486679401141124\n",
      "epochs 259\n",
      "training loss 0.006540443902158697\n",
      "testing loss 0.00725465667341565\n",
      "epochs 260\n",
      "training loss 0.006618147596680081\n",
      "epochs 261\n",
      "training loss 0.0066509205419117284\n",
      "epochs 262\n",
      "training loss 0.006474837818250943\n",
      "epochs 263\n",
      "training loss 0.006598386726707311\n",
      "epochs 264\n",
      "training loss 0.006592360411849903\n",
      "epochs 265\n",
      "training loss 0.006533761153147644\n",
      "epochs 266\n",
      "training loss 0.006583699636305697\n",
      "epochs 267\n",
      "training loss 0.006595548064346032\n",
      "epochs 268\n",
      "training loss 0.006545684062224731\n",
      "epochs 269\n",
      "training loss 0.006576337776922459\n",
      "testing loss 0.0072314040883979264\n",
      "epochs 270\n",
      "training loss 0.006594367118659896\n",
      "epochs 271\n",
      "training loss 0.006526650923536383\n",
      "epochs 272\n",
      "training loss 0.006634663357710789\n",
      "epochs 273\n",
      "training loss 0.006570100558548033\n",
      "epochs 274\n",
      "training loss 0.006617463642156663\n",
      "epochs 275\n",
      "training loss 0.006567694679522564\n",
      "epochs 276\n",
      "training loss 0.006606586149392656\n",
      "epochs 277\n",
      "training loss 0.006553533567698802\n",
      "epochs 278\n",
      "training loss 0.006621813111072321\n",
      "epochs 279\n",
      "training loss 0.006584813957396997\n",
      "testing loss 0.0073578227815013515\n",
      "epochs 280\n",
      "training loss 0.006560761829354096\n",
      "epochs 281\n",
      "training loss 0.006648129619386239\n",
      "epochs 282\n",
      "training loss 0.006627773041596302\n",
      "epochs 283\n",
      "training loss 0.006497122946062258\n",
      "epochs 284\n",
      "training loss 0.006614245614815379\n",
      "epochs 285\n",
      "training loss 0.006540305674710172\n",
      "epochs 286\n",
      "training loss 0.006475422745797002\n",
      "epochs 287\n",
      "training loss 0.006621126175750511\n",
      "epochs 288\n",
      "training loss 0.006523426062721861\n",
      "epochs 289\n",
      "training loss 0.006626793752016788\n",
      "testing loss 0.0076321400569897176\n",
      "epochs 290\n",
      "training loss 0.006557547051734136\n",
      "epochs 291\n",
      "training loss 0.006621932812207761\n",
      "epochs 292\n",
      "training loss 0.006609240731883986\n",
      "epochs 293\n",
      "training loss 0.006531134086426087\n",
      "epochs 294\n",
      "training loss 0.006574597934014687\n",
      "epochs 295\n",
      "training loss 0.006561043571417523\n",
      "epochs 296\n",
      "training loss 0.0065636612187595445\n",
      "epochs 297\n",
      "training loss 0.006550376415309301\n",
      "epochs 298\n",
      "training loss 0.006656754185519207\n",
      "epochs 299\n",
      "training loss 0.00660213434185527\n",
      "testing loss 0.007718087644613487\n",
      "epochs 300\n",
      "training loss 0.0065951747151012435\n",
      "epochs 301\n",
      "training loss 0.006405430217131995\n",
      "epochs 302\n",
      "training loss 0.006571058140739255\n",
      "epochs 303\n",
      "training loss 0.006630792419106266\n",
      "epochs 304\n",
      "training loss 0.006598607403449828\n",
      "epochs 305\n",
      "training loss 0.006575566927738216\n",
      "epochs 306\n",
      "training loss 0.006588816568099732\n",
      "epochs 307\n",
      "training loss 0.006522737386831759\n",
      "epochs 308\n",
      "training loss 0.006536146374291947\n",
      "epochs 309\n",
      "training loss 0.006581237061603113\n",
      "testing loss 0.007936755837608419\n",
      "epochs 310\n",
      "training loss 0.006578075100093129\n",
      "epochs 311\n",
      "training loss 0.0064780329719276416\n",
      "epochs 312\n",
      "training loss 0.006540152745669578\n",
      "epochs 313\n",
      "training loss 0.006586144835756112\n",
      "epochs 314\n",
      "training loss 0.00656684035099873\n",
      "epochs 315\n",
      "training loss 0.006594937485444577\n",
      "epochs 316\n",
      "training loss 0.00655966619239118\n",
      "epochs 317\n",
      "training loss 0.006587155218841932\n",
      "epochs 318\n",
      "training loss 0.00657620658747014\n",
      "epochs 319\n",
      "training loss 0.006523248185417799\n",
      "testing loss 0.00735531041666152\n",
      "epochs 320\n",
      "training loss 0.006563339677163737\n",
      "epochs 321\n",
      "training loss 0.006543946530284943\n",
      "epochs 322\n",
      "training loss 0.006559940246089523\n",
      "epochs 323\n",
      "training loss 0.006569590737638389\n",
      "epochs 324\n",
      "training loss 0.0065936920276240055\n",
      "epochs 325\n",
      "training loss 0.006573572124306418\n",
      "epochs 326\n",
      "training loss 0.006554691861935616\n",
      "epochs 327\n",
      "training loss 0.0065582137849283795\n",
      "epochs 328\n",
      "training loss 0.006613700297140265\n",
      "epochs 329\n",
      "training loss 0.00657789445535979\n",
      "testing loss 0.007502294787047233\n",
      "epochs 330\n",
      "training loss 0.006616963596949439\n",
      "epochs 331\n",
      "training loss 0.006651877781226212\n",
      "epochs 332\n",
      "training loss 0.0065842617291102246\n",
      "epochs 333\n",
      "training loss 0.006552395915420086\n",
      "epochs 334\n",
      "training loss 0.0066151738433608935\n",
      "epochs 335\n",
      "training loss 0.006529640019616525\n",
      "epochs 336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006573342899706973\n",
      "epochs 337\n",
      "training loss 0.006476160321076114\n",
      "epochs 338\n",
      "training loss 0.006644068989976335\n",
      "epochs 339\n",
      "training loss 0.0066472104284204655\n",
      "testing loss 0.007452625276005649\n",
      "epochs 340\n",
      "training loss 0.006643609057455168\n",
      "epochs 341\n",
      "training loss 0.006431681619204105\n",
      "epochs 342\n",
      "training loss 0.006625445785020736\n",
      "epochs 343\n",
      "training loss 0.006610166868492169\n",
      "epochs 344\n",
      "training loss 0.006597515072469744\n",
      "epochs 345\n",
      "training loss 0.0066628186545252665\n",
      "epochs 346\n",
      "training loss 0.00661892944551993\n",
      "epochs 347\n",
      "training loss 0.006477860277405399\n",
      "epochs 348\n",
      "training loss 0.0065154865462655675\n",
      "epochs 349\n",
      "training loss 0.006516161088132523\n",
      "testing loss 0.007526349471155422\n",
      "epochs 350\n",
      "training loss 0.006484328083040074\n",
      "epochs 351\n",
      "training loss 0.006532548982324503\n",
      "epochs 352\n",
      "training loss 0.006505370165906439\n",
      "epochs 353\n",
      "training loss 0.006461290829095956\n",
      "epochs 354\n",
      "training loss 0.006541187178603965\n",
      "epochs 355\n",
      "training loss 0.006519784313332187\n",
      "epochs 356\n",
      "training loss 0.006566428880512964\n",
      "epochs 357\n",
      "training loss 0.0065866018964634435\n",
      "epochs 358\n",
      "training loss 0.006554799739308202\n",
      "epochs 359\n",
      "training loss 0.006577979965775208\n",
      "testing loss 0.007693881961576482\n",
      "epochs 360\n",
      "training loss 0.006546423750318089\n",
      "epochs 361\n",
      "training loss 0.00663030635612227\n",
      "epochs 362\n",
      "training loss 0.006646066098360735\n",
      "epochs 363\n",
      "training loss 0.006636446709164325\n",
      "epochs 364\n",
      "training loss 0.006578868409649874\n",
      "epochs 365\n",
      "training loss 0.006580454556379003\n",
      "epochs 366\n",
      "training loss 0.00656684075332159\n",
      "epochs 367\n",
      "training loss 0.006565462561157223\n",
      "epochs 368\n",
      "training loss 0.006531972244897104\n",
      "epochs 369\n",
      "training loss 0.006514507660778143\n",
      "testing loss 0.007481126118028629\n",
      "epochs 370\n",
      "training loss 0.006560650083383596\n",
      "epochs 371\n",
      "training loss 0.006535069438795868\n",
      "epochs 372\n",
      "training loss 0.006563198443439911\n",
      "epochs 373\n",
      "training loss 0.006562198733409873\n",
      "epochs 374\n",
      "training loss 0.006553364506113063\n",
      "epochs 375\n",
      "training loss 0.0066577929512974116\n",
      "epochs 376\n",
      "training loss 0.006519791230666918\n",
      "epochs 377\n",
      "training loss 0.006602691769554622\n",
      "epochs 378\n",
      "training loss 0.006575642288082186\n",
      "epochs 379\n",
      "training loss 0.006512688880497561\n",
      "testing loss 0.007926072533975573\n",
      "epochs 380\n",
      "training loss 0.006597409677680435\n",
      "epochs 381\n",
      "training loss 0.00653917400242823\n",
      "epochs 382\n",
      "training loss 0.006548387187074496\n",
      "epochs 383\n",
      "training loss 0.006603213228104143\n",
      "epochs 384\n",
      "training loss 0.006515917505525438\n",
      "epochs 385\n",
      "training loss 0.0065728143695458815\n",
      "epochs 386\n",
      "training loss 0.006578860899623155\n",
      "epochs 387\n",
      "training loss 0.006593957595034619\n",
      "epochs 388\n",
      "training loss 0.006556328169514615\n",
      "epochs 389\n",
      "training loss 0.006601366943023012\n",
      "testing loss 0.007566039623851155\n",
      "epochs 390\n",
      "training loss 0.00664407877134029\n",
      "epochs 391\n",
      "training loss 0.006585123846491397\n",
      "epochs 392\n",
      "training loss 0.006576775497470991\n",
      "epochs 393\n",
      "training loss 0.0064129545498429865\n",
      "epochs 394\n",
      "training loss 0.006536833945121107\n",
      "epochs 395\n",
      "training loss 0.006600830230527871\n",
      "epochs 396\n",
      "training loss 0.006698496992159126\n",
      "epochs 397\n",
      "training loss 0.006586461881031268\n",
      "epochs 398\n",
      "training loss 0.0065150150528890975\n",
      "epochs 399\n",
      "training loss 0.0066574589964313756\n",
      "testing loss 0.0074932683417771725\n",
      "epochs 400\n",
      "training loss 0.00654246280639288\n",
      "epochs 401\n",
      "training loss 0.006494641209893087\n",
      "epochs 402\n",
      "training loss 0.006524536755830502\n",
      "epochs 403\n",
      "training loss 0.006593033040825583\n",
      "epochs 404\n",
      "training loss 0.0066091337040955584\n",
      "epochs 405\n",
      "training loss 0.006643354880346104\n",
      "epochs 406\n",
      "training loss 0.006524058660396055\n",
      "epochs 407\n",
      "training loss 0.00659891030656338\n",
      "epochs 408\n",
      "training loss 0.006584207087155081\n",
      "epochs 409\n",
      "training loss 0.006611269429512888\n",
      "testing loss 0.007399852352930193\n",
      "epochs 410\n",
      "training loss 0.006677083843415971\n",
      "epochs 411\n",
      "training loss 0.006651738508870559\n",
      "epochs 412\n",
      "training loss 0.006581040541570645\n",
      "epochs 413\n",
      "training loss 0.006655659515263462\n",
      "epochs 414\n",
      "training loss 0.006541305489832462\n",
      "epochs 415\n",
      "training loss 0.006603220156054253\n",
      "epochs 416\n",
      "training loss 0.006575417209849736\n",
      "epochs 417\n",
      "training loss 0.006547067747847673\n",
      "epochs 418\n",
      "training loss 0.006617356463276008\n",
      "epochs 419\n",
      "training loss 0.006501274426130855\n",
      "testing loss 0.007814240025964083\n",
      "epochs 420\n",
      "training loss 0.006612781914004824\n",
      "epochs 421\n",
      "training loss 0.006561440100757501\n",
      "epochs 422\n",
      "training loss 0.006587476098391944\n",
      "epochs 423\n",
      "training loss 0.006515563907608457\n",
      "epochs 424\n",
      "training loss 0.00652984471172151\n",
      "epochs 425\n",
      "training loss 0.00653959299214071\n",
      "epochs 426\n",
      "training loss 0.006636410629968399\n",
      "epochs 427\n",
      "training loss 0.006545941973116396\n",
      "epochs 428\n",
      "training loss 0.006588365653696779\n",
      "epochs 429\n",
      "training loss 0.006611302105064496\n",
      "testing loss 0.007342191366809057\n",
      "epochs 430\n",
      "training loss 0.006560300697295114\n",
      "epochs 431\n",
      "training loss 0.006466168362157573\n",
      "epochs 432\n",
      "training loss 0.006413565691005677\n",
      "epochs 433\n",
      "training loss 0.00659913213648989\n",
      "epochs 434\n",
      "training loss 0.006590204504474228\n",
      "epochs 435\n",
      "training loss 0.006510390193445048\n",
      "epochs 436\n",
      "training loss 0.006599505371088586\n",
      "epochs 437\n",
      "training loss 0.0065675134870356745\n",
      "epochs 438\n",
      "training loss 0.006616335502672816\n",
      "epochs 439\n",
      "training loss 0.006611310220521657\n",
      "testing loss 0.007554964718891057\n",
      "epochs 440\n",
      "training loss 0.006541491021886241\n",
      "epochs 441\n",
      "training loss 0.006491704201707928\n",
      "epochs 442\n",
      "training loss 0.006548661840895593\n",
      "epochs 443\n",
      "training loss 0.006623107301620455\n",
      "epochs 444\n",
      "training loss 0.0065681890723943755\n",
      "epochs 445\n",
      "training loss 0.006619993058007811\n",
      "epochs 446\n",
      "training loss 0.006570092724044567\n",
      "epochs 447\n",
      "training loss 0.006615649472488979\n",
      "epochs 448\n",
      "training loss 0.006477905626657864\n",
      "epochs 449\n",
      "training loss 0.006612442211264965\n",
      "testing loss 0.008475189894794467\n",
      "epochs 450\n",
      "training loss 0.006595434725007161\n",
      "epochs 451\n",
      "training loss 0.006561541060795356\n",
      "epochs 452\n",
      "training loss 0.00653680464490614\n",
      "epochs 453\n",
      "training loss 0.0065250433057880645\n",
      "epochs 454\n",
      "training loss 0.006630787465262784\n",
      "epochs 455\n",
      "training loss 0.006588106737528732\n",
      "epochs 456\n",
      "training loss 0.006551140412620317\n",
      "epochs 457\n",
      "training loss 0.00647975005227846\n",
      "epochs 458\n",
      "training loss 0.0065062173711571926\n",
      "epochs 459\n",
      "training loss 0.006584710186052965\n",
      "testing loss 0.007782216423276997\n",
      "epochs 460\n",
      "training loss 0.006520046940282848\n",
      "epochs 461\n",
      "training loss 0.006586739583589182\n",
      "epochs 462\n",
      "training loss 0.0066326791109194505\n",
      "epochs 463\n",
      "training loss 0.006593617676094877\n",
      "epochs 464\n",
      "training loss 0.006608853202969923\n",
      "epochs 465\n",
      "training loss 0.006607144574229931\n",
      "epochs 466\n",
      "training loss 0.006499457564403085\n",
      "epochs 467\n",
      "training loss 0.006642923911886239\n",
      "epochs 468\n",
      "training loss 0.006572883976354493\n",
      "epochs 469\n",
      "training loss 0.006496722381109071\n",
      "testing loss 0.007266650409834024\n",
      "epochs 470\n",
      "training loss 0.006569811112196454\n",
      "epochs 471\n",
      "training loss 0.006464823459821405\n",
      "epochs 472\n",
      "training loss 0.006614707625827733\n",
      "epochs 473\n",
      "training loss 0.006570831895522818\n",
      "epochs 474\n",
      "training loss 0.006556366398324766\n",
      "epochs 475\n",
      "training loss 0.006620248596008448\n",
      "epochs 476\n",
      "training loss 0.0065051352776485105\n",
      "epochs 477\n",
      "training loss 0.006537931138652414\n",
      "epochs 478\n",
      "training loss 0.006520763031096625\n",
      "epochs 479\n",
      "training loss 0.006603926524596351\n",
      "testing loss 0.007282055080505022\n",
      "epochs 480\n",
      "training loss 0.0065923218153937976\n",
      "epochs 481\n",
      "training loss 0.006547294455186936\n",
      "epochs 482\n",
      "training loss 0.0065079200421927046\n",
      "epochs 483\n",
      "training loss 0.006618332405826558\n",
      "epochs 484\n",
      "training loss 0.006638070229811758\n",
      "epochs 485\n",
      "training loss 0.006537203484506412\n",
      "epochs 486\n",
      "training loss 0.006571532367929635\n",
      "epochs 487\n",
      "training loss 0.006522958460943292\n",
      "epochs 488\n",
      "training loss 0.0066134565485794225\n",
      "epochs 489\n",
      "training loss 0.0065904080214567685\n",
      "testing loss 0.007388058120999089\n",
      "epochs 490\n",
      "training loss 0.006671883784712923\n",
      "epochs 491\n",
      "training loss 0.006591061499227393\n",
      "epochs 492\n",
      "training loss 0.006626612861360505\n",
      "epochs 493\n",
      "training loss 0.006521795127586119\n",
      "epochs 494\n",
      "training loss 0.006627831243949526\n",
      "epochs 495\n",
      "training loss 0.006639064489198583\n",
      "epochs 496\n",
      "training loss 0.006547950787079098\n",
      "epochs 497\n",
      "training loss 0.006598969380439211\n",
      "epochs 498\n",
      "training loss 0.006583290643803209\n",
      "epochs 499\n",
      "training loss 0.0065657840726761245\n",
      "testing loss 0.007446193966869239\n",
      "epochs 500\n",
      "training loss 0.0065801242535571515\n",
      "epochs 501\n",
      "training loss 0.006641046969944719\n",
      "epochs 502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006554855537631954\n",
      "epochs 503\n",
      "training loss 0.006510293226912731\n",
      "epochs 504\n",
      "training loss 0.007146237280055609\n",
      "epochs 505\n",
      "training loss 0.006518146347631007\n",
      "epochs 506\n",
      "training loss 0.006558205031840793\n",
      "epochs 507\n",
      "training loss 0.0065793750728381445\n",
      "epochs 508\n",
      "training loss 0.00665103023663718\n",
      "epochs 509\n",
      "training loss 0.006570431062000546\n",
      "testing loss 0.00728666788901711\n",
      "epochs 510\n",
      "training loss 0.006588077022606297\n",
      "epochs 511\n",
      "training loss 0.006523149781917027\n",
      "epochs 512\n",
      "training loss 0.006550826336466637\n",
      "epochs 513\n",
      "training loss 0.006595251206690757\n",
      "epochs 514\n",
      "training loss 0.006534411213827803\n",
      "epochs 515\n",
      "training loss 0.006507090323655597\n",
      "epochs 516\n",
      "training loss 0.006617390828442089\n",
      "epochs 517\n",
      "training loss 0.006596533445737645\n",
      "epochs 518\n",
      "training loss 0.006595994977458836\n",
      "epochs 519\n",
      "training loss 0.006605922981550334\n",
      "testing loss 0.007095547452410485\n",
      "epochs 520\n",
      "training loss 0.006624949820365157\n",
      "epochs 521\n",
      "training loss 0.0065947593374483535\n",
      "epochs 522\n",
      "training loss 0.006580608804557348\n",
      "epochs 523\n",
      "training loss 0.006564280446010181\n",
      "epochs 524\n",
      "training loss 0.006623684844401266\n",
      "epochs 525\n",
      "training loss 0.006617361889857697\n",
      "epochs 526\n",
      "training loss 0.0065578490008789715\n",
      "epochs 527\n",
      "training loss 0.006579485351055589\n",
      "epochs 528\n",
      "training loss 0.006499873180902\n",
      "epochs 529\n",
      "training loss 0.006583114220806055\n",
      "testing loss 0.008266446925369455\n",
      "epochs 530\n",
      "training loss 0.006605630090862151\n",
      "epochs 531\n",
      "training loss 0.006552017202224344\n",
      "epochs 532\n",
      "training loss 0.0066170558286689244\n",
      "epochs 533\n",
      "training loss 0.0065656852629601865\n",
      "epochs 534\n",
      "training loss 0.006627039880899424\n",
      "epochs 535\n",
      "training loss 0.006664209537375096\n",
      "epochs 536\n",
      "training loss 0.006630782627480778\n",
      "epochs 537\n",
      "training loss 0.006483483167880393\n",
      "epochs 538\n",
      "training loss 0.006548766839023577\n",
      "epochs 539\n",
      "training loss 0.006536349957089837\n",
      "testing loss 0.008248238228195419\n",
      "epochs 540\n",
      "training loss 0.00659648969622278\n",
      "epochs 541\n",
      "training loss 0.006569428893448756\n",
      "epochs 542\n",
      "training loss 0.006593917312502499\n",
      "epochs 543\n",
      "training loss 0.0065776877661481905\n",
      "epochs 544\n",
      "training loss 0.006534472145748786\n",
      "epochs 545\n",
      "training loss 0.006558561354725534\n",
      "epochs 546\n",
      "training loss 0.006553846593283819\n",
      "epochs 547\n",
      "training loss 0.006571029301231733\n",
      "epochs 548\n",
      "training loss 0.0065823768702191286\n",
      "epochs 549\n",
      "training loss 0.006617861674634292\n",
      "testing loss 0.00750604222267734\n",
      "epochs 550\n",
      "training loss 0.006511540417163137\n",
      "epochs 551\n",
      "training loss 0.006478317294943523\n",
      "epochs 552\n",
      "training loss 0.00651182870891534\n",
      "epochs 553\n",
      "training loss 0.006639328956502096\n",
      "epochs 554\n",
      "training loss 0.006539394059586466\n",
      "epochs 555\n",
      "training loss 0.006605272472193493\n",
      "epochs 556\n",
      "training loss 0.006603339353666887\n",
      "epochs 557\n",
      "training loss 0.006548422066732607\n",
      "epochs 558\n",
      "training loss 0.0065324834436829285\n",
      "epochs 559\n",
      "training loss 0.006645788295310568\n",
      "testing loss 0.00715110638073203\n",
      "epochs 560\n",
      "training loss 0.00653233885054553\n",
      "epochs 561\n",
      "training loss 0.006572805394596876\n",
      "epochs 562\n",
      "training loss 0.0065895019868377445\n",
      "epochs 563\n",
      "training loss 0.006605087070001182\n",
      "epochs 564\n",
      "training loss 0.006599815870213427\n",
      "epochs 565\n",
      "training loss 0.006592171324705734\n",
      "epochs 566\n",
      "training loss 0.006553500549624303\n",
      "epochs 567\n",
      "training loss 0.006609515009566825\n",
      "epochs 568\n",
      "training loss 0.006653437106785192\n",
      "epochs 569\n",
      "training loss 0.006615395577395148\n",
      "testing loss 0.007545333415936969\n",
      "epochs 570\n",
      "training loss 0.006553453854341796\n",
      "epochs 571\n",
      "training loss 0.006546750176049724\n",
      "epochs 572\n",
      "training loss 0.0065569814247161285\n",
      "epochs 573\n",
      "training loss 0.00658241959110378\n",
      "epochs 574\n",
      "training loss 0.00658991469330746\n",
      "epochs 575\n",
      "training loss 0.0066052620305529705\n",
      "epochs 576\n",
      "training loss 0.006590662385750347\n",
      "epochs 577\n",
      "training loss 0.006543464309360413\n",
      "epochs 578\n",
      "training loss 0.006626590734664747\n",
      "epochs 579\n",
      "training loss 0.006472144517226649\n",
      "testing loss 0.007654034705165483\n",
      "epochs 580\n",
      "training loss 0.006527266811896512\n",
      "epochs 581\n",
      "training loss 0.006571150952058428\n",
      "epochs 582\n",
      "training loss 0.006562662332129062\n",
      "epochs 583\n",
      "training loss 0.0066104826707064565\n",
      "epochs 584\n",
      "training loss 0.006541963796538944\n",
      "epochs 585\n",
      "training loss 0.006567679383469275\n",
      "epochs 586\n",
      "training loss 0.006588515117170012\n",
      "epochs 587\n",
      "training loss 0.006580087187838681\n",
      "epochs 588\n",
      "training loss 0.006477350196065127\n",
      "epochs 589\n",
      "training loss 0.006522687145658857\n",
      "testing loss 0.007462527607864839\n",
      "epochs 590\n",
      "training loss 0.0064959096043628265\n",
      "epochs 591\n",
      "training loss 0.006621002142832792\n",
      "epochs 592\n",
      "training loss 0.006560908946120902\n",
      "epochs 593\n",
      "training loss 0.006605142157094547\n",
      "epochs 594\n",
      "training loss 0.00656447466781312\n",
      "epochs 595\n",
      "training loss 0.006499393284745447\n",
      "epochs 596\n",
      "training loss 0.006610495163945872\n",
      "epochs 597\n",
      "training loss 0.006575856244582174\n",
      "epochs 598\n",
      "training loss 0.006569971924925104\n",
      "epochs 599\n",
      "training loss 0.006611308317184221\n",
      "testing loss 0.007642884491051131\n",
      "epochs 600\n",
      "training loss 0.006603025220544007\n",
      "epochs 601\n",
      "training loss 0.006553526966348517\n",
      "epochs 602\n",
      "training loss 0.006548240061107695\n",
      "epochs 603\n",
      "training loss 0.006660106220695907\n",
      "epochs 604\n",
      "training loss 0.006602691737708485\n",
      "epochs 605\n",
      "training loss 0.0066305971722115725\n",
      "epochs 606\n",
      "training loss 0.006583561350826435\n",
      "epochs 607\n",
      "training loss 0.006542362438339321\n",
      "epochs 608\n",
      "training loss 0.006605046107377121\n",
      "epochs 609\n",
      "training loss 0.006525374254378799\n",
      "testing loss 0.0075703339951760165\n",
      "epochs 610\n",
      "training loss 0.006566653269453478\n",
      "epochs 611\n",
      "training loss 0.006612367243690014\n",
      "epochs 612\n",
      "training loss 0.006619942433973576\n",
      "epochs 613\n",
      "training loss 0.006627626775829037\n",
      "epochs 614\n",
      "training loss 0.006535668597672712\n",
      "epochs 615\n",
      "training loss 0.006579747399821945\n",
      "epochs 616\n",
      "training loss 0.006495275718335481\n",
      "epochs 617\n",
      "training loss 0.0065912101782241266\n",
      "epochs 618\n",
      "training loss 0.006552340956772805\n",
      "epochs 619\n",
      "training loss 0.006480382574229618\n",
      "testing loss 0.007464508676293788\n",
      "epochs 620\n",
      "training loss 0.0065656555625454366\n",
      "epochs 621\n",
      "training loss 0.00656937788478396\n",
      "epochs 622\n",
      "training loss 0.00665174099286922\n",
      "epochs 623\n",
      "training loss 0.006607947654535253\n",
      "epochs 624\n",
      "training loss 0.006535250519467433\n",
      "epochs 625\n",
      "training loss 0.006608949705256336\n",
      "epochs 626\n",
      "training loss 0.0065633146867926\n",
      "epochs 627\n",
      "training loss 0.006493959463368479\n",
      "epochs 628\n",
      "training loss 0.006644092156271532\n",
      "epochs 629\n",
      "training loss 0.006575909934691677\n",
      "testing loss 0.007449826400674192\n",
      "epochs 630\n",
      "training loss 0.006422369842811015\n",
      "epochs 631\n",
      "training loss 0.006642658371721765\n",
      "epochs 632\n",
      "training loss 0.006639157299456217\n",
      "epochs 633\n",
      "training loss 0.006535267691612017\n",
      "epochs 634\n",
      "training loss 0.0065823553889384065\n",
      "epochs 635\n",
      "training loss 0.006588032671553295\n",
      "epochs 636\n",
      "training loss 0.00656292335084982\n",
      "epochs 637\n",
      "training loss 0.006586093540476086\n",
      "epochs 638\n",
      "training loss 0.00661240993520545\n",
      "epochs 639\n",
      "training loss 0.0066132076887136446\n",
      "testing loss 0.007251013784524027\n",
      "epochs 640\n",
      "training loss 0.006541420580001157\n",
      "epochs 641\n",
      "training loss 0.006488788742388069\n",
      "epochs 642\n",
      "training loss 0.00663484779744208\n",
      "epochs 643\n",
      "training loss 0.006555914254660936\n",
      "epochs 644\n",
      "training loss 0.00658595831788748\n",
      "epochs 645\n",
      "training loss 0.006751533888922976\n",
      "epochs 646\n",
      "training loss 0.006639509327712506\n",
      "epochs 647\n",
      "training loss 0.006595668964311594\n",
      "epochs 648\n",
      "training loss 0.006592897297729565\n",
      "epochs 649\n",
      "training loss 0.006545622295227276\n",
      "testing loss 0.007315821986123655\n",
      "epochs 650\n",
      "training loss 0.006596107323198242\n",
      "epochs 651\n",
      "training loss 0.006602897494889818\n",
      "epochs 652\n",
      "training loss 0.006489275990402196\n",
      "epochs 653\n",
      "training loss 0.006513886771866865\n",
      "epochs 654\n",
      "training loss 0.006539119886642034\n",
      "epochs 655\n",
      "training loss 0.006606148652474772\n",
      "epochs 656\n",
      "training loss 0.006566421237794009\n",
      "epochs 657\n",
      "training loss 0.006633478798549679\n",
      "epochs 658\n",
      "training loss 0.006490297207543711\n",
      "epochs 659\n",
      "training loss 0.006511926944693126\n",
      "testing loss 0.007744265231468999\n",
      "epochs 660\n",
      "training loss 0.006499736063468262\n",
      "epochs 661\n",
      "training loss 0.006589863612104241\n",
      "epochs 662\n",
      "training loss 0.006584742301466414\n",
      "epochs 663\n",
      "training loss 0.0066091453562431195\n",
      "epochs 664\n",
      "training loss 0.006515559270103267\n",
      "epochs 665\n",
      "training loss 0.006544163236875755\n",
      "epochs 666\n",
      "training loss 0.0065406157283429455\n",
      "epochs 667\n",
      "training loss 0.00653529050689202\n",
      "epochs 668\n",
      "training loss 0.0066118532194441755\n",
      "epochs 669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.006609882565645507\n",
      "testing loss 0.007345991361892868\n",
      "epochs 670\n",
      "training loss 0.006598087347191858\n",
      "epochs 671\n",
      "training loss 0.006562367818349446\n",
      "epochs 672\n",
      "training loss 0.006546816706167687\n",
      "epochs 673\n",
      "training loss 0.006597120375698008\n",
      "epochs 674\n",
      "training loss 0.0065391126338613294\n",
      "epochs 675\n",
      "training loss 0.006566629828573813\n",
      "epochs 676\n",
      "training loss 0.00658190844185639\n",
      "epochs 677\n",
      "training loss 0.00659012279188753\n",
      "epochs 678\n",
      "training loss 0.006597082065857132\n",
      "epochs 679\n",
      "training loss 0.006641759377929713\n",
      "testing loss 0.00850573293072112\n",
      "epochs 680\n",
      "training loss 0.006569565101144524\n",
      "epochs 681\n",
      "training loss 0.006543656774318146\n",
      "epochs 682\n",
      "training loss 0.0064993786677225605\n",
      "epochs 683\n",
      "training loss 0.00657626271982448\n",
      "epochs 684\n",
      "training loss 0.006570228683694314\n",
      "epochs 685\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "#accs = []\n",
    "for t in range(5000):\n",
    "    print('epochs', t)\n",
    "    train_loss = train_func(model, train_loader)\n",
    "    if (t+1) % 10 == 0:\n",
    "        eval_loss = eval_func(model, eval_loader)\n",
    "        #acc = accuracy(model)\n",
    "        \n",
    "        eval_losses.append(eval_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        #accs.append(acc)\n",
    "        #print('accuracy: ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa1d6916d68>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(eval_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa1d6936a58>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(eval_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "label = y_train.cpu()\n",
    "label = y_scaler.inverse_transform(label)\n",
    "with torch.no_grad():\n",
    "    pred = model(X_train)\n",
    "    pred = pred.cpu().numpy()\n",
    "    pred = y_scaler.inverse_transform(pred)\n",
    "    plot(label, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test_func(model, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>total_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X5gsdTWGS3W7JJQB</td>\n",
       "      <td>1.787341e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BTshNOJyKHnT2YIT</td>\n",
       "      <td>4.164898e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dhdymr0lV8N5kZOT</td>\n",
       "      <td>1.366612e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VEwyGGMcD56w5BOc</td>\n",
       "      <td>1.343023e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wmUeMoJZfsqaSX9b</td>\n",
       "      <td>1.932745e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EtBjGAHmHCe9t7TZ</td>\n",
       "      <td>5.281519e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hPNH34vmaZtvBtqc</td>\n",
       "      <td>1.480271e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wXjeI38bYDMJJwZC</td>\n",
       "      <td>1.103791e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fxZSGX6aPAFKU8W4</td>\n",
       "      <td>2.926602e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ewr0Fx6ign87OwaV</td>\n",
       "      <td>6.620901e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gHKurnEP4AowzsLg</td>\n",
       "      <td>1.666348e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PmLfTgY2FElLrTl0</td>\n",
       "      <td>8.056260e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>eM2NppIOwzW0o8iy</td>\n",
       "      <td>9.510944e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dxxwNun97NH4WTrZ</td>\n",
       "      <td>4.979172e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jykBfhh3vdeFUi3H</td>\n",
       "      <td>6.267576e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NlXbvdFfmJZf3L18</td>\n",
       "      <td>2.568831e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D7jaFWHCzSqLBwdt</td>\n",
       "      <td>1.908631e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>L10dBBdqGmemweSl</td>\n",
       "      <td>6.590401e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>OgB0AdiPKlElakKN</td>\n",
       "      <td>2.311245e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>StiWNN1GQrpPBOYt</td>\n",
       "      <td>3.890137e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>a016eMAVQKnfwMnt</td>\n",
       "      <td>3.130959e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gsCFcQHnOH3AKMcZ</td>\n",
       "      <td>7.562794e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>IbNsDXfsPwSuFpow</td>\n",
       "      <td>8.608911e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>EgAVWOVxD1Jy5YkE</td>\n",
       "      <td>2.583701e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BrKghvR76XdbQPnx</td>\n",
       "      <td>8.499508e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>a7fxkXTnUGWHUmKG</td>\n",
       "      <td>3.249687e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>WgzXa170DfpzpURE</td>\n",
       "      <td>1.055089e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>JPWqZbLq0VNC0yKI</td>\n",
       "      <td>1.426529e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>JQgTtbVstqFZwEK1</td>\n",
       "      <td>2.501520e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bCSDbEthlS3nSIor</td>\n",
       "      <td>4.044996e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>QL412tWF5RDIX7IO</td>\n",
       "      <td>2.855370e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>d3c2ceGtckONZzsr</td>\n",
       "      <td>1.074882e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>P1j8YRbxDAovumaI</td>\n",
       "      <td>1.087405e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>IxcBhEoFLcrI9TPr</td>\n",
       "      <td>3.645594e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>rKiV0KDbAl2myBQI</td>\n",
       "      <td>2.926602e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>GSdIXmKr0g5jQQcF</td>\n",
       "      <td>3.384842e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>Am6Wcg3TO64qvzd8</td>\n",
       "      <td>3.890967e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>RZqACAhkL4Tgw4Jr</td>\n",
       "      <td>9.674688e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>u7NKZfWoMUlZy9rJ</td>\n",
       "      <td>5.230691e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>C1BqV4MWH15rjAgz</td>\n",
       "      <td>5.239147e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>wz8A2UbwsgR0lXGJ</td>\n",
       "      <td>4.983484e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>MGJ8ABBTmC2yIaSm</td>\n",
       "      <td>9.950168e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>MjHL2HP1PGIp8aBt</td>\n",
       "      <td>2.409051e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>FMz7nnURFn85LaGt</td>\n",
       "      <td>5.450432e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>kydULx0r0G7OklRD</td>\n",
       "      <td>7.823972e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>nVNYRuk2fRbtlV00</td>\n",
       "      <td>2.945555e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>F8SGEOGPxrPfiRv2</td>\n",
       "      <td>2.375709e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>w7VMfiMvRb765ejK</td>\n",
       "      <td>3.244109e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>lgZWdUKliWt2y5sM</td>\n",
       "      <td>1.530564e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>TER8YrP9mw7UwWwr</td>\n",
       "      <td>6.472142e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>TXHk3oUpVsm5Cmag</td>\n",
       "      <td>5.305676e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>JtgDm9aQcGE9zELB</td>\n",
       "      <td>2.370131e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>wTQmcqbN0OCuSF1t</td>\n",
       "      <td>1.513394e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>WgsI1cBtzSfiWA1j</td>\n",
       "      <td>2.509481e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>qNgt1ajb5uVMKbqm</td>\n",
       "      <td>4.627620e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>UEeCDaAJzPwdKKKA</td>\n",
       "      <td>2.926602e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>i0fgbPaQsDWs7Q87</td>\n",
       "      <td>4.304960e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>YunNwAhcqkf6YclI</td>\n",
       "      <td>2.926602e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>A2NotxtRY9MYoWMl</td>\n",
       "      <td>5.816262e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>kKvgBXiA50gRmQhP</td>\n",
       "      <td>6.059150e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           building_id   total_price\n",
       "0     X5gsdTWGS3W7JJQB  1.787341e+07\n",
       "1     BTshNOJyKHnT2YIT  4.164898e+06\n",
       "2     dhdymr0lV8N5kZOT  1.366612e+07\n",
       "3     VEwyGGMcD56w5BOc  1.343023e+07\n",
       "4     wmUeMoJZfsqaSX9b  1.932745e+06\n",
       "5     EtBjGAHmHCe9t7TZ  5.281519e+06\n",
       "6     hPNH34vmaZtvBtqc  1.480271e+07\n",
       "7     wXjeI38bYDMJJwZC  1.103791e+07\n",
       "8     fxZSGX6aPAFKU8W4  2.926602e+06\n",
       "9     ewr0Fx6ign87OwaV  6.620901e+06\n",
       "10    gHKurnEP4AowzsLg  1.666348e+06\n",
       "11    PmLfTgY2FElLrTl0  8.056260e+06\n",
       "12    eM2NppIOwzW0o8iy  9.510944e+06\n",
       "13    dxxwNun97NH4WTrZ  4.979172e+06\n",
       "14    jykBfhh3vdeFUi3H  6.267576e+06\n",
       "15    NlXbvdFfmJZf3L18  2.568831e+07\n",
       "16    D7jaFWHCzSqLBwdt  1.908631e+06\n",
       "17    L10dBBdqGmemweSl  6.590401e+06\n",
       "18    OgB0AdiPKlElakKN  2.311245e+06\n",
       "19    StiWNN1GQrpPBOYt  3.890137e+06\n",
       "20    a016eMAVQKnfwMnt  3.130959e+07\n",
       "21    gsCFcQHnOH3AKMcZ  7.562794e+06\n",
       "22    IbNsDXfsPwSuFpow  8.608911e+06\n",
       "23    EgAVWOVxD1Jy5YkE  2.583701e+06\n",
       "24    BrKghvR76XdbQPnx  8.499508e+06\n",
       "25    a7fxkXTnUGWHUmKG  3.249687e+07\n",
       "26    WgzXa170DfpzpURE  1.055089e+07\n",
       "27    JPWqZbLq0VNC0yKI  1.426529e+07\n",
       "28    JQgTtbVstqFZwEK1  2.501520e+06\n",
       "29    bCSDbEthlS3nSIor  4.044996e+06\n",
       "...                ...           ...\n",
       "9970  QL412tWF5RDIX7IO  2.855370e+06\n",
       "9971  d3c2ceGtckONZzsr  1.074882e+07\n",
       "9972  P1j8YRbxDAovumaI  1.087405e+07\n",
       "9973  IxcBhEoFLcrI9TPr  3.645594e+06\n",
       "9974  rKiV0KDbAl2myBQI  2.926602e+06\n",
       "9975  GSdIXmKr0g5jQQcF  3.384842e+06\n",
       "9976  Am6Wcg3TO64qvzd8  3.890967e+07\n",
       "9977  RZqACAhkL4Tgw4Jr  9.674688e+07\n",
       "9978  u7NKZfWoMUlZy9rJ  5.230691e+06\n",
       "9979  C1BqV4MWH15rjAgz  5.239147e+06\n",
       "9980  wz8A2UbwsgR0lXGJ  4.983484e+06\n",
       "9981  MGJ8ABBTmC2yIaSm  9.950168e+06\n",
       "9982  MjHL2HP1PGIp8aBt  2.409051e+06\n",
       "9983  FMz7nnURFn85LaGt  5.450432e+07\n",
       "9984  kydULx0r0G7OklRD  7.823972e+06\n",
       "9985  nVNYRuk2fRbtlV00  2.945555e+07\n",
       "9986  F8SGEOGPxrPfiRv2  2.375709e+06\n",
       "9987  w7VMfiMvRb765ejK  3.244109e+07\n",
       "9988  lgZWdUKliWt2y5sM  1.530564e+07\n",
       "9989  TER8YrP9mw7UwWwr  6.472142e+06\n",
       "9990  TXHk3oUpVsm5Cmag  5.305676e+06\n",
       "9991  JtgDm9aQcGE9zELB  2.370131e+06\n",
       "9992  wTQmcqbN0OCuSF1t  1.513394e+07\n",
       "9993  WgsI1cBtzSfiWA1j  2.509481e+07\n",
       "9994  qNgt1ajb5uVMKbqm  4.627620e+06\n",
       "9995  UEeCDaAJzPwdKKKA  2.926602e+06\n",
       "9996  i0fgbPaQsDWs7Q87  4.304960e+07\n",
       "9997  YunNwAhcqkf6YclI  2.926602e+06\n",
       "9998  A2NotxtRY9MYoWMl  5.816262e+06\n",
       "9999  kKvgBXiA50gRmQhP  6.059150e+06\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('./dataset-0510/submit_test.csv')\n",
    "submission['total_price'] = pred\n",
    "submission.to_csv('submission/DNN2_result.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size use 128 or 32 , learning rate use 0.003 which find loss will stock in 0.6\n",
    "\n",
    "Result 1 DNN 233->256->128->1, lr=0.001, batch_size=128, predict score : 13\n",
    "change: \n",
    "- replacing Standard to MinMax \n",
    "- adding DropOut 0.3 layer\n",
    "- batch size change to 512\n",
    "\n",
    "Result 2 lr=0.001, batch_size=64, DNN 233->256->128->64->1\n",
    "after 1k loss : 0.00011785521522113447, can't decrease...\n",
    "- x_scale false\n",
    "- y_scale true\n",
    "\n",
    "Result 3 lr=0.001 batch_size=128 DNN 211->256->512->512->256->128->1\n",
    "after 1w loss : 0.0003, test loss : 0.0007 score: 1670\n",
    "\n",
    "Result 4 lr=0.001 batch_size=128 DNN 211->256->512->512->256->128->64->32->1\n",
    "train_loss: 0.0004, test loss: 0.0002 score: 1600\n",
    "\n",
    "Result 5 lr=0.001 batch_size=128 DNN 211->256->512->512->256->128->64->32->1 + batch_noram\n",
    "train_loss: 0.0005, test loss: 0.003 score: 1000\n",
    "\n",
    "Result 6 lr=0.001 batch_size=128 DNN 211->256->512->512->256->128->64->1 + batch_noram + weight_decay\n",
    "after 2k\n",
    "train_loss: 0.004, test loss 0.006 ,look like L2 regular not work which will increase loss\n",
    "\n",
    "Result 6 lr=0.0015 batch_size=128 DNN 211->256->512->512->256->128->64->32->1 + batch_noram\n",
    "after 3k\n",
    "train_loss 0.0004 test loss 0.002 score : 1400\n",
    "I think we should reduce lr or add L2 regularzation\n",
    "\n",
    "Result 7 lr=0.0015 batch_size= 128 DNN 211->256->512->512->256->128->64->32->1 + batch_noram + weight_decay(0.0005)\n",
    "after 3.5k \n",
    "train loss 0.0007, test loss 0.002, score: 2217\n",
    "regularzation is work, but test loss is not reduce, next tuning lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why output is negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
