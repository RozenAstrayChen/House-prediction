{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler,scale\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "use_gpu = True\n",
    "y_scale = True\n",
    "lr = 0.0003\n",
    "weight_decay = 0.0005\n",
    "\n",
    "# Batch size and learning rate is hyperparameters in deep learning\n",
    "# suggest batch_size is reduced, lr is also reduced which will reduce concussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('./dataset-0510/train.csv')\n",
    "X_test = pd.read_csv('./dataset-0510/test.csv')\n",
    "\n",
    "\n",
    "columns = X.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_id', 'building_material', 'city', 'txn_dt', 'total_floor',\n",
       "       'building_type', 'building_use', 'building_complete_dt', 'parking_way',\n",
       "       'parking_area',\n",
       "       ...\n",
       "       'XIV_500', 'XIV_index_500', 'XIV_1000', 'XIV_index_1000', 'XIV_5000',\n",
       "       'XIV_index_5000', 'XIV_10000', 'XIV_index_10000', 'XIV_MIN',\n",
       "       'total_price'],\n",
       "      dtype='object', length=235)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputer, Scaler, Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/islab/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# step1. Imputation transformer for completing missing values.\n",
    "step1 = ('Imputer', Imputer())\n",
    "# step2. MinMaxScaler\n",
    "step2 = ('MinMaxScaler', MinMaxScaler())\n",
    "# step3. feature selection\n",
    "#step3 = ('FeatureSelection', SelectFromModel(RandomForestRegressor()))\n",
    "step3 = ('FeatureSelection', VarianceThreshold())\n",
    "\n",
    "pipeline = Pipeline(steps=[step1, step2, step3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['total_price']\n",
    "X = X.drop(columns=['building_id', 'total_price'], axis=1)\n",
    "X_test = X_test.drop(columns=['building_id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X sacle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 211)\n",
      "(10000, 211)\n"
     ]
    }
   ],
   "source": [
    "X = pipeline.fit_transform(X)\n",
    "print(X.shape)\n",
    "\n",
    "X_test = pipeline.transform(X_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pd.DataFrame(X, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = MinMaxScaler()\n",
    "if y_scale:\n",
    "    y = y_scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_eval = torch.from_numpy(X_eval).float().to(device)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).float().to(device)\n",
    "y_eval = torch.from_numpy(y_eval).float().to(device)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42000, 211])\n",
      "torch.Size([10000, 211])\n",
      "torch.Size([42000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data.TensorDataset(X_train, y_train)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "eval_dataset = Data.TensorDataset(X_eval, y_eval)\n",
    "eval_loader = Data.DataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # apply a uniform distribution to the weights and a bias=0\n",
    "        m.weight.data.uniform_(0.0, 1.0)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(211, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=512)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=512)\n",
    "        \n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=256)\n",
    "        \n",
    "        self.fc5 = nn.Linear(256, 128)\n",
    "        self.bn5 = nn.BatchNorm1d(num_features=128)\n",
    "        \n",
    "        self.fc6 = nn.Linear(128, 64)\n",
    "        self.bn6 = nn.BatchNorm1d(num_features=64)\n",
    "        \n",
    "        self.fc7 = nn.Linear(64, 32)\n",
    "        self.bn7 = nn.BatchNorm1d(num_features=32)\n",
    "        \n",
    "        self.fc8 = nn.Linear(32, 1)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(0)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = F.relu(self.bn5(self.fc5(x)))\n",
    "        x = F.relu(self.bn6(self.fc6(x)))\n",
    "        x = F.relu(self.bn7(self.fc7(x)))\n",
    "        x = self.fc8(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN().to(device)\n",
    "model.apply(weights_init_uniform)\n",
    "criterion = nn.MSELoss()\n",
    "optim = optim.Adam(model.parameters(), lr= lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_func(model, loader):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        optim.zero_grad()\n",
    "        pred = model(batch_x)\n",
    "        loss = torch.sqrt(criterion(pred, batch_y))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    print('training loss', np.array(train_loss).mean())\n",
    "    return np.array(train_loss).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_func(model, loader):\n",
    "    model.eval()\n",
    "    eval_loss = []\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            pred = model(batch_x)\n",
    "            loss = torch.sqrt(criterion(pred, batch_y))\n",
    "            \n",
    "            eval_loss.append(loss.item())\n",
    "        print('testing loss', np.array(eval_loss).mean())\n",
    "    return np.array(eval_loss).mean()\n",
    "\n",
    "def test_func(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X)\n",
    "        \n",
    "        pred = pred.cpu().numpy()\n",
    "        if y_scale:\n",
    "            pred = y_scaler.inverse_transform(pred)            \n",
    "    return pred\n",
    "\n",
    "\n",
    "def accuracy(model, pct_close=0.5):\n",
    "    #pred, y_eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_eval)\n",
    "        \n",
    "    n_correct = torch.sum((torch.abs(pred - y_eval) < torch.abs(pct_close * y_eval)))\n",
    "    result = (n_correct.item()/len(y_eval))  # scalar\n",
    "    return result \n",
    "\n",
    "def plot(label, pred):\n",
    "    plt.plot(label, label='actual')\n",
    "    plt.plot(pred, label='pred')\n",
    "    plt.legend(frameon=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 0\n",
      "training loss 6.75363528329913\n",
      "epochs 1\n",
      "training loss 3.6869358552625475\n",
      "epochs 2\n",
      "training loss 1.1813627465608272\n",
      "epochs 3\n",
      "training loss 0.13784821392645588\n",
      "epochs 4\n",
      "training loss 0.0691052162425315\n",
      "epochs 5\n",
      "training loss 0.041945271728996265\n",
      "epochs 6\n",
      "training loss 0.033604479621314534\n",
      "epochs 7\n",
      "training loss 0.028863234027914454\n",
      "epochs 8\n",
      "training loss 0.027894222194549703\n",
      "epochs 9\n",
      "training loss 0.026340313208371536\n",
      "testing loss 0.017205774757667637\n",
      "epochs 10\n",
      "training loss 0.025927641789929935\n",
      "epochs 11\n",
      "training loss 0.024666568632364997\n",
      "epochs 12\n",
      "training loss 0.02345344823739834\n",
      "epochs 13\n",
      "training loss 0.023495212543365125\n",
      "epochs 14\n",
      "training loss 0.021514452822608335\n",
      "epochs 15\n",
      "training loss 0.02159960110782218\n",
      "epochs 16\n",
      "training loss 0.02081311467998183\n",
      "epochs 17\n",
      "training loss 0.02054332399112385\n",
      "epochs 18\n",
      "training loss 0.021119379955074978\n",
      "epochs 19\n",
      "training loss 0.01892864763932048\n",
      "testing loss 0.04147569840470104\n",
      "epochs 20\n",
      "training loss 0.01844585027990121\n",
      "epochs 21\n",
      "training loss 0.017812170124383457\n",
      "epochs 22\n",
      "training loss 0.017282048105134836\n",
      "epochs 23\n",
      "training loss 0.017451302841939825\n",
      "epochs 24\n",
      "training loss 0.016494122756316714\n",
      "epochs 25\n",
      "training loss 0.016322447079740534\n",
      "epochs 26\n",
      "training loss 0.015348536713971647\n",
      "epochs 27\n",
      "training loss 0.014953255888335067\n",
      "epochs 28\n",
      "training loss 0.01458248249495736\n",
      "epochs 29\n",
      "training loss 0.014075046887842084\n",
      "testing loss 0.022216032212930368\n",
      "epochs 30\n",
      "training loss 0.013696036767214537\n",
      "epochs 31\n",
      "training loss 0.013225195814761982\n",
      "epochs 32\n",
      "training loss 0.012296095093440457\n",
      "epochs 33\n",
      "training loss 0.012259653217433795\n",
      "epochs 34\n",
      "training loss 0.011695792375551396\n",
      "epochs 35\n",
      "training loss 0.011263209365387546\n",
      "epochs 36\n",
      "training loss 0.010211762149838056\n",
      "epochs 37\n",
      "training loss 0.009586924053643657\n",
      "epochs 38\n",
      "training loss 0.00948070932185351\n",
      "epochs 39\n",
      "training loss 0.008925803558402737\n",
      "testing loss 0.016118319473949306\n",
      "epochs 40\n",
      "training loss 0.008575510355363683\n",
      "epochs 41\n",
      "training loss 0.008536169454059068\n",
      "epochs 42\n",
      "training loss 0.008758307380126194\n",
      "epochs 43\n",
      "training loss 0.008492087193773238\n",
      "epochs 44\n",
      "training loss 0.008601798771492752\n",
      "epochs 45\n",
      "training loss 0.008436962484458523\n",
      "epochs 46\n",
      "training loss 0.008330664511820964\n",
      "epochs 47\n",
      "training loss 0.007908938460386044\n",
      "epochs 48\n",
      "training loss 0.007714350506844492\n",
      "epochs 49\n",
      "training loss 0.007752023391017979\n",
      "testing loss 0.01118366193382981\n",
      "epochs 50\n",
      "training loss 0.0077643843203511285\n",
      "epochs 51\n",
      "training loss 0.0075765711483248375\n",
      "epochs 52\n",
      "training loss 0.007519125794754066\n",
      "epochs 53\n",
      "training loss 0.007453892367260706\n",
      "epochs 54\n",
      "training loss 0.007593190917422525\n",
      "epochs 55\n",
      "training loss 0.007520144403343143\n",
      "epochs 56\n",
      "training loss 0.007494102105127483\n",
      "epochs 57\n",
      "training loss 0.007241556180307024\n",
      "epochs 58\n",
      "training loss 0.007678289768232265\n",
      "epochs 59\n",
      "training loss 0.007257363011658554\n",
      "testing loss 0.012196854166422329\n",
      "epochs 60\n",
      "training loss 0.007012470214879622\n",
      "epochs 61\n",
      "training loss 0.007432167997193101\n",
      "epochs 62\n",
      "training loss 0.007051068465692475\n",
      "epochs 63\n",
      "training loss 0.007198394124047961\n",
      "epochs 64\n",
      "training loss 0.0070647916128139905\n",
      "epochs 65\n",
      "training loss 0.0070184245861296774\n",
      "epochs 66\n",
      "training loss 0.007077878094094276\n",
      "epochs 67\n",
      "training loss 0.007028381598513223\n",
      "epochs 68\n",
      "training loss 0.00674208874374028\n",
      "epochs 69\n",
      "training loss 0.006720104377566749\n",
      "testing loss 0.007002542971085149\n",
      "epochs 70\n",
      "training loss 0.00652597862965283\n",
      "epochs 71\n",
      "training loss 0.006693705785481718\n",
      "epochs 72\n",
      "training loss 0.006639709136743201\n",
      "epochs 73\n",
      "training loss 0.006809923424892195\n",
      "epochs 74\n",
      "training loss 0.006524448738770282\n",
      "epochs 75\n",
      "training loss 0.006400424371385737\n",
      "epochs 76\n",
      "training loss 0.006634190626227294\n",
      "epochs 77\n",
      "training loss 0.006574462951333644\n",
      "epochs 78\n",
      "training loss 0.006770120998405735\n",
      "epochs 79\n",
      "training loss 0.0066634066251466065\n",
      "testing loss 0.012635403451450328\n",
      "epochs 80\n",
      "training loss 0.006547397358192974\n",
      "epochs 81\n",
      "training loss 0.006639112899926318\n",
      "epochs 82\n",
      "training loss 0.0065087124494422445\n",
      "epochs 83\n",
      "training loss 0.006316610850467372\n",
      "epochs 84\n",
      "training loss 0.0064173859061452485\n",
      "epochs 85\n",
      "training loss 0.006434090306351137\n",
      "epochs 86\n",
      "training loss 0.006433789031990552\n",
      "epochs 87\n",
      "training loss 0.006326253571558891\n",
      "epochs 88\n",
      "training loss 0.006381825916893713\n",
      "epochs 89\n",
      "training loss 0.006174520982534124\n",
      "testing loss 0.018966375103761965\n",
      "epochs 90\n",
      "training loss 0.006461116433636169\n",
      "epochs 91\n",
      "training loss 0.006440721695328352\n",
      "epochs 92\n",
      "training loss 0.006200253654916779\n",
      "epochs 93\n",
      "training loss 0.006194872567911369\n",
      "epochs 94\n",
      "training loss 0.005937113223518146\n",
      "epochs 95\n",
      "training loss 0.006483599720494454\n",
      "epochs 96\n",
      "training loss 0.006316612117943612\n",
      "epochs 97\n",
      "training loss 0.006330182030655466\n",
      "epochs 98\n",
      "training loss 0.006202855976128605\n",
      "epochs 99\n",
      "training loss 0.006021901321849336\n",
      "testing loss 0.011176692678573285\n",
      "epochs 100\n",
      "training loss 0.006007914598408441\n",
      "epochs 101\n",
      "training loss 0.006091000741534501\n",
      "epochs 102\n",
      "training loss 0.006033141586694606\n",
      "epochs 103\n",
      "training loss 0.0060335621365139366\n",
      "epochs 104\n",
      "training loss 0.005973889045190847\n",
      "epochs 105\n",
      "training loss 0.006050784873286415\n",
      "epochs 106\n",
      "training loss 0.00595200480044024\n",
      "epochs 107\n",
      "training loss 0.005994676245025452\n",
      "epochs 108\n",
      "training loss 0.005990397861640686\n",
      "epochs 109\n",
      "training loss 0.005980480263097421\n",
      "testing loss 0.016085102447434097\n",
      "epochs 110\n",
      "training loss 0.005964110405141867\n",
      "epochs 111\n",
      "training loss 0.005894554238964586\n",
      "epochs 112\n",
      "training loss 0.005807977503428864\n",
      "epochs 113\n",
      "training loss 0.005836463576157246\n",
      "epochs 114\n",
      "training loss 0.005808439010918412\n",
      "epochs 115\n",
      "training loss 0.005841471380440909\n",
      "epochs 116\n",
      "training loss 0.005955454151395222\n",
      "epochs 117\n",
      "training loss 0.005901411951712041\n",
      "epochs 118\n",
      "training loss 0.005688775826166285\n",
      "epochs 119\n",
      "training loss 0.005844061838966825\n",
      "testing loss 0.00888501399969484\n",
      "epochs 120\n",
      "training loss 0.005748950816849445\n",
      "epochs 121\n",
      "training loss 0.0056718028858898486\n",
      "epochs 122\n",
      "training loss 0.0057430648691471\n",
      "epochs 123\n",
      "training loss 0.005545821661868424\n",
      "epochs 124\n",
      "training loss 0.005533421632556363\n",
      "epochs 125\n",
      "training loss 0.005624273354201762\n",
      "epochs 126\n",
      "training loss 0.005358025512663721\n",
      "epochs 127\n",
      "training loss 0.005385416655059576\n",
      "epochs 128\n",
      "training loss 0.005377996347754016\n",
      "epochs 129\n",
      "training loss 0.00548954447872914\n",
      "testing loss 0.007086126034722683\n",
      "epochs 130\n",
      "training loss 0.005490302637935613\n",
      "epochs 131\n",
      "training loss 0.005615532009701009\n",
      "epochs 132\n",
      "training loss 0.005529510190619841\n",
      "epochs 133\n",
      "training loss 0.005360360992973314\n",
      "epochs 134\n",
      "training loss 0.005411704610481202\n",
      "epochs 135\n",
      "training loss 0.005419534330658253\n",
      "epochs 136\n",
      "training loss 0.005438403127004469\n",
      "epochs 137\n",
      "training loss 0.005347134786060697\n",
      "epochs 138\n",
      "training loss 0.005444211525117871\n",
      "epochs 139\n",
      "training loss 0.005316450943502295\n",
      "testing loss 0.007931856032302405\n",
      "epochs 140\n",
      "training loss 0.005301483028913681\n",
      "epochs 141\n",
      "training loss 0.0054108684873321634\n",
      "epochs 142\n",
      "training loss 0.005436039845017563\n",
      "epochs 143\n",
      "training loss 0.005353158250335026\n",
      "epochs 144\n",
      "training loss 0.005373947618537693\n",
      "epochs 145\n",
      "training loss 0.005206820363954479\n",
      "epochs 146\n",
      "training loss 0.005264554265815776\n",
      "epochs 147\n",
      "training loss 0.005319850183145842\n",
      "epochs 148\n",
      "training loss 0.005234522989874643\n",
      "epochs 149\n",
      "training loss 0.005147379266429342\n",
      "testing loss 0.005651257250207008\n",
      "epochs 150\n",
      "training loss 0.0052791882319075814\n",
      "epochs 151\n",
      "training loss 0.005366671249400655\n",
      "epochs 152\n",
      "training loss 0.0053126324993617675\n",
      "epochs 153\n",
      "training loss 0.005154419592355228\n",
      "epochs 154\n",
      "training loss 0.005391694372873697\n",
      "epochs 155\n",
      "training loss 0.0051966232501004335\n",
      "epochs 156\n",
      "training loss 0.005162850750530971\n",
      "epochs 157\n",
      "training loss 0.005287912263433055\n",
      "epochs 158\n",
      "training loss 0.005199135866760641\n",
      "epochs 159\n",
      "training loss 0.005133657768937705\n",
      "testing loss 0.01146810281863238\n",
      "epochs 160\n",
      "training loss 0.005165635613798189\n",
      "epochs 161\n",
      "training loss 0.005346479361075738\n",
      "epochs 162\n",
      "training loss 0.005223489535963984\n",
      "epochs 163\n",
      "training loss 0.005095903873146373\n",
      "epochs 164\n",
      "training loss 0.005155077262492822\n",
      "epochs 165\n",
      "training loss 0.005182928204009916\n",
      "epochs 166\n",
      "training loss 0.005264422429886882\n",
      "epochs 167\n",
      "training loss 0.00527892337961966\n",
      "epochs 168\n",
      "training loss 0.0052347293054249055\n",
      "epochs 169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.005168740677938862\n",
      "testing loss 0.015239170237266004\n",
      "epochs 170\n",
      "training loss 0.005216853882435223\n",
      "epochs 171\n",
      "training loss 0.005114225293495508\n",
      "epochs 172\n",
      "training loss 0.005171938462106449\n",
      "epochs 173\n",
      "training loss 0.005114019021468299\n",
      "epochs 174\n",
      "training loss 0.0051220076821750105\n",
      "epochs 175\n",
      "training loss 0.0050507204477621475\n",
      "epochs 176\n",
      "training loss 0.0050807637219121935\n",
      "epochs 177\n",
      "training loss 0.005081992598864785\n",
      "epochs 178\n",
      "training loss 0.005140461270360553\n",
      "epochs 179\n",
      "training loss 0.005090730121829136\n",
      "testing loss 0.010922608286115928\n",
      "epochs 180\n",
      "training loss 0.0053989498272374\n",
      "epochs 181\n",
      "training loss 0.005199334064730667\n",
      "epochs 182\n",
      "training loss 0.005261849211090128\n",
      "epochs 183\n",
      "training loss 0.005161168580622862\n",
      "epochs 184\n",
      "training loss 0.00512668860214267\n",
      "epochs 185\n",
      "training loss 0.004973837641198018\n",
      "epochs 186\n",
      "training loss 0.005039150042864169\n",
      "epochs 187\n",
      "training loss 0.005109140457299546\n",
      "epochs 188\n",
      "training loss 0.005096160901546206\n",
      "epochs 189\n",
      "training loss 0.005046314448415441\n",
      "testing loss 0.013233677193469613\n",
      "epochs 190\n",
      "training loss 0.005034941008688152\n",
      "epochs 191\n",
      "training loss 0.004947957250510419\n",
      "epochs 192\n",
      "training loss 0.005027020300914021\n",
      "epochs 193\n",
      "training loss 0.005115736798895664\n",
      "epochs 194\n",
      "training loss 0.005003686838975816\n",
      "epochs 195\n",
      "training loss 0.005047172595998754\n",
      "epochs 196\n",
      "training loss 0.005009407148552113\n",
      "epochs 197\n",
      "training loss 0.005079485762986074\n",
      "epochs 198\n",
      "training loss 0.004999131307155913\n",
      "epochs 199\n",
      "training loss 0.004969571120145315\n",
      "testing loss 0.02046688108420964\n",
      "epochs 200\n",
      "training loss 0.00487105602237161\n",
      "epochs 201\n",
      "training loss 0.0050119213323960915\n",
      "epochs 202\n",
      "training loss 0.005084170913889913\n",
      "epochs 203\n",
      "training loss 0.0049971240642692856\n",
      "epochs 204\n",
      "training loss 0.005090905251873142\n",
      "epochs 205\n",
      "training loss 0.005042991580761471\n",
      "epochs 206\n",
      "training loss 0.005018256786902149\n",
      "epochs 207\n",
      "training loss 0.004989064960262107\n",
      "epochs 208\n",
      "training loss 0.004979516737627313\n",
      "epochs 209\n",
      "training loss 0.0048161995636814454\n",
      "testing loss 0.009268746892954653\n",
      "epochs 210\n",
      "training loss 0.004993525099456537\n",
      "epochs 211\n",
      "training loss 0.004802238497965174\n",
      "epochs 212\n",
      "training loss 0.0048583199158254275\n",
      "epochs 213\n",
      "training loss 0.005024148829955053\n",
      "epochs 214\n",
      "training loss 0.004870457031571598\n",
      "epochs 215\n",
      "training loss 0.004826636412421576\n",
      "epochs 216\n",
      "training loss 0.00480113964843476\n",
      "epochs 217\n",
      "training loss 0.004829751573910648\n",
      "epochs 218\n",
      "training loss 0.004948004131208211\n",
      "epochs 219\n",
      "training loss 0.004858757625404734\n",
      "testing loss 0.005929273952802657\n",
      "epochs 220\n",
      "training loss 0.004735703742850606\n",
      "epochs 221\n",
      "training loss 0.004723916597392602\n",
      "epochs 222\n",
      "training loss 0.00484717012903905\n",
      "epochs 223\n",
      "training loss 0.004895811005266196\n",
      "epochs 224\n",
      "training loss 0.004858770716997673\n",
      "epochs 225\n",
      "training loss 0.004831193416948376\n",
      "epochs 226\n",
      "training loss 0.004740257241471605\n",
      "epochs 227\n",
      "training loss 0.004696849680514185\n",
      "epochs 228\n",
      "training loss 0.004789982945021344\n",
      "epochs 229\n",
      "training loss 0.0047473872450568165\n",
      "testing loss 0.009538350498026356\n",
      "epochs 230\n",
      "training loss 0.004730499559365168\n",
      "epochs 231\n",
      "training loss 0.004722060068116996\n",
      "epochs 232\n",
      "training loss 0.0047703858215141035\n",
      "epochs 233\n",
      "training loss 0.0047064287995630845\n",
      "epochs 234\n",
      "training loss 0.004678163414546444\n",
      "epochs 235\n",
      "training loss 0.004720164950169895\n",
      "epochs 236\n",
      "training loss 0.004811692073881785\n",
      "epochs 237\n",
      "training loss 0.004778138942886314\n",
      "epochs 238\n",
      "training loss 0.0047003886334054555\n",
      "epochs 239\n",
      "training loss 0.00468848378993401\n",
      "testing loss 0.007622140099737027\n",
      "epochs 240\n",
      "training loss 0.004628923566626968\n",
      "epochs 241\n",
      "training loss 0.004820174354306878\n",
      "epochs 242\n",
      "training loss 0.0046713099539659264\n",
      "epochs 243\n",
      "training loss 0.00467795426539771\n",
      "epochs 244\n",
      "training loss 0.004581941489489766\n",
      "epochs 245\n",
      "training loss 0.004766095944858612\n",
      "epochs 246\n",
      "training loss 0.004699742260154076\n",
      "epochs 247\n",
      "training loss 0.004592482807358167\n",
      "epochs 248\n",
      "training loss 0.004704260905301566\n",
      "epochs 249\n",
      "training loss 0.004621358557907146\n",
      "testing loss 0.005038469628569611\n",
      "epochs 250\n",
      "training loss 0.004481747073452569\n",
      "epochs 251\n",
      "training loss 0.004676365080900988\n",
      "epochs 252\n",
      "training loss 0.004750346583753009\n",
      "epochs 253\n",
      "training loss 0.00461576884686041\n",
      "epochs 254\n",
      "training loss 0.004630509174448696\n",
      "epochs 255\n",
      "training loss 0.004687121893083931\n",
      "epochs 256\n",
      "training loss 0.004661380321475306\n",
      "epochs 257\n",
      "training loss 0.004636564488752001\n",
      "epochs 258\n",
      "training loss 0.004510896803746085\n",
      "epochs 259\n",
      "training loss 0.004547020753047296\n",
      "testing loss 0.005925374736362104\n",
      "epochs 260\n",
      "training loss 0.004662487839877967\n",
      "epochs 261\n",
      "training loss 0.004601092903314323\n",
      "epochs 262\n",
      "training loss 0.004533086834393306\n",
      "epochs 263\n",
      "training loss 0.004722948720053088\n",
      "epochs 264\n",
      "training loss 0.004592638033212907\n",
      "epochs 265\n",
      "training loss 0.004521676748608859\n",
      "epochs 266\n",
      "training loss 0.004629809083487827\n",
      "epochs 267\n",
      "training loss 0.004684981772192049\n",
      "epochs 268\n",
      "training loss 0.004643766875421249\n",
      "epochs 269\n",
      "training loss 0.004539526321735561\n",
      "testing loss 0.0051331765659100624\n",
      "epochs 270\n",
      "training loss 0.004627039973442881\n",
      "epochs 271\n",
      "training loss 0.004566103679278022\n",
      "epochs 272\n",
      "training loss 0.004475819087434435\n",
      "epochs 273\n",
      "training loss 0.004484324244531806\n",
      "epochs 274\n",
      "training loss 0.004555878917669657\n",
      "epochs 275\n",
      "training loss 0.004547304182955967\n",
      "epochs 276\n",
      "training loss 0.004524837991103847\n",
      "epochs 277\n",
      "training loss 0.004420046347807696\n",
      "epochs 278\n",
      "training loss 0.004545731006450612\n",
      "epochs 279\n",
      "training loss 0.004384653779004324\n",
      "testing loss 0.008153588989921284\n",
      "epochs 280\n",
      "training loss 0.004457400038861033\n",
      "epochs 281\n",
      "training loss 0.0044923091794174495\n",
      "epochs 282\n",
      "training loss 0.004449925687953852\n",
      "epochs 283\n",
      "training loss 0.0044779292853424565\n",
      "epochs 284\n",
      "training loss 0.004422985152114964\n",
      "epochs 285\n",
      "training loss 0.0045037618349943204\n",
      "epochs 286\n",
      "training loss 0.004581248590448095\n",
      "epochs 287\n",
      "training loss 0.004494000488812936\n",
      "epochs 288\n",
      "training loss 0.004539182830013131\n",
      "epochs 289\n",
      "training loss 0.00443678769059366\n",
      "testing loss 0.00462274876974708\n",
      "epochs 290\n",
      "training loss 0.004390619705094302\n",
      "epochs 291\n",
      "training loss 0.0044295341770750635\n",
      "epochs 292\n",
      "training loss 0.004410818980307542\n",
      "epochs 293\n",
      "training loss 0.004483333694335288\n",
      "epochs 294\n",
      "training loss 0.004448959999151619\n",
      "epochs 295\n",
      "training loss 0.0044539251455352116\n",
      "epochs 296\n",
      "training loss 0.004272509834765935\n",
      "epochs 297\n",
      "training loss 0.004469133202640458\n",
      "epochs 298\n",
      "training loss 0.004524027332479536\n",
      "epochs 299\n",
      "training loss 0.004369539076017957\n",
      "testing loss 0.025432887599400596\n",
      "epochs 300\n",
      "training loss 0.004436891861075937\n",
      "epochs 301\n",
      "training loss 0.004377755843524717\n",
      "epochs 302\n",
      "training loss 0.004343610389248714\n",
      "epochs 303\n",
      "training loss 0.004331986142087467\n",
      "epochs 304\n",
      "training loss 0.0045948868587692366\n",
      "epochs 305\n",
      "training loss 0.0043513760243834064\n",
      "epochs 306\n",
      "training loss 0.004384527583733926\n",
      "epochs 307\n",
      "training loss 0.004365990109993626\n",
      "epochs 308\n",
      "training loss 0.004429125850156833\n",
      "epochs 309\n",
      "training loss 0.004398165052717036\n",
      "testing loss 0.0050889059128437905\n",
      "epochs 310\n",
      "training loss 0.004331563772792195\n",
      "epochs 311\n",
      "training loss 0.0043879143057226865\n",
      "epochs 312\n",
      "training loss 0.004420766964665187\n",
      "epochs 313\n",
      "training loss 0.0045133854381423045\n",
      "epochs 314\n",
      "training loss 0.004315617095146861\n",
      "epochs 315\n",
      "training loss 0.004392588882293767\n",
      "epochs 316\n",
      "training loss 0.004323368243827827\n",
      "epochs 317\n",
      "training loss 0.004297038313074994\n",
      "epochs 318\n",
      "training loss 0.004322309176137495\n",
      "epochs 319\n",
      "training loss 0.004348610909613229\n",
      "testing loss 0.004748167553487241\n",
      "epochs 320\n",
      "training loss 0.0043520232309420285\n",
      "epochs 321\n",
      "training loss 0.0044769490371942295\n",
      "epochs 322\n",
      "training loss 0.004333273296762473\n",
      "epochs 323\n",
      "training loss 0.004345600797559805\n",
      "epochs 324\n",
      "training loss 0.004305480354137175\n",
      "epochs 325\n",
      "training loss 0.004359785741970713\n",
      "epochs 326\n",
      "training loss 0.004310787307582569\n",
      "epochs 327\n",
      "training loss 0.004411399355979132\n",
      "epochs 328\n",
      "training loss 0.004426376646778074\n",
      "epochs 329\n",
      "training loss 0.004267189722143023\n",
      "testing loss 0.01437299195941247\n",
      "epochs 330\n",
      "training loss 0.004436563055730201\n",
      "epochs 331\n",
      "training loss 0.004319115749229096\n",
      "epochs 332\n",
      "training loss 0.004334098224579088\n",
      "epochs 333\n",
      "training loss 0.004309429112668956\n",
      "epochs 334\n",
      "training loss 0.004400593859191678\n",
      "epochs 335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.004257697038816736\n",
      "epochs 336\n",
      "training loss 0.004320853593261024\n",
      "epochs 337\n",
      "training loss 0.004243422692392508\n",
      "epochs 338\n",
      "training loss 0.004268712791583096\n",
      "epochs 339\n",
      "training loss 0.004205928928230105\n",
      "testing loss 0.0076681173915135944\n",
      "epochs 340\n",
      "training loss 0.004278396059529669\n",
      "epochs 341\n",
      "training loss 0.004285195032431883\n",
      "epochs 342\n",
      "training loss 0.00433009425369247\n",
      "epochs 343\n",
      "training loss 0.004137673554305089\n",
      "epochs 344\n",
      "training loss 0.004262539542543399\n",
      "epochs 345\n",
      "training loss 0.004377872917000035\n",
      "epochs 346\n",
      "training loss 0.004211352106628198\n",
      "epochs 347\n",
      "training loss 0.004356747639993924\n",
      "epochs 348\n",
      "training loss 0.004182690762671271\n",
      "epochs 349\n",
      "training loss 0.004222691936296691\n",
      "testing loss 0.0061414563972238745\n",
      "epochs 350\n",
      "training loss 0.004258737520040168\n",
      "epochs 351\n",
      "training loss 0.004308115394069816\n",
      "epochs 352\n",
      "training loss 0.004210405452261852\n",
      "epochs 353\n",
      "training loss 0.0043032719292975165\n",
      "epochs 354\n",
      "training loss 0.004207802246081272\n",
      "epochs 355\n",
      "training loss 0.00423403021576602\n",
      "epochs 356\n",
      "training loss 0.004358057563595112\n",
      "epochs 357\n",
      "training loss 0.004277121535740159\n",
      "epochs 358\n",
      "training loss 0.004254024523935963\n",
      "epochs 359\n",
      "training loss 0.00424053615078013\n",
      "testing loss 0.01370371180645963\n",
      "epochs 360\n",
      "training loss 0.004113541816228202\n",
      "epochs 361\n",
      "training loss 0.004089160393116752\n",
      "epochs 362\n",
      "training loss 0.004086245267473637\n",
      "epochs 363\n",
      "training loss 0.004196244415099588\n",
      "epochs 364\n",
      "training loss 0.004236406418438265\n",
      "epochs 365\n",
      "training loss 0.004205871461168792\n",
      "epochs 366\n",
      "training loss 0.004262134554869521\n",
      "epochs 367\n",
      "training loss 0.004151916049284957\n",
      "epochs 368\n",
      "training loss 0.004100507144719974\n",
      "epochs 369\n",
      "training loss 0.004126990775897601\n",
      "testing loss 0.005206933699165147\n",
      "epochs 370\n",
      "training loss 0.004144769030662769\n",
      "epochs 371\n",
      "training loss 0.004261912042020302\n",
      "epochs 372\n",
      "training loss 0.004087738956729556\n",
      "epochs 373\n",
      "training loss 0.004155051511624618\n",
      "epochs 374\n",
      "training loss 0.004283473865867884\n",
      "epochs 375\n",
      "training loss 0.004094751731147225\n",
      "epochs 376\n",
      "training loss 0.00409998685563136\n",
      "epochs 377\n",
      "training loss 0.00408337456860656\n",
      "epochs 378\n",
      "training loss 0.004086568153330258\n",
      "epochs 379\n",
      "training loss 0.004214352371856162\n",
      "testing loss 0.00515202643739469\n",
      "epochs 380\n",
      "training loss 0.0042402750117517445\n",
      "epochs 381\n",
      "training loss 0.0041292990557141715\n",
      "epochs 382\n",
      "training loss 0.0041771081724206955\n",
      "epochs 383\n",
      "training loss 0.0042581744966207575\n",
      "epochs 384\n",
      "training loss 0.004170312840593441\n",
      "epochs 385\n",
      "training loss 0.004186115799100905\n",
      "epochs 386\n",
      "training loss 0.004109143760369519\n",
      "epochs 387\n",
      "training loss 0.004107193458021412\n",
      "epochs 388\n",
      "training loss 0.004094894035255089\n",
      "epochs 389\n",
      "training loss 0.0042294227464356685\n",
      "testing loss 0.007334853086541308\n",
      "epochs 390\n",
      "training loss 0.004166042405296695\n",
      "epochs 391\n",
      "training loss 0.004171505986180689\n",
      "epochs 392\n",
      "training loss 0.004126742512969963\n",
      "epochs 393\n",
      "training loss 0.004114286451441969\n",
      "epochs 394\n",
      "training loss 0.004091153573810963\n",
      "epochs 395\n",
      "training loss 0.00405024501918263\n",
      "epochs 396\n",
      "training loss 0.004117673280646056\n",
      "epochs 397\n",
      "training loss 0.004150966917289178\n",
      "epochs 398\n",
      "training loss 0.004045578286668857\n",
      "epochs 399\n",
      "training loss 0.00409531248146017\n",
      "testing loss 0.006668867926436959\n",
      "epochs 400\n",
      "training loss 0.0040930462427901555\n",
      "epochs 401\n",
      "training loss 0.003995087399008743\n",
      "epochs 402\n",
      "training loss 0.004037148784492221\n",
      "epochs 403\n",
      "training loss 0.004158845458655281\n",
      "epochs 404\n",
      "training loss 0.003971222262634219\n",
      "epochs 405\n",
      "training loss 0.00411323061613451\n",
      "epochs 406\n",
      "training loss 0.00406175306226809\n",
      "epochs 407\n",
      "training loss 0.004033978699386618\n",
      "epochs 408\n",
      "training loss 0.003970895871225642\n",
      "epochs 409\n",
      "training loss 0.00402518390256085\n",
      "testing loss 0.026495312426424195\n",
      "epochs 410\n",
      "training loss 0.004146455763060724\n",
      "epochs 411\n",
      "training loss 0.003948508730330887\n",
      "epochs 412\n",
      "training loss 0.004035780118922717\n",
      "epochs 413\n",
      "training loss 0.003940293839623116\n",
      "epochs 414\n",
      "training loss 0.00400500893019604\n",
      "epochs 415\n",
      "training loss 0.003959316924486117\n",
      "epochs 416\n",
      "training loss 0.004008825503906807\n",
      "epochs 417\n",
      "training loss 0.003992482406613742\n",
      "epochs 418\n",
      "training loss 0.003893067714925233\n",
      "epochs 419\n",
      "training loss 0.0038702090183771174\n",
      "testing loss 0.004511198583752551\n",
      "epochs 420\n",
      "training loss 0.00393496756724968\n",
      "epochs 421\n",
      "training loss 0.0039034205033107007\n",
      "epochs 422\n",
      "training loss 0.003956249172694551\n",
      "epochs 423\n",
      "training loss 0.003968113489126316\n",
      "epochs 424\n",
      "training loss 0.003950007483122536\n",
      "epochs 425\n",
      "training loss 0.003920564115525854\n",
      "epochs 426\n",
      "training loss 0.003915527771088354\n",
      "epochs 427\n",
      "training loss 0.004150218223816061\n",
      "epochs 428\n",
      "training loss 0.0038265348376711906\n",
      "epochs 429\n",
      "training loss 0.003945406058188924\n",
      "testing loss 0.005532581098245602\n",
      "epochs 430\n",
      "training loss 0.004006325802978094\n",
      "epochs 431\n",
      "training loss 0.004018797352697734\n",
      "epochs 432\n",
      "training loss 0.003912826669529805\n",
      "epochs 433\n",
      "training loss 0.0038291311063478194\n",
      "epochs 434\n",
      "training loss 0.004066322898311048\n",
      "epochs 435\n",
      "training loss 0.0038592094292943463\n",
      "epochs 436\n",
      "training loss 0.0038757758345285085\n",
      "epochs 437\n",
      "training loss 0.003778250121205796\n",
      "epochs 438\n",
      "training loss 0.0037829352501711857\n",
      "epochs 439\n",
      "training loss 0.0039771489019146174\n",
      "testing loss 0.0056314452713130845\n",
      "epochs 440\n",
      "training loss 0.0038468730501479563\n",
      "epochs 441\n",
      "training loss 0.00389405657214574\n",
      "epochs 442\n",
      "training loss 0.003929526708584199\n",
      "epochs 443\n",
      "training loss 0.003840008572219534\n",
      "epochs 444\n",
      "training loss 0.003808310207285236\n",
      "epochs 445\n",
      "training loss 0.0037737400111767794\n",
      "epochs 446\n",
      "training loss 0.00389244788082877\n",
      "epochs 447\n",
      "training loss 0.003826734983209396\n",
      "epochs 448\n",
      "training loss 0.00393419964791001\n",
      "epochs 449\n",
      "training loss 0.003844458975342292\n",
      "testing loss 0.007470462695820957\n",
      "epochs 450\n",
      "training loss 0.0037444653930736984\n",
      "epochs 451\n",
      "training loss 0.0038414911471737557\n",
      "epochs 452\n",
      "training loss 0.003851125188069956\n",
      "epochs 453\n",
      "training loss 0.0038029528007132076\n",
      "epochs 454\n",
      "training loss 0.0037902243701475008\n",
      "epochs 455\n",
      "training loss 0.0038426906202345137\n",
      "epochs 456\n",
      "training loss 0.004008495155792672\n",
      "epochs 457\n",
      "training loss 0.0037723156891757547\n",
      "epochs 458\n",
      "training loss 0.003873866738958177\n",
      "epochs 459\n",
      "training loss 0.0038335412188000005\n",
      "testing loss 0.004536196945321\n",
      "epochs 460\n",
      "training loss 0.003988889701336078\n",
      "epochs 461\n",
      "training loss 0.0037324386489603067\n",
      "epochs 462\n",
      "training loss 0.003779014998177876\n",
      "epochs 463\n",
      "training loss 0.0037326792857997507\n",
      "epochs 464\n",
      "training loss 0.003674402589185056\n",
      "epochs 465\n",
      "training loss 0.003715697726543347\n",
      "epochs 466\n",
      "training loss 0.0038637921789457233\n",
      "epochs 467\n",
      "training loss 0.0037372460929212645\n",
      "epochs 468\n",
      "training loss 0.003838341805226195\n",
      "epochs 469\n",
      "training loss 0.003899116886108774\n",
      "testing loss 0.00678268890391956\n",
      "epochs 470\n",
      "training loss 0.0038500720975456313\n",
      "epochs 471\n",
      "training loss 0.003746822339448636\n",
      "epochs 472\n",
      "training loss 0.0036989215600069353\n",
      "epochs 473\n",
      "training loss 0.0037087999294175113\n",
      "epochs 474\n",
      "training loss 0.003772294429756452\n",
      "epochs 475\n",
      "training loss 0.0037138934808395416\n",
      "epochs 476\n",
      "training loss 0.0037182738514769946\n",
      "epochs 477\n",
      "training loss 0.0037827446715662815\n",
      "epochs 478\n",
      "training loss 0.003774348963094496\n",
      "epochs 479\n",
      "training loss 0.003668107777426308\n",
      "testing loss 0.005573289937848318\n",
      "epochs 480\n",
      "training loss 0.003690735856991114\n",
      "epochs 481\n",
      "training loss 0.003803340393673391\n",
      "epochs 482\n",
      "training loss 0.0036444405969911706\n",
      "epochs 483\n",
      "training loss 0.0037978430944537365\n",
      "epochs 484\n",
      "training loss 0.0037174681700499995\n",
      "epochs 485\n",
      "training loss 0.003807964140625733\n",
      "epochs 486\n",
      "training loss 0.003720354844354614\n",
      "epochs 487\n",
      "training loss 0.003689061502832636\n",
      "epochs 488\n",
      "training loss 0.0036346282633757474\n",
      "epochs 489\n",
      "training loss 0.0037304901275189315\n",
      "testing loss 0.012149899778557374\n",
      "epochs 490\n",
      "training loss 0.0038029834812812755\n",
      "epochs 491\n",
      "training loss 0.003738222151887136\n",
      "epochs 492\n",
      "training loss 0.0036530741076423903\n",
      "epochs 493\n",
      "training loss 0.00373710932225657\n",
      "epochs 494\n",
      "training loss 0.0036523963868233026\n",
      "epochs 495\n",
      "training loss 0.0036662166834154977\n",
      "epochs 496\n",
      "training loss 0.0037050544698991584\n",
      "epochs 497\n",
      "training loss 0.003739206701740736\n",
      "epochs 498\n",
      "training loss 0.003811485144326684\n",
      "epochs 499\n",
      "training loss 0.0036592209194348077\n",
      "testing loss 0.004737285923556233\n",
      "epochs 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0036154675787869604\n",
      "epochs 501\n",
      "training loss 0.003697183506498198\n",
      "epochs 502\n",
      "training loss 0.0037118727362525045\n",
      "epochs 503\n",
      "training loss 0.0035230986417291014\n",
      "epochs 504\n",
      "training loss 0.003563648540920105\n",
      "epochs 505\n",
      "training loss 0.003627714103351044\n",
      "epochs 506\n",
      "training loss 0.0036897945502988067\n",
      "epochs 507\n",
      "training loss 0.0036538760694406224\n",
      "epochs 508\n",
      "training loss 0.0036287278338196748\n",
      "epochs 509\n",
      "training loss 0.003705689505925883\n",
      "testing loss 0.0067873492734572775\n",
      "epochs 510\n",
      "training loss 0.0037396260583914794\n",
      "epochs 511\n",
      "training loss 0.0036906728472866366\n",
      "epochs 512\n",
      "training loss 0.003615891594173653\n",
      "epochs 513\n",
      "training loss 0.0036128352160528757\n",
      "epochs 514\n",
      "training loss 0.0036955813641623117\n",
      "epochs 515\n",
      "training loss 0.0036896622819701454\n",
      "epochs 516\n",
      "training loss 0.003546137547072329\n",
      "epochs 517\n",
      "training loss 0.0036535120695138684\n",
      "epochs 518\n",
      "training loss 0.0036733797825674744\n",
      "epochs 519\n",
      "training loss 0.0036487688620413417\n",
      "testing loss 0.005805859216720077\n",
      "epochs 520\n",
      "training loss 0.0035597189565933674\n",
      "epochs 521\n",
      "training loss 0.0035262672152047508\n",
      "epochs 522\n",
      "training loss 0.003507871527541825\n",
      "epochs 523\n",
      "training loss 0.003596775493114666\n",
      "epochs 524\n",
      "training loss 0.003573236933917849\n",
      "epochs 525\n",
      "training loss 0.003635154809522525\n",
      "epochs 526\n",
      "training loss 0.0036390288803715112\n",
      "epochs 527\n",
      "training loss 0.003624422629264758\n",
      "epochs 528\n",
      "training loss 0.0036893316645792325\n",
      "epochs 529\n",
      "training loss 0.0037107933581577426\n",
      "testing loss 0.008845537580576138\n",
      "epochs 530\n",
      "training loss 0.003694623082771467\n",
      "epochs 531\n",
      "training loss 0.0035376646110114266\n",
      "epochs 532\n",
      "training loss 0.003572756670469186\n",
      "epochs 533\n",
      "training loss 0.003646377469469598\n",
      "epochs 534\n",
      "training loss 0.0035810946585259494\n",
      "epochs 535\n",
      "training loss 0.0037965489445978885\n",
      "epochs 536\n",
      "training loss 0.0037322465435100727\n",
      "epochs 537\n",
      "training loss 0.0034853749700445445\n",
      "epochs 538\n",
      "training loss 0.003657793203697152\n",
      "epochs 539\n",
      "training loss 0.003673532014531522\n",
      "testing loss 0.004362550896843433\n",
      "epochs 540\n",
      "training loss 0.0035648657604174456\n",
      "epochs 541\n",
      "training loss 0.0035268759809994905\n",
      "epochs 542\n",
      "training loss 0.003488120134979328\n",
      "epochs 543\n",
      "training loss 0.0035374144638540355\n",
      "epochs 544\n",
      "training loss 0.0036474085107904303\n",
      "epochs 545\n",
      "training loss 0.0036031785537842605\n",
      "epochs 546\n",
      "training loss 0.0035757471716716476\n",
      "epochs 547\n",
      "training loss 0.0035114118282323684\n",
      "epochs 548\n",
      "training loss 0.003612808077836743\n",
      "epochs 549\n",
      "training loss 0.0035500794367462757\n",
      "testing loss 0.0046631693592334685\n",
      "epochs 550\n",
      "training loss 0.0034600366462491975\n",
      "epochs 551\n",
      "training loss 0.0036247440130452663\n",
      "epochs 552\n",
      "training loss 0.003571410419626054\n",
      "epochs 553\n",
      "training loss 0.003565541401683809\n",
      "epochs 554\n",
      "training loss 0.003610113405305496\n",
      "epochs 555\n",
      "training loss 0.0034522823734622235\n",
      "epochs 556\n",
      "training loss 0.003594398270658356\n",
      "epochs 557\n",
      "training loss 0.0034068657363422418\n",
      "epochs 558\n",
      "training loss 0.0033682371768534274\n",
      "epochs 559\n",
      "training loss 0.003514302576365555\n",
      "testing loss 0.005515733580204084\n",
      "epochs 560\n",
      "training loss 0.0036487161966697467\n",
      "epochs 561\n",
      "training loss 0.0035809250028328377\n",
      "epochs 562\n",
      "training loss 0.00351236342830374\n",
      "epochs 563\n",
      "training loss 0.0034962912977568373\n",
      "epochs 564\n",
      "training loss 0.003455918806316303\n",
      "epochs 565\n",
      "training loss 0.003558591424663039\n",
      "epochs 566\n",
      "training loss 0.003434723342194202\n",
      "epochs 567\n",
      "training loss 0.003481436769214762\n",
      "epochs 568\n",
      "training loss 0.0036579551529790357\n",
      "epochs 569\n",
      "training loss 0.0035685050849007345\n",
      "testing loss 0.0056625606231259324\n",
      "epochs 570\n",
      "training loss 0.003500081973481572\n",
      "epochs 571\n",
      "training loss 0.0036411212925660484\n",
      "epochs 572\n",
      "training loss 0.0035040013585522726\n",
      "epochs 573\n",
      "training loss 0.0035061075834778212\n",
      "epochs 574\n",
      "training loss 0.003516524411613365\n",
      "epochs 575\n",
      "training loss 0.003576184255297446\n",
      "epochs 576\n",
      "training loss 0.0035331604693022677\n",
      "epochs 577\n",
      "training loss 0.0034452083019545377\n",
      "epochs 578\n",
      "training loss 0.003558514972704273\n",
      "epochs 579\n",
      "training loss 0.003463936153315234\n",
      "testing loss 0.008012273104478282\n",
      "epochs 580\n",
      "training loss 0.003495565386302203\n",
      "epochs 581\n",
      "training loss 0.003494899738353527\n",
      "epochs 582\n",
      "training loss 0.003592547474748888\n",
      "epochs 583\n",
      "training loss 0.0035126731875366715\n",
      "epochs 584\n",
      "training loss 0.003556506506925998\n",
      "epochs 585\n",
      "training loss 0.00340896319013406\n",
      "epochs 586\n",
      "training loss 0.003442269478539588\n",
      "epochs 587\n",
      "training loss 0.003486060551905546\n",
      "epochs 588\n",
      "training loss 0.003442474495475166\n",
      "epochs 589\n",
      "training loss 0.0035219381437396296\n",
      "testing loss 0.007133884259651528\n",
      "epochs 590\n",
      "training loss 0.003428056737405211\n",
      "epochs 591\n",
      "training loss 0.00356394974344996\n",
      "epochs 592\n",
      "training loss 0.003463634123493518\n",
      "epochs 593\n",
      "training loss 0.0035666903911871794\n",
      "epochs 594\n",
      "training loss 0.0035266792231825345\n",
      "epochs 595\n",
      "training loss 0.0035116377815258854\n",
      "epochs 596\n",
      "training loss 0.003464128610842202\n",
      "epochs 597\n",
      "training loss 0.0032593291061774666\n",
      "epochs 598\n",
      "training loss 0.003357316262235916\n",
      "epochs 599\n",
      "training loss 0.0033964793442672054\n",
      "testing loss 0.015402461174241405\n",
      "epochs 600\n",
      "training loss 0.0035602359520837575\n",
      "epochs 601\n",
      "training loss 0.0034197554583901874\n",
      "epochs 602\n",
      "training loss 0.003433520443550676\n",
      "epochs 603\n",
      "training loss 0.003434938108647599\n",
      "epochs 604\n",
      "training loss 0.003419422491815926\n",
      "epochs 605\n",
      "training loss 0.0035096753312920228\n",
      "epochs 606\n",
      "training loss 0.0033767230279381335\n",
      "epochs 607\n",
      "training loss 0.003333564988538442\n",
      "epochs 608\n",
      "training loss 0.0034143279082252216\n",
      "epochs 609\n",
      "training loss 0.0033400085674015673\n",
      "testing loss 0.005210356672908714\n",
      "epochs 610\n",
      "training loss 0.0032753364567285385\n",
      "epochs 611\n",
      "training loss 0.0034559024529712767\n",
      "epochs 612\n",
      "training loss 0.0034879236896083574\n",
      "epochs 613\n",
      "training loss 0.0034678485722204606\n",
      "epochs 614\n",
      "training loss 0.003416825461308939\n",
      "epochs 615\n",
      "training loss 0.0035057444239352297\n",
      "epochs 616\n",
      "training loss 0.003406328798093377\n",
      "epochs 617\n",
      "training loss 0.0034546787120928383\n",
      "epochs 618\n",
      "training loss 0.003354310107595743\n",
      "epochs 619\n",
      "training loss 0.0033911933717643418\n",
      "testing loss 0.004245720987029849\n",
      "epochs 620\n",
      "training loss 0.0034086081644486193\n",
      "epochs 621\n",
      "training loss 0.0033777140857266844\n",
      "epochs 622\n",
      "training loss 0.00341502958830831\n",
      "epochs 623\n",
      "training loss 0.003484078939143557\n",
      "epochs 624\n",
      "training loss 0.003426604087557927\n",
      "epochs 625\n",
      "training loss 0.0034835238641660127\n",
      "epochs 626\n",
      "training loss 0.003411258097403997\n",
      "epochs 627\n",
      "training loss 0.003413431495031778\n",
      "epochs 628\n",
      "training loss 0.003468326090532143\n",
      "epochs 629\n",
      "training loss 0.003447175553878118\n",
      "testing loss 0.004614220336839187\n",
      "epochs 630\n",
      "training loss 0.0034049707525027604\n",
      "epochs 631\n",
      "training loss 0.0033408003971744952\n",
      "epochs 632\n",
      "training loss 0.0033539532385497583\n",
      "epochs 633\n",
      "training loss 0.003392772108959006\n",
      "epochs 634\n",
      "training loss 0.0033991528681329365\n",
      "epochs 635\n",
      "training loss 0.0033172815231362917\n",
      "epochs 636\n",
      "training loss 0.003449697774625454\n",
      "epochs 637\n",
      "training loss 0.0034908537464895736\n",
      "epochs 638\n",
      "training loss 0.0033423220749383958\n",
      "epochs 639\n",
      "training loss 0.0033957873316096724\n",
      "testing loss 0.005041725684208333\n",
      "epochs 640\n",
      "training loss 0.0033593913780859356\n",
      "epochs 641\n",
      "training loss 0.0033153574340509698\n",
      "epochs 642\n",
      "training loss 0.0033806012331233514\n",
      "epochs 643\n",
      "training loss 0.00334782444300147\n",
      "epochs 644\n",
      "training loss 0.0033587060000402087\n",
      "epochs 645\n",
      "training loss 0.0033362968658496228\n",
      "epochs 646\n",
      "training loss 0.0032648614368551497\n",
      "epochs 647\n",
      "training loss 0.0033174759285852858\n",
      "epochs 648\n",
      "training loss 0.003264536932507094\n",
      "epochs 649\n",
      "training loss 0.003328417732145377\n",
      "testing loss 0.00559408442077643\n",
      "epochs 650\n",
      "training loss 0.0034345942708638166\n",
      "epochs 651\n",
      "training loss 0.0033486338288418517\n",
      "epochs 652\n",
      "training loss 0.003373978470910714\n",
      "epochs 653\n",
      "training loss 0.00348749767180736\n",
      "epochs 654\n",
      "training loss 0.0033703060809375665\n",
      "epochs 655\n",
      "training loss 0.0032311247390663777\n",
      "epochs 656\n",
      "training loss 0.003284927569386354\n",
      "epochs 657\n",
      "training loss 0.0033720295043311176\n",
      "epochs 658\n",
      "training loss 0.0033831619070191785\n",
      "epochs 659\n",
      "training loss 0.0033221532722317974\n",
      "testing loss 0.00532642269427789\n",
      "epochs 660\n",
      "training loss 0.0032817988941661086\n",
      "epochs 661\n",
      "training loss 0.0034565192248847756\n",
      "epochs 662\n",
      "training loss 0.003419787058603913\n",
      "epochs 663\n",
      "training loss 0.003392291857541106\n",
      "epochs 664\n",
      "training loss 0.0033039029678443056\n",
      "epochs 665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0032388348713386973\n",
      "epochs 666\n",
      "training loss 0.0033234494864249936\n",
      "epochs 667\n",
      "training loss 0.003435226595368927\n",
      "epochs 668\n",
      "training loss 0.0032945539006867127\n",
      "epochs 669\n",
      "training loss 0.003294582855548019\n",
      "testing loss 0.004451914839050237\n",
      "epochs 670\n",
      "training loss 0.003244652338877977\n",
      "epochs 671\n",
      "training loss 0.0033818552947085137\n",
      "epochs 672\n",
      "training loss 0.003196388639801247\n",
      "epochs 673\n",
      "training loss 0.003303011363417499\n",
      "epochs 674\n",
      "training loss 0.0033659651789865186\n",
      "epochs 675\n",
      "training loss 0.0033077198653241285\n",
      "epochs 676\n",
      "training loss 0.0032545505042009533\n",
      "epochs 677\n",
      "training loss 0.003372040595986674\n",
      "epochs 678\n",
      "training loss 0.003427491795786835\n",
      "epochs 679\n",
      "training loss 0.0033583810123385674\n",
      "testing loss 0.004306671319443548\n",
      "epochs 680\n",
      "training loss 0.003335462836760994\n",
      "epochs 681\n",
      "training loss 0.0033224387732008896\n",
      "epochs 682\n",
      "training loss 0.0033587874826270306\n",
      "epochs 683\n",
      "training loss 0.0032674381792500025\n",
      "epochs 684\n",
      "training loss 0.003278321491010872\n",
      "epochs 685\n",
      "training loss 0.00324984478989282\n",
      "epochs 686\n",
      "training loss 0.0032286184502335257\n",
      "epochs 687\n",
      "training loss 0.0033249450344337564\n",
      "epochs 688\n",
      "training loss 0.0034193601830803015\n",
      "epochs 689\n",
      "training loss 0.0032321776577984873\n",
      "testing loss 0.013374956321393978\n",
      "epochs 690\n",
      "training loss 0.0032915900108237443\n",
      "epochs 691\n",
      "training loss 0.0032354712898143793\n",
      "epochs 692\n",
      "training loss 0.00323644681942669\n",
      "epochs 693\n",
      "training loss 0.0034071686107942714\n",
      "epochs 694\n",
      "training loss 0.0033679862246964705\n",
      "epochs 695\n",
      "training loss 0.0032014518553011777\n",
      "epochs 696\n",
      "training loss 0.0033011232519743108\n",
      "epochs 697\n",
      "training loss 0.003234639355493851\n",
      "epochs 698\n",
      "training loss 0.0032735385749443776\n",
      "epochs 699\n",
      "training loss 0.003312453057894364\n",
      "testing loss 0.005653181261092372\n",
      "epochs 700\n",
      "training loss 0.00327569200999831\n",
      "epochs 701\n",
      "training loss 0.0032358351053873866\n",
      "epochs 702\n",
      "training loss 0.0032824581247643996\n",
      "epochs 703\n",
      "training loss 0.003239139668943536\n",
      "epochs 704\n",
      "training loss 0.0032280265897835143\n",
      "epochs 705\n",
      "training loss 0.0032815978325207045\n",
      "epochs 706\n",
      "training loss 0.003266423771520199\n",
      "epochs 707\n",
      "training loss 0.003148594115702237\n",
      "epochs 708\n",
      "training loss 0.0033434519684366814\n",
      "epochs 709\n",
      "training loss 0.003217676052212217\n",
      "testing loss 0.007145451991125307\n",
      "epochs 710\n",
      "training loss 0.0032593793830888113\n",
      "epochs 711\n",
      "training loss 0.0032547499240010887\n",
      "epochs 712\n",
      "training loss 0.0031884988657719707\n",
      "epochs 713\n",
      "training loss 0.003210460310688633\n",
      "epochs 714\n",
      "training loss 0.0032132341329002678\n",
      "epochs 715\n",
      "training loss 0.003282363019815694\n",
      "epochs 716\n",
      "training loss 0.003302259519653774\n",
      "epochs 717\n",
      "training loss 0.003246551688822796\n",
      "epochs 718\n",
      "training loss 0.003407137966672338\n",
      "epochs 719\n",
      "training loss 0.003252083454762322\n",
      "testing loss 0.005052871678473679\n",
      "epochs 720\n",
      "training loss 0.003260027507523862\n",
      "epochs 721\n",
      "training loss 0.0031982306638692923\n",
      "epochs 722\n",
      "training loss 0.0032828662598981184\n",
      "epochs 723\n",
      "training loss 0.0032613530088397235\n",
      "epochs 724\n",
      "training loss 0.003283933658537803\n",
      "epochs 725\n",
      "training loss 0.003134327602366093\n",
      "epochs 726\n",
      "training loss 0.003231402427470453\n",
      "epochs 727\n",
      "training loss 0.0034196164121421046\n",
      "epochs 728\n",
      "training loss 0.003235547101665798\n",
      "epochs 729\n",
      "training loss 0.0031976302134546827\n",
      "testing loss 0.00476176781309042\n",
      "epochs 730\n",
      "training loss 0.003346177653974279\n",
      "epochs 731\n",
      "training loss 0.0031878196083767888\n",
      "epochs 732\n",
      "training loss 0.003151802565317918\n",
      "epochs 733\n",
      "training loss 0.0032254568482310395\n",
      "epochs 734\n",
      "training loss 0.00334168600338146\n",
      "epochs 735\n",
      "training loss 0.003240975831553204\n",
      "epochs 736\n",
      "training loss 0.0031949300931109893\n",
      "epochs 737\n",
      "training loss 0.003198054498825845\n",
      "epochs 738\n",
      "training loss 0.0031265608374012412\n",
      "epochs 739\n",
      "training loss 0.003242424643587174\n",
      "testing loss 0.004089267440602606\n",
      "epochs 740\n",
      "training loss 0.0032412353791593713\n",
      "epochs 741\n",
      "training loss 0.0032771639458659615\n",
      "epochs 742\n",
      "training loss 0.003265589521277843\n",
      "epochs 743\n",
      "training loss 0.0032046473557469195\n",
      "epochs 744\n",
      "training loss 0.0032625074323555485\n",
      "epochs 745\n",
      "training loss 0.0031550688195639606\n",
      "epochs 746\n",
      "training loss 0.003154816579053852\n",
      "epochs 747\n",
      "training loss 0.0031987932163197266\n",
      "epochs 748\n",
      "training loss 0.003205901324270593\n",
      "epochs 749\n",
      "training loss 0.0031501685080223218\n",
      "testing loss 0.005173581738209893\n",
      "epochs 750\n",
      "training loss 0.0031914835237391226\n",
      "epochs 751\n",
      "training loss 0.0031568149131606686\n",
      "epochs 752\n",
      "training loss 0.0033653587676828168\n",
      "epochs 753\n",
      "training loss 0.0033155537785758407\n",
      "epochs 754\n",
      "training loss 0.003197942349824794\n",
      "epochs 755\n",
      "training loss 0.0031053059873111704\n",
      "epochs 756\n",
      "training loss 0.00316606032645109\n",
      "epochs 757\n",
      "training loss 0.0032109905481564963\n",
      "epochs 758\n",
      "training loss 0.0031833534145240864\n",
      "epochs 759\n",
      "training loss 0.0031227928449719645\n",
      "testing loss 0.005528821304427288\n",
      "epochs 760\n",
      "training loss 0.0031536966119963664\n",
      "epochs 761\n",
      "training loss 0.0032034550894668896\n",
      "epochs 762\n",
      "training loss 0.003210211317422927\n",
      "epochs 763\n",
      "training loss 0.0033103443440163118\n",
      "epochs 764\n",
      "training loss 0.0031794821627658867\n",
      "epochs 765\n",
      "training loss 0.003148183082570081\n",
      "epochs 766\n",
      "training loss 0.0031382069462276203\n",
      "epochs 767\n",
      "training loss 0.0031856953344763595\n",
      "epochs 768\n",
      "training loss 0.0031364882933853427\n",
      "epochs 769\n",
      "training loss 0.003203078750223714\n",
      "testing loss 0.007003146534164746\n",
      "epochs 770\n",
      "training loss 0.0031639530665261\n",
      "epochs 771\n",
      "training loss 0.0033774204112696927\n",
      "epochs 772\n",
      "training loss 0.00311870558546873\n",
      "epochs 773\n",
      "training loss 0.003097587176966608\n",
      "epochs 774\n",
      "training loss 0.003057950534390733\n",
      "epochs 775\n",
      "training loss 0.003167603465326493\n",
      "epochs 776\n",
      "training loss 0.003233485443639289\n",
      "epochs 777\n",
      "training loss 0.0032535425702599654\n",
      "epochs 778\n",
      "training loss 0.0031680467414335663\n",
      "epochs 779\n",
      "training loss 0.00313113291718145\n",
      "testing loss 0.006941622375768233\n",
      "epochs 780\n",
      "training loss 0.0031503481018331503\n",
      "epochs 781\n",
      "training loss 0.003196508045121461\n",
      "epochs 782\n",
      "training loss 0.0032427341951125252\n",
      "epochs 783\n",
      "training loss 0.0032711964745306635\n",
      "epochs 784\n",
      "training loss 0.003081192325101458\n",
      "epochs 785\n",
      "training loss 0.0031001314391328255\n",
      "epochs 786\n",
      "training loss 0.0031552735902227484\n",
      "epochs 787\n",
      "training loss 0.0031806275812557992\n",
      "epochs 788\n",
      "training loss 0.0031167858731041245\n",
      "epochs 789\n",
      "training loss 0.00323568366568445\n",
      "testing loss 0.007177223491066313\n",
      "epochs 790\n",
      "training loss 0.00317981315523352\n",
      "epochs 791\n",
      "training loss 0.003112083424440384\n",
      "epochs 792\n",
      "training loss 0.0032542111436301407\n",
      "epochs 793\n",
      "training loss 0.0032961597608821324\n",
      "epochs 794\n",
      "training loss 0.003059849961827818\n",
      "epochs 795\n",
      "training loss 0.0031299208963889978\n",
      "epochs 796\n",
      "training loss 0.0032030304421113076\n",
      "epochs 797\n",
      "training loss 0.0031194194682836987\n",
      "epochs 798\n",
      "training loss 0.0031822232910259244\n",
      "epochs 799\n",
      "training loss 0.0030378523749075717\n",
      "testing loss 0.0051118718470109905\n",
      "epochs 800\n",
      "training loss 0.003146423417565308\n",
      "epochs 801\n",
      "training loss 0.003136761946530056\n",
      "epochs 802\n",
      "training loss 0.0030528552600927084\n",
      "epochs 803\n",
      "training loss 0.0030827003529034893\n",
      "epochs 804\n",
      "training loss 0.003285638525541362\n",
      "epochs 805\n",
      "training loss 0.0031441577100180837\n",
      "epochs 806\n",
      "training loss 0.003142724208191241\n",
      "epochs 807\n",
      "training loss 0.0031963908732769654\n",
      "epochs 808\n",
      "training loss 0.003099835306507929\n",
      "epochs 809\n",
      "training loss 0.0031143638859488817\n",
      "testing loss 0.007591310320488104\n",
      "epochs 810\n",
      "training loss 0.0030724638903168195\n",
      "epochs 811\n",
      "training loss 0.0030773955059031134\n",
      "epochs 812\n",
      "training loss 0.0030168578991847605\n",
      "epochs 813\n",
      "training loss 0.0030868252830248675\n",
      "epochs 814\n",
      "training loss 0.003009834193580679\n",
      "epochs 815\n",
      "training loss 0.0030495806336742586\n",
      "epochs 816\n",
      "training loss 0.0029887532649737272\n",
      "epochs 817\n",
      "training loss 0.0030302833939129686\n",
      "epochs 818\n",
      "training loss 0.0031258657866222697\n",
      "epochs 819\n",
      "training loss 0.0030669686359729854\n",
      "testing loss 0.0057907842268097275\n",
      "epochs 820\n",
      "training loss 0.0029964309097635687\n",
      "epochs 821\n",
      "training loss 0.0030987481544173597\n",
      "epochs 822\n",
      "training loss 0.003092079258829604\n",
      "epochs 823\n",
      "training loss 0.003016507283835403\n",
      "epochs 824\n",
      "training loss 0.0031252384708067658\n",
      "epochs 825\n",
      "training loss 0.0030456673092772125\n",
      "epochs 826\n",
      "training loss 0.003008871768122839\n",
      "epochs 827\n",
      "training loss 0.003134702898117524\n",
      "epochs 828\n",
      "training loss 0.003006355821064427\n",
      "epochs 829\n",
      "training loss 0.0029342804669535588\n",
      "testing loss 0.004007599797473018\n",
      "epochs 830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0030583095362432594\n",
      "epochs 831\n",
      "training loss 0.003025151875172954\n",
      "epochs 832\n",
      "training loss 0.0030374660777313053\n",
      "epochs 833\n",
      "training loss 0.0030359489849264704\n",
      "epochs 834\n",
      "training loss 0.0030341553455784827\n",
      "epochs 835\n",
      "training loss 0.003133777999970213\n",
      "epochs 836\n",
      "training loss 0.003043459775775535\n",
      "epochs 837\n",
      "training loss 0.0029856168309730508\n",
      "epochs 838\n",
      "training loss 0.002996792938768309\n",
      "epochs 839\n",
      "training loss 0.003031512201157905\n",
      "testing loss 0.005581233072497532\n",
      "epochs 840\n",
      "training loss 0.0030192770102800163\n",
      "epochs 841\n",
      "training loss 0.00293480349445608\n",
      "epochs 842\n",
      "training loss 0.002999175578429389\n",
      "epochs 843\n",
      "training loss 0.0030226821241203954\n",
      "epochs 844\n",
      "training loss 0.0029846547864303898\n",
      "epochs 845\n",
      "training loss 0.003156128692607704\n",
      "epochs 846\n",
      "training loss 0.0030047066642769744\n",
      "epochs 847\n",
      "training loss 0.002968833193686822\n",
      "epochs 848\n",
      "training loss 0.0029251340604712762\n",
      "epochs 849\n",
      "training loss 0.0030057456844704267\n",
      "testing loss 0.005663208125145934\n",
      "epochs 850\n",
      "training loss 0.0030302271309894675\n",
      "epochs 851\n",
      "training loss 0.0030503395610803168\n",
      "epochs 852\n",
      "training loss 0.003045070859268138\n",
      "epochs 853\n",
      "training loss 0.0028534785606954983\n",
      "epochs 854\n",
      "training loss 0.0029512355114704163\n",
      "epochs 855\n",
      "training loss 0.0029495221900252633\n",
      "epochs 856\n",
      "training loss 0.002934497684144095\n",
      "epochs 857\n",
      "training loss 0.003120856779616179\n",
      "epochs 858\n",
      "training loss 0.0029234978470153044\n",
      "epochs 859\n",
      "training loss 0.002953637885828545\n",
      "testing loss 0.004278386183452944\n",
      "epochs 860\n",
      "training loss 0.0029289564726404324\n",
      "epochs 861\n",
      "training loss 0.003025731481406498\n",
      "epochs 862\n",
      "training loss 0.0029242496203636826\n",
      "epochs 863\n",
      "training loss 0.0030284774225024015\n",
      "epochs 864\n",
      "training loss 0.0029797090049748136\n",
      "epochs 865\n",
      "training loss 0.0028997731294573533\n",
      "epochs 866\n",
      "training loss 0.002962146906923831\n",
      "epochs 867\n",
      "training loss 0.002908270608097326\n",
      "epochs 868\n",
      "training loss 0.002913716721906126\n",
      "epochs 869\n",
      "training loss 0.002963291724583298\n",
      "testing loss 0.004095612315903583\n",
      "epochs 870\n",
      "training loss 0.0029534717590412223\n",
      "epochs 871\n",
      "training loss 0.002876291176591544\n",
      "epochs 872\n",
      "training loss 0.003025781230202743\n",
      "epochs 873\n",
      "training loss 0.0029998054180512246\n",
      "epochs 874\n",
      "training loss 0.0029615870723642386\n",
      "epochs 875\n",
      "training loss 0.0029573216641570884\n",
      "epochs 876\n",
      "training loss 0.0030528421500997794\n",
      "epochs 877\n",
      "training loss 0.0028832973585050767\n",
      "epochs 878\n",
      "training loss 0.0028607540878560675\n",
      "epochs 879\n",
      "training loss 0.0028679272454333886\n",
      "testing loss 0.0061453380605456884\n",
      "epochs 880\n",
      "training loss 0.002958082036316214\n",
      "epochs 881\n",
      "training loss 0.0030691891131599896\n",
      "epochs 882\n",
      "training loss 0.002922911291070124\n",
      "epochs 883\n",
      "training loss 0.0029805727666781757\n",
      "epochs 884\n",
      "training loss 0.003079361820488738\n",
      "epochs 885\n",
      "training loss 0.0029199129670411123\n",
      "epochs 886\n",
      "training loss 0.0029195295370786837\n",
      "epochs 887\n",
      "training loss 0.002882223941122485\n",
      "epochs 888\n",
      "training loss 0.002931862360780838\n",
      "epochs 889\n",
      "training loss 0.0028844439991789453\n",
      "testing loss 0.00555281561356804\n",
      "epochs 890\n",
      "training loss 0.002975024448178346\n",
      "epochs 891\n",
      "training loss 0.002897395585531986\n",
      "epochs 892\n",
      "training loss 0.002854567754123428\n",
      "epochs 893\n",
      "training loss 0.0028232250629434516\n",
      "epochs 894\n",
      "training loss 0.002966718343412079\n",
      "epochs 895\n",
      "training loss 0.002952678999538027\n",
      "epochs 896\n",
      "training loss 0.0029750132105613304\n",
      "epochs 897\n",
      "training loss 0.0029024338957607337\n",
      "epochs 898\n",
      "training loss 0.002892448807292332\n",
      "epochs 899\n",
      "training loss 0.0028250817004959227\n",
      "testing loss 0.004435675705003665\n",
      "epochs 900\n",
      "training loss 0.0029813183868221804\n",
      "epochs 901\n",
      "training loss 0.0029259583342036285\n",
      "epochs 902\n",
      "training loss 0.002988383325388638\n",
      "epochs 903\n",
      "training loss 0.0029080701989438783\n",
      "epochs 904\n",
      "training loss 0.002870228087938586\n",
      "epochs 905\n",
      "training loss 0.002902756258456099\n",
      "epochs 906\n",
      "training loss 0.0029289006973166687\n",
      "epochs 907\n",
      "training loss 0.002866858661055882\n",
      "epochs 908\n",
      "training loss 0.002920124718903827\n",
      "epochs 909\n",
      "training loss 0.002912493683942693\n",
      "testing loss 0.004286017213991004\n",
      "epochs 910\n",
      "training loss 0.002983072843568224\n",
      "epochs 911\n",
      "training loss 0.0028898936744473627\n",
      "epochs 912\n",
      "training loss 0.0028844713429795822\n",
      "epochs 913\n",
      "training loss 0.0029484617257049153\n",
      "epochs 914\n",
      "training loss 0.0029052022606060814\n",
      "epochs 915\n",
      "training loss 0.002903825513902474\n",
      "epochs 916\n",
      "training loss 0.0028717568843755702\n",
      "epochs 917\n",
      "training loss 0.0028752350999611377\n",
      "epochs 918\n",
      "training loss 0.002841479893197733\n",
      "epochs 919\n",
      "training loss 0.0028857115138106976\n",
      "testing loss 0.0045706751518585585\n",
      "epochs 920\n",
      "training loss 0.0028565078269616674\n",
      "epochs 921\n",
      "training loss 0.002948426802523751\n",
      "epochs 922\n",
      "training loss 0.002903543519723798\n",
      "epochs 923\n",
      "training loss 0.0028574831182587223\n",
      "epochs 924\n",
      "training loss 0.0029145532962087666\n",
      "epochs 925\n",
      "training loss 0.002809677476496478\n",
      "epochs 926\n",
      "training loss 0.002855428895420511\n",
      "epochs 927\n",
      "training loss 0.0029142383697632374\n",
      "epochs 928\n",
      "training loss 0.002937241658033367\n",
      "epochs 929\n",
      "training loss 0.0028992111164450893\n",
      "testing loss 0.005254573840978192\n",
      "epochs 930\n",
      "training loss 0.0028560731248961236\n",
      "epochs 931\n",
      "training loss 0.0028647263176982663\n",
      "epochs 932\n",
      "training loss 0.0028723849724598886\n",
      "epochs 933\n",
      "training loss 0.0028817709252283567\n",
      "epochs 934\n",
      "training loss 0.002772952003919698\n",
      "epochs 935\n",
      "training loss 0.0028432779807433216\n",
      "epochs 936\n",
      "training loss 0.002930580120318566\n",
      "epochs 937\n",
      "training loss 0.0027382796206355865\n",
      "epochs 938\n",
      "training loss 0.0029307269635624436\n",
      "epochs 939\n",
      "training loss 0.002877823006726892\n",
      "testing loss 0.010991595100928495\n",
      "epochs 940\n",
      "training loss 0.0028812983108678757\n",
      "epochs 941\n",
      "training loss 0.00294602153459026\n",
      "epochs 942\n",
      "training loss 0.002869511072348384\n",
      "epochs 943\n",
      "training loss 0.0028370752968502693\n",
      "epochs 944\n",
      "training loss 0.0029391767706588973\n",
      "epochs 945\n",
      "training loss 0.0028980035829502173\n",
      "epochs 946\n",
      "training loss 0.0028471652617236445\n",
      "epochs 947\n",
      "training loss 0.0028795475231506223\n",
      "epochs 948\n",
      "training loss 0.0028339132357326456\n",
      "epochs 949\n",
      "training loss 0.0029470049004752677\n",
      "testing loss 0.004124662628337899\n",
      "epochs 950\n",
      "training loss 0.002810118580873954\n",
      "epochs 951\n",
      "training loss 0.002949044846867333\n",
      "epochs 952\n",
      "training loss 0.002875601982494029\n",
      "epochs 953\n",
      "training loss 0.0029626055611764565\n",
      "epochs 954\n",
      "training loss 0.0027620234561370727\n",
      "epochs 955\n",
      "training loss 0.0027637694434030687\n",
      "epochs 956\n",
      "training loss 0.0027915806653569693\n",
      "epochs 957\n",
      "training loss 0.0028668247164357285\n",
      "epochs 958\n",
      "training loss 0.0029567211535886655\n",
      "epochs 959\n",
      "training loss 0.002740055062570312\n",
      "testing loss 0.004059631205725332\n",
      "epochs 960\n",
      "training loss 0.0027842553435771028\n",
      "epochs 961\n",
      "training loss 0.0029210492356051905\n",
      "epochs 962\n",
      "training loss 0.002761820440731184\n",
      "epochs 963\n",
      "training loss 0.0028371410224994355\n",
      "epochs 964\n",
      "training loss 0.0029910444688918156\n",
      "epochs 965\n",
      "training loss 0.0028088723552076424\n",
      "epochs 966\n",
      "training loss 0.002818697873654416\n",
      "epochs 967\n",
      "training loss 0.002844883067785312\n",
      "epochs 968\n",
      "training loss 0.0028268408793163426\n",
      "epochs 969\n",
      "training loss 0.0027815523020580614\n",
      "testing loss 0.003921763905396699\n",
      "epochs 970\n",
      "training loss 0.0029164352411783574\n",
      "epochs 971\n",
      "training loss 0.002719898944533795\n",
      "epochs 972\n",
      "training loss 0.0028375965572460513\n",
      "epochs 973\n",
      "training loss 0.002902762652806496\n",
      "epochs 974\n",
      "training loss 0.0027551563494869914\n",
      "epochs 975\n",
      "training loss 0.002734108588808304\n",
      "epochs 976\n",
      "training loss 0.002762652814351475\n",
      "epochs 977\n",
      "training loss 0.002799070468717354\n",
      "epochs 978\n",
      "training loss 0.0028617419281232776\n",
      "epochs 979\n",
      "training loss 0.0028399836596124167\n",
      "testing loss 0.0037828853216801657\n",
      "epochs 980\n",
      "training loss 0.0028325043276156156\n",
      "epochs 981\n",
      "training loss 0.0027326896194351615\n",
      "epochs 982\n",
      "training loss 0.0027087527386685634\n",
      "epochs 983\n",
      "training loss 0.0027292322202537513\n",
      "epochs 984\n",
      "training loss 0.0027710781250457565\n",
      "epochs 985\n",
      "training loss 0.0025962452723723934\n",
      "epochs 986\n",
      "training loss 0.0025929911582274956\n",
      "epochs 987\n",
      "training loss 0.002608023840073485\n",
      "epochs 988\n",
      "training loss 0.00257700349478119\n",
      "epochs 989\n",
      "training loss 0.0025220814006725796\n",
      "testing loss 0.004040270725080202\n",
      "epochs 990\n",
      "training loss 0.0025554918968911343\n",
      "epochs 991\n",
      "training loss 0.0025692452583886615\n",
      "epochs 992\n",
      "training loss 0.002474624049668639\n",
      "epochs 993\n",
      "training loss 0.002582715951277107\n",
      "epochs 994\n",
      "training loss 0.0024232930368616452\n",
      "epochs 995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0024914055939559967\n",
      "epochs 996\n",
      "training loss 0.0023942949477863415\n",
      "epochs 997\n",
      "training loss 0.002400073028200711\n",
      "epochs 998\n",
      "training loss 0.002378222779731767\n",
      "epochs 999\n",
      "training loss 0.0023748011987848846\n",
      "testing loss 0.004033724556128818\n",
      "epochs 1000\n",
      "training loss 0.002427139066125756\n",
      "epochs 1001\n",
      "training loss 0.002610973730551696\n",
      "epochs 1002\n",
      "training loss 0.0025245739762051354\n",
      "epochs 1003\n",
      "training loss 0.0023573868781311216\n",
      "epochs 1004\n",
      "training loss 0.0024600481235147608\n",
      "epochs 1005\n",
      "training loss 0.0026381639636328335\n",
      "epochs 1006\n",
      "training loss 0.002318174876541601\n",
      "epochs 1007\n",
      "training loss 0.002347903499184431\n",
      "epochs 1008\n",
      "training loss 0.0023155520959561946\n",
      "epochs 1009\n",
      "training loss 0.002396650166685288\n",
      "testing loss 0.0064159702189293105\n",
      "epochs 1010\n",
      "training loss 0.0022764765685084516\n",
      "epochs 1011\n",
      "training loss 0.002359038990947749\n",
      "epochs 1012\n",
      "training loss 0.002336338783067999\n",
      "epochs 1013\n",
      "training loss 0.0023166633157645517\n",
      "epochs 1014\n",
      "training loss 0.0025242905547887677\n",
      "epochs 1015\n",
      "training loss 0.0023470191241844944\n",
      "epochs 1016\n",
      "training loss 0.002277625743250184\n",
      "epochs 1017\n",
      "training loss 0.0024347444134622835\n",
      "epochs 1018\n",
      "training loss 0.0023723777783765687\n",
      "epochs 1019\n",
      "training loss 0.0023778328749189848\n",
      "testing loss 0.004059408172132804\n",
      "epochs 1020\n",
      "training loss 0.00235374008077494\n",
      "epochs 1021\n",
      "training loss 0.002401866151818207\n",
      "epochs 1022\n",
      "training loss 0.0023203274052190992\n",
      "epochs 1023\n",
      "training loss 0.002338840855472356\n",
      "epochs 1024\n",
      "training loss 0.002336062969162913\n",
      "epochs 1025\n",
      "training loss 0.0023806796771527966\n",
      "epochs 1026\n",
      "training loss 0.002242879445259785\n",
      "epochs 1027\n",
      "training loss 0.002339152244165947\n",
      "epochs 1028\n",
      "training loss 0.0023917614069758806\n",
      "epochs 1029\n",
      "training loss 0.00238411491885385\n",
      "testing loss 0.004278569033118428\n",
      "epochs 1030\n",
      "training loss 0.0023403474059674923\n",
      "epochs 1031\n",
      "training loss 0.0023090850686057155\n",
      "epochs 1032\n",
      "training loss 0.00234776784490375\n",
      "epochs 1033\n",
      "training loss 0.0022849934716994867\n",
      "epochs 1034\n",
      "training loss 0.002311563733568851\n",
      "epochs 1035\n",
      "training loss 0.0022190097134877395\n",
      "epochs 1036\n",
      "training loss 0.00228267399283511\n",
      "epochs 1037\n",
      "training loss 0.002338385945971557\n",
      "epochs 1038\n",
      "training loss 0.002290468314438945\n",
      "epochs 1039\n",
      "training loss 0.0023330201714732725\n",
      "testing loss 0.0033670489152840583\n",
      "epochs 1040\n",
      "training loss 0.0024003233993425965\n",
      "epochs 1041\n",
      "training loss 0.002295745249008222\n",
      "epochs 1042\n",
      "training loss 0.002249475163910085\n",
      "epochs 1043\n",
      "training loss 0.002370781478391377\n",
      "epochs 1044\n",
      "training loss 0.0023005976912064486\n",
      "epochs 1045\n",
      "training loss 0.002225682765880945\n",
      "epochs 1046\n",
      "training loss 0.0023346137916060634\n",
      "epochs 1047\n",
      "training loss 0.0023449725941344416\n",
      "epochs 1048\n",
      "training loss 0.002226141778756454\n",
      "epochs 1049\n",
      "training loss 0.0023542665220063893\n",
      "testing loss 0.003480234915714262\n",
      "epochs 1050\n",
      "training loss 0.0022402651159313582\n",
      "epochs 1051\n",
      "training loss 0.002153022863900736\n",
      "epochs 1052\n",
      "training loss 0.002255840262215539\n",
      "epochs 1053\n",
      "training loss 0.002247631185020017\n",
      "epochs 1054\n",
      "training loss 0.0022450328610291123\n",
      "epochs 1055\n",
      "training loss 0.0021933517385186417\n",
      "epochs 1056\n",
      "training loss 0.002234619442553268\n",
      "epochs 1057\n",
      "training loss 0.002212188100105429\n",
      "epochs 1058\n",
      "training loss 0.002347013871341174\n",
      "epochs 1059\n",
      "training loss 0.002274738931476413\n",
      "testing loss 0.0038623736018698054\n",
      "epochs 1060\n",
      "training loss 0.002264424670724011\n",
      "epochs 1061\n",
      "training loss 0.0022331632538925075\n",
      "epochs 1062\n",
      "training loss 0.0023163061217110503\n",
      "epochs 1063\n",
      "training loss 0.002266966193591706\n",
      "epochs 1064\n",
      "training loss 0.0022780330165920602\n",
      "epochs 1065\n",
      "training loss 0.0022704231968194836\n",
      "epochs 1066\n",
      "training loss 0.002177138631140083\n",
      "epochs 1067\n",
      "training loss 0.002153675833725481\n",
      "epochs 1068\n",
      "training loss 0.0021727992735497406\n",
      "epochs 1069\n",
      "training loss 0.002294871896663879\n",
      "testing loss 0.0037944838404655457\n",
      "epochs 1070\n",
      "training loss 0.0022615555701791493\n",
      "epochs 1071\n",
      "training loss 0.002218853145849107\n",
      "epochs 1072\n",
      "training loss 0.0021583885034067857\n",
      "epochs 1073\n",
      "training loss 0.002242500923848555\n",
      "epochs 1074\n",
      "training loss 0.0022268452351462764\n",
      "epochs 1075\n",
      "training loss 0.0022461864356684716\n",
      "epochs 1076\n",
      "training loss 0.002160479995955665\n",
      "epochs 1077\n",
      "training loss 0.002278586138693735\n",
      "epochs 1078\n",
      "training loss 0.002208489492977191\n",
      "epochs 1079\n",
      "training loss 0.0021716913521165272\n",
      "testing loss 0.004496872221971763\n",
      "epochs 1080\n",
      "training loss 0.002147502622193456\n",
      "epochs 1081\n",
      "training loss 0.0023790924614550133\n",
      "epochs 1082\n",
      "training loss 0.0021695484413261495\n",
      "epochs 1083\n",
      "training loss 0.002204636495070685\n",
      "epochs 1084\n",
      "training loss 0.0023248137258301774\n",
      "epochs 1085\n",
      "training loss 0.0022193204649047513\n",
      "epochs 1086\n",
      "training loss 0.0021737567577256303\n",
      "epochs 1087\n",
      "training loss 0.002303560824192902\n",
      "epochs 1088\n",
      "training loss 0.0021473678500507605\n",
      "epochs 1089\n",
      "training loss 0.002152299272260235\n",
      "testing loss 0.003955916769089217\n",
      "epochs 1090\n",
      "training loss 0.0021688606921586246\n",
      "epochs 1091\n",
      "training loss 0.002324934238865179\n",
      "epochs 1092\n",
      "training loss 0.0020475830393772584\n",
      "epochs 1093\n",
      "training loss 0.002112916007986162\n",
      "epochs 1094\n",
      "training loss 0.0023040666172739494\n",
      "epochs 1095\n",
      "training loss 0.0021777662206554395\n",
      "epochs 1096\n",
      "training loss 0.0021095850510596324\n",
      "epochs 1097\n",
      "training loss 0.002140127467578928\n",
      "epochs 1098\n",
      "training loss 0.0021421516444718063\n",
      "epochs 1099\n",
      "training loss 0.002162759976410576\n",
      "testing loss 0.0046586279867956715\n",
      "epochs 1100\n",
      "training loss 0.0022084642784446365\n",
      "epochs 1101\n",
      "training loss 0.0021406448613229493\n",
      "epochs 1102\n",
      "training loss 0.002093644764403159\n",
      "epochs 1103\n",
      "training loss 0.0021583626742437937\n",
      "epochs 1104\n",
      "training loss 0.002133766598263501\n",
      "epochs 1105\n",
      "training loss 0.0022870099637657404\n",
      "epochs 1106\n",
      "training loss 0.002153743801696515\n",
      "epochs 1107\n",
      "training loss 0.0021622359761855024\n",
      "epochs 1108\n",
      "training loss 0.0021826096376585335\n",
      "epochs 1109\n",
      "training loss 0.002181219220823838\n",
      "testing loss 0.006651155568051634\n",
      "epochs 1110\n",
      "training loss 0.0022006166518926283\n",
      "epochs 1111\n",
      "training loss 0.0021790980121140628\n",
      "epochs 1112\n",
      "training loss 0.0021622837903289447\n",
      "epochs 1113\n",
      "training loss 0.002137578410799912\n",
      "epochs 1114\n",
      "training loss 0.0020806655039342747\n",
      "epochs 1115\n",
      "training loss 0.002119647747855407\n",
      "epochs 1116\n",
      "training loss 0.0020915346604412403\n",
      "epochs 1117\n",
      "training loss 0.0021311297167395456\n",
      "epochs 1118\n",
      "training loss 0.0021680533497786788\n",
      "epochs 1119\n",
      "training loss 0.0020892075386809857\n",
      "testing loss 0.006133911138770322\n",
      "epochs 1120\n",
      "training loss 0.002215496673090185\n",
      "epochs 1121\n",
      "training loss 0.0021956578497902284\n",
      "epochs 1122\n",
      "training loss 0.0022047729948662946\n",
      "epochs 1123\n",
      "training loss 0.0023015723875115164\n",
      "epochs 1124\n",
      "training loss 0.002202020244004833\n",
      "epochs 1125\n",
      "training loss 0.0021831164678620986\n",
      "epochs 1126\n",
      "training loss 0.0020817241769094347\n",
      "epochs 1127\n",
      "training loss 0.002223393148431232\n",
      "epochs 1128\n",
      "training loss 0.0021142800537427733\n",
      "epochs 1129\n",
      "training loss 0.0021869840045915323\n",
      "testing loss 0.0043192391419215196\n",
      "epochs 1130\n",
      "training loss 0.0021039582689867374\n",
      "epochs 1131\n",
      "training loss 0.0021370963959906554\n",
      "epochs 1132\n",
      "training loss 0.00213630530097887\n",
      "epochs 1133\n",
      "training loss 0.0021134069181290827\n",
      "epochs 1134\n",
      "training loss 0.0021334133386232616\n",
      "epochs 1135\n",
      "training loss 0.002100598216368208\n",
      "epochs 1136\n",
      "training loss 0.002098429548245569\n",
      "epochs 1137\n",
      "training loss 0.0021393114589805797\n",
      "epochs 1138\n",
      "training loss 0.0021471944177596856\n",
      "epochs 1139\n",
      "training loss 0.0021967563217669996\n",
      "testing loss 0.0037046109874409143\n",
      "epochs 1140\n",
      "training loss 0.002142946600296894\n",
      "epochs 1141\n",
      "training loss 0.0020925337537177654\n",
      "epochs 1142\n",
      "training loss 0.0020813735877446785\n",
      "epochs 1143\n",
      "training loss 0.0021559045578234213\n",
      "epochs 1144\n",
      "training loss 0.0022406856820276037\n",
      "epochs 1145\n",
      "training loss 0.0021226057644061587\n",
      "epochs 1146\n",
      "training loss 0.002055632089912937\n",
      "epochs 1147\n",
      "training loss 0.0021747585108622593\n",
      "epochs 1148\n",
      "training loss 0.002220551176902507\n",
      "epochs 1149\n",
      "training loss 0.0021229978310408518\n",
      "testing loss 0.004242122412745094\n",
      "epochs 1150\n",
      "training loss 0.0020964023403081716\n",
      "epochs 1151\n",
      "training loss 0.0021007501146555127\n",
      "epochs 1152\n",
      "training loss 0.00202797941465814\n",
      "epochs 1153\n",
      "training loss 0.002068734482761563\n",
      "epochs 1154\n",
      "training loss 0.002185983069900616\n",
      "epochs 1155\n",
      "training loss 0.002093675554132706\n",
      "epochs 1156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0020247426338346622\n",
      "epochs 1157\n",
      "training loss 0.002074475925257231\n",
      "epochs 1158\n",
      "training loss 0.0020680683121824\n",
      "epochs 1159\n",
      "training loss 0.002144827846420708\n",
      "testing loss 0.004162722103733649\n",
      "epochs 1160\n",
      "training loss 0.0021021549101978383\n",
      "epochs 1161\n",
      "training loss 0.0020294413278388896\n",
      "epochs 1162\n",
      "training loss 0.002060869666372672\n",
      "epochs 1163\n",
      "training loss 0.0020772895390397126\n",
      "epochs 1164\n",
      "training loss 0.002107364043103684\n",
      "epochs 1165\n",
      "training loss 0.0021390351838372805\n",
      "epochs 1166\n",
      "training loss 0.002074826309899021\n",
      "epochs 1167\n",
      "training loss 0.0021030555835197537\n",
      "epochs 1168\n",
      "training loss 0.002116067362963406\n",
      "epochs 1169\n",
      "training loss 0.0021363541285371506\n",
      "testing loss 0.0037247273362578546\n",
      "epochs 1170\n",
      "training loss 0.0020532609548456512\n",
      "epochs 1171\n",
      "training loss 0.002089193326988651\n",
      "epochs 1172\n",
      "training loss 0.0021778512963710923\n",
      "epochs 1173\n",
      "training loss 0.002134314808275517\n",
      "epochs 1174\n",
      "training loss 0.002110807765784778\n",
      "epochs 1175\n",
      "training loss 0.0021031311340559285\n",
      "epochs 1176\n",
      "training loss 0.0021530505783550194\n",
      "epochs 1177\n",
      "training loss 0.002150381104079412\n",
      "epochs 1178\n",
      "training loss 0.00203102635252247\n",
      "epochs 1179\n",
      "training loss 0.002081328557661273\n",
      "testing loss 0.003211226387788271\n",
      "epochs 1180\n",
      "training loss 0.002097306952112578\n",
      "epochs 1181\n",
      "training loss 0.0019836587917008708\n",
      "epochs 1182\n",
      "training loss 0.002069366306043867\n",
      "epochs 1183\n",
      "training loss 0.002080077653089793\n",
      "epochs 1184\n",
      "training loss 0.002061055090149587\n",
      "epochs 1185\n",
      "training loss 0.00204548508294725\n",
      "epochs 1186\n",
      "training loss 0.002137353695082651\n",
      "epochs 1187\n",
      "training loss 0.002220828338429943\n",
      "epochs 1188\n",
      "training loss 0.00202905670506172\n",
      "epochs 1189\n",
      "training loss 0.001974899829032698\n",
      "testing loss 0.004171854762710525\n",
      "epochs 1190\n",
      "training loss 0.002177363062895959\n",
      "epochs 1191\n",
      "training loss 0.002118664147724372\n",
      "epochs 1192\n",
      "training loss 0.0019800040121105327\n",
      "epochs 1193\n",
      "training loss 0.00209987682228806\n",
      "epochs 1194\n",
      "training loss 0.0021085048762806845\n",
      "epochs 1195\n",
      "training loss 0.0020004132427105335\n",
      "epochs 1196\n",
      "training loss 0.0021002215011636967\n",
      "epochs 1197\n",
      "training loss 0.0021197722444882597\n",
      "epochs 1198\n",
      "training loss 0.001987813396583167\n",
      "epochs 1199\n",
      "training loss 0.002084215696919483\n",
      "testing loss 0.003526850676177241\n",
      "epochs 1200\n",
      "training loss 0.0020856061553310794\n",
      "epochs 1201\n",
      "training loss 0.002070440796818188\n",
      "epochs 1202\n",
      "training loss 0.002194863962579949\n",
      "epochs 1203\n",
      "training loss 0.002041889498424985\n",
      "epochs 1204\n",
      "training loss 0.0020869136287258957\n",
      "epochs 1205\n",
      "training loss 0.0020220337083880257\n",
      "epochs 1206\n",
      "training loss 0.0019416410437400106\n",
      "epochs 1207\n",
      "training loss 0.0020754694636829062\n",
      "epochs 1208\n",
      "training loss 0.0020658301225280258\n",
      "epochs 1209\n",
      "training loss 0.002052000700071522\n",
      "testing loss 0.004216714615187218\n",
      "epochs 1210\n",
      "training loss 0.002147968748788205\n",
      "epochs 1211\n",
      "training loss 0.002010418284906635\n",
      "epochs 1212\n",
      "training loss 0.0020266040780306947\n",
      "epochs 1213\n",
      "training loss 0.0020349071690908952\n",
      "epochs 1214\n",
      "training loss 0.002125272932136856\n",
      "epochs 1215\n",
      "training loss 0.0020872879882009084\n",
      "epochs 1216\n",
      "training loss 0.0020802186522137452\n",
      "epochs 1217\n",
      "training loss 0.002078074666262409\n",
      "epochs 1218\n",
      "training loss 0.0019085967727854713\n",
      "epochs 1219\n",
      "training loss 0.001992480810958111\n",
      "testing loss 0.004995685095522315\n",
      "epochs 1220\n",
      "training loss 0.002083064709424882\n",
      "epochs 1221\n",
      "training loss 0.0019495013282467303\n",
      "epochs 1222\n",
      "training loss 0.0020152129240668063\n",
      "epochs 1223\n",
      "training loss 0.0020615027748235137\n",
      "epochs 1224\n",
      "training loss 0.0020584463193873935\n",
      "epochs 1225\n",
      "training loss 0.002049827765274261\n",
      "epochs 1226\n",
      "training loss 0.0020319256113451485\n",
      "epochs 1227\n",
      "training loss 0.0020666428858281244\n",
      "epochs 1228\n",
      "training loss 0.001967823806241449\n",
      "epochs 1229\n",
      "training loss 0.002026554767826521\n",
      "testing loss 0.00321353712459326\n",
      "epochs 1230\n",
      "training loss 0.0019882304679197586\n",
      "epochs 1231\n",
      "training loss 0.0021047087720281084\n",
      "epochs 1232\n",
      "training loss 0.00205769336366273\n",
      "epochs 1233\n",
      "training loss 0.0018873368975519567\n",
      "epochs 1234\n",
      "training loss 0.002011845605575273\n",
      "epochs 1235\n",
      "training loss 0.002049454218760348\n",
      "epochs 1236\n",
      "training loss 0.0020020895373602725\n",
      "epochs 1237\n",
      "training loss 0.0018960733750784107\n",
      "epochs 1238\n",
      "training loss 0.0021385401283889036\n",
      "epochs 1239\n",
      "training loss 0.0019287507224539267\n",
      "testing loss 0.003849930907698705\n",
      "epochs 1240\n",
      "training loss 0.0019831539879731357\n",
      "epochs 1241\n",
      "training loss 0.0020649127927920173\n",
      "epochs 1242\n",
      "training loss 0.0021305334595765205\n",
      "epochs 1243\n",
      "training loss 0.002033297280005341\n",
      "epochs 1244\n",
      "training loss 0.001997977115162917\n",
      "epochs 1245\n",
      "training loss 0.00205882052600095\n",
      "epochs 1246\n",
      "training loss 0.0019729993076808183\n",
      "epochs 1247\n",
      "training loss 0.001996813429375165\n",
      "epochs 1248\n",
      "training loss 0.0020173802093594226\n",
      "epochs 1249\n",
      "training loss 0.0020736701990687217\n",
      "testing loss 0.0033417307888983306\n",
      "epochs 1250\n",
      "training loss 0.001924342692031392\n",
      "epochs 1251\n",
      "training loss 0.00199065260988261\n",
      "epochs 1252\n",
      "training loss 0.0020586617234712155\n",
      "epochs 1253\n",
      "training loss 0.0019884049306102777\n",
      "epochs 1254\n",
      "training loss 0.002024179353169236\n",
      "epochs 1255\n",
      "training loss 0.0020311004544132727\n",
      "epochs 1256\n",
      "training loss 0.0019942794041495804\n",
      "epochs 1257\n",
      "training loss 0.0020077108001438364\n",
      "epochs 1258\n",
      "training loss 0.002010287448769509\n",
      "epochs 1259\n",
      "training loss 0.0020017234676115575\n",
      "testing loss 0.003541282143655187\n",
      "epochs 1260\n",
      "training loss 0.0019609435253753965\n",
      "epochs 1261\n",
      "training loss 0.002015915870918442\n",
      "epochs 1262\n",
      "training loss 0.0019867997593529477\n",
      "epochs 1263\n",
      "training loss 0.0020107077535505102\n",
      "epochs 1264\n",
      "training loss 0.001992923983919086\n",
      "epochs 1265\n",
      "training loss 0.0019703895514576178\n",
      "epochs 1266\n",
      "training loss 0.0019029699470125999\n",
      "epochs 1267\n",
      "training loss 0.002027410945903204\n",
      "epochs 1268\n",
      "training loss 0.0019949914326347798\n",
      "epochs 1269\n",
      "training loss 0.0019309530489353356\n",
      "testing loss 0.005770306013762317\n",
      "epochs 1270\n",
      "training loss 0.001996715527804891\n",
      "epochs 1271\n",
      "training loss 0.0019829715697638303\n",
      "epochs 1272\n",
      "training loss 0.0019505574459232685\n",
      "epochs 1273\n",
      "training loss 0.0019746831622495568\n",
      "epochs 1274\n",
      "training loss 0.0019005800437785143\n",
      "epochs 1275\n",
      "training loss 0.001901088777142591\n",
      "epochs 1276\n",
      "training loss 0.001969092024672822\n",
      "epochs 1277\n",
      "training loss 0.0019617413082528056\n",
      "epochs 1278\n",
      "training loss 0.0019754683158337777\n",
      "epochs 1279\n",
      "training loss 0.002020126690955034\n",
      "testing loss 0.0031776222877587515\n",
      "epochs 1280\n",
      "training loss 0.0020116203551967613\n",
      "epochs 1281\n",
      "training loss 0.00203238490269743\n",
      "epochs 1282\n",
      "training loss 0.0019134506868059807\n",
      "epochs 1283\n",
      "training loss 0.0019653929711679016\n",
      "epochs 1284\n",
      "training loss 0.002017106004922699\n",
      "epochs 1285\n",
      "training loss 0.0019283415254284727\n",
      "epochs 1286\n",
      "training loss 0.002023711134814077\n",
      "epochs 1287\n",
      "training loss 0.0019096952346776327\n",
      "epochs 1288\n",
      "training loss 0.0019550928639001735\n",
      "epochs 1289\n",
      "training loss 0.0019680552273397094\n",
      "testing loss 0.003735219084670939\n",
      "epochs 1290\n",
      "training loss 0.002067185448458571\n",
      "epochs 1291\n",
      "training loss 0.0019309559373799315\n",
      "epochs 1292\n",
      "training loss 0.0019321109587185107\n",
      "epochs 1293\n",
      "training loss 0.0020294617095432815\n",
      "epochs 1294\n",
      "training loss 0.0020190096617036563\n",
      "epochs 1295\n",
      "training loss 0.0020290926142346602\n",
      "epochs 1296\n",
      "training loss 0.0020641264548853584\n",
      "epochs 1297\n",
      "training loss 0.0019464321226790253\n",
      "epochs 1298\n",
      "training loss 0.0019572889453541814\n",
      "epochs 1299\n",
      "training loss 0.0019892051828017394\n",
      "testing loss 0.0037750420672488763\n",
      "epochs 1300\n",
      "training loss 0.001948848420521952\n",
      "epochs 1301\n",
      "training loss 0.002028245387930072\n",
      "epochs 1302\n",
      "training loss 0.0018509347871084637\n",
      "epochs 1303\n",
      "training loss 0.001954954670951631\n",
      "epochs 1304\n",
      "training loss 0.0019751941906585497\n",
      "epochs 1305\n",
      "training loss 0.0019264231571477575\n",
      "epochs 1306\n",
      "training loss 0.001929028869088524\n",
      "epochs 1307\n",
      "training loss 0.001972673774525625\n",
      "epochs 1308\n",
      "training loss 0.001992804098076054\n",
      "epochs 1309\n",
      "training loss 0.002009162624645412\n",
      "testing loss 0.003251747884589465\n",
      "epochs 1310\n",
      "training loss 0.001911967752344261\n",
      "epochs 1311\n",
      "training loss 0.0019002214256719344\n",
      "epochs 1312\n",
      "training loss 0.001993293999581057\n",
      "epochs 1313\n",
      "training loss 0.00188946957900939\n",
      "epochs 1314\n",
      "training loss 0.0018907246021709566\n",
      "epochs 1315\n",
      "training loss 0.00197959345507812\n",
      "epochs 1316\n",
      "training loss 0.0019719658448151384\n",
      "epochs 1317\n",
      "training loss 0.0019723341273395825\n",
      "epochs 1318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.001975555395379119\n",
      "epochs 1319\n",
      "training loss 0.001983436620666852\n",
      "testing loss 0.0032189938120861005\n",
      "epochs 1320\n",
      "training loss 0.001996113862460168\n",
      "epochs 1321\n",
      "training loss 0.0019442144198212941\n",
      "epochs 1322\n",
      "training loss 0.0019519454631515662\n",
      "epochs 1323\n",
      "training loss 0.001960197481323928\n",
      "epochs 1324\n",
      "training loss 0.0020283418801318753\n",
      "epochs 1325\n",
      "training loss 0.00197747988377961\n",
      "epochs 1326\n",
      "training loss 0.0018919691037228132\n",
      "epochs 1327\n",
      "training loss 0.001964973743317088\n",
      "epochs 1328\n",
      "training loss 0.001952029890621415\n",
      "epochs 1329\n",
      "training loss 0.001940896932218269\n",
      "testing loss 0.00392698057009079\n",
      "epochs 1330\n",
      "training loss 0.0020091381071894046\n",
      "epochs 1331\n",
      "training loss 0.0018716774536593956\n",
      "epochs 1332\n",
      "training loss 0.0019034312223791964\n",
      "epochs 1333\n",
      "training loss 0.001968288594944612\n",
      "epochs 1334\n",
      "training loss 0.001963729532803536\n",
      "epochs 1335\n",
      "training loss 0.001941499892639217\n",
      "epochs 1336\n",
      "training loss 0.0018973619178911292\n",
      "epochs 1337\n",
      "training loss 0.0018951190542854301\n",
      "epochs 1338\n",
      "training loss 0.0019299533029899318\n",
      "epochs 1339\n",
      "training loss 0.0019335944129799502\n",
      "testing loss 0.003714100366907445\n",
      "epochs 1340\n",
      "training loss 0.0019814905952549\n",
      "epochs 1341\n",
      "training loss 0.001981248581600814\n",
      "epochs 1342\n",
      "training loss 0.0019416476059903173\n",
      "epochs 1343\n",
      "training loss 0.0018796953862733959\n",
      "epochs 1344\n",
      "training loss 0.0019968520797927804\n",
      "epochs 1345\n",
      "training loss 0.001964856977864681\n",
      "epochs 1346\n",
      "training loss 0.001897516185707925\n",
      "epochs 1347\n",
      "training loss 0.001944455102306867\n",
      "epochs 1348\n",
      "training loss 0.001980754398382736\n",
      "epochs 1349\n",
      "training loss 0.001957932287384119\n",
      "testing loss 0.004334136597640443\n",
      "epochs 1350\n",
      "training loss 0.0019702466698731423\n",
      "epochs 1351\n",
      "training loss 0.0019219524738743991\n",
      "epochs 1352\n",
      "training loss 0.0018883661404506822\n",
      "epochs 1353\n",
      "training loss 0.0018978657885270595\n",
      "epochs 1354\n",
      "training loss 0.001914064697128661\n",
      "epochs 1355\n",
      "training loss 0.0019498712055549298\n",
      "epochs 1356\n",
      "training loss 0.001887922842051313\n",
      "epochs 1357\n",
      "training loss 0.0018908942125717852\n",
      "epochs 1358\n",
      "training loss 0.0019497162979846328\n",
      "epochs 1359\n",
      "training loss 0.001940095449173625\n",
      "testing loss 0.005684797103840409\n",
      "epochs 1360\n",
      "training loss 0.0019342282130227263\n",
      "epochs 1361\n",
      "training loss 0.0019015040330724695\n",
      "epochs 1362\n",
      "training loss 0.0019175428200065862\n",
      "epochs 1363\n",
      "training loss 0.0018946953539985677\n",
      "epochs 1364\n",
      "training loss 0.0019899203787390766\n",
      "epochs 1365\n",
      "training loss 0.0019731083078066446\n",
      "epochs 1366\n",
      "training loss 0.0018845060244018614\n",
      "epochs 1367\n",
      "training loss 0.0019345238318632847\n",
      "epochs 1368\n",
      "training loss 0.0018917741899740937\n",
      "epochs 1369\n",
      "training loss 0.0018274483389835409\n",
      "testing loss 0.0032904606373610756\n",
      "epochs 1370\n",
      "training loss 0.001943242251745632\n",
      "epochs 1371\n",
      "training loss 0.001878690935592068\n",
      "epochs 1372\n",
      "training loss 0.0019481809331050376\n",
      "epochs 1373\n",
      "training loss 0.0019263951665167004\n",
      "epochs 1374\n",
      "training loss 0.0019578404930182436\n",
      "epochs 1375\n",
      "training loss 0.00196024795745055\n",
      "epochs 1376\n",
      "training loss 0.001996655964560093\n",
      "epochs 1377\n",
      "training loss 0.001983561728922452\n",
      "epochs 1378\n",
      "training loss 0.0019175625456805626\n",
      "epochs 1379\n",
      "training loss 0.0019060240102730872\n",
      "testing loss 0.004179335854029455\n",
      "epochs 1380\n",
      "training loss 0.001919691202494013\n",
      "epochs 1381\n",
      "training loss 0.0019579301956237087\n",
      "epochs 1382\n",
      "training loss 0.0018884258535492452\n",
      "epochs 1383\n",
      "training loss 0.001880315204623713\n",
      "epochs 1384\n",
      "training loss 0.0018667147139591197\n",
      "epochs 1385\n",
      "training loss 0.0018446464287923893\n",
      "epochs 1386\n",
      "training loss 0.0018827870295674606\n",
      "epochs 1387\n",
      "training loss 0.001899700483737519\n",
      "epochs 1388\n",
      "training loss 0.0018673439569966612\n",
      "epochs 1389\n",
      "training loss 0.0018679157876420912\n",
      "testing loss 0.004817221482145659\n",
      "epochs 1390\n",
      "training loss 0.0018792439720630193\n",
      "epochs 1391\n",
      "training loss 0.0018851148006350572\n",
      "epochs 1392\n",
      "training loss 0.0019507565025005229\n",
      "epochs 1393\n",
      "training loss 0.0018928270872985225\n",
      "epochs 1394\n",
      "training loss 0.0018863330730010605\n",
      "epochs 1395\n",
      "training loss 0.0019357333603416964\n",
      "epochs 1396\n",
      "training loss 0.0018711802179891175\n",
      "epochs 1397\n",
      "training loss 0.001960452189025013\n",
      "epochs 1398\n",
      "training loss 0.001871163725051859\n",
      "epochs 1399\n",
      "training loss 0.0018832438915902833\n",
      "testing loss 0.003223091992219342\n",
      "epochs 1400\n",
      "training loss 0.001788679689931822\n",
      "epochs 1401\n",
      "training loss 0.0018900118263630837\n",
      "epochs 1402\n",
      "training loss 0.001882280729602071\n",
      "epochs 1403\n",
      "training loss 0.001910225117061072\n",
      "epochs 1404\n",
      "training loss 0.0018895245749874204\n",
      "epochs 1405\n",
      "training loss 0.0018865429020648126\n",
      "epochs 1406\n",
      "training loss 0.0018842535737072512\n",
      "epochs 1407\n",
      "training loss 0.0018447582697705294\n",
      "epochs 1408\n",
      "training loss 0.0019198723369501644\n",
      "epochs 1409\n",
      "training loss 0.0019323033843993416\n",
      "testing loss 0.003764485561368155\n",
      "epochs 1410\n",
      "training loss 0.0019032439870878737\n",
      "epochs 1411\n",
      "training loss 0.001907250799180651\n",
      "epochs 1412\n",
      "training loss 0.0018540285225167848\n",
      "epochs 1413\n",
      "training loss 0.0019013790688321764\n",
      "epochs 1414\n",
      "training loss 0.001857132718142541\n",
      "epochs 1415\n",
      "training loss 0.0018968301725086946\n",
      "epochs 1416\n",
      "training loss 0.0019272816509693454\n",
      "epochs 1417\n",
      "training loss 0.0019184992309676705\n",
      "epochs 1418\n",
      "training loss 0.0018619369304283186\n",
      "epochs 1419\n",
      "training loss 0.0018138915454646103\n",
      "testing loss 0.003240121926877859\n",
      "epochs 1420\n",
      "training loss 0.0019299828172817056\n",
      "epochs 1421\n",
      "training loss 0.0018699055496535313\n",
      "epochs 1422\n",
      "training loss 0.0018460839823324312\n",
      "epochs 1423\n",
      "training loss 0.0018619653799975897\n",
      "epochs 1424\n",
      "training loss 0.001924541676777916\n",
      "epochs 1425\n",
      "training loss 0.0018824716735529319\n",
      "epochs 1426\n",
      "training loss 0.0018441686304116276\n",
      "epochs 1427\n",
      "training loss 0.0018144287593594801\n",
      "epochs 1428\n",
      "training loss 0.0018520585442100591\n",
      "epochs 1429\n",
      "training loss 0.00187282701303169\n",
      "testing loss 0.0037341895389390437\n",
      "epochs 1430\n",
      "training loss 0.0018581412062063066\n",
      "epochs 1431\n",
      "training loss 0.0018770596158257642\n",
      "epochs 1432\n",
      "training loss 0.0019147828317567209\n",
      "epochs 1433\n",
      "training loss 0.0018630174418917\n",
      "epochs 1434\n",
      "training loss 0.0019073595169373989\n",
      "epochs 1435\n",
      "training loss 0.00191101768088745\n",
      "epochs 1436\n",
      "training loss 0.0018923039835421964\n",
      "epochs 1437\n",
      "training loss 0.0018748474170419878\n",
      "epochs 1438\n",
      "training loss 0.0019192380951307034\n",
      "epochs 1439\n",
      "training loss 0.002057364289217139\n",
      "testing loss 0.004827226728451907\n",
      "epochs 1440\n",
      "training loss 0.0019535634172712325\n",
      "epochs 1441\n",
      "training loss 0.0019750253689803728\n",
      "epochs 1442\n",
      "training loss 0.0018697503420218576\n",
      "epochs 1443\n",
      "training loss 0.0019039079233067037\n",
      "epochs 1444\n",
      "training loss 0.0018441015316632019\n",
      "epochs 1445\n",
      "training loss 0.001887825938503578\n",
      "epochs 1446\n",
      "training loss 0.0018415802455999772\n",
      "epochs 1447\n",
      "training loss 0.0019008889362657595\n",
      "epochs 1448\n",
      "training loss 0.001950747414851578\n",
      "epochs 1449\n",
      "training loss 0.001851323553952629\n",
      "testing loss 0.0030925957793997375\n",
      "epochs 1450\n",
      "training loss 0.001909412087845732\n",
      "epochs 1451\n",
      "training loss 0.0018722732034073095\n",
      "epochs 1452\n",
      "training loss 0.0018865465594166862\n",
      "epochs 1453\n",
      "training loss 0.0018214492695575926\n",
      "epochs 1454\n",
      "training loss 0.00188936488200432\n",
      "epochs 1455\n",
      "training loss 0.0019484057291453325\n",
      "epochs 1456\n",
      "training loss 0.001837716079430264\n",
      "epochs 1457\n",
      "training loss 0.001911392974869651\n",
      "epochs 1458\n",
      "training loss 0.0018736891521743888\n",
      "epochs 1459\n",
      "training loss 0.0018298191861971353\n",
      "testing loss 0.008163891570867166\n",
      "epochs 1460\n",
      "training loss 0.001910751723715509\n",
      "epochs 1461\n",
      "training loss 0.0017999574141491963\n",
      "epochs 1462\n",
      "training loss 0.00184631545615374\n",
      "epochs 1463\n",
      "training loss 0.0018145298402357315\n",
      "epochs 1464\n",
      "training loss 0.0018543208063591418\n",
      "epochs 1465\n",
      "training loss 0.0018489265428377616\n",
      "epochs 1466\n",
      "training loss 0.0019265966092542064\n",
      "epochs 1467\n",
      "training loss 0.0019876354950406562\n",
      "epochs 1468\n",
      "training loss 0.0019454335228406958\n",
      "epochs 1469\n",
      "training loss 0.0018237229653543547\n",
      "testing loss 0.0038655984656330434\n",
      "epochs 1470\n",
      "training loss 0.0018704983900799373\n",
      "epochs 1471\n",
      "training loss 0.0018084162625407738\n",
      "epochs 1472\n",
      "training loss 0.0019484343363529802\n",
      "epochs 1473\n",
      "training loss 0.0017810704575435437\n",
      "epochs 1474\n",
      "training loss 0.0018710020638005892\n",
      "epochs 1475\n",
      "training loss 0.0018474492555806883\n",
      "epochs 1476\n",
      "training loss 0.0018334584611419894\n",
      "epochs 1477\n",
      "training loss 0.001855245351014336\n",
      "epochs 1478\n",
      "training loss 0.0018157638072625007\n",
      "epochs 1479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0018276419024560657\n",
      "testing loss 0.003871247135201826\n",
      "epochs 1480\n",
      "training loss 0.0018063480443870337\n",
      "epochs 1481\n",
      "training loss 0.0018470239748408321\n",
      "epochs 1482\n",
      "training loss 0.0018701436623327977\n",
      "epochs 1483\n",
      "training loss 0.0019525063907413385\n",
      "epochs 1484\n",
      "training loss 0.0017965547184921281\n",
      "epochs 1485\n",
      "training loss 0.0018126566345537348\n",
      "epochs 1486\n",
      "training loss 0.0019146522443733067\n",
      "epochs 1487\n",
      "training loss 0.0018834497931407691\n",
      "epochs 1488\n",
      "training loss 0.0018448805477891943\n",
      "epochs 1489\n",
      "training loss 0.0017907786316459144\n",
      "testing loss 0.0033769151385781085\n",
      "epochs 1490\n",
      "training loss 0.0017835086048439168\n",
      "epochs 1491\n",
      "training loss 0.0018605559251885465\n",
      "epochs 1492\n",
      "training loss 0.0018591377160534558\n",
      "epochs 1493\n",
      "training loss 0.0018562273358385501\n",
      "epochs 1494\n",
      "training loss 0.001809635319206354\n",
      "epochs 1495\n",
      "training loss 0.0018482048255193265\n",
      "epochs 1496\n",
      "training loss 0.001785400088792978\n",
      "epochs 1497\n",
      "training loss 0.0018755430564437185\n",
      "epochs 1498\n",
      "training loss 0.0018130506285873884\n",
      "epochs 1499\n",
      "training loss 0.001853814128309341\n",
      "testing loss 0.0034630402868138033\n",
      "epochs 1500\n",
      "training loss 0.0018172087059370426\n",
      "epochs 1501\n",
      "training loss 0.0018364200478215846\n",
      "epochs 1502\n",
      "training loss 0.0018033942577109238\n",
      "epochs 1503\n",
      "training loss 0.0018789007147635393\n",
      "epochs 1504\n",
      "training loss 0.0017950154988146119\n",
      "epochs 1505\n",
      "training loss 0.0018165538938132917\n",
      "epochs 1506\n",
      "training loss 0.0018389067502188464\n",
      "epochs 1507\n",
      "training loss 0.0018287460734759172\n",
      "epochs 1508\n",
      "training loss 0.0017675702772556173\n",
      "epochs 1509\n",
      "training loss 0.0018844126371983835\n",
      "testing loss 0.0031049397426446013\n",
      "epochs 1510\n",
      "training loss 0.0018065762033793612\n",
      "epochs 1511\n",
      "training loss 0.0017667442539319011\n",
      "epochs 1512\n",
      "training loss 0.001792058316986487\n",
      "epochs 1513\n",
      "training loss 0.0018781606521242294\n",
      "epochs 1514\n",
      "training loss 0.0018017158086700393\n",
      "epochs 1515\n",
      "training loss 0.001803110607002372\n",
      "epochs 1516\n",
      "training loss 0.0018131186025738036\n",
      "epochs 1517\n",
      "training loss 0.00178600187391456\n",
      "epochs 1518\n",
      "training loss 0.001877647525408676\n",
      "epochs 1519\n",
      "training loss 0.0017814995247890499\n",
      "testing loss 0.003430048462023285\n",
      "epochs 1520\n",
      "training loss 0.0018518821530590418\n",
      "epochs 1521\n",
      "training loss 0.0018904568849691958\n",
      "epochs 1522\n",
      "training loss 0.0018053284754600137\n",
      "epochs 1523\n",
      "training loss 0.0018512441171262425\n",
      "epochs 1524\n",
      "training loss 0.0018213824919243783\n",
      "epochs 1525\n",
      "training loss 0.001841548573201675\n",
      "epochs 1526\n",
      "training loss 0.0017730880632719143\n",
      "epochs 1527\n",
      "training loss 0.001862643830800899\n",
      "epochs 1528\n",
      "training loss 0.0018016185587071062\n",
      "epochs 1529\n",
      "training loss 0.0017825387472054698\n",
      "testing loss 0.0032870328464175425\n",
      "epochs 1530\n",
      "training loss 0.0018117678950407676\n",
      "epochs 1531\n",
      "training loss 0.0017415916162902506\n",
      "epochs 1532\n",
      "training loss 0.001867805842393698\n",
      "epochs 1533\n",
      "training loss 0.0018458262855175736\n",
      "epochs 1534\n",
      "training loss 0.0018218833086927249\n",
      "epochs 1535\n",
      "training loss 0.0018267957980201033\n",
      "epochs 1536\n",
      "training loss 0.0018640670633102035\n",
      "epochs 1537\n",
      "training loss 0.0017823948323907822\n",
      "epochs 1538\n",
      "training loss 0.0017579417758179677\n",
      "epochs 1539\n",
      "training loss 0.0017953472012741702\n",
      "testing loss 0.0031770169181775643\n",
      "epochs 1540\n",
      "training loss 0.0018097309860624486\n",
      "epochs 1541\n",
      "training loss 0.001813380184794258\n",
      "epochs 1542\n",
      "training loss 0.0018237275853441705\n",
      "epochs 1543\n",
      "training loss 0.0018301088545870567\n",
      "epochs 1544\n",
      "training loss 0.0017772573140155423\n",
      "epochs 1545\n",
      "training loss 0.0017207534504292878\n",
      "epochs 1546\n",
      "training loss 0.0018717623474982237\n",
      "epochs 1547\n",
      "training loss 0.0018385297551221783\n",
      "epochs 1548\n",
      "training loss 0.0017805266207137843\n",
      "epochs 1549\n",
      "training loss 0.0017480173779476015\n",
      "testing loss 0.0033510540651672696\n",
      "epochs 1550\n",
      "training loss 0.001806957216927708\n",
      "epochs 1551\n",
      "training loss 0.0017960228112251654\n",
      "epochs 1552\n",
      "training loss 0.001807394177061265\n",
      "epochs 1553\n",
      "training loss 0.0018619993508023445\n",
      "epochs 1554\n",
      "training loss 0.0017483012644944876\n",
      "epochs 1555\n",
      "training loss 0.0018379053295205\n",
      "epochs 1556\n",
      "training loss 0.0018377358581811347\n",
      "epochs 1557\n",
      "training loss 0.0018153983008365592\n",
      "epochs 1558\n",
      "training loss 0.0017323114949570103\n",
      "epochs 1559\n",
      "training loss 0.0018145803728007845\n",
      "testing loss 0.0034565815944819058\n",
      "epochs 1560\n",
      "training loss 0.0018117914988664983\n",
      "epochs 1561\n",
      "training loss 0.001809050616529405\n",
      "epochs 1562\n",
      "training loss 0.0018096480427994917\n",
      "epochs 1563\n",
      "training loss 0.001790179987968792\n",
      "epochs 1564\n",
      "training loss 0.0018370440026312092\n",
      "epochs 1565\n",
      "training loss 0.0018186872604934324\n",
      "epochs 1566\n",
      "training loss 0.0018298387333558232\n",
      "epochs 1567\n",
      "training loss 0.0018332472308482239\n",
      "epochs 1568\n",
      "training loss 0.0017790136558759539\n",
      "epochs 1569\n",
      "training loss 0.0017767187031368105\n",
      "testing loss 0.003158501112843527\n",
      "epochs 1570\n",
      "training loss 0.0017037615675399912\n",
      "epochs 1571\n",
      "training loss 0.0018557787439035744\n",
      "epochs 1572\n",
      "training loss 0.0017867413859695514\n",
      "epochs 1573\n",
      "training loss 0.001757240930457506\n",
      "epochs 1574\n",
      "training loss 0.0017513378953566848\n",
      "epochs 1575\n",
      "training loss 0.0018385100609404927\n",
      "epochs 1576\n",
      "training loss 0.0017516033295442925\n",
      "epochs 1577\n",
      "training loss 0.0018139771284184513\n",
      "epochs 1578\n",
      "training loss 0.0018147150267589284\n",
      "epochs 1579\n",
      "training loss 0.00177217707074018\n",
      "testing loss 0.004664527569362458\n",
      "epochs 1580\n",
      "training loss 0.0018385432861918035\n",
      "epochs 1581\n",
      "training loss 0.0017250367844729234\n",
      "epochs 1582\n",
      "training loss 0.0018222805998630617\n",
      "epochs 1583\n",
      "training loss 0.0017059203571679585\n",
      "epochs 1584\n",
      "training loss 0.001756640790896575\n",
      "epochs 1585\n",
      "training loss 0.0017731777161620217\n",
      "epochs 1586\n",
      "training loss 0.0017768838987784817\n",
      "epochs 1587\n",
      "training loss 0.001810874376661147\n",
      "epochs 1588\n",
      "training loss 0.0018685992823426801\n",
      "epochs 1589\n",
      "training loss 0.0018442066106449891\n",
      "testing loss 0.0037007742447461535\n",
      "epochs 1590\n",
      "training loss 0.001764919441639188\n",
      "epochs 1591\n",
      "training loss 0.001753439862276838\n",
      "epochs 1592\n",
      "training loss 0.0017957934151873513\n",
      "epochs 1593\n",
      "training loss 0.001809352331605137\n",
      "epochs 1594\n",
      "training loss 0.0018770586485878245\n",
      "epochs 1595\n",
      "training loss 0.0018539453274924252\n",
      "epochs 1596\n",
      "training loss 0.0017745783655142757\n",
      "epochs 1597\n",
      "training loss 0.0017476901856086876\n",
      "epochs 1598\n",
      "training loss 0.0017620109937770865\n",
      "epochs 1599\n",
      "training loss 0.0018123732513527309\n",
      "testing loss 0.0031636025354510556\n",
      "epochs 1600\n",
      "training loss 0.0017663044230460463\n",
      "epochs 1601\n",
      "training loss 0.001845530476130964\n",
      "epochs 1602\n",
      "training loss 0.0017778375193103016\n",
      "epochs 1603\n",
      "training loss 0.0017564560388962209\n",
      "epochs 1604\n",
      "training loss 0.0018136507085252747\n",
      "epochs 1605\n",
      "training loss 0.0017551187985636136\n",
      "epochs 1606\n",
      "training loss 0.001780600262250989\n",
      "epochs 1607\n",
      "training loss 0.001732280874365833\n",
      "epochs 1608\n",
      "training loss 0.0017274802295550546\n",
      "epochs 1609\n",
      "training loss 0.0017602388243867497\n",
      "testing loss 0.0036448575325098865\n",
      "epochs 1610\n",
      "training loss 0.0018144623135106135\n",
      "epochs 1611\n",
      "training loss 0.0017136943604058295\n",
      "epochs 1612\n",
      "training loss 0.0017994128866121173\n",
      "epochs 1613\n",
      "training loss 0.0017812591768648352\n",
      "epochs 1614\n",
      "training loss 0.001796122360833012\n",
      "epochs 1615\n",
      "training loss 0.0017956071393493574\n",
      "epochs 1616\n",
      "training loss 0.0018252009475648993\n",
      "epochs 1617\n",
      "training loss 0.0018428859966495683\n",
      "epochs 1618\n",
      "training loss 0.0018387714274918326\n",
      "epochs 1619\n",
      "training loss 0.0018127651732644254\n",
      "testing loss 0.004435078697510275\n",
      "epochs 1620\n",
      "training loss 0.001806738112145882\n",
      "epochs 1621\n",
      "training loss 0.001753854996837551\n",
      "epochs 1622\n",
      "training loss 0.0017007253174160502\n",
      "epochs 1623\n",
      "training loss 0.0018209357861653083\n",
      "epochs 1624\n",
      "training loss 0.001734208608949189\n",
      "epochs 1625\n",
      "training loss 0.001745453084915265\n",
      "epochs 1626\n",
      "training loss 0.0017481226579138958\n",
      "epochs 1627\n",
      "training loss 0.0017416768082443024\n",
      "epochs 1628\n",
      "training loss 0.0017802056521715138\n",
      "epochs 1629\n",
      "training loss 0.0016928143710135779\n",
      "testing loss 0.0031718162733876537\n",
      "epochs 1630\n",
      "training loss 0.0017728299729803459\n",
      "epochs 1631\n",
      "training loss 0.0017455356318707433\n",
      "epochs 1632\n",
      "training loss 0.0017207303462340568\n",
      "epochs 1633\n",
      "training loss 0.0017346047380893833\n",
      "epochs 1634\n",
      "training loss 0.0018299634191193434\n",
      "epochs 1635\n",
      "training loss 0.0018369181949990634\n",
      "epochs 1636\n",
      "training loss 0.0017324899173607783\n",
      "epochs 1637\n",
      "training loss 0.001734202926890715\n",
      "epochs 1638\n",
      "training loss 0.001749169355503207\n",
      "epochs 1639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0017755371440586981\n",
      "testing loss 0.004070308135412898\n",
      "epochs 1640\n",
      "training loss 0.0016902067642276795\n",
      "epochs 1641\n",
      "training loss 0.0017201250575066723\n",
      "epochs 1642\n",
      "training loss 0.001764839492443847\n",
      "epochs 1643\n",
      "training loss 0.0018249888386716075\n",
      "epochs 1644\n",
      "training loss 0.0017094353467259997\n",
      "epochs 1645\n",
      "training loss 0.0017508845307473602\n",
      "epochs 1646\n",
      "training loss 0.0017622557591143032\n",
      "epochs 1647\n",
      "training loss 0.001834434547354\n",
      "epochs 1648\n",
      "training loss 0.0017969477270647748\n",
      "epochs 1649\n",
      "training loss 0.001775991568613605\n",
      "testing loss 0.0032649704416989225\n",
      "epochs 1650\n",
      "training loss 0.0018627411814330084\n",
      "epochs 1651\n",
      "training loss 0.0017381781493851457\n",
      "epochs 1652\n",
      "training loss 0.0018273227029666872\n",
      "epochs 1653\n",
      "training loss 0.0017180796247329507\n",
      "epochs 1654\n",
      "training loss 0.0017409875014468485\n",
      "epochs 1655\n",
      "training loss 0.0017200326331180597\n",
      "epochs 1656\n",
      "training loss 0.0017039327504322793\n",
      "epochs 1657\n",
      "training loss 0.001742488120068261\n",
      "epochs 1658\n",
      "training loss 0.0017918030050934198\n",
      "epochs 1659\n",
      "training loss 0.0017503257872717947\n",
      "testing loss 0.0030945367642169724\n",
      "epochs 1660\n",
      "training loss 0.0017199620031638461\n",
      "epochs 1661\n",
      "training loss 0.001766894609096961\n",
      "epochs 1662\n",
      "training loss 0.001784467156042308\n",
      "epochs 1663\n",
      "training loss 0.0018084375587600569\n",
      "epochs 1664\n",
      "training loss 0.00169505905179522\n",
      "epochs 1665\n",
      "training loss 0.0017249493287893234\n",
      "epochs 1666\n",
      "training loss 0.0017099529768391244\n",
      "epochs 1667\n",
      "training loss 0.0017390312394481096\n",
      "epochs 1668\n",
      "training loss 0.0017410052864526408\n",
      "epochs 1669\n",
      "training loss 0.0016998232525948854\n",
      "testing loss 0.0035705940371894454\n",
      "epochs 1670\n",
      "training loss 0.0017804210005245968\n",
      "epochs 1671\n",
      "training loss 0.0017556717911576653\n",
      "epochs 1672\n",
      "training loss 0.0017414239408960707\n",
      "epochs 1673\n",
      "training loss 0.00174115172631212\n",
      "epochs 1674\n",
      "training loss 0.0017573700695493934\n",
      "epochs 1675\n",
      "training loss 0.00169233957784658\n",
      "epochs 1676\n",
      "training loss 0.0016895075327586551\n",
      "epochs 1677\n",
      "training loss 0.0018268328961154792\n",
      "epochs 1678\n",
      "training loss 0.0017742468919930555\n",
      "epochs 1679\n",
      "training loss 0.0017893873986655048\n",
      "testing loss 0.003254950454818574\n",
      "epochs 1680\n",
      "training loss 0.001720636852169098\n",
      "epochs 1681\n",
      "training loss 0.0017652049831236433\n",
      "epochs 1682\n",
      "training loss 0.0017366458555346633\n",
      "epochs 1683\n",
      "training loss 0.0017176131244768646\n",
      "epochs 1684\n",
      "training loss 0.001745585660382222\n",
      "epochs 1685\n",
      "training loss 0.0017658336444384218\n",
      "epochs 1686\n",
      "training loss 0.001703899463081002\n",
      "epochs 1687\n",
      "training loss 0.0017396174737473096\n",
      "epochs 1688\n",
      "training loss 0.0017225786130295041\n",
      "epochs 1689\n",
      "training loss 0.0017877264554459478\n",
      "testing loss 0.0034407514621378156\n",
      "epochs 1690\n",
      "training loss 0.0017461850957896867\n",
      "epochs 1691\n",
      "training loss 0.0017298828317899835\n",
      "epochs 1692\n",
      "training loss 0.001705607259752212\n",
      "epochs 1693\n",
      "training loss 0.001703160992217757\n",
      "epochs 1694\n",
      "training loss 0.0017882980783837975\n",
      "epochs 1695\n",
      "training loss 0.001726095883478623\n",
      "epochs 1696\n",
      "training loss 0.0017209255688903022\n",
      "epochs 1697\n",
      "training loss 0.0017251631173894014\n",
      "epochs 1698\n",
      "training loss 0.0017691335558047729\n",
      "epochs 1699\n",
      "training loss 0.001734634913719262\n",
      "testing loss 0.0037588966227671567\n",
      "epochs 1700\n",
      "training loss 0.0016906128728855685\n",
      "epochs 1701\n",
      "training loss 0.0016877890405624448\n",
      "epochs 1702\n",
      "training loss 0.001670427085041977\n",
      "epochs 1703\n",
      "training loss 0.0017968073569020217\n",
      "epochs 1704\n",
      "training loss 0.0017506403285559935\n",
      "epochs 1705\n",
      "training loss 0.0017557932227767869\n",
      "epochs 1706\n",
      "training loss 0.001741847824121296\n",
      "epochs 1707\n",
      "training loss 0.001689845542345581\n",
      "epochs 1708\n",
      "training loss 0.0016996612129053582\n",
      "epochs 1709\n",
      "training loss 0.0016543401334799916\n",
      "testing loss 0.0037912728012281845\n",
      "epochs 1710\n",
      "training loss 0.0017385450220103503\n",
      "epochs 1711\n",
      "training loss 0.0016829015233831362\n",
      "epochs 1712\n",
      "training loss 0.001695783312912283\n",
      "epochs 1713\n",
      "training loss 0.0017437746647128262\n",
      "epochs 1714\n",
      "training loss 0.0017812294711423953\n",
      "epochs 1715\n",
      "training loss 0.0017924132119258441\n",
      "epochs 1716\n",
      "training loss 0.0017558268807817104\n",
      "epochs 1717\n",
      "training loss 0.0017809546593290763\n",
      "epochs 1718\n",
      "training loss 0.0017659869795172592\n",
      "epochs 1719\n",
      "training loss 0.001730919226092544\n",
      "testing loss 0.003077953651379001\n",
      "epochs 1720\n",
      "training loss 0.001689014247424708\n",
      "epochs 1721\n",
      "training loss 0.0017581121103645593\n",
      "epochs 1722\n",
      "training loss 0.0016615080858062737\n",
      "epochs 1723\n",
      "training loss 0.001773136811214853\n",
      "epochs 1724\n",
      "training loss 0.001715027448179138\n",
      "epochs 1725\n",
      "training loss 0.0016927864815824661\n",
      "epochs 1726\n",
      "training loss 0.0017048046084851207\n",
      "epochs 1727\n",
      "training loss 0.001744014881890958\n",
      "epochs 1728\n",
      "training loss 0.0017171103909249363\n",
      "epochs 1729\n",
      "training loss 0.0017392619776236613\n",
      "testing loss 0.0035046331918012377\n",
      "epochs 1730\n",
      "training loss 0.0016816034155832688\n",
      "epochs 1731\n",
      "training loss 0.0017072288078853243\n",
      "epochs 1732\n",
      "training loss 0.0017046866574718144\n",
      "epochs 1733\n",
      "training loss 0.0016960155835947005\n",
      "epochs 1734\n",
      "training loss 0.0017789630318417193\n",
      "epochs 1735\n",
      "training loss 0.0016982489683747427\n",
      "epochs 1736\n",
      "training loss 0.0017319019644317362\n",
      "epochs 1737\n",
      "training loss 0.001740977894705876\n",
      "epochs 1738\n",
      "training loss 0.0017196699921367266\n",
      "epochs 1739\n",
      "training loss 0.0016623262086342511\n",
      "testing loss 0.003970271096967742\n",
      "epochs 1740\n",
      "training loss 0.001695283840758596\n",
      "epochs 1741\n",
      "training loss 0.0016639289963851743\n",
      "epochs 1742\n",
      "training loss 0.0017457884797574472\n",
      "epochs 1743\n",
      "training loss 0.0017036055708318201\n",
      "epochs 1744\n",
      "training loss 0.0017089728001677814\n",
      "epochs 1745\n",
      "training loss 0.0016968996197871274\n",
      "epochs 1746\n",
      "training loss 0.0016801373533816798\n",
      "epochs 1747\n",
      "training loss 0.0017132379559058367\n",
      "epochs 1748\n",
      "training loss 0.0016607085620067266\n",
      "epochs 1749\n",
      "training loss 0.0016851358644318934\n",
      "testing loss 0.004226756007212432\n",
      "epochs 1750\n",
      "training loss 0.0016683980415286468\n",
      "epochs 1751\n",
      "training loss 0.0017287389047608052\n",
      "epochs 1752\n",
      "training loss 0.0016904661582646987\n",
      "epochs 1753\n",
      "training loss 0.0016764181609106982\n",
      "epochs 1754\n",
      "training loss 0.0017437708582148793\n",
      "epochs 1755\n",
      "training loss 0.0017023017938888566\n",
      "epochs 1756\n",
      "training loss 0.001738472103026602\n",
      "epochs 1757\n",
      "training loss 0.0017232415215027346\n",
      "epochs 1758\n",
      "training loss 0.0017578132341949887\n",
      "epochs 1759\n",
      "training loss 0.0017183381085012302\n",
      "testing loss 0.0035226217001273646\n",
      "epochs 1760\n",
      "training loss 0.0017226917149377335\n",
      "epochs 1761\n",
      "training loss 0.001627899772886719\n",
      "epochs 1762\n",
      "training loss 0.0017194313591269714\n",
      "epochs 1763\n",
      "training loss 0.0016899876990766013\n",
      "epochs 1764\n",
      "training loss 0.001720632425025332\n",
      "epochs 1765\n",
      "training loss 0.0016961740745630647\n",
      "epochs 1766\n",
      "training loss 0.0017064009983471869\n",
      "epochs 1767\n",
      "training loss 0.0017231726453860367\n",
      "epochs 1768\n",
      "training loss 0.001695703154240132\n",
      "epochs 1769\n",
      "training loss 0.0017283269491522237\n",
      "testing loss 0.0036296317798039275\n",
      "epochs 1770\n",
      "training loss 0.001710597770168279\n",
      "epochs 1771\n",
      "training loss 0.001670953692227183\n",
      "epochs 1772\n",
      "training loss 0.001654925579410386\n",
      "epochs 1773\n",
      "training loss 0.0017308101891667376\n",
      "epochs 1774\n",
      "training loss 0.0017515413780161675\n",
      "epochs 1775\n",
      "training loss 0.0017121852234736265\n",
      "epochs 1776\n",
      "training loss 0.0017155938349044273\n",
      "epochs 1777\n",
      "training loss 0.0017049416371035222\n",
      "epochs 1778\n",
      "training loss 0.0017473033500557884\n",
      "epochs 1779\n",
      "training loss 0.0017334375575419776\n",
      "testing loss 0.004713474791860274\n",
      "epochs 1780\n",
      "training loss 0.001735690776272861\n",
      "epochs 1781\n",
      "training loss 0.0016401428391372836\n",
      "epochs 1782\n",
      "training loss 0.0016523044314702526\n",
      "epochs 1783\n",
      "training loss 0.0016692000054499547\n",
      "epochs 1784\n",
      "training loss 0.001669741039819687\n",
      "epochs 1785\n",
      "training loss 0.0017193121977835486\n",
      "epochs 1786\n",
      "training loss 0.001691646170682107\n",
      "epochs 1787\n",
      "training loss 0.001581208364815822\n",
      "epochs 1788\n",
      "training loss 0.0016857377935687822\n",
      "epochs 1789\n",
      "training loss 0.0016319708322602292\n",
      "testing loss 0.004150982658008893\n",
      "epochs 1790\n",
      "training loss 0.0017762297968235783\n",
      "epochs 1791\n",
      "training loss 0.0017331427790855814\n",
      "epochs 1792\n",
      "training loss 0.0016723863724236996\n",
      "epochs 1793\n",
      "training loss 0.0017693441612065676\n",
      "epochs 1794\n",
      "training loss 0.001690251832172603\n",
      "epochs 1795\n",
      "training loss 0.00167270817586552\n",
      "epochs 1796\n",
      "training loss 0.0016686863683115997\n",
      "epochs 1797\n",
      "training loss 0.0017281853628208956\n",
      "epochs 1798\n",
      "training loss 0.0017013543946767583\n",
      "epochs 1799\n",
      "training loss 0.0017013421625756632\n",
      "testing loss 0.00412860645744157\n",
      "epochs 1800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0016669150637207966\n",
      "epochs 1801\n",
      "training loss 0.0016903120914324098\n",
      "epochs 1802\n",
      "training loss 0.0016354081729755348\n",
      "epochs 1803\n",
      "training loss 0.0016956569688496574\n",
      "epochs 1804\n",
      "training loss 0.0016594160120654003\n",
      "epochs 1805\n",
      "training loss 0.0016783671067903063\n",
      "epochs 1806\n",
      "training loss 0.0016820299324839967\n",
      "epochs 1807\n",
      "training loss 0.0016956477554084727\n",
      "epochs 1808\n",
      "training loss 0.001728258567535259\n",
      "epochs 1809\n",
      "training loss 0.0017121750703944874\n",
      "testing loss 0.0043065342119174965\n",
      "epochs 1810\n",
      "training loss 0.0016567199377733705\n",
      "epochs 1811\n",
      "training loss 0.0017680348515281266\n",
      "epochs 1812\n",
      "training loss 0.0016324241126542147\n",
      "epochs 1813\n",
      "training loss 0.0016763282608591857\n",
      "epochs 1814\n",
      "training loss 0.0016863243181985948\n",
      "epochs 1815\n",
      "training loss 0.00169464341955016\n",
      "epochs 1816\n",
      "training loss 0.0016447724710787592\n",
      "epochs 1817\n",
      "training loss 0.0017450294221360745\n",
      "epochs 1818\n",
      "training loss 0.0017228494617680377\n",
      "epochs 1819\n",
      "training loss 0.0016262201347464757\n",
      "testing loss 0.003715451362589044\n",
      "epochs 1820\n",
      "training loss 0.0016986143583853624\n",
      "epochs 1821\n",
      "training loss 0.0016434618153702751\n",
      "epochs 1822\n",
      "training loss 0.0016724620197367454\n",
      "epochs 1823\n",
      "training loss 0.0016816644391503559\n",
      "epochs 1824\n",
      "training loss 0.0016425893796638991\n",
      "epochs 1825\n",
      "training loss 0.0016394088348716293\n",
      "epochs 1826\n",
      "training loss 0.0016949000512886691\n",
      "epochs 1827\n",
      "training loss 0.001639434469242419\n",
      "epochs 1828\n",
      "training loss 0.0016044295857530256\n",
      "epochs 1829\n",
      "training loss 0.0017139514158748793\n",
      "testing loss 0.00395400207308088\n",
      "epochs 1830\n",
      "training loss 0.0016950959911906814\n",
      "epochs 1831\n",
      "training loss 0.00162523604383109\n",
      "epochs 1832\n",
      "training loss 0.0016669093775930942\n",
      "epochs 1833\n",
      "training loss 0.0016933193191104657\n",
      "epochs 1834\n",
      "training loss 0.00160366351985851\n",
      "epochs 1835\n",
      "training loss 0.0017151537425264059\n",
      "epochs 1836\n",
      "training loss 0.0017286203433632126\n",
      "epochs 1837\n",
      "training loss 0.0017618326633732687\n",
      "epochs 1838\n",
      "training loss 0.0016247532172991943\n",
      "epochs 1839\n",
      "training loss 0.0017401926323140215\n",
      "testing loss 0.003016634827730715\n",
      "epochs 1840\n",
      "training loss 0.001648975987734675\n",
      "epochs 1841\n",
      "training loss 0.0016966563703260161\n",
      "epochs 1842\n",
      "training loss 0.0017891560743441154\n",
      "epochs 1843\n",
      "training loss 0.0017334800289652244\n",
      "epochs 1844\n",
      "training loss 0.0016017202294989288\n",
      "epochs 1845\n",
      "training loss 0.0016257628001623693\n",
      "epochs 1846\n",
      "training loss 0.0016723799161502588\n",
      "epochs 1847\n",
      "training loss 0.0017117393354525773\n",
      "epochs 1848\n",
      "training loss 0.0016554875922457266\n",
      "epochs 1849\n",
      "training loss 0.0017122098318680464\n",
      "testing loss 0.0034373834578928057\n",
      "epochs 1850\n",
      "training loss 0.0016238315671039773\n",
      "epochs 1851\n",
      "training loss 0.0016391651291261487\n",
      "epochs 1852\n",
      "training loss 0.0016589451400424414\n",
      "epochs 1853\n",
      "training loss 0.0016573409968845569\n",
      "epochs 1854\n",
      "training loss 0.0017840539570190125\n",
      "epochs 1855\n",
      "training loss 0.001669816620255846\n",
      "epochs 1856\n",
      "training loss 0.0016954746619249074\n",
      "epochs 1857\n",
      "training loss 0.0016179539080663926\n",
      "epochs 1858\n",
      "training loss 0.001648636231033307\n",
      "epochs 1859\n",
      "training loss 0.0016674522109157023\n",
      "testing loss 0.004561249052788666\n",
      "epochs 1860\n",
      "training loss 0.0016566009793606074\n",
      "epochs 1861\n",
      "training loss 0.0016620296920864847\n",
      "epochs 1862\n",
      "training loss 0.0016063413947911248\n",
      "epochs 1863\n",
      "training loss 0.0017848004184317942\n",
      "epochs 1864\n",
      "training loss 0.001761153967354707\n",
      "epochs 1865\n",
      "training loss 0.0016355809456977347\n",
      "epochs 1866\n",
      "training loss 0.001708493640364677\n",
      "epochs 1867\n",
      "training loss 0.0016307349199652784\n",
      "epochs 1868\n",
      "training loss 0.0016655608433819627\n",
      "epochs 1869\n",
      "training loss 0.0017253087853339123\n",
      "testing loss 0.004442624524830186\n",
      "epochs 1870\n",
      "training loss 0.0016291153606234011\n",
      "epochs 1871\n",
      "training loss 0.0016059272954067763\n",
      "epochs 1872\n",
      "training loss 0.001646419512751968\n",
      "epochs 1873\n",
      "training loss 0.0016623869675245577\n",
      "epochs 1874\n",
      "training loss 0.001643641749757843\n",
      "epochs 1875\n",
      "training loss 0.0016112732377234495\n",
      "epochs 1876\n",
      "training loss 0.0016292756355061877\n",
      "epochs 1877\n",
      "training loss 0.001686977541553972\n",
      "epochs 1878\n",
      "training loss 0.0016843635821714997\n",
      "epochs 1879\n",
      "training loss 0.001680061659891736\n",
      "testing loss 0.0034863465855307614\n",
      "epochs 1880\n",
      "training loss 0.001645992147151698\n",
      "epochs 1881\n",
      "training loss 0.001618154659919656\n",
      "epochs 1882\n",
      "training loss 0.0016873781091610032\n",
      "epochs 1883\n",
      "training loss 0.0016989111854329615\n",
      "epochs 1884\n",
      "training loss 0.0016622554774800922\n",
      "epochs 1885\n",
      "training loss 0.0016073645936469563\n",
      "epochs 1886\n",
      "training loss 0.001622227259938489\n",
      "epochs 1887\n",
      "training loss 0.0016058779489333914\n",
      "epochs 1888\n",
      "training loss 0.0016377019728845308\n",
      "epochs 1889\n",
      "training loss 0.001635452614259258\n",
      "testing loss 0.0040863044692058086\n",
      "epochs 1890\n",
      "training loss 0.001700923921955087\n",
      "epochs 1891\n",
      "training loss 0.0016873326583547478\n",
      "epochs 1892\n",
      "training loss 0.0016466761483827828\n",
      "epochs 1893\n",
      "training loss 0.0016323677106692505\n",
      "epochs 1894\n",
      "training loss 0.0016120967406023712\n",
      "epochs 1895\n",
      "training loss 0.001559402734022814\n",
      "epochs 1896\n",
      "training loss 0.001638279855534389\n",
      "epochs 1897\n",
      "training loss 0.001571369023553427\n",
      "epochs 1898\n",
      "training loss 0.0016370848448250704\n",
      "epochs 1899\n",
      "training loss 0.0016713781711520871\n",
      "testing loss 0.0034759741297969264\n",
      "epochs 1900\n",
      "training loss 0.0016407414822836369\n",
      "epochs 1901\n",
      "training loss 0.0016401374658094124\n",
      "epochs 1902\n",
      "training loss 0.0016081970835056462\n",
      "epochs 1903\n",
      "training loss 0.0016227965600545148\n",
      "epochs 1904\n",
      "training loss 0.0016153352793325927\n",
      "epochs 1905\n",
      "training loss 0.0015272891847239835\n",
      "epochs 1906\n",
      "training loss 0.0016102597551239163\n",
      "epochs 1907\n",
      "training loss 0.0016041464718287999\n",
      "epochs 1908\n",
      "training loss 0.0017322147837245559\n",
      "epochs 1909\n",
      "training loss 0.0016336375060151571\n",
      "testing loss 0.003205203867339073\n",
      "epochs 1910\n",
      "training loss 0.0016283844643637628\n",
      "epochs 1911\n",
      "training loss 0.0016214181189595136\n",
      "epochs 1912\n",
      "training loss 0.0015150231058358309\n",
      "epochs 1913\n",
      "training loss 0.0016504474800564121\n",
      "epochs 1914\n",
      "training loss 0.0015643983891822603\n",
      "epochs 1915\n",
      "training loss 0.0016658826588545458\n",
      "epochs 1916\n",
      "training loss 0.0016109874347468277\n",
      "epochs 1917\n",
      "training loss 0.0016024085902892008\n",
      "epochs 1918\n",
      "training loss 0.0015944500857508405\n",
      "epochs 1919\n",
      "training loss 0.0016519105184673243\n",
      "testing loss 0.003263870725599745\n",
      "epochs 1920\n",
      "training loss 0.0016386980269780796\n",
      "epochs 1921\n",
      "training loss 0.0016846424890441486\n",
      "epochs 1922\n",
      "training loss 0.0016423177859026905\n",
      "epochs 1923\n",
      "training loss 0.0016141588103286582\n",
      "epochs 1924\n",
      "training loss 0.0015861472485751186\n",
      "epochs 1925\n",
      "training loss 0.0016328178379418595\n",
      "epochs 1926\n",
      "training loss 0.00164462938691747\n",
      "epochs 1927\n",
      "training loss 0.0016353466681779383\n",
      "epochs 1928\n",
      "training loss 0.0016685096325146141\n",
      "epochs 1929\n",
      "training loss 0.0015738356391769071\n",
      "testing loss 0.0030178172243032473\n",
      "epochs 1930\n",
      "training loss 0.0016578615230499656\n",
      "epochs 1931\n",
      "training loss 0.0016248188183945816\n",
      "epochs 1932\n",
      "training loss 0.001632636510462735\n",
      "epochs 1933\n",
      "training loss 0.001668438349714599\n",
      "epochs 1934\n",
      "training loss 0.0015946109478410027\n",
      "epochs 1935\n",
      "training loss 0.0015939254192338186\n",
      "epochs 1936\n",
      "training loss 0.0015802817772309604\n",
      "epochs 1937\n",
      "training loss 0.001611273279831119\n",
      "epochs 1938\n",
      "training loss 0.0016244568621051868\n",
      "epochs 1939\n",
      "training loss 0.001610600566463177\n",
      "testing loss 0.003639788623628411\n",
      "epochs 1940\n",
      "training loss 0.001657768907938379\n",
      "epochs 1941\n",
      "training loss 0.0015953778859658172\n",
      "epochs 1942\n",
      "training loss 0.0015938572143629126\n",
      "epochs 1943\n",
      "training loss 0.0016102323271079402\n",
      "epochs 1944\n",
      "training loss 0.0016052303780903503\n",
      "epochs 1945\n",
      "training loss 0.001652936124891088\n",
      "epochs 1946\n",
      "training loss 0.0016118970741715996\n",
      "epochs 1947\n",
      "training loss 0.0016327485411023115\n",
      "epochs 1948\n",
      "training loss 0.0016928836253916103\n",
      "epochs 1949\n",
      "training loss 0.0017008536578775992\n",
      "testing loss 0.004466500414819424\n",
      "epochs 1950\n",
      "training loss 0.001604443458107082\n",
      "epochs 1951\n",
      "training loss 0.0015451696256917299\n",
      "epochs 1952\n",
      "training loss 0.0016130704848848758\n",
      "epochs 1953\n",
      "training loss 0.001581388853503091\n",
      "epochs 1954\n",
      "training loss 0.0016677888403264494\n",
      "epochs 1955\n",
      "training loss 0.0016418577608508043\n",
      "epochs 1956\n",
      "training loss 0.0016311086674636983\n",
      "epochs 1957\n",
      "training loss 0.0016249751422333454\n",
      "epochs 1958\n",
      "training loss 0.0016190894565539361\n",
      "epochs 1959\n",
      "training loss 0.0016435925881689737\n",
      "testing loss 0.003321503152813208\n",
      "epochs 1960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0018912405258619247\n",
      "epochs 1961\n",
      "training loss 0.0015982482191330915\n",
      "epochs 1962\n",
      "training loss 0.0015479877932948318\n",
      "epochs 1963\n",
      "training loss 0.0015573632934535529\n",
      "epochs 1964\n",
      "training loss 0.0016245969192911856\n",
      "epochs 1965\n",
      "training loss 0.0015906547332440125\n",
      "epochs 1966\n",
      "training loss 0.0016253402311210505\n",
      "epochs 1967\n",
      "training loss 0.0015899513614233752\n",
      "epochs 1968\n",
      "training loss 0.0015997368595378875\n",
      "epochs 1969\n",
      "training loss 0.0015982861845049226\n",
      "testing loss 0.003378158560722855\n",
      "epochs 1970\n",
      "training loss 0.0015748200715536473\n",
      "epochs 1971\n",
      "training loss 0.0015961691366698264\n",
      "epochs 1972\n",
      "training loss 0.0016134559885615705\n",
      "epochs 1973\n",
      "training loss 0.0016514126479973887\n",
      "epochs 1974\n",
      "training loss 0.0016344223489226225\n",
      "epochs 1975\n",
      "training loss 0.0016068384988307048\n",
      "epochs 1976\n",
      "training loss 0.0016110783114131981\n",
      "epochs 1977\n",
      "training loss 0.001650343708358535\n",
      "epochs 1978\n",
      "training loss 0.0015425764669672697\n",
      "epochs 1979\n",
      "training loss 0.0015945589073613784\n",
      "testing loss 0.0030764460729613423\n",
      "epochs 1980\n",
      "training loss 0.0016676597541345334\n",
      "epochs 1981\n",
      "training loss 0.0016254103067755632\n",
      "epochs 1982\n",
      "training loss 0.0016071819107608706\n",
      "epochs 1983\n",
      "training loss 0.0016026110430800082\n",
      "epochs 1984\n",
      "training loss 0.001617713005311708\n",
      "epochs 1985\n",
      "training loss 0.001596804969380737\n",
      "epochs 1986\n",
      "training loss 0.001587780952201675\n",
      "epochs 1987\n",
      "training loss 0.001636475680069007\n",
      "epochs 1988\n",
      "training loss 0.0015614264477313267\n",
      "epochs 1989\n",
      "training loss 0.0015682374904962613\n",
      "testing loss 0.003683863519790008\n",
      "epochs 1990\n",
      "training loss 0.0016005987405499562\n",
      "epochs 1991\n",
      "training loss 0.0015586288242479135\n",
      "epochs 1992\n",
      "training loss 0.001579427964437617\n",
      "epochs 1993\n",
      "training loss 0.0015906163172493477\n",
      "epochs 1994\n",
      "training loss 0.001575395471974075\n",
      "epochs 1995\n",
      "training loss 0.0015581330918922093\n",
      "epochs 1996\n",
      "training loss 0.0016198253517724166\n",
      "epochs 1997\n",
      "training loss 0.001646280527011545\n",
      "epochs 1998\n",
      "training loss 0.0015649799051224665\n",
      "epochs 1999\n",
      "training loss 0.0015736561515198675\n",
      "testing loss 0.0037466690525518242\n",
      "epochs 2000\n",
      "training loss 0.0015897760786948369\n",
      "epochs 2001\n",
      "training loss 0.0015142076057141312\n",
      "epochs 2002\n",
      "training loss 0.0015235795074363316\n",
      "epochs 2003\n",
      "training loss 0.0015643425278739273\n",
      "epochs 2004\n",
      "training loss 0.0015841049726381558\n",
      "epochs 2005\n",
      "training loss 0.0016407311025661583\n",
      "epochs 2006\n",
      "training loss 0.0015654627410312803\n",
      "epochs 2007\n",
      "training loss 0.0015995366499535627\n",
      "epochs 2008\n",
      "training loss 0.0015715865377976493\n",
      "epochs 2009\n",
      "training loss 0.0015438372687872578\n",
      "testing loss 0.0029920832912853743\n",
      "epochs 2010\n",
      "training loss 0.0015772457213684532\n",
      "epochs 2011\n",
      "training loss 0.0016137207414187763\n",
      "epochs 2012\n",
      "training loss 0.0015573902167082625\n",
      "epochs 2013\n",
      "training loss 0.001610706059267818\n",
      "epochs 2014\n",
      "training loss 0.0016407151515439923\n",
      "epochs 2015\n",
      "training loss 0.0015983115688835393\n",
      "epochs 2016\n",
      "training loss 0.0015933563340792154\n",
      "epochs 2017\n",
      "training loss 0.0015689382518183066\n",
      "epochs 2018\n",
      "training loss 0.0015502744308307677\n",
      "epochs 2019\n",
      "training loss 0.0015919130800807096\n",
      "testing loss 0.0030992462535897364\n",
      "epochs 2020\n",
      "training loss 0.00158441449939429\n",
      "epochs 2021\n",
      "training loss 0.0015913849274732602\n",
      "epochs 2022\n",
      "training loss 0.0015526838913081514\n",
      "epochs 2023\n",
      "training loss 0.0016190217456519944\n",
      "epochs 2024\n",
      "training loss 0.0015741800209988577\n",
      "epochs 2025\n",
      "training loss 0.001541365054966797\n",
      "epochs 2026\n",
      "training loss 0.0015117235438923682\n",
      "epochs 2027\n",
      "training loss 0.0015617857001067952\n",
      "epochs 2028\n",
      "training loss 0.001567070993048487\n",
      "epochs 2029\n",
      "training loss 0.0015372195934641845\n",
      "testing loss 0.002965319357607646\n",
      "epochs 2030\n",
      "training loss 0.0015892073948015558\n",
      "epochs 2031\n",
      "training loss 0.0015719937311703379\n",
      "epochs 2032\n",
      "training loss 0.0015217615468399231\n",
      "epochs 2033\n",
      "training loss 0.0016270109238584263\n",
      "epochs 2034\n",
      "training loss 0.0015546199347254875\n",
      "epochs 2035\n",
      "training loss 0.0015806339180102645\n",
      "epochs 2036\n",
      "training loss 0.0015963555085769993\n",
      "epochs 2037\n",
      "training loss 0.0015374693480294799\n",
      "epochs 2038\n",
      "training loss 0.0015855504942585156\n",
      "epochs 2039\n",
      "training loss 0.0015793071759335334\n",
      "testing loss 0.002912616228820188\n",
      "epochs 2040\n",
      "training loss 0.0016443371074981875\n",
      "epochs 2041\n",
      "training loss 0.0015195121285916055\n",
      "epochs 2042\n",
      "training loss 0.0016253501846310667\n",
      "epochs 2043\n",
      "training loss 0.0015990714070891062\n",
      "epochs 2044\n",
      "training loss 0.0016028349753823075\n",
      "epochs 2045\n",
      "training loss 0.0015470282093969976\n",
      "epochs 2046\n",
      "training loss 0.0015569045592317397\n",
      "epochs 2047\n",
      "training loss 0.0016422926163085732\n",
      "epochs 2048\n",
      "training loss 0.0015450047285191285\n",
      "epochs 2049\n",
      "training loss 0.0016140131882071993\n",
      "testing loss 0.003082308145438103\n",
      "epochs 2050\n",
      "training loss 0.001602712482579143\n",
      "epochs 2051\n",
      "training loss 0.001540927618556574\n",
      "epochs 2052\n",
      "training loss 0.001596030568290717\n",
      "epochs 2053\n",
      "training loss 0.0015592382191807756\n",
      "epochs 2054\n",
      "training loss 0.0015792194962654387\n",
      "epochs 2055\n",
      "training loss 0.001620435881489282\n",
      "epochs 2056\n",
      "training loss 0.0015435581841070126\n",
      "epochs 2057\n",
      "training loss 0.0016205245777568284\n",
      "epochs 2058\n",
      "training loss 0.0016178187711085093\n",
      "epochs 2059\n",
      "training loss 0.00157614087733358\n",
      "testing loss 0.002936410748627671\n",
      "epochs 2060\n",
      "training loss 0.0015178264798469117\n",
      "epochs 2061\n",
      "training loss 0.0015296296276347456\n",
      "epochs 2062\n",
      "training loss 0.001492005185504362\n",
      "epochs 2063\n",
      "training loss 0.0015763925589439899\n",
      "epochs 2064\n",
      "training loss 0.0015461633451096146\n",
      "epochs 2065\n",
      "training loss 0.0015239481032913761\n",
      "epochs 2066\n",
      "training loss 0.0015298644304388744\n",
      "epochs 2067\n",
      "training loss 0.0015471510788691895\n",
      "epochs 2068\n",
      "training loss 0.0015760824152574193\n",
      "epochs 2069\n",
      "training loss 0.001548085080765437\n",
      "testing loss 0.003761121023162301\n",
      "epochs 2070\n",
      "training loss 0.0015668824274732665\n",
      "epochs 2071\n",
      "training loss 0.0015928434699173666\n",
      "epochs 2072\n",
      "training loss 0.0015769631412208623\n",
      "epochs 2073\n",
      "training loss 0.001591837287513896\n",
      "epochs 2074\n",
      "training loss 0.0016346099214291478\n",
      "epochs 2075\n",
      "training loss 0.0015970134835528906\n",
      "epochs 2076\n",
      "training loss 0.0015434320665058032\n",
      "epochs 2077\n",
      "training loss 0.001516195534095791\n",
      "epochs 2078\n",
      "training loss 0.0015979415990446204\n",
      "epochs 2079\n",
      "training loss 0.0015560163396954152\n",
      "testing loss 0.003342191175993611\n",
      "epochs 2080\n",
      "training loss 0.0015910536069904191\n",
      "epochs 2081\n",
      "training loss 0.001586620046050885\n",
      "epochs 2082\n",
      "training loss 0.001607013066082706\n",
      "epochs 2083\n",
      "training loss 0.0015965148050756984\n",
      "epochs 2084\n",
      "training loss 0.0015418220988664447\n",
      "epochs 2085\n",
      "training loss 0.0015680927349435657\n",
      "epochs 2086\n",
      "training loss 0.001576635590259065\n",
      "epochs 2087\n",
      "training loss 0.0014949588470959238\n",
      "epochs 2088\n",
      "training loss 0.0016025191859064742\n",
      "epochs 2089\n",
      "training loss 0.0016193830835955687\n",
      "testing loss 0.0028881212859084263\n",
      "epochs 2090\n",
      "training loss 0.0015077357457124127\n",
      "epochs 2091\n",
      "training loss 0.0015449013561364207\n",
      "epochs 2092\n",
      "training loss 0.0015399857929491616\n",
      "epochs 2093\n",
      "training loss 0.001585312095671711\n",
      "epochs 2094\n",
      "training loss 0.001549086200163946\n",
      "epochs 2095\n",
      "training loss 0.00161262849394634\n",
      "epochs 2096\n",
      "training loss 0.0015201598257576555\n",
      "epochs 2097\n",
      "training loss 0.0015273349592108336\n",
      "epochs 2098\n",
      "training loss 0.0015052604352090067\n",
      "epochs 2099\n",
      "training loss 0.0015960089785562075\n",
      "testing loss 0.0035090058668787384\n",
      "epochs 2100\n",
      "training loss 0.001541285574440434\n",
      "epochs 2101\n",
      "training loss 0.0015420524716960396\n",
      "epochs 2102\n",
      "training loss 0.0015302259591054228\n",
      "epochs 2103\n",
      "training loss 0.0015372015213123934\n",
      "epochs 2104\n",
      "training loss 0.0015956508321262964\n",
      "epochs 2105\n",
      "training loss 0.001538821421938702\n",
      "epochs 2106\n",
      "training loss 0.0016041095108487347\n",
      "epochs 2107\n",
      "training loss 0.0015325851649637574\n",
      "epochs 2108\n",
      "training loss 0.0015354780027342786\n",
      "epochs 2109\n",
      "training loss 0.0015175585370683494\n",
      "testing loss 0.0036086659095785085\n",
      "epochs 2110\n",
      "training loss 0.00157571151197285\n",
      "epochs 2111\n",
      "training loss 0.0015435525740561922\n",
      "epochs 2112\n",
      "training loss 0.0015122727559961514\n",
      "epochs 2113\n",
      "training loss 0.001530194895852946\n",
      "epochs 2114\n",
      "training loss 0.0015051139553649334\n",
      "epochs 2115\n",
      "training loss 0.0015417931733743534\n",
      "epochs 2116\n",
      "training loss 0.0015619222407716137\n",
      "epochs 2117\n",
      "training loss 0.0015702246775706283\n",
      "epochs 2118\n",
      "training loss 0.0015993527034835432\n",
      "epochs 2119\n",
      "training loss 0.0016639675105720939\n",
      "testing loss 0.0035025397908431313\n",
      "epochs 2120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0015876263853850405\n",
      "epochs 2121\n",
      "training loss 0.0015160981137560267\n",
      "epochs 2122\n",
      "training loss 0.001489839006334226\n",
      "epochs 2123\n",
      "training loss 0.0016300150891765952\n",
      "epochs 2124\n",
      "training loss 0.0015407221927519854\n",
      "epochs 2125\n",
      "training loss 0.0015605451595066483\n",
      "epochs 2126\n",
      "training loss 0.0015125388964603013\n",
      "epochs 2127\n",
      "training loss 0.0015336639714203662\n",
      "epochs 2128\n",
      "training loss 0.0015076765433904972\n",
      "epochs 2129\n",
      "training loss 0.001572675720256354\n",
      "testing loss 0.0030906929478465403\n",
      "epochs 2130\n",
      "training loss 0.0015935609683303848\n",
      "epochs 2131\n",
      "training loss 0.0015494019970705443\n",
      "epochs 2132\n",
      "training loss 0.0015489950666054064\n",
      "epochs 2133\n",
      "training loss 0.0015368543446381041\n",
      "epochs 2134\n",
      "training loss 0.0015598337976672435\n",
      "epochs 2135\n",
      "training loss 0.0015457194819720886\n",
      "epochs 2136\n",
      "training loss 0.001563748131586822\n",
      "epochs 2137\n",
      "training loss 0.001558790852083601\n",
      "epochs 2138\n",
      "training loss 0.0015363935657173938\n",
      "epochs 2139\n",
      "training loss 0.001580017766565662\n",
      "testing loss 0.0028389683513446373\n",
      "epochs 2140\n",
      "training loss 0.0014885037107390314\n",
      "epochs 2141\n",
      "training loss 0.0015290458713188247\n",
      "epochs 2142\n",
      "training loss 0.0015132797690530202\n",
      "epochs 2143\n",
      "training loss 0.0015704432312373673\n",
      "epochs 2144\n",
      "training loss 0.0015217464211634657\n",
      "epochs 2145\n",
      "training loss 0.00151504636183098\n",
      "epochs 2146\n",
      "training loss 0.0014901255695488995\n",
      "epochs 2147\n",
      "training loss 0.0016290213936433129\n",
      "epochs 2148\n",
      "training loss 0.0014653271623238876\n",
      "epochs 2149\n",
      "training loss 0.0015409240333892767\n",
      "testing loss 0.0037711827247073307\n",
      "epochs 2150\n",
      "training loss 0.0015563875203952193\n",
      "epochs 2151\n",
      "training loss 0.0015113532127767212\n",
      "epochs 2152\n",
      "training loss 0.0015393814157289778\n",
      "epochs 2153\n",
      "training loss 0.0014772189180947003\n",
      "epochs 2154\n",
      "training loss 0.0014354746297248115\n",
      "epochs 2155\n",
      "training loss 0.001482327357585624\n",
      "epochs 2156\n",
      "training loss 0.0015358849753070861\n",
      "epochs 2157\n",
      "training loss 0.0015508452995459472\n",
      "epochs 2158\n",
      "training loss 0.001552187566083536\n",
      "epochs 2159\n",
      "training loss 0.0015495805986972928\n",
      "testing loss 0.002936271609612972\n",
      "epochs 2160\n",
      "training loss 0.0015009042211046788\n",
      "epochs 2161\n",
      "training loss 0.0015696283933383875\n",
      "epochs 2162\n",
      "training loss 0.0015195542833227155\n",
      "epochs 2163\n",
      "training loss 0.0015747128859499191\n",
      "epochs 2164\n",
      "training loss 0.0015425368310886473\n",
      "epochs 2165\n",
      "training loss 0.001536150270963999\n",
      "epochs 2166\n",
      "training loss 0.0015235034777927228\n",
      "epochs 2167\n",
      "training loss 0.0014672158774282885\n",
      "epochs 2168\n",
      "training loss 0.0015362865718981777\n",
      "epochs 2169\n",
      "training loss 0.0015367996267830007\n",
      "testing loss 0.003938875878593019\n",
      "epochs 2170\n",
      "training loss 0.0015141248540587633\n",
      "epochs 2171\n",
      "training loss 0.001466710689069992\n",
      "epochs 2172\n",
      "training loss 0.0014883096034052615\n",
      "epochs 2173\n",
      "training loss 0.00151971509569752\n",
      "epochs 2174\n",
      "training loss 0.001484447457519074\n",
      "epochs 2175\n",
      "training loss 0.0015728893542257462\n",
      "epochs 2176\n",
      "training loss 0.0014877014416255804\n",
      "epochs 2177\n",
      "training loss 0.0015213830671825172\n",
      "epochs 2178\n",
      "training loss 0.0015036499302545534\n",
      "epochs 2179\n",
      "training loss 0.0015455254172767573\n",
      "testing loss 0.00323025574775558\n",
      "epochs 2180\n",
      "training loss 0.0014737683198326988\n",
      "epochs 2181\n",
      "training loss 0.0015010947982941989\n",
      "epochs 2182\n",
      "training loss 0.0015013000423988853\n",
      "epochs 2183\n",
      "training loss 0.0015565177132403846\n",
      "epochs 2184\n",
      "training loss 0.0016627290827861921\n",
      "epochs 2185\n",
      "training loss 0.0015689222084423443\n",
      "epochs 2186\n",
      "training loss 0.0015741811842674611\n",
      "epochs 2187\n",
      "training loss 0.001509375733820195\n",
      "epochs 2188\n",
      "training loss 0.0014974920294604243\n",
      "epochs 2189\n",
      "training loss 0.0015598886982837866\n",
      "testing loss 0.0028571914988801093\n",
      "epochs 2190\n",
      "training loss 0.0014951633868702208\n",
      "epochs 2191\n",
      "training loss 0.001517731828705654\n",
      "epochs 2192\n",
      "training loss 0.0015604614420287244\n",
      "epochs 2193\n",
      "training loss 0.0015043209918695402\n",
      "epochs 2194\n",
      "training loss 0.0015119691407675983\n",
      "epochs 2195\n",
      "training loss 0.0014893077501438353\n",
      "epochs 2196\n",
      "training loss 0.0015807438420279002\n",
      "epochs 2197\n",
      "training loss 0.001518991817387619\n",
      "epochs 2198\n",
      "training loss 0.0014791673282515201\n",
      "epochs 2199\n",
      "training loss 0.0014794564828571033\n",
      "testing loss 0.0030600578893233647\n",
      "epochs 2200\n",
      "training loss 0.001523123522420728\n",
      "epochs 2201\n",
      "training loss 0.0015356580011679658\n",
      "epochs 2202\n",
      "training loss 0.0014694806117221918\n",
      "epochs 2203\n",
      "training loss 0.0015647360111309334\n",
      "epochs 2204\n",
      "training loss 0.0015353277894614497\n",
      "epochs 2205\n",
      "training loss 0.0015095639834109705\n",
      "epochs 2206\n",
      "training loss 0.0015131522392525726\n",
      "epochs 2207\n",
      "training loss 0.0014675096453006145\n",
      "epochs 2208\n",
      "training loss 0.001508921043703893\n",
      "epochs 2209\n",
      "training loss 0.001537596549460874\n",
      "testing loss 0.003585955197427501\n",
      "epochs 2210\n",
      "training loss 0.0015295303644652061\n",
      "epochs 2211\n",
      "training loss 0.0015148140865485652\n",
      "epochs 2212\n",
      "training loss 0.0015382379840841477\n",
      "epochs 2213\n",
      "training loss 0.0015637941487235412\n",
      "epochs 2214\n",
      "training loss 0.0014629732519473122\n",
      "epochs 2215\n",
      "training loss 0.0015087340479662875\n",
      "epochs 2216\n",
      "training loss 0.0015073868888005682\n",
      "epochs 2217\n",
      "training loss 0.0015283864220437286\n",
      "epochs 2218\n",
      "training loss 0.0014723322731456903\n",
      "epochs 2219\n",
      "training loss 0.0014974751849772018\n",
      "testing loss 0.0029376848305225477\n",
      "epochs 2220\n",
      "training loss 0.001480571497663414\n",
      "epochs 2221\n",
      "training loss 0.0015431925308349965\n",
      "epochs 2222\n",
      "training loss 0.0014990477719751829\n",
      "epochs 2223\n",
      "training loss 0.0014695806023990311\n",
      "epochs 2224\n",
      "training loss 0.0014879968738123932\n",
      "epochs 2225\n",
      "training loss 0.0015121836660749713\n",
      "epochs 2226\n",
      "training loss 0.0015020965712462017\n",
      "epochs 2227\n",
      "training loss 0.001496720738853342\n",
      "epochs 2228\n",
      "training loss 0.0014999101411177576\n",
      "epochs 2229\n",
      "training loss 0.00153471943253572\n",
      "testing loss 0.002988434617543025\n",
      "epochs 2230\n",
      "training loss 0.0015343329757749242\n",
      "epochs 2231\n",
      "training loss 0.001496663101238275\n",
      "epochs 2232\n",
      "training loss 0.0014886047066922517\n",
      "epochs 2233\n",
      "training loss 0.0015331122788563823\n",
      "epochs 2234\n",
      "training loss 0.0015134671881673206\n",
      "epochs 2235\n",
      "training loss 0.0014654467183824822\n",
      "epochs 2236\n",
      "training loss 0.0014773403238830664\n",
      "epochs 2237\n",
      "training loss 0.0015143136965184499\n",
      "epochs 2238\n",
      "training loss 0.001506981001119483\n",
      "epochs 2239\n",
      "training loss 0.0015170327442596272\n",
      "testing loss 0.003942262789181678\n",
      "epochs 2240\n",
      "training loss 0.0015260433041459475\n",
      "epochs 2241\n",
      "training loss 0.0015055200399612969\n",
      "epochs 2242\n",
      "training loss 0.001500680800817488\n",
      "epochs 2243\n",
      "training loss 0.0014840532548932252\n",
      "epochs 2244\n",
      "training loss 0.0015257153713843562\n",
      "epochs 2245\n",
      "training loss 0.001495026275451864\n",
      "epochs 2246\n",
      "training loss 0.0015756327911537694\n",
      "epochs 2247\n",
      "training loss 0.001534010882179414\n",
      "epochs 2248\n",
      "training loss 0.001491405553004696\n",
      "epochs 2249\n",
      "training loss 0.001654257243647775\n",
      "testing loss 0.0033148982998062954\n",
      "epochs 2250\n",
      "training loss 0.00150181993871848\n",
      "epochs 2251\n",
      "training loss 0.001512902593141065\n",
      "epochs 2252\n",
      "training loss 0.001426796820212869\n",
      "epochs 2253\n",
      "training loss 0.0014941367086470354\n",
      "epochs 2254\n",
      "training loss 0.001514978930998118\n",
      "epochs 2255\n",
      "training loss 0.0015281095029007767\n",
      "epochs 2256\n",
      "training loss 0.0015696143468921172\n",
      "epochs 2257\n",
      "training loss 0.0014895686535108098\n",
      "epochs 2258\n",
      "training loss 0.0014739374272414911\n",
      "epochs 2259\n",
      "training loss 0.001487999603026306\n",
      "testing loss 0.004700718285740479\n",
      "epochs 2260\n",
      "training loss 0.0014964812330825192\n",
      "epochs 2261\n",
      "training loss 0.00145232862910386\n",
      "epochs 2262\n",
      "training loss 0.0014908219042579476\n",
      "epochs 2263\n",
      "training loss 0.0014978974415725353\n",
      "epochs 2264\n",
      "training loss 0.0014673717794826645\n",
      "epochs 2265\n",
      "training loss 0.0014695832744668206\n",
      "epochs 2266\n",
      "training loss 0.0014554178669530682\n",
      "epochs 2267\n",
      "training loss 0.0015624577703596996\n",
      "epochs 2268\n",
      "training loss 0.0014703269134272784\n",
      "epochs 2269\n",
      "training loss 0.001496687267502164\n",
      "testing loss 0.0035057467646262747\n",
      "epochs 2270\n",
      "training loss 0.0014844191829266316\n",
      "epochs 2271\n",
      "training loss 0.0015444733711287922\n",
      "epochs 2272\n",
      "training loss 0.0014889348584218771\n",
      "epochs 2273\n",
      "training loss 0.0014694532880907749\n",
      "epochs 2274\n",
      "training loss 0.001502469482606611\n",
      "epochs 2275\n",
      "training loss 0.0014322600725508864\n",
      "epochs 2276\n",
      "training loss 0.0015233329196154821\n",
      "epochs 2277\n",
      "training loss 0.0015096173104745163\n",
      "epochs 2278\n",
      "training loss 0.001501159694528428\n",
      "epochs 2279\n",
      "training loss 0.001494446611895274\n",
      "testing loss 0.0034399938708541133\n",
      "epochs 2280\n",
      "training loss 0.0015313068040686243\n",
      "epochs 2281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0014562702725010167\n",
      "epochs 2282\n",
      "training loss 0.0014114439008703688\n",
      "epochs 2283\n",
      "training loss 0.0014191433278869237\n",
      "epochs 2284\n",
      "training loss 0.001484029012198608\n",
      "epochs 2285\n",
      "training loss 0.0015125899843865932\n",
      "epochs 2286\n",
      "training loss 0.001559338831918818\n",
      "epochs 2287\n",
      "training loss 0.0014488993803149592\n",
      "epochs 2288\n",
      "training loss 0.0014493973010310218\n",
      "epochs 2289\n",
      "training loss 0.001493961932095147\n",
      "testing loss 0.0031059852137448308\n",
      "epochs 2290\n",
      "training loss 0.001436452047105335\n",
      "epochs 2291\n",
      "training loss 0.0015098992931531986\n",
      "epochs 2292\n",
      "training loss 0.0014529699685425698\n",
      "epochs 2293\n",
      "training loss 0.0015064047804841131\n",
      "epochs 2294\n",
      "training loss 0.0014736342182281033\n",
      "epochs 2295\n",
      "training loss 0.0014593856823899554\n",
      "epochs 2296\n",
      "training loss 0.0015145237545205398\n",
      "epochs 2297\n",
      "training loss 0.0014868209224851027\n",
      "epochs 2298\n",
      "training loss 0.0014287729809162601\n",
      "epochs 2299\n",
      "training loss 0.001529375814456255\n",
      "testing loss 0.0028415130045106435\n",
      "epochs 2300\n",
      "training loss 0.001526729326899019\n",
      "epochs 2301\n",
      "training loss 0.0014217021113606934\n",
      "epochs 2302\n",
      "training loss 0.001495093399323353\n",
      "epochs 2303\n",
      "training loss 0.0014725341450983495\n",
      "epochs 2304\n",
      "training loss 0.0014770612531797365\n",
      "epochs 2305\n",
      "training loss 0.0015096198142885505\n",
      "epochs 2306\n",
      "training loss 0.0014998047275746123\n",
      "epochs 2307\n",
      "training loss 0.001460507380284969\n",
      "epochs 2308\n",
      "training loss 0.001476110577000175\n",
      "epochs 2309\n",
      "training loss 0.0015036613515176241\n",
      "testing loss 0.003274542388526049\n",
      "epochs 2310\n",
      "training loss 0.0014362769389997794\n",
      "epochs 2311\n",
      "training loss 0.0014892560872178536\n",
      "epochs 2312\n",
      "training loss 0.0014785743855636306\n",
      "epochs 2313\n",
      "training loss 0.001416153525176393\n",
      "epochs 2314\n",
      "training loss 0.0015207905162021088\n",
      "epochs 2315\n",
      "training loss 0.0014501923281560712\n",
      "epochs 2316\n",
      "training loss 0.0014890343199220788\n",
      "epochs 2317\n",
      "training loss 0.0014258459890487754\n",
      "epochs 2318\n",
      "training loss 0.0014272565720956716\n",
      "epochs 2319\n",
      "training loss 0.0015020831463303656\n",
      "testing loss 0.002947943734668909\n",
      "epochs 2320\n",
      "training loss 0.0014191049042845706\n",
      "epochs 2321\n",
      "training loss 0.0015132729966182115\n",
      "epochs 2322\n",
      "training loss 0.0014801229887053166\n",
      "epochs 2323\n",
      "training loss 0.0014573240795634167\n",
      "epochs 2324\n",
      "training loss 0.0014090229531376422\n",
      "epochs 2325\n",
      "training loss 0.0014458664975661253\n",
      "epochs 2326\n",
      "training loss 0.0015163864032082305\n",
      "epochs 2327\n",
      "training loss 0.0014969739713706076\n",
      "epochs 2328\n",
      "training loss 0.001484265036125155\n",
      "epochs 2329\n",
      "training loss 0.0014989383741264941\n",
      "testing loss 0.0028080429843831676\n",
      "epochs 2330\n",
      "training loss 0.0014069771359102817\n",
      "epochs 2331\n",
      "training loss 0.0015057731937478357\n",
      "epochs 2332\n",
      "training loss 0.0014935054012721455\n",
      "epochs 2333\n",
      "training loss 0.0014256041241869173\n",
      "epochs 2334\n",
      "training loss 0.0014099072787760669\n",
      "epochs 2335\n",
      "training loss 0.0014237739976586089\n",
      "epochs 2336\n",
      "training loss 0.0014512938982311715\n",
      "epochs 2337\n",
      "training loss 0.0015010386969013802\n",
      "epochs 2338\n",
      "training loss 0.0015033359737008085\n",
      "epochs 2339\n",
      "training loss 0.0015052463719550531\n",
      "testing loss 0.0038005294375689632\n",
      "epochs 2340\n",
      "training loss 0.001431073962129075\n",
      "epochs 2341\n",
      "training loss 0.0014962330623317544\n",
      "epochs 2342\n",
      "training loss 0.001488347928461514\n",
      "epochs 2343\n",
      "training loss 0.0014028281193172792\n",
      "epochs 2344\n",
      "training loss 0.0015132655805375958\n",
      "epochs 2345\n",
      "training loss 0.0014555144281548343\n",
      "epochs 2346\n",
      "training loss 0.0015035547175597523\n",
      "epochs 2347\n",
      "training loss 0.0015079450687764376\n",
      "epochs 2348\n",
      "training loss 0.001450520849717345\n",
      "epochs 2349\n",
      "training loss 0.0014243406326068413\n",
      "testing loss 0.002887747102294857\n",
      "epochs 2350\n",
      "training loss 0.0014420951071540658\n",
      "epochs 2351\n",
      "training loss 0.001520966603930213\n",
      "epochs 2352\n",
      "training loss 0.0014851004446822709\n",
      "epochs 2353\n",
      "training loss 0.0014599846659361251\n",
      "epochs 2354\n",
      "training loss 0.0014420675216766364\n",
      "epochs 2355\n",
      "training loss 0.001497906987805848\n",
      "epochs 2356\n",
      "training loss 0.001473207820042696\n",
      "epochs 2357\n",
      "training loss 0.0014140978179543668\n",
      "epochs 2358\n",
      "training loss 0.0015283541341303737\n",
      "epochs 2359\n",
      "training loss 0.0015688729932458114\n",
      "testing loss 0.003106412071988964\n",
      "epochs 2360\n",
      "training loss 0.0014559798401576611\n",
      "epochs 2361\n",
      "training loss 0.0014304393505405828\n",
      "epochs 2362\n",
      "training loss 0.0015051145657492195\n",
      "epochs 2363\n",
      "training loss 0.0014270139059496302\n",
      "epochs 2364\n",
      "training loss 0.0014097658334523552\n",
      "epochs 2365\n",
      "training loss 0.0014990922576665743\n",
      "epochs 2366\n",
      "training loss 0.0014418729730969505\n",
      "epochs 2367\n",
      "training loss 0.0014463774258367105\n",
      "epochs 2368\n",
      "training loss 0.0014838684261082968\n",
      "epochs 2369\n",
      "training loss 0.0014226713276533326\n",
      "testing loss 0.002970530643253356\n",
      "epochs 2370\n",
      "training loss 0.0014620860322027561\n",
      "epochs 2371\n",
      "training loss 0.0014430846943587947\n",
      "epochs 2372\n",
      "training loss 0.001451835159593089\n",
      "epochs 2373\n",
      "training loss 0.0014754557797379542\n",
      "epochs 2374\n",
      "training loss 0.0014435791105844398\n",
      "epochs 2375\n",
      "training loss 0.001467056184799034\n",
      "epochs 2376\n",
      "training loss 0.0014217696732934848\n",
      "epochs 2377\n",
      "training loss 0.0014432588150571908\n",
      "epochs 2378\n",
      "training loss 0.0014387756523444094\n",
      "epochs 2379\n",
      "training loss 0.0015015053464804626\n",
      "testing loss 0.002999410805199956\n",
      "epochs 2380\n",
      "training loss 0.0014262996850348442\n",
      "epochs 2381\n",
      "training loss 0.00150410892756084\n",
      "epochs 2382\n",
      "training loss 0.0014203337002064364\n",
      "epochs 2383\n",
      "training loss 0.0014261148599652987\n",
      "epochs 2384\n",
      "training loss 0.001416745744780057\n",
      "epochs 2385\n",
      "training loss 0.001440618723088813\n",
      "epochs 2386\n",
      "training loss 0.0014569992983308945\n",
      "epochs 2387\n",
      "training loss 0.0014438665334678801\n",
      "epochs 2388\n",
      "training loss 0.0014791221764451034\n",
      "epochs 2389\n",
      "training loss 0.0014478579389305055\n",
      "testing loss 0.0027509538612843667\n",
      "epochs 2390\n",
      "training loss 0.0014849188518186742\n",
      "epochs 2391\n",
      "training loss 0.0014641104629801128\n",
      "epochs 2392\n",
      "training loss 0.0014107894903617856\n",
      "epochs 2393\n",
      "training loss 0.0014241924578406052\n",
      "epochs 2394\n",
      "training loss 0.0014427661431152205\n",
      "epochs 2395\n",
      "training loss 0.0014532792794526665\n",
      "epochs 2396\n",
      "training loss 0.0014683729064888617\n",
      "epochs 2397\n",
      "training loss 0.0014422777673940097\n",
      "epochs 2398\n",
      "training loss 0.0014680848974595838\n",
      "epochs 2399\n",
      "training loss 0.0014939607090265759\n",
      "testing loss 0.00299011638630776\n",
      "epochs 2400\n",
      "training loss 0.0014404400855466996\n",
      "epochs 2401\n",
      "training loss 0.0014651137486236074\n",
      "epochs 2402\n",
      "training loss 0.0014559819032565484\n",
      "epochs 2403\n",
      "training loss 0.001432422615055527\n",
      "epochs 2404\n",
      "training loss 0.0014231785807028233\n",
      "epochs 2405\n",
      "training loss 0.0014097704081498879\n",
      "epochs 2406\n",
      "training loss 0.0014714326111155372\n",
      "epochs 2407\n",
      "training loss 0.0014646640098738657\n",
      "epochs 2408\n",
      "training loss 0.0014318701426150407\n",
      "epochs 2409\n",
      "training loss 0.001443572960387757\n",
      "testing loss 0.0035007846449231004\n",
      "epochs 2410\n",
      "training loss 0.0014141342490501022\n",
      "epochs 2411\n",
      "training loss 0.0014026244857424932\n",
      "epochs 2412\n",
      "training loss 0.0014478526330102904\n",
      "epochs 2413\n",
      "training loss 0.0014738118931322747\n",
      "epochs 2414\n",
      "training loss 0.0014758862814749944\n",
      "epochs 2415\n",
      "training loss 0.0014639219329664116\n",
      "epochs 2416\n",
      "training loss 0.0014846411561607567\n",
      "epochs 2417\n",
      "training loss 0.001445970491833113\n",
      "epochs 2418\n",
      "training loss 0.0013889344541800398\n",
      "epochs 2419\n",
      "training loss 0.0014973708887026846\n",
      "testing loss 0.002939885331925454\n",
      "epochs 2420\n",
      "training loss 0.001506555739487309\n",
      "epochs 2421\n",
      "training loss 0.0014296016285385608\n",
      "epochs 2422\n",
      "training loss 0.0014194816180736981\n",
      "epochs 2423\n",
      "training loss 0.001406770325860286\n",
      "epochs 2424\n",
      "training loss 0.0014018576541253137\n",
      "epochs 2425\n",
      "training loss 0.0014839582744982987\n",
      "epochs 2426\n",
      "training loss 0.0015001072775498937\n",
      "epochs 2427\n",
      "training loss 0.0014976876593933732\n",
      "epochs 2428\n",
      "training loss 0.001463177288729573\n",
      "epochs 2429\n",
      "training loss 0.0014263130724430084\n",
      "testing loss 0.003936712090949799\n",
      "epochs 2430\n",
      "training loss 0.0014300711939853013\n",
      "epochs 2431\n",
      "training loss 0.001441771539259351\n",
      "epochs 2432\n",
      "training loss 0.001430345999429393\n",
      "epochs 2433\n",
      "training loss 0.0014807131811254586\n",
      "epochs 2434\n",
      "training loss 0.0014295652887350836\n",
      "epochs 2435\n",
      "training loss 0.0014367625600301681\n",
      "epochs 2436\n",
      "training loss 0.0013712115904619198\n",
      "epochs 2437\n",
      "training loss 0.0014181120273739235\n",
      "epochs 2438\n",
      "training loss 0.0014768798273314344\n",
      "epochs 2439\n",
      "training loss 0.0013992526760610742\n",
      "testing loss 0.002827263050811722\n",
      "epochs 2440\n",
      "training loss 0.0014031472899682118\n",
      "epochs 2441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0014638206795902532\n",
      "epochs 2442\n",
      "training loss 0.0014293347941824776\n",
      "epochs 2443\n",
      "training loss 0.0015174630464275445\n",
      "epochs 2444\n",
      "training loss 0.0014282316385236169\n",
      "epochs 2445\n",
      "training loss 0.001385176083353266\n",
      "epochs 2446\n",
      "training loss 0.0014422521399232164\n",
      "epochs 2447\n",
      "training loss 0.0014500696474607003\n",
      "epochs 2448\n",
      "training loss 0.001418541810095967\n",
      "epochs 2449\n",
      "training loss 0.0014095251966129117\n",
      "testing loss 0.0030031565210207347\n",
      "epochs 2450\n",
      "training loss 0.0013923211482149693\n",
      "epochs 2451\n",
      "training loss 0.0014266310115330669\n",
      "epochs 2452\n",
      "training loss 0.0014646995655546887\n",
      "epochs 2453\n",
      "training loss 0.0014225816827247593\n",
      "epochs 2454\n",
      "training loss 0.0014790268699811223\n",
      "epochs 2455\n",
      "training loss 0.0014755192193652766\n",
      "epochs 2456\n",
      "training loss 0.001402426564833857\n",
      "epochs 2457\n",
      "training loss 0.0014894665312658894\n",
      "epochs 2458\n",
      "training loss 0.0014105510309134755\n",
      "epochs 2459\n",
      "training loss 0.0014558544498868287\n",
      "testing loss 0.0034304921769583267\n",
      "epochs 2460\n",
      "training loss 0.001502229287720775\n",
      "epochs 2461\n",
      "training loss 0.0014179616764550152\n",
      "epochs 2462\n",
      "training loss 0.0014475110103022363\n",
      "epochs 2463\n",
      "training loss 0.001409064288007656\n",
      "epochs 2464\n",
      "training loss 0.001530403893024839\n",
      "epochs 2465\n",
      "training loss 0.0014191185531849013\n",
      "epochs 2466\n",
      "training loss 0.0014325805138626722\n",
      "epochs 2467\n",
      "training loss 0.0013973983668150397\n",
      "epochs 2468\n",
      "training loss 0.001419005297530601\n",
      "epochs 2469\n",
      "training loss 0.0013921116963510139\n",
      "testing loss 0.003531106369543477\n",
      "epochs 2470\n",
      "training loss 0.0014026601615540208\n",
      "epochs 2471\n",
      "training loss 0.00139420409067633\n",
      "epochs 2472\n",
      "training loss 0.0014681975239757617\n",
      "epochs 2473\n",
      "training loss 0.001374740160274175\n",
      "epochs 2474\n",
      "training loss 0.0014040953363645766\n",
      "epochs 2475\n",
      "training loss 0.001452353657690361\n",
      "epochs 2476\n",
      "training loss 0.0014904678606719776\n",
      "epochs 2477\n",
      "training loss 0.0014592315094038777\n",
      "epochs 2478\n",
      "training loss 0.0013596085476969286\n",
      "epochs 2479\n",
      "training loss 0.0014277469659773321\n",
      "testing loss 0.003067382497808744\n",
      "epochs 2480\n",
      "training loss 0.0013438152378611834\n",
      "epochs 2481\n",
      "training loss 0.0014174695819896563\n",
      "epochs 2482\n",
      "training loss 0.0014151470933115183\n",
      "epochs 2483\n",
      "training loss 0.001475940930153212\n",
      "epochs 2484\n",
      "training loss 0.0014211222080734635\n",
      "epochs 2485\n",
      "training loss 0.001361528989868833\n",
      "epochs 2486\n",
      "training loss 0.0013554976442064466\n",
      "epochs 2487\n",
      "training loss 0.0014454198884070032\n",
      "epochs 2488\n",
      "training loss 0.0014431283761121582\n",
      "epochs 2489\n",
      "training loss 0.0014249834512985993\n",
      "testing loss 0.0028960406970140254\n",
      "epochs 2490\n",
      "training loss 0.0013850465223000676\n",
      "epochs 2491\n",
      "training loss 0.0014116519918427506\n",
      "epochs 2492\n",
      "training loss 0.0014234266824536253\n",
      "epochs 2493\n",
      "training loss 0.0013880808308712096\n",
      "epochs 2494\n",
      "training loss 0.001341056374487634\n",
      "epochs 2495\n",
      "training loss 0.0013835808029752168\n",
      "epochs 2496\n",
      "training loss 0.0014002527164063193\n",
      "epochs 2497\n",
      "training loss 0.0014222051363047721\n",
      "epochs 2498\n",
      "training loss 0.0013760677416735302\n",
      "epochs 2499\n",
      "training loss 0.0014451517518977762\n",
      "testing loss 0.0033105302796903866\n",
      "epochs 2500\n",
      "training loss 0.0014046173194907378\n",
      "epochs 2501\n",
      "training loss 0.0014317125965923206\n",
      "epochs 2502\n",
      "training loss 0.0014180838408891258\n",
      "epochs 2503\n",
      "training loss 0.0013836880374096876\n",
      "epochs 2504\n",
      "training loss 0.0014282900818459417\n",
      "epochs 2505\n",
      "training loss 0.001398208523862698\n",
      "epochs 2506\n",
      "training loss 0.0013480717098654606\n",
      "epochs 2507\n",
      "training loss 0.0014349964037212036\n",
      "epochs 2508\n",
      "training loss 0.001351850093158364\n",
      "epochs 2509\n",
      "training loss 0.0014393013195378144\n",
      "testing loss 0.0034497577593007936\n",
      "epochs 2510\n",
      "training loss 0.0013941477598144042\n",
      "epochs 2511\n",
      "training loss 0.0014163405141909254\n",
      "epochs 2512\n",
      "training loss 0.0013877539575244315\n",
      "epochs 2513\n",
      "training loss 0.0014777169762799193\n",
      "epochs 2514\n",
      "training loss 0.001360280857149704\n",
      "epochs 2515\n",
      "training loss 0.0013722565897675316\n",
      "epochs 2516\n",
      "training loss 0.001403755535963232\n",
      "epochs 2517\n",
      "training loss 0.0013972699888458186\n",
      "epochs 2518\n",
      "training loss 0.0013395844878045713\n",
      "epochs 2519\n",
      "training loss 0.001428033199938337\n",
      "testing loss 0.0029618749151448884\n",
      "epochs 2520\n",
      "training loss 0.0014434463251099035\n",
      "epochs 2521\n",
      "training loss 0.0014863196855246748\n",
      "epochs 2522\n",
      "training loss 0.0014765842283062502\n",
      "epochs 2523\n",
      "training loss 0.0013850319040387212\n",
      "epochs 2524\n",
      "training loss 0.0013862590913768394\n",
      "epochs 2525\n",
      "training loss 0.0013890585295592737\n",
      "epochs 2526\n",
      "training loss 0.0014454637189525942\n",
      "epochs 2527\n",
      "training loss 0.0014057262258541892\n",
      "epochs 2528\n",
      "training loss 0.00137138209343919\n",
      "epochs 2529\n",
      "training loss 0.0014109796576207577\n",
      "testing loss 0.0030181264514697994\n",
      "epochs 2530\n",
      "training loss 0.0013156259447650411\n",
      "epochs 2531\n",
      "training loss 0.0014484337995276082\n",
      "epochs 2532\n",
      "training loss 0.0015200952003925111\n",
      "epochs 2533\n",
      "training loss 0.0013853036616306106\n",
      "epochs 2534\n",
      "training loss 0.0014676855044442267\n",
      "epochs 2535\n",
      "training loss 0.0014072567540133162\n",
      "epochs 2536\n",
      "training loss 0.0014276396018583159\n",
      "epochs 2537\n",
      "training loss 0.0013846162891321134\n",
      "epochs 2538\n",
      "training loss 0.0014087124453435757\n",
      "epochs 2539\n",
      "training loss 0.001396247194612846\n",
      "testing loss 0.002910473392632334\n",
      "epochs 2540\n",
      "training loss 0.0014063344055030914\n",
      "epochs 2541\n",
      "training loss 0.0013844544249501838\n",
      "epochs 2542\n",
      "training loss 0.0014658684419088723\n",
      "epochs 2543\n",
      "training loss 0.0013587024775163856\n",
      "epochs 2544\n",
      "training loss 0.0013791998642380702\n",
      "epochs 2545\n",
      "training loss 0.001389872712312453\n",
      "epochs 2546\n",
      "training loss 0.0013920638938844884\n",
      "epochs 2547\n",
      "training loss 0.001361691536973471\n",
      "epochs 2548\n",
      "training loss 0.0013505553311489997\n",
      "epochs 2549\n",
      "training loss 0.0013939784344364989\n",
      "testing loss 0.002986774918976643\n",
      "epochs 2550\n",
      "training loss 0.0013544163063358187\n",
      "epochs 2551\n",
      "training loss 0.0013867461290295409\n",
      "epochs 2552\n",
      "training loss 0.0013755799708519935\n",
      "epochs 2553\n",
      "training loss 0.0013577317444630262\n",
      "epochs 2554\n",
      "training loss 0.0013817326037643483\n",
      "epochs 2555\n",
      "training loss 0.0014531774192306816\n",
      "epochs 2556\n",
      "training loss 0.001458225612907945\n",
      "epochs 2557\n",
      "training loss 0.0013470892692876465\n",
      "epochs 2558\n",
      "training loss 0.0013825114804750314\n",
      "epochs 2559\n",
      "training loss 0.0013509263271412114\n",
      "testing loss 0.002912317256522464\n",
      "epochs 2560\n",
      "training loss 0.0013699085806570042\n",
      "epochs 2561\n",
      "training loss 0.0014226395982858302\n",
      "epochs 2562\n",
      "training loss 0.0013700350623657095\n",
      "epochs 2563\n",
      "training loss 0.0014343723386001\n",
      "epochs 2564\n",
      "training loss 0.0014726233102117967\n",
      "epochs 2565\n",
      "training loss 0.0013954644884042284\n",
      "epochs 2566\n",
      "training loss 0.0014382843025478797\n",
      "epochs 2567\n",
      "training loss 0.0014026980078566853\n",
      "epochs 2568\n",
      "training loss 0.0013838148312105324\n",
      "epochs 2569\n",
      "training loss 0.0014305006340075295\n",
      "testing loss 0.003264918047373335\n",
      "epochs 2570\n",
      "training loss 0.0014528897826242796\n",
      "epochs 2571\n",
      "training loss 0.0014263725566032333\n",
      "epochs 2572\n",
      "training loss 0.0013145743235860778\n",
      "epochs 2573\n",
      "training loss 0.0014153858807750364\n",
      "epochs 2574\n",
      "training loss 0.0014053569240764486\n",
      "epochs 2575\n",
      "training loss 0.0013855750535226973\n",
      "epochs 2576\n",
      "training loss 0.0013864850605934246\n",
      "epochs 2577\n",
      "training loss 0.0013789695494392134\n",
      "epochs 2578\n",
      "training loss 0.0013965270979157266\n",
      "epochs 2579\n",
      "training loss 0.0014506336634180375\n",
      "testing loss 0.0028437582226073805\n",
      "epochs 2580\n",
      "training loss 0.0013665238205671853\n",
      "epochs 2581\n",
      "training loss 0.0014090052935702437\n",
      "epochs 2582\n",
      "training loss 0.0014852379881465485\n",
      "epochs 2583\n",
      "training loss 0.0014879482822693704\n",
      "epochs 2584\n",
      "training loss 0.0014122320860298775\n",
      "epochs 2585\n",
      "training loss 0.001354029828344265\n",
      "epochs 2586\n",
      "training loss 0.0013285092788318166\n",
      "epochs 2587\n",
      "training loss 0.0013898524277387782\n",
      "epochs 2588\n",
      "training loss 0.0013903688064756546\n",
      "epochs 2589\n",
      "training loss 0.0013804549663072865\n",
      "testing loss 0.0029996547786888816\n",
      "epochs 2590\n",
      "training loss 0.0013977182902732586\n",
      "epochs 2591\n",
      "training loss 0.0013804270591838766\n",
      "epochs 2592\n",
      "training loss 0.0013601203600516526\n",
      "epochs 2593\n",
      "training loss 0.0013189080306102371\n",
      "epochs 2594\n",
      "training loss 0.001350290148368949\n",
      "epochs 2595\n",
      "training loss 0.0013624494214185534\n",
      "epochs 2596\n",
      "training loss 0.0013865598264761767\n",
      "epochs 2597\n",
      "training loss 0.0013854502621361573\n",
      "epochs 2598\n",
      "training loss 0.0013998062192394447\n",
      "epochs 2599\n",
      "training loss 0.0013679655786818829\n",
      "testing loss 0.003611304571006632\n",
      "epochs 2600\n",
      "training loss 0.0014012861092104988\n",
      "epochs 2601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0014108229624206557\n",
      "epochs 2602\n",
      "training loss 0.0013431283347393559\n",
      "epochs 2603\n",
      "training loss 0.0013198103303851214\n",
      "epochs 2604\n",
      "training loss 0.0013760768641763025\n",
      "epochs 2605\n",
      "training loss 0.0013968467531587057\n",
      "epochs 2606\n",
      "training loss 0.001355755346152071\n",
      "epochs 2607\n",
      "training loss 0.0013572162568872075\n",
      "epochs 2608\n",
      "training loss 0.0013984070804556068\n",
      "epochs 2609\n",
      "training loss 0.0013615700308697745\n",
      "testing loss 0.0028435107318895824\n",
      "epochs 2610\n",
      "training loss 0.001338426872775083\n",
      "epochs 2611\n",
      "training loss 0.0013792020801984136\n",
      "epochs 2612\n",
      "training loss 0.0013395214538616453\n",
      "epochs 2613\n",
      "training loss 0.001389403439142478\n",
      "epochs 2614\n",
      "training loss 0.0013990325960797742\n",
      "epochs 2615\n",
      "training loss 0.0013589486134759402\n",
      "epochs 2616\n",
      "training loss 0.0013940681062573651\n",
      "epochs 2617\n",
      "training loss 0.001413043397853836\n",
      "epochs 2618\n",
      "training loss 0.0013662145022263236\n",
      "epochs 2619\n",
      "training loss 0.0014013210394685821\n",
      "testing loss 0.0028727940838546196\n",
      "epochs 2620\n",
      "training loss 0.0013524654442033982\n",
      "epochs 2621\n",
      "training loss 0.0013826666102605932\n",
      "epochs 2622\n",
      "training loss 0.001339047181378982\n",
      "epochs 2623\n",
      "training loss 0.0013886854538374147\n",
      "epochs 2624\n",
      "training loss 0.0014229103035398258\n",
      "epochs 2625\n",
      "training loss 0.0014128025340222072\n",
      "epochs 2626\n",
      "training loss 0.0014014023704324086\n",
      "epochs 2627\n",
      "training loss 0.0013453413896534398\n",
      "epochs 2628\n",
      "training loss 0.0013639194210410955\n",
      "epochs 2629\n",
      "training loss 0.0013657521706295277\n",
      "testing loss 0.0031252322877024083\n",
      "epochs 2630\n",
      "training loss 0.0014311650201415014\n",
      "epochs 2631\n",
      "training loss 0.0013759048343537019\n",
      "epochs 2632\n",
      "training loss 0.0013565569617120282\n",
      "epochs 2633\n",
      "training loss 0.0013682770869754097\n",
      "epochs 2634\n",
      "training loss 0.0014095937187067237\n",
      "epochs 2635\n",
      "training loss 0.0013846440573709827\n",
      "epochs 2636\n",
      "training loss 0.0014082642311391655\n",
      "epochs 2637\n",
      "training loss 0.0014213804284034235\n",
      "epochs 2638\n",
      "training loss 0.0013490242953978389\n",
      "epochs 2639\n",
      "training loss 0.0014227274095846233\n",
      "testing loss 0.0029342742356082015\n",
      "epochs 2640\n",
      "training loss 0.0013626493042261313\n",
      "epochs 2641\n",
      "training loss 0.0013970246133012385\n",
      "epochs 2642\n",
      "training loss 0.0013709228930253207\n",
      "epochs 2643\n",
      "training loss 0.0013946257374181461\n",
      "epochs 2644\n",
      "training loss 0.0013564191164171441\n",
      "epochs 2645\n",
      "training loss 0.0013819704581360618\n",
      "epochs 2646\n",
      "training loss 0.0013569378986527244\n",
      "epochs 2647\n",
      "training loss 0.00138078724942807\n",
      "epochs 2648\n",
      "training loss 0.0013899617684413439\n",
      "epochs 2649\n",
      "training loss 0.001355014797859177\n",
      "testing loss 0.0028503752996359047\n",
      "epochs 2650\n",
      "training loss 0.0013624361336180277\n",
      "epochs 2651\n",
      "training loss 0.0013572817401518893\n",
      "epochs 2652\n",
      "training loss 0.0013650520878071165\n",
      "epochs 2653\n",
      "training loss 0.0013527225410724258\n",
      "epochs 2654\n",
      "training loss 0.0013944351734978494\n",
      "epochs 2655\n",
      "training loss 0.0013306919114853858\n",
      "epochs 2656\n",
      "training loss 0.0013756024176014253\n",
      "epochs 2657\n",
      "training loss 0.0014224257189242627\n",
      "epochs 2658\n",
      "training loss 0.0013543318705505898\n",
      "epochs 2659\n",
      "training loss 0.0013747652916529282\n",
      "testing loss 0.0030808223152189707\n",
      "epochs 2660\n",
      "training loss 0.0012861269447983648\n",
      "epochs 2661\n",
      "training loss 0.0013407044401774495\n",
      "epochs 2662\n",
      "training loss 0.0014172697699512708\n",
      "epochs 2663\n",
      "training loss 0.0013408451601169369\n",
      "epochs 2664\n",
      "training loss 0.0014051200947964898\n",
      "epochs 2665\n",
      "training loss 0.001356754955866779\n",
      "epochs 2666\n",
      "training loss 0.0013366085105635068\n",
      "epochs 2667\n",
      "training loss 0.0013212351139782047\n",
      "epochs 2668\n",
      "training loss 0.0012996497510587032\n",
      "epochs 2669\n",
      "training loss 0.0013923898689932894\n",
      "testing loss 0.0035146999301666274\n",
      "epochs 2670\n",
      "training loss 0.001318381406086579\n",
      "epochs 2671\n",
      "training loss 0.0013578634025843241\n",
      "epochs 2672\n",
      "training loss 0.0013564862961962954\n",
      "epochs 2673\n",
      "training loss 0.0013867458905373617\n",
      "epochs 2674\n",
      "training loss 0.0013888176533430362\n",
      "epochs 2675\n",
      "training loss 0.0013908988146513338\n",
      "epochs 2676\n",
      "training loss 0.0014621151869870315\n",
      "epochs 2677\n",
      "training loss 0.001381202720496294\n",
      "epochs 2678\n",
      "training loss 0.001341021699883259\n",
      "epochs 2679\n",
      "training loss 0.0013200678490385368\n",
      "testing loss 0.0047247903223367445\n",
      "epochs 2680\n",
      "training loss 0.0013561348468320031\n",
      "epochs 2681\n",
      "training loss 0.00136955026314257\n",
      "epochs 2682\n",
      "training loss 0.001346733277956573\n",
      "epochs 2683\n",
      "training loss 0.0013617961004016904\n",
      "epochs 2684\n",
      "training loss 0.0014128299397458874\n",
      "epochs 2685\n",
      "training loss 0.0013246275691561808\n",
      "epochs 2686\n",
      "training loss 0.0013534803713790755\n",
      "epochs 2687\n",
      "training loss 0.0013231023191403609\n",
      "epochs 2688\n",
      "training loss 0.00136812271212021\n",
      "epochs 2689\n",
      "training loss 0.0013112936860324084\n",
      "testing loss 0.003130327692215747\n",
      "epochs 2690\n",
      "training loss 0.0013414037954925608\n",
      "epochs 2691\n",
      "training loss 0.0013314838764274715\n",
      "epochs 2692\n",
      "training loss 0.0013150513748210887\n",
      "epochs 2693\n",
      "training loss 0.0013568385146447886\n",
      "epochs 2694\n",
      "training loss 0.001291926043486013\n",
      "epochs 2695\n",
      "training loss 0.0013180083860954593\n",
      "epochs 2696\n",
      "training loss 0.0014026585731394929\n",
      "epochs 2697\n",
      "training loss 0.0013200107427231832\n",
      "epochs 2698\n",
      "training loss 0.0012875420241659128\n",
      "epochs 2699\n",
      "training loss 0.0013462978439606546\n",
      "testing loss 0.0032634716398406323\n",
      "epochs 2700\n",
      "training loss 0.0013254521091576202\n",
      "epochs 2701\n",
      "training loss 0.001338686988480944\n",
      "epochs 2702\n",
      "training loss 0.001327349136103692\n",
      "epochs 2703\n",
      "training loss 0.0013828322998712286\n",
      "epochs 2704\n",
      "training loss 0.001390794535007577\n",
      "epochs 2705\n",
      "training loss 0.0013677871598165743\n",
      "epochs 2706\n",
      "training loss 0.001332408063344397\n",
      "epochs 2707\n",
      "training loss 0.0013661988542732248\n",
      "epochs 2708\n",
      "training loss 0.0013491418159575312\n",
      "epochs 2709\n",
      "training loss 0.0013892288642827885\n",
      "testing loss 0.0030031685745475667\n",
      "epochs 2710\n",
      "training loss 0.0013216361258388425\n",
      "epochs 2711\n",
      "training loss 0.0013809567471289594\n",
      "epochs 2712\n",
      "training loss 0.0013508525523810017\n",
      "epochs 2713\n",
      "training loss 0.0013174370625112942\n",
      "epochs 2714\n",
      "training loss 0.0013412347531914259\n",
      "epochs 2715\n",
      "training loss 0.0013310030227637422\n",
      "epochs 2716\n",
      "training loss 0.0013443729307680847\n",
      "epochs 2717\n",
      "training loss 0.001245154572989067\n",
      "epochs 2718\n",
      "training loss 0.001368651297481272\n",
      "epochs 2719\n",
      "training loss 0.0013410720944483864\n",
      "testing loss 0.003129480902851949\n",
      "epochs 2720\n",
      "training loss 0.001375413075511239\n",
      "epochs 2721\n",
      "training loss 0.0013118435858358206\n",
      "epochs 2722\n",
      "training loss 0.00132613053448402\n",
      "epochs 2723\n",
      "training loss 0.0013382092437078457\n",
      "epochs 2724\n",
      "training loss 0.0013629260745768557\n",
      "epochs 2725\n",
      "training loss 0.0013277760411734413\n",
      "epochs 2726\n",
      "training loss 0.0013736384106222484\n",
      "epochs 2727\n",
      "training loss 0.0013531507580260829\n",
      "epochs 2728\n",
      "training loss 0.0013289115744456082\n",
      "epochs 2729\n",
      "training loss 0.0013910193991632197\n",
      "testing loss 0.0029608123603025913\n",
      "epochs 2730\n",
      "training loss 0.0013433090160957517\n",
      "epochs 2731\n",
      "training loss 0.0013699794032803446\n",
      "epochs 2732\n",
      "training loss 0.001346639491261003\n",
      "epochs 2733\n",
      "training loss 0.0013568591226336736\n",
      "epochs 2734\n",
      "training loss 0.0013482732338106153\n",
      "epochs 2735\n",
      "training loss 0.0013502424214485364\n",
      "epochs 2736\n",
      "training loss 0.0013591543216496071\n",
      "epochs 2737\n",
      "training loss 0.0012563812174186065\n",
      "epochs 2738\n",
      "training loss 0.0013286646947018395\n",
      "epochs 2739\n",
      "training loss 0.0013570574274651796\n",
      "testing loss 0.002797582107084202\n",
      "epochs 2740\n",
      "training loss 0.0013473726852194017\n",
      "epochs 2741\n",
      "training loss 0.001283461999220315\n",
      "epochs 2742\n",
      "training loss 0.0014130311344373733\n",
      "epochs 2743\n",
      "training loss 0.0013609044598825956\n",
      "epochs 2744\n",
      "training loss 0.0013649210452008709\n",
      "epochs 2745\n",
      "training loss 0.0013174737052069156\n",
      "epochs 2746\n",
      "training loss 0.00133379178346732\n",
      "epochs 2747\n",
      "training loss 0.0013433299582921489\n",
      "epochs 2748\n",
      "training loss 0.0014108655391130765\n",
      "epochs 2749\n",
      "training loss 0.0013046861632577055\n",
      "testing loss 0.002780930117657738\n",
      "epochs 2750\n",
      "training loss 0.001323652488574397\n",
      "epochs 2751\n",
      "training loss 0.0013236355414758452\n",
      "epochs 2752\n",
      "training loss 0.0013322411418190414\n",
      "epochs 2753\n",
      "training loss 0.0013681773735368823\n",
      "epochs 2754\n",
      "training loss 0.0013442150101994127\n",
      "epochs 2755\n",
      "training loss 0.0013341362220815477\n",
      "epochs 2756\n",
      "training loss 0.0013438981857241377\n",
      "epochs 2757\n",
      "training loss 0.001372460162303889\n",
      "epochs 2758\n",
      "training loss 0.0013715015599747558\n",
      "epochs 2759\n",
      "training loss 0.0013331001467711226\n",
      "testing loss 0.00305014113971554\n",
      "epochs 2760\n",
      "training loss 0.0013313993440422945\n",
      "epochs 2761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0013186246355553937\n",
      "epochs 2762\n",
      "training loss 0.0013870532192023738\n",
      "epochs 2763\n",
      "training loss 0.0013188925723185746\n",
      "epochs 2764\n",
      "training loss 0.0013488687197663637\n",
      "epochs 2765\n",
      "training loss 0.0013129611778792028\n",
      "epochs 2766\n",
      "training loss 0.0013268693096547934\n",
      "epochs 2767\n",
      "training loss 0.0013259595867223743\n",
      "epochs 2768\n",
      "training loss 0.0013011068795343414\n",
      "epochs 2769\n",
      "training loss 0.0013088068972967315\n",
      "testing loss 0.0029623750588982133\n",
      "epochs 2770\n",
      "training loss 0.0013114569098674508\n",
      "epochs 2771\n",
      "training loss 0.001385861393460568\n",
      "epochs 2772\n",
      "training loss 0.0013416317747311389\n",
      "epochs 2773\n",
      "training loss 0.0013449032471435976\n",
      "epochs 2774\n",
      "training loss 0.001315060227693237\n",
      "epochs 2775\n",
      "training loss 0.001372764294324471\n",
      "epochs 2776\n",
      "training loss 0.0013588258876652093\n",
      "epochs 2777\n",
      "training loss 0.0012964983430045647\n",
      "epochs 2778\n",
      "training loss 0.0013731802475078195\n",
      "epochs 2779\n",
      "training loss 0.0013282067369949921\n",
      "testing loss 0.0030573087931009576\n",
      "epochs 2780\n",
      "training loss 0.0013531563401230722\n",
      "epochs 2781\n",
      "training loss 0.0013089358522117953\n",
      "epochs 2782\n",
      "training loss 0.0013027956596388846\n",
      "epochs 2783\n",
      "training loss 0.0013440943690721727\n",
      "epochs 2784\n",
      "training loss 0.0013821131833206216\n",
      "epochs 2785\n",
      "training loss 0.0013431975125097373\n",
      "epochs 2786\n",
      "training loss 0.0013216206096933652\n",
      "epochs 2787\n",
      "training loss 0.0013649422419894385\n",
      "epochs 2788\n",
      "training loss 0.001343783262039968\n",
      "epochs 2789\n",
      "training loss 0.0012790275048351306\n",
      "testing loss 0.0028211630526632863\n",
      "epochs 2790\n",
      "training loss 0.0013415120674033951\n",
      "epochs 2791\n",
      "training loss 0.0013507511921433625\n",
      "epochs 2792\n",
      "training loss 0.0013047124267666172\n",
      "epochs 2793\n",
      "training loss 0.0013288998768288404\n",
      "epochs 2794\n",
      "training loss 0.0014054378622712562\n",
      "epochs 2795\n",
      "training loss 0.0013398750770786114\n",
      "epochs 2796\n",
      "training loss 0.0013449445549443954\n",
      "epochs 2797\n",
      "training loss 0.0012787613181940826\n",
      "epochs 2798\n",
      "training loss 0.0013418361384670695\n",
      "epochs 2799\n",
      "training loss 0.0013072650388129463\n",
      "testing loss 0.003185133285430782\n",
      "epochs 2800\n",
      "training loss 0.0012910733799843576\n",
      "epochs 2801\n",
      "training loss 0.0013276600182667872\n",
      "epochs 2802\n",
      "training loss 0.0012991959151265559\n",
      "epochs 2803\n",
      "training loss 0.001329248905355977\n",
      "epochs 2804\n",
      "training loss 0.0013457913911338092\n",
      "epochs 2805\n",
      "training loss 0.001329473921842307\n",
      "epochs 2806\n",
      "training loss 0.0013857592411387406\n",
      "epochs 2807\n",
      "training loss 0.001304893971153317\n",
      "epochs 2808\n",
      "training loss 0.0013492386025591757\n",
      "epochs 2809\n",
      "training loss 0.0013032292164819694\n",
      "testing loss 0.002823710771019947\n",
      "epochs 2810\n",
      "training loss 0.001331267538721076\n",
      "epochs 2811\n",
      "training loss 0.0013219105114446992\n",
      "epochs 2812\n",
      "training loss 0.0013437992357082755\n",
      "epochs 2813\n",
      "training loss 0.0013207002310436163\n",
      "epochs 2814\n",
      "training loss 0.0013553629530944768\n",
      "epochs 2815\n",
      "training loss 0.0013437612307056965\n",
      "epochs 2816\n",
      "training loss 0.0013080944309271532\n",
      "epochs 2817\n",
      "training loss 0.001268534636794445\n",
      "epochs 2818\n",
      "training loss 0.0013256355646663659\n",
      "epochs 2819\n",
      "training loss 0.0012814604321998576\n",
      "testing loss 0.0026442094203183784\n",
      "epochs 2820\n",
      "training loss 0.0013252606353224298\n",
      "epochs 2821\n",
      "training loss 0.0013067982747646948\n",
      "epochs 2822\n",
      "training loss 0.0013148941325751278\n",
      "epochs 2823\n",
      "training loss 0.0013445798473257153\n",
      "epochs 2824\n",
      "training loss 0.0013484411843200314\n",
      "epochs 2825\n",
      "training loss 0.0013106660071709462\n",
      "epochs 2826\n",
      "training loss 0.0012589339313725729\n",
      "epochs 2827\n",
      "training loss 0.001323181941912801\n",
      "epochs 2828\n",
      "training loss 0.0013227162340947493\n",
      "epochs 2829\n",
      "training loss 0.0013031725533586844\n",
      "testing loss 0.0033084931585224387\n",
      "epochs 2830\n",
      "training loss 0.0013223987815428899\n",
      "epochs 2831\n",
      "training loss 0.0013522804737031617\n",
      "epochs 2832\n",
      "training loss 0.0013473008527196831\n",
      "epochs 2833\n",
      "training loss 0.0012901210992902767\n",
      "epochs 2834\n",
      "training loss 0.0013310977329972758\n",
      "epochs 2835\n",
      "training loss 0.0012743910296896534\n",
      "epochs 2836\n",
      "training loss 0.0012705457555328278\n",
      "epochs 2837\n",
      "training loss 0.0013087890471832821\n",
      "epochs 2838\n",
      "training loss 0.0012761663315013776\n",
      "epochs 2839\n",
      "training loss 0.0012916626587742232\n",
      "testing loss 0.0031032451170047493\n",
      "epochs 2840\n",
      "training loss 0.0012885856537338018\n",
      "epochs 2841\n",
      "training loss 0.0013197884405892354\n",
      "epochs 2842\n",
      "training loss 0.0013030757871031829\n",
      "epochs 2843\n",
      "training loss 0.0013121033392034154\n",
      "epochs 2844\n",
      "training loss 0.0013371612724499906\n",
      "epochs 2845\n",
      "training loss 0.0013298071106318328\n",
      "epochs 2846\n",
      "training loss 0.0013742058058085322\n",
      "epochs 2847\n",
      "training loss 0.0013351905153283006\n",
      "epochs 2848\n",
      "training loss 0.0012736253767333767\n",
      "epochs 2849\n",
      "training loss 0.0012677033558504878\n",
      "testing loss 0.0029189087923596354\n",
      "epochs 2850\n",
      "training loss 0.0013178428794231867\n",
      "epochs 2851\n",
      "training loss 0.0013646930288084666\n",
      "epochs 2852\n",
      "training loss 0.001299828395146967\n",
      "epochs 2853\n",
      "training loss 0.0012680376695163298\n",
      "epochs 2854\n",
      "training loss 0.0013576629007232338\n",
      "epochs 2855\n",
      "training loss 0.001274477630481407\n",
      "epochs 2856\n",
      "training loss 0.0013109393938696492\n",
      "epochs 2857\n",
      "training loss 0.0012488665735408506\n",
      "epochs 2858\n",
      "training loss 0.0012757322297355098\n",
      "epochs 2859\n",
      "training loss 0.0012754837550310628\n",
      "testing loss 0.002961682045505323\n",
      "epochs 2860\n",
      "training loss 0.00134106700862036\n",
      "epochs 2861\n",
      "training loss 0.0012566391721871714\n",
      "epochs 2862\n",
      "training loss 0.0012801906225231893\n",
      "epochs 2863\n",
      "training loss 0.0012595287209594662\n",
      "epochs 2864\n",
      "training loss 0.0012845049511731848\n",
      "epochs 2865\n",
      "training loss 0.0012838299823910745\n",
      "epochs 2866\n",
      "training loss 0.0012786746832561936\n",
      "epochs 2867\n",
      "training loss 0.001240572312706657\n",
      "epochs 2868\n",
      "training loss 0.0013154226806054406\n",
      "epochs 2869\n",
      "training loss 0.0012789594611410605\n",
      "testing loss 0.0031619047810462243\n",
      "epochs 2870\n",
      "training loss 0.0013581690092579572\n",
      "epochs 2871\n",
      "training loss 0.0012696103669142514\n",
      "epochs 2872\n",
      "training loss 0.0013131468359021453\n",
      "epochs 2873\n",
      "training loss 0.0013737546389364838\n",
      "epochs 2874\n",
      "training loss 0.001300312081524553\n",
      "epochs 2875\n",
      "training loss 0.001342709793703557\n",
      "epochs 2876\n",
      "training loss 0.0013254179368375895\n",
      "epochs 2877\n",
      "training loss 0.0013207802239389337\n",
      "epochs 2878\n",
      "training loss 0.0013606048487749597\n",
      "epochs 2879\n",
      "training loss 0.0012828559812164007\n",
      "testing loss 0.0027600742025294255\n",
      "epochs 2880\n",
      "training loss 0.0012934563304991165\n",
      "epochs 2881\n",
      "training loss 0.0012872044685633575\n",
      "epochs 2882\n",
      "training loss 0.0013029806899390887\n",
      "epochs 2883\n",
      "training loss 0.001292087704029591\n",
      "epochs 2884\n",
      "training loss 0.0012643089051582032\n",
      "epochs 2885\n",
      "training loss 0.0012957813889835608\n",
      "epochs 2886\n",
      "training loss 0.001308236842073421\n",
      "epochs 2887\n",
      "training loss 0.0013451283823452482\n",
      "epochs 2888\n",
      "training loss 0.0012959068057928403\n",
      "epochs 2889\n",
      "training loss 0.0013047956184294403\n",
      "testing loss 0.002718183969507483\n",
      "epochs 2890\n",
      "training loss 0.0012953879179342404\n",
      "epochs 2891\n",
      "training loss 0.0013027567742675498\n",
      "epochs 2892\n",
      "training loss 0.0012970876465636478\n",
      "epochs 2893\n",
      "training loss 0.0013323304321939595\n",
      "epochs 2894\n",
      "training loss 0.0012845574760679324\n",
      "epochs 2895\n",
      "training loss 0.0012705023322639403\n",
      "epochs 2896\n",
      "training loss 0.0012939286244520777\n",
      "epochs 2897\n",
      "training loss 0.0012688802374388151\n",
      "epochs 2898\n",
      "training loss 0.0013829488958314196\n",
      "epochs 2899\n",
      "training loss 0.0014662207350188633\n",
      "testing loss 0.002975998829855379\n",
      "epochs 2900\n",
      "training loss 0.0013246792113821737\n",
      "epochs 2901\n",
      "training loss 0.0012702297606494131\n",
      "epochs 2902\n",
      "training loss 0.001270645671725092\n",
      "epochs 2903\n",
      "training loss 0.001258280982247817\n",
      "epochs 2904\n",
      "training loss 0.001327871801798716\n",
      "epochs 2905\n",
      "training loss 0.0012494521180173455\n",
      "epochs 2906\n",
      "training loss 0.001301179534725924\n",
      "epochs 2907\n",
      "training loss 0.001327611802154331\n",
      "epochs 2908\n",
      "training loss 0.001286026412914125\n",
      "epochs 2909\n",
      "training loss 0.0012966147800879185\n",
      "testing loss 0.0030824220607202526\n",
      "epochs 2910\n",
      "training loss 0.00129927813247452\n",
      "epochs 2911\n",
      "training loss 0.0012896830050804083\n",
      "epochs 2912\n",
      "training loss 0.0012656871733994514\n",
      "epochs 2913\n",
      "training loss 0.0012830542264248622\n",
      "epochs 2914\n",
      "training loss 0.001278262479424669\n",
      "epochs 2915\n",
      "training loss 0.0012932211664182596\n",
      "epochs 2916\n",
      "training loss 0.0013201216364556805\n",
      "epochs 2917\n",
      "training loss 0.0012927464684358257\n",
      "epochs 2918\n",
      "training loss 0.0012734556072787877\n",
      "epochs 2919\n",
      "training loss 0.0013267869557222877\n",
      "testing loss 0.0031440178137140495\n",
      "epochs 2920\n",
      "training loss 0.0012528957881523144\n",
      "epochs 2921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0012705717872726337\n",
      "epochs 2922\n",
      "training loss 0.001303727547659349\n",
      "epochs 2923\n",
      "training loss 0.001271987018263177\n",
      "epochs 2924\n",
      "training loss 0.0012384799401428674\n",
      "epochs 2925\n",
      "training loss 0.0012742601270294865\n",
      "epochs 2926\n",
      "training loss 0.0013142548903814406\n",
      "epochs 2927\n",
      "training loss 0.0012482654971727326\n",
      "epochs 2928\n",
      "training loss 0.0013141051793929563\n",
      "epochs 2929\n",
      "training loss 0.0012762418809760146\n",
      "testing loss 0.0026204032457684636\n",
      "epochs 2930\n",
      "training loss 0.0012582830619774646\n",
      "epochs 2931\n",
      "training loss 0.0013017277525225514\n",
      "epochs 2932\n",
      "training loss 0.001308934774220069\n",
      "epochs 2933\n",
      "training loss 0.0012554568311094808\n",
      "epochs 2934\n",
      "training loss 0.0012668248076320237\n",
      "epochs 2935\n",
      "training loss 0.0012816966144724728\n",
      "epochs 2936\n",
      "training loss 0.0012882436676259438\n",
      "epochs 2937\n",
      "training loss 0.001285568862130135\n",
      "epochs 2938\n",
      "training loss 0.0013492102028821852\n",
      "epochs 2939\n",
      "training loss 0.0012856054850103825\n",
      "testing loss 0.0027824764392596294\n",
      "epochs 2940\n",
      "training loss 0.001336400007714424\n",
      "epochs 2941\n",
      "training loss 0.0012938358402021207\n",
      "epochs 2942\n",
      "training loss 0.0013553126036447095\n",
      "epochs 2943\n",
      "training loss 0.0012710282683802774\n",
      "epochs 2944\n",
      "training loss 0.0012624851879302842\n",
      "epochs 2945\n",
      "training loss 0.0012813142839094517\n",
      "epochs 2946\n",
      "training loss 0.0012787124065973862\n",
      "epochs 2947\n",
      "training loss 0.001326796336178767\n",
      "epochs 2948\n",
      "training loss 0.0012495988768689607\n",
      "epochs 2949\n",
      "training loss 0.001258092670026306\n",
      "testing loss 0.0029055836617926808\n",
      "epochs 2950\n",
      "training loss 0.0012361561123346825\n",
      "epochs 2951\n",
      "training loss 0.0012326205041723773\n",
      "epochs 2952\n",
      "training loss 0.0012407171130208667\n",
      "epochs 2953\n",
      "training loss 0.001285426073253302\n",
      "epochs 2954\n",
      "training loss 0.0012829076466193043\n",
      "epochs 2955\n",
      "training loss 0.0013336479990448701\n",
      "epochs 2956\n",
      "training loss 0.0012692265068520552\n",
      "epochs 2957\n",
      "training loss 0.0012751705254538488\n",
      "epochs 2958\n",
      "training loss 0.0012745412988701445\n",
      "epochs 2959\n",
      "training loss 0.0012727845384099587\n",
      "testing loss 0.0031695913982491756\n",
      "epochs 2960\n",
      "training loss 0.0012632925767525126\n",
      "epochs 2961\n",
      "training loss 0.0012584868282444865\n",
      "epochs 2962\n",
      "training loss 0.0012704775925926684\n",
      "epochs 2963\n",
      "training loss 0.0013098447692600336\n",
      "epochs 2964\n",
      "training loss 0.0012825519769342113\n",
      "epochs 2965\n",
      "training loss 0.0012244223364017406\n",
      "epochs 2966\n",
      "training loss 0.0012468175313613834\n",
      "epochs 2967\n",
      "training loss 0.001261695061306438\n",
      "epochs 2968\n",
      "training loss 0.0012896019322472117\n",
      "epochs 2969\n",
      "training loss 0.001299478578427948\n",
      "testing loss 0.0030187610210685705\n",
      "epochs 2970\n",
      "training loss 0.0012866556561285897\n",
      "epochs 2971\n",
      "training loss 0.0012463906317762463\n",
      "epochs 2972\n",
      "training loss 0.0013238264431421132\n",
      "epochs 2973\n",
      "training loss 0.0012426045353490418\n",
      "epochs 2974\n",
      "training loss 0.0012820345120671513\n",
      "epochs 2975\n",
      "training loss 0.0012613860658500175\n",
      "epochs 2976\n",
      "training loss 0.0012487821032556403\n",
      "epochs 2977\n",
      "training loss 0.001262138161198222\n",
      "epochs 2978\n",
      "training loss 0.001329181148100214\n",
      "epochs 2979\n",
      "training loss 0.0012976545346886667\n",
      "testing loss 0.0029059318459843755\n",
      "epochs 2980\n",
      "training loss 0.0012615563011428167\n",
      "epochs 2981\n",
      "training loss 0.00126969200537022\n",
      "epochs 2982\n",
      "training loss 0.0013315809048828316\n",
      "epochs 2983\n",
      "training loss 0.0012345078652852008\n",
      "epochs 2984\n",
      "training loss 0.0012610292173270985\n",
      "epochs 2985\n",
      "training loss 0.0013165991128093952\n",
      "epochs 2986\n",
      "training loss 0.001302271039298761\n",
      "epochs 2987\n",
      "training loss 0.0012391405176558168\n",
      "epochs 2988\n",
      "training loss 0.0012655783615196775\n",
      "epochs 2989\n",
      "training loss 0.0012866805376920928\n",
      "testing loss 0.002806578807413895\n",
      "epochs 2990\n",
      "training loss 0.001261496295944411\n",
      "epochs 2991\n",
      "training loss 0.001292443517022299\n",
      "epochs 2992\n",
      "training loss 0.0012205074506593829\n",
      "epochs 2993\n",
      "training loss 0.0012562218207815\n",
      "epochs 2994\n",
      "training loss 0.0012785965070060503\n",
      "epochs 2995\n",
      "training loss 0.0012634788150828343\n",
      "epochs 2996\n",
      "training loss 0.0012203032137772301\n",
      "epochs 2997\n",
      "training loss 0.0012641285488093244\n",
      "epochs 2998\n",
      "training loss 0.0012852070599406573\n",
      "epochs 2999\n",
      "training loss 0.0013140573254418432\n",
      "testing loss 0.0033048677302778082\n",
      "epochs 3000\n",
      "training loss 0.0013544713127523048\n",
      "epochs 3001\n",
      "training loss 0.0012695450388110247\n",
      "epochs 3002\n",
      "training loss 0.0012396349531660666\n",
      "epochs 3003\n",
      "training loss 0.0012658522531565697\n",
      "epochs 3004\n",
      "training loss 0.0011787535099023925\n",
      "epochs 3005\n",
      "training loss 0.0013174132548702886\n",
      "epochs 3006\n",
      "training loss 0.0012419839659606681\n",
      "epochs 3007\n",
      "training loss 0.0012314282861923215\n",
      "epochs 3008\n",
      "training loss 0.0012579667357173056\n",
      "epochs 3009\n",
      "training loss 0.001296089172340635\n",
      "testing loss 0.0027423938001499425\n",
      "epochs 3010\n",
      "training loss 0.0012459313984547026\n",
      "epochs 3011\n",
      "training loss 0.0013047324093327629\n",
      "epochs 3012\n",
      "training loss 0.001331376789546767\n",
      "epochs 3013\n",
      "training loss 0.0012820750180529967\n",
      "epochs 3014\n",
      "training loss 0.0012331280135832682\n",
      "epochs 3015\n",
      "training loss 0.0012975183162005975\n",
      "epochs 3016\n",
      "training loss 0.0012549951212200419\n",
      "epochs 3017\n",
      "training loss 0.00124161710678161\n",
      "epochs 3018\n",
      "training loss 0.0012287930171444197\n",
      "epochs 3019\n",
      "training loss 0.0012678509865785554\n",
      "testing loss 0.002813279536281917\n",
      "epochs 3020\n",
      "training loss 0.0012147916959652444\n",
      "epochs 3021\n",
      "training loss 0.0012301218727430877\n",
      "epochs 3022\n",
      "training loss 0.0012679362618633317\n",
      "epochs 3023\n",
      "training loss 0.0012786691892668492\n",
      "epochs 3024\n",
      "training loss 0.0012978312177626037\n",
      "epochs 3025\n",
      "training loss 0.00125830947286322\n",
      "epochs 3026\n",
      "training loss 0.001298686713978116\n",
      "epochs 3027\n",
      "training loss 0.001211988578138641\n",
      "epochs 3028\n",
      "training loss 0.0012260560835513504\n",
      "epochs 3029\n",
      "training loss 0.0012651219906350513\n",
      "testing loss 0.0028742050412705446\n",
      "epochs 3030\n",
      "training loss 0.0012831704089160455\n",
      "epochs 3031\n",
      "training loss 0.0013010095007714778\n",
      "epochs 3032\n",
      "training loss 0.001199161766010611\n",
      "epochs 3033\n",
      "training loss 0.0012335351690944386\n",
      "epochs 3034\n",
      "training loss 0.0012824836742248965\n",
      "epochs 3035\n",
      "training loss 0.0012543964458737059\n",
      "epochs 3036\n",
      "training loss 0.0012077710314902922\n",
      "epochs 3037\n",
      "training loss 0.0012429523868075824\n",
      "epochs 3038\n",
      "training loss 0.0012393894328984877\n",
      "epochs 3039\n",
      "training loss 0.001266691158065646\n",
      "testing loss 0.0033627490187724328\n",
      "epochs 3040\n",
      "training loss 0.001282643032292793\n",
      "epochs 3041\n",
      "training loss 0.0013044329006635332\n",
      "epochs 3042\n",
      "training loss 0.0012183827072053605\n",
      "epochs 3043\n",
      "training loss 0.001236139093936169\n",
      "epochs 3044\n",
      "training loss 0.001184106973215009\n",
      "epochs 3045\n",
      "training loss 0.0012435055515476953\n",
      "epochs 3046\n",
      "training loss 0.001304689850509563\n",
      "epochs 3047\n",
      "training loss 0.0013353310972678625\n",
      "epochs 3048\n",
      "training loss 0.001272262174545541\n",
      "epochs 3049\n",
      "training loss 0.001233066120085881\n",
      "testing loss 0.0028888939050891508\n",
      "epochs 3050\n",
      "training loss 0.0012149672140013483\n",
      "epochs 3051\n",
      "training loss 0.0012269784582461766\n",
      "epochs 3052\n",
      "training loss 0.001251672696050449\n",
      "epochs 3053\n",
      "training loss 0.0012852323724885433\n",
      "epochs 3054\n",
      "training loss 0.0012513481464101097\n",
      "epochs 3055\n",
      "training loss 0.001256192031020644\n",
      "epochs 3056\n",
      "training loss 0.0012223153600842893\n",
      "epochs 3057\n",
      "training loss 0.001243705877547342\n",
      "epochs 3058\n",
      "training loss 0.001276813077529479\n",
      "epochs 3059\n",
      "training loss 0.001258147650081268\n",
      "testing loss 0.0027773671990742655\n",
      "epochs 3060\n",
      "training loss 0.001230306923035589\n",
      "epochs 3061\n",
      "training loss 0.001249517143405351\n",
      "epochs 3062\n",
      "training loss 0.00129488407578291\n",
      "epochs 3063\n",
      "training loss 0.0011864921158055088\n",
      "epochs 3064\n",
      "training loss 0.0012511685984223334\n",
      "epochs 3065\n",
      "training loss 0.0012712537413278996\n",
      "epochs 3066\n",
      "training loss 0.001317867493125296\n",
      "epochs 3067\n",
      "training loss 0.001246571554809435\n",
      "epochs 3068\n",
      "training loss 0.0012265043492403482\n",
      "epochs 3069\n",
      "training loss 0.001225842230374384\n",
      "testing loss 0.0030342128791628364\n",
      "epochs 3070\n",
      "training loss 0.0012125736590769243\n",
      "epochs 3071\n",
      "training loss 0.0012962714640500086\n",
      "epochs 3072\n",
      "training loss 0.0013177334840514196\n",
      "epochs 3073\n",
      "training loss 0.0012142077462725046\n",
      "epochs 3074\n",
      "training loss 0.0012194683205967595\n",
      "epochs 3075\n",
      "training loss 0.0012337252830995932\n",
      "epochs 3076\n",
      "training loss 0.001222023336141792\n",
      "epochs 3077\n",
      "training loss 0.0012308781326813539\n",
      "epochs 3078\n",
      "training loss 0.0012315510778184017\n",
      "epochs 3079\n",
      "training loss 0.0012659118252475166\n",
      "testing loss 0.003395107304776444\n",
      "epochs 3080\n",
      "training loss 0.0012607257722984536\n",
      "epochs 3081\n",
      "training loss 0.0013021736217897646\n",
      "epochs 3082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0012842121894618547\n",
      "epochs 3083\n",
      "training loss 0.0012225884417677723\n",
      "epochs 3084\n",
      "training loss 0.0011941901882997173\n",
      "epochs 3085\n",
      "training loss 0.0012517419682977024\n",
      "epochs 3086\n",
      "training loss 0.001255654330764498\n",
      "epochs 3087\n",
      "training loss 0.0012826949015109711\n",
      "epochs 3088\n",
      "training loss 0.0012820467500067048\n",
      "epochs 3089\n",
      "training loss 0.0012117701706102848\n",
      "testing loss 0.0028491599847734342\n",
      "epochs 3090\n",
      "training loss 0.0012080700851136133\n",
      "epochs 3091\n",
      "training loss 0.001212338423519183\n",
      "epochs 3092\n",
      "training loss 0.001248381496194784\n",
      "epochs 3093\n",
      "training loss 0.0012362774652230771\n",
      "epochs 3094\n",
      "training loss 0.0012275894973243057\n",
      "epochs 3095\n",
      "training loss 0.0012435145410043852\n",
      "epochs 3096\n",
      "training loss 0.0012374424498024365\n",
      "epochs 3097\n",
      "training loss 0.0012280109351200808\n",
      "epochs 3098\n",
      "training loss 0.001252417001125932\n",
      "epochs 3099\n",
      "training loss 0.0011685697542642183\n",
      "testing loss 0.002745084067004079\n",
      "epochs 3100\n",
      "training loss 0.0012621832855815765\n",
      "epochs 3101\n",
      "training loss 0.0012612809944759183\n",
      "epochs 3102\n",
      "training loss 0.0012806283126409678\n",
      "epochs 3103\n",
      "training loss 0.001212569627179099\n",
      "epochs 3104\n",
      "training loss 0.0012184077439303185\n",
      "epochs 3105\n",
      "training loss 0.001280902301939346\n",
      "epochs 3106\n",
      "training loss 0.001263784520833337\n",
      "epochs 3107\n",
      "training loss 0.0012052695885779036\n",
      "epochs 3108\n",
      "training loss 0.0013020748920430144\n",
      "epochs 3109\n",
      "training loss 0.001275898915599454\n",
      "testing loss 0.0028352541609581376\n",
      "epochs 3110\n",
      "training loss 0.0012608919583549751\n",
      "epochs 3111\n",
      "training loss 0.0012364739994093843\n",
      "epochs 3112\n",
      "training loss 0.0012842751452048226\n",
      "epochs 3113\n",
      "training loss 0.0012382124982332659\n",
      "epochs 3114\n",
      "training loss 0.0012165119646451123\n",
      "epochs 3115\n",
      "training loss 0.0012342643964395924\n",
      "epochs 3116\n",
      "training loss 0.0011884603386824119\n",
      "epochs 3117\n",
      "training loss 0.0012336735725813294\n",
      "epochs 3118\n",
      "training loss 0.0011964103854176416\n",
      "epochs 3119\n",
      "training loss 0.0012202695187954033\n",
      "testing loss 0.003318510736596394\n",
      "epochs 3120\n",
      "training loss 0.0012194068858606634\n",
      "epochs 3121\n",
      "training loss 0.0012132981036631191\n",
      "epochs 3122\n",
      "training loss 0.0011449586575296015\n",
      "epochs 3123\n",
      "training loss 0.001254977426975739\n",
      "epochs 3124\n",
      "training loss 0.001278194752599659\n",
      "epochs 3125\n",
      "training loss 0.0012412836119153263\n",
      "epochs 3126\n",
      "training loss 0.0012488218086649943\n",
      "epochs 3127\n",
      "training loss 0.0011965238855795024\n",
      "epochs 3128\n",
      "training loss 0.0012092233021916273\n",
      "epochs 3129\n",
      "training loss 0.0012097970114052748\n",
      "testing loss 0.0032330162225436446\n",
      "epochs 3130\n",
      "training loss 0.0012639475682761666\n",
      "epochs 3131\n",
      "training loss 0.0012293927101517453\n",
      "epochs 3132\n",
      "training loss 0.001193895037774291\n",
      "epochs 3133\n",
      "training loss 0.0012025907814380993\n",
      "epochs 3134\n",
      "training loss 0.0012174502735544212\n",
      "epochs 3135\n",
      "training loss 0.0011989496404865593\n",
      "epochs 3136\n",
      "training loss 0.0011951365289816175\n",
      "epochs 3137\n",
      "training loss 0.0011795709199076774\n",
      "epochs 3138\n",
      "training loss 0.0012755645223182703\n",
      "epochs 3139\n",
      "training loss 0.001226172609096195\n",
      "testing loss 0.002836869891686695\n",
      "epochs 3140\n",
      "training loss 0.0012008575175883175\n",
      "epochs 3141\n",
      "training loss 0.0012479765587670327\n",
      "epochs 3142\n",
      "training loss 0.00121251745825339\n",
      "epochs 3143\n",
      "training loss 0.0011931503445297865\n",
      "epochs 3144\n",
      "training loss 0.0012215308547458728\n",
      "epochs 3145\n",
      "training loss 0.0012335931432170164\n",
      "epochs 3146\n",
      "training loss 0.001218853677951343\n",
      "epochs 3147\n",
      "training loss 0.0011914973847828466\n",
      "epochs 3148\n",
      "training loss 0.0012440644769000858\n",
      "epochs 3149\n",
      "training loss 0.0012541118238579855\n",
      "testing loss 0.0027704442985824816\n",
      "epochs 3150\n",
      "training loss 0.0012223340611972797\n",
      "epochs 3151\n",
      "training loss 0.0012243289325675024\n",
      "epochs 3152\n",
      "training loss 0.0012218638163662904\n",
      "epochs 3153\n",
      "training loss 0.001191407379639083\n",
      "epochs 3154\n",
      "training loss 0.0012033698823101999\n",
      "epochs 3155\n",
      "training loss 0.0012110577153868543\n",
      "epochs 3156\n",
      "training loss 0.00120200160508664\n",
      "epochs 3157\n",
      "training loss 0.001243197804813678\n",
      "epochs 3158\n",
      "training loss 0.0012405665388251555\n",
      "epochs 3159\n",
      "training loss 0.001260168882975734\n",
      "testing loss 0.002764196719244413\n",
      "epochs 3160\n",
      "training loss 0.001215387927120591\n",
      "epochs 3161\n",
      "training loss 0.0012363741380632444\n",
      "epochs 3162\n",
      "training loss 0.0011872926287814455\n",
      "epochs 3163\n",
      "training loss 0.001215996635415362\n",
      "epochs 3164\n",
      "training loss 0.0012172965071373548\n",
      "epochs 3165\n",
      "training loss 0.001214294789548635\n",
      "epochs 3166\n",
      "training loss 0.0011945345899370416\n",
      "epochs 3167\n",
      "training loss 0.001216335102525118\n",
      "epochs 3168\n",
      "training loss 0.0011921574938037406\n",
      "epochs 3169\n",
      "training loss 0.0012368599443319946\n",
      "testing loss 0.0028689187226473545\n",
      "epochs 3170\n",
      "training loss 0.0012328165778281636\n",
      "epochs 3171\n",
      "training loss 0.0012129479500827434\n",
      "epochs 3172\n",
      "training loss 0.001219784182611015\n",
      "epochs 3173\n",
      "training loss 0.001218570669473214\n",
      "epochs 3174\n",
      "training loss 0.001226615467311186\n",
      "epochs 3175\n",
      "training loss 0.0012338431368052597\n",
      "epochs 3176\n",
      "training loss 0.0012008191564397768\n",
      "epochs 3177\n",
      "training loss 0.0011643000041901793\n",
      "epochs 3178\n",
      "training loss 0.0012067958838546983\n",
      "epochs 3179\n",
      "training loss 0.0012076088989161997\n",
      "testing loss 0.002593939176080286\n",
      "epochs 3180\n",
      "training loss 0.0012315796271722345\n",
      "epochs 3181\n",
      "training loss 0.0011356558911162669\n",
      "epochs 3182\n",
      "training loss 0.0011663196789239473\n",
      "epochs 3183\n",
      "training loss 0.001242056562590744\n",
      "epochs 3184\n",
      "training loss 0.001171348235919494\n",
      "epochs 3185\n",
      "training loss 0.001220918032814859\n",
      "epochs 3186\n",
      "training loss 0.0012273877435561983\n",
      "epochs 3187\n",
      "training loss 0.0012216242297416649\n",
      "epochs 3188\n",
      "training loss 0.0011713727271909001\n",
      "epochs 3189\n",
      "training loss 0.0011845936189833065\n",
      "testing loss 0.003870101118004544\n",
      "epochs 3190\n",
      "training loss 0.001215801208413073\n",
      "epochs 3191\n",
      "training loss 0.001252814388896217\n",
      "epochs 3192\n",
      "training loss 0.0012311880771526471\n",
      "epochs 3193\n",
      "training loss 0.00119317939121412\n",
      "epochs 3194\n",
      "training loss 0.0011595852576099948\n",
      "epochs 3195\n",
      "training loss 0.0012506709501676624\n",
      "epochs 3196\n",
      "training loss 0.0012322334465811363\n",
      "epochs 3197\n",
      "training loss 0.0012210331459835422\n",
      "epochs 3198\n",
      "training loss 0.0012194380641130781\n",
      "epochs 3199\n",
      "training loss 0.0011938808957896107\n",
      "testing loss 0.003969880721716752\n",
      "epochs 3200\n",
      "training loss 0.0012633913057915709\n",
      "epochs 3201\n",
      "training loss 0.0012116201216989058\n",
      "epochs 3202\n",
      "training loss 0.0011997224595312788\n",
      "epochs 3203\n",
      "training loss 0.0012439747171484976\n",
      "epochs 3204\n",
      "training loss 0.0012341915800711735\n",
      "epochs 3205\n",
      "training loss 0.0011820934738009873\n",
      "epochs 3206\n",
      "training loss 0.0012136833778707066\n",
      "epochs 3207\n",
      "training loss 0.0012006408605359405\n",
      "epochs 3208\n",
      "training loss 0.0012093762314091452\n",
      "epochs 3209\n",
      "training loss 0.0011566909121018254\n",
      "testing loss 0.0031087866060415955\n",
      "epochs 3210\n",
      "training loss 0.0012126886156687687\n",
      "epochs 3211\n",
      "training loss 0.0012253485294483535\n",
      "epochs 3212\n",
      "training loss 0.0012671511785175345\n",
      "epochs 3213\n",
      "training loss 0.0012112223556672865\n",
      "epochs 3214\n",
      "training loss 0.0011610399687869367\n",
      "epochs 3215\n",
      "training loss 0.001207328249882952\n",
      "epochs 3216\n",
      "training loss 0.0012662215869573702\n",
      "epochs 3217\n",
      "training loss 0.0012418208803024744\n",
      "epochs 3218\n",
      "training loss 0.0011991516665391352\n",
      "epochs 3219\n",
      "training loss 0.001197404836412055\n",
      "testing loss 0.002859620572272556\n",
      "epochs 3220\n",
      "training loss 0.0012175932075080991\n",
      "epochs 3221\n",
      "training loss 0.0011946819726190894\n",
      "epochs 3222\n",
      "training loss 0.0011704198690202076\n",
      "epochs 3223\n",
      "training loss 0.0011822404431909503\n",
      "epochs 3224\n",
      "training loss 0.0012188897277780534\n",
      "epochs 3225\n",
      "training loss 0.001177779584273832\n",
      "epochs 3226\n",
      "training loss 0.0012264631967779283\n",
      "epochs 3227\n",
      "training loss 0.001180349303357088\n",
      "epochs 3228\n",
      "training loss 0.0011946471639070943\n",
      "epochs 3229\n",
      "training loss 0.0012443511452070101\n",
      "testing loss 0.0029494763683884386\n",
      "epochs 3230\n",
      "training loss 0.0011882695359160902\n",
      "epochs 3231\n",
      "training loss 0.0011976573697296976\n",
      "epochs 3232\n",
      "training loss 0.0011809368914709423\n",
      "epochs 3233\n",
      "training loss 0.0012062961871857475\n",
      "epochs 3234\n",
      "training loss 0.001212087298508473\n",
      "epochs 3235\n",
      "training loss 0.0012444006995648983\n",
      "epochs 3236\n",
      "training loss 0.0012298673417880611\n",
      "epochs 3237\n",
      "training loss 0.001251293891739372\n",
      "epochs 3238\n",
      "training loss 0.0012236636926032218\n",
      "epochs 3239\n",
      "training loss 0.001198183284438354\n",
      "testing loss 0.00303248622197103\n",
      "epochs 3240\n",
      "training loss 0.0011393553989594038\n",
      "epochs 3241\n",
      "training loss 0.0012042418619091298\n",
      "epochs 3242\n",
      "training loss 0.0011974927526261759\n",
      "epochs 3243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0012574766312816426\n",
      "epochs 3244\n",
      "training loss 0.0011963657723417036\n",
      "epochs 3245\n",
      "training loss 0.0012941278851984527\n",
      "epochs 3246\n",
      "training loss 0.001192769603088985\n",
      "epochs 3247\n",
      "training loss 0.0011621990784617725\n",
      "epochs 3248\n",
      "training loss 0.0011673095601746717\n",
      "epochs 3249\n",
      "training loss 0.0012142295490222838\n",
      "testing loss 0.0035192286917702\n",
      "epochs 3250\n",
      "training loss 0.0011588935179446221\n",
      "epochs 3251\n",
      "training loss 0.001238538416372867\n",
      "epochs 3252\n",
      "training loss 0.001185333856599445\n",
      "epochs 3253\n",
      "training loss 0.0011932996484184904\n",
      "epochs 3254\n",
      "training loss 0.001194066241366568\n",
      "epochs 3255\n",
      "training loss 0.0012211977422101519\n",
      "epochs 3256\n",
      "training loss 0.0011706452423601913\n",
      "epochs 3257\n",
      "training loss 0.0011428841336638778\n",
      "epochs 3258\n",
      "training loss 0.0012105326286777514\n",
      "epochs 3259\n",
      "training loss 0.0012208014587931177\n",
      "testing loss 0.002807718115455821\n",
      "epochs 3260\n",
      "training loss 0.0011781349066591123\n",
      "epochs 3261\n",
      "training loss 0.0011839373592757209\n",
      "epochs 3262\n",
      "training loss 0.0012319229014563207\n",
      "epochs 3263\n",
      "training loss 0.0012097423016886287\n",
      "epochs 3264\n",
      "training loss 0.001223732088020179\n",
      "epochs 3265\n",
      "training loss 0.0011804337899192126\n",
      "epochs 3266\n",
      "training loss 0.0012120208106751024\n",
      "epochs 3267\n",
      "training loss 0.0012003716246273062\n",
      "epochs 3268\n",
      "training loss 0.0011640547556763\n",
      "epochs 3269\n",
      "training loss 0.0012569465831213697\n",
      "testing loss 0.003059744113290035\n",
      "epochs 3270\n",
      "training loss 0.0012185122058047462\n",
      "epochs 3271\n",
      "training loss 0.0011807230842939512\n",
      "epochs 3272\n",
      "training loss 0.0011903226620087394\n",
      "epochs 3273\n",
      "training loss 0.0012230187204049327\n",
      "epochs 3274\n",
      "training loss 0.0012414055622726294\n",
      "epochs 3275\n",
      "training loss 0.0011894910693264885\n",
      "epochs 3276\n",
      "training loss 0.001205798162262049\n",
      "epochs 3277\n",
      "training loss 0.0012003292315809402\n",
      "epochs 3278\n",
      "training loss 0.0012133061927587564\n",
      "epochs 3279\n",
      "training loss 0.0012319492590882586\n",
      "testing loss 0.0027995082666991767\n",
      "epochs 3280\n",
      "training loss 0.0012503884590123312\n",
      "epochs 3281\n",
      "training loss 0.0012583997716991327\n",
      "epochs 3282\n",
      "training loss 0.001159955164818178\n",
      "epochs 3283\n",
      "training loss 0.0012051079888958315\n",
      "epochs 3284\n",
      "training loss 0.001192086379611327\n",
      "epochs 3285\n",
      "training loss 0.0011700317847449049\n",
      "epochs 3286\n",
      "training loss 0.0011728563449291745\n",
      "epochs 3287\n",
      "training loss 0.0012478431806005085\n",
      "epochs 3288\n",
      "training loss 0.001134001144270291\n",
      "epochs 3289\n",
      "training loss 0.0011728044511187018\n",
      "testing loss 0.002856302740554332\n",
      "epochs 3290\n",
      "training loss 0.0012108162171094138\n",
      "epochs 3291\n",
      "training loss 0.0011876521352183153\n",
      "epochs 3292\n",
      "training loss 0.0011603168721769379\n",
      "epochs 3293\n",
      "training loss 0.001183259230649161\n",
      "epochs 3294\n",
      "training loss 0.0011707515189336402\n",
      "epochs 3295\n",
      "training loss 0.0011804373287326885\n",
      "epochs 3296\n",
      "training loss 0.001191082542321976\n",
      "epochs 3297\n",
      "training loss 0.00121370851756484\n",
      "epochs 3298\n",
      "training loss 0.0011839458678557472\n",
      "epochs 3299\n",
      "training loss 0.0012115687368958509\n",
      "testing loss 0.0028386056210171987\n",
      "epochs 3300\n",
      "training loss 0.0012112158229631177\n",
      "epochs 3301\n",
      "training loss 0.0012051135750620493\n",
      "epochs 3302\n",
      "training loss 0.0011586470453006337\n",
      "epochs 3303\n",
      "training loss 0.0011733947684464687\n",
      "epochs 3304\n",
      "training loss 0.0011845979477578949\n",
      "epochs 3305\n",
      "training loss 0.0011769778998908347\n",
      "epochs 3306\n",
      "training loss 0.0011390558257132859\n",
      "epochs 3307\n",
      "training loss 0.0011758065844764281\n",
      "epochs 3308\n",
      "training loss 0.0012538440229716544\n",
      "epochs 3309\n",
      "training loss 0.0011844351768456852\n",
      "testing loss 0.0029184571203135007\n",
      "epochs 3310\n",
      "training loss 0.0012295383474885805\n",
      "epochs 3311\n",
      "training loss 0.0011740572087814902\n",
      "epochs 3312\n",
      "training loss 0.0011934272442112102\n",
      "epochs 3313\n",
      "training loss 0.001223155236831303\n",
      "epochs 3314\n",
      "training loss 0.0012262659955919809\n",
      "epochs 3315\n",
      "training loss 0.001281830576838682\n",
      "epochs 3316\n",
      "training loss 0.0012987239084965166\n",
      "epochs 3317\n",
      "training loss 0.0011641916823870641\n",
      "epochs 3318\n",
      "training loss 0.001191737821522398\n",
      "epochs 3319\n",
      "training loss 0.0011805690677077893\n",
      "testing loss 0.0027235043297046202\n",
      "epochs 3320\n",
      "training loss 0.0011321920069998928\n",
      "epochs 3321\n",
      "training loss 0.001141885404937156\n",
      "epochs 3322\n",
      "training loss 0.0011470275846137282\n",
      "epochs 3323\n",
      "training loss 0.0011747404604783638\n",
      "epochs 3324\n",
      "training loss 0.001172138724970068\n",
      "epochs 3325\n",
      "training loss 0.0012185453946099228\n",
      "epochs 3326\n",
      "training loss 0.0012111596095780517\n",
      "epochs 3327\n",
      "training loss 0.0011677913888609517\n",
      "epochs 3328\n",
      "training loss 0.0012215558935939066\n",
      "epochs 3329\n",
      "training loss 0.0011925576817340537\n",
      "testing loss 0.002842978655248352\n",
      "epochs 3330\n",
      "training loss 0.0011461962854467038\n",
      "epochs 3331\n",
      "training loss 0.0012088469135869396\n",
      "epochs 3332\n",
      "training loss 0.0011738794984927226\n",
      "epochs 3333\n",
      "training loss 0.0011952348762208818\n",
      "epochs 3334\n",
      "training loss 0.0011830005112194702\n",
      "epochs 3335\n",
      "training loss 0.0011372340368574038\n",
      "epochs 3336\n",
      "training loss 0.0011729363362321852\n",
      "epochs 3337\n",
      "training loss 0.0011620598678522402\n",
      "epochs 3338\n",
      "training loss 0.0011759150844409857\n",
      "epochs 3339\n",
      "training loss 0.0011971872489217183\n",
      "testing loss 0.0027402271161595664\n",
      "epochs 3340\n",
      "training loss 0.0012285175109083982\n",
      "epochs 3341\n",
      "training loss 0.0011544763352962545\n",
      "epochs 3342\n",
      "training loss 0.0011783152385926197\n",
      "epochs 3343\n",
      "training loss 0.0011829806008379013\n",
      "epochs 3344\n",
      "training loss 0.0011513684161494296\n",
      "epochs 3345\n",
      "training loss 0.0011603385875267641\n",
      "epochs 3346\n",
      "training loss 0.0012165768946716306\n",
      "epochs 3347\n",
      "training loss 0.001215792612256171\n",
      "epochs 3348\n",
      "training loss 0.0012171670251687292\n",
      "epochs 3349\n",
      "training loss 0.00114907350339488\n",
      "testing loss 0.003039007276587241\n",
      "epochs 3350\n",
      "training loss 0.0011863884693691026\n",
      "epochs 3351\n",
      "training loss 0.0012178520564454123\n",
      "epochs 3352\n",
      "training loss 0.0011710540664320002\n",
      "epochs 3353\n",
      "training loss 0.0011213032967880888\n",
      "epochs 3354\n",
      "training loss 0.001203753704068571\n",
      "epochs 3355\n",
      "training loss 0.0011863008286240638\n",
      "epochs 3356\n",
      "training loss 0.0011617516121108051\n",
      "epochs 3357\n",
      "training loss 0.0011593336235918672\n",
      "epochs 3358\n",
      "training loss 0.0011495633038768607\n",
      "epochs 3359\n",
      "training loss 0.0011746991088891284\n",
      "testing loss 0.0030260076193165376\n",
      "epochs 3360\n",
      "training loss 0.001161025525148573\n",
      "epochs 3361\n",
      "training loss 0.0011864076156203145\n",
      "epochs 3362\n",
      "training loss 0.0012242387961468866\n",
      "epochs 3363\n",
      "training loss 0.0012266470040095613\n",
      "epochs 3364\n",
      "training loss 0.001192166671152637\n",
      "epochs 3365\n",
      "training loss 0.0011661801977991763\n",
      "epochs 3366\n",
      "training loss 0.0012087252093295037\n",
      "epochs 3367\n",
      "training loss 0.0014575793336026688\n",
      "epochs 3368\n",
      "training loss 0.0011595878728855073\n",
      "epochs 3369\n",
      "training loss 0.0012044187689675614\n",
      "testing loss 0.0029385315298915225\n",
      "epochs 3370\n",
      "training loss 0.0011511059758525148\n",
      "epochs 3371\n",
      "training loss 0.0011962860556231601\n",
      "epochs 3372\n",
      "training loss 0.0011470244357385036\n",
      "epochs 3373\n",
      "training loss 0.0011994326618106578\n",
      "epochs 3374\n",
      "training loss 0.0011351966126793627\n",
      "epochs 3375\n",
      "training loss 0.001170615970275979\n",
      "epochs 3376\n",
      "training loss 0.001149371307765029\n",
      "epochs 3377\n",
      "training loss 0.001187726061778443\n",
      "epochs 3378\n",
      "training loss 0.0011879949878949366\n",
      "epochs 3379\n",
      "training loss 0.0011561986637134728\n",
      "testing loss 0.002720621763728559\n",
      "epochs 3380\n",
      "training loss 0.0011465210065254121\n",
      "epochs 3381\n",
      "training loss 0.0011722076103752224\n",
      "epochs 3382\n",
      "training loss 0.0012156234887473875\n",
      "epochs 3383\n",
      "training loss 0.001160813265163055\n",
      "epochs 3384\n",
      "training loss 0.0011431136876939143\n",
      "epochs 3385\n",
      "training loss 0.0011864957324650965\n",
      "epochs 3386\n",
      "training loss 0.0011969444016065437\n",
      "epochs 3387\n",
      "training loss 0.0011968599104444217\n",
      "epochs 3388\n",
      "training loss 0.0011415295267483293\n",
      "epochs 3389\n",
      "training loss 0.0011446005751458096\n",
      "testing loss 0.002980217615849213\n",
      "epochs 3390\n",
      "training loss 0.0011038062641097058\n",
      "epochs 3391\n",
      "training loss 0.0011946940502664215\n",
      "epochs 3392\n",
      "training loss 0.0012089729002651428\n",
      "epochs 3393\n",
      "training loss 0.00113424223597872\n",
      "epochs 3394\n",
      "training loss 0.0012005084570381212\n",
      "epochs 3395\n",
      "training loss 0.0011939531371583398\n",
      "epochs 3396\n",
      "training loss 0.0011821079623777883\n",
      "epochs 3397\n",
      "training loss 0.0011545138910913834\n",
      "epochs 3398\n",
      "training loss 0.0011641231174778907\n",
      "epochs 3399\n",
      "training loss 0.001187180560103428\n",
      "testing loss 0.0033752808045159947\n",
      "epochs 3400\n",
      "training loss 0.0011490698387891644\n",
      "epochs 3401\n",
      "training loss 0.0011943921464138685\n",
      "epochs 3402\n",
      "training loss 0.0011371871305057796\n",
      "epochs 3403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.001228169044111728\n",
      "epochs 3404\n",
      "training loss 0.001153988485036299\n",
      "epochs 3405\n",
      "training loss 0.0011598769197449948\n",
      "epochs 3406\n",
      "training loss 0.0011590539854965357\n",
      "epochs 3407\n",
      "training loss 0.0011992248234842936\n",
      "epochs 3408\n",
      "training loss 0.001155136104801947\n",
      "epochs 3409\n",
      "training loss 0.001188200224038089\n",
      "testing loss 0.002778744112026501\n",
      "epochs 3410\n",
      "training loss 0.0011864106364033007\n",
      "epochs 3411\n",
      "training loss 0.00115574684204947\n",
      "epochs 3412\n",
      "training loss 0.001144957150499645\n",
      "epochs 3413\n",
      "training loss 0.001175900576048811\n",
      "epochs 3414\n",
      "training loss 0.0011506655699669703\n",
      "epochs 3415\n",
      "training loss 0.0011914534930219042\n",
      "epochs 3416\n",
      "training loss 0.0011494107598206736\n",
      "epochs 3417\n",
      "training loss 0.0011569935604462847\n",
      "epochs 3418\n",
      "training loss 0.0011528584922842206\n",
      "epochs 3419\n",
      "training loss 0.0012168291618971092\n",
      "testing loss 0.0028286919646718085\n",
      "epochs 3420\n",
      "training loss 0.0011801794650794593\n",
      "epochs 3421\n",
      "training loss 0.0011900155371590022\n",
      "epochs 3422\n",
      "training loss 0.0011445670958330973\n",
      "epochs 3423\n",
      "training loss 0.0011641373562394418\n",
      "epochs 3424\n",
      "training loss 0.0011392055618248337\n",
      "epochs 3425\n",
      "training loss 0.001200078892462114\n",
      "epochs 3426\n",
      "training loss 0.001164645111219431\n",
      "epochs 3427\n",
      "training loss 0.0012095120569512723\n",
      "epochs 3428\n",
      "training loss 0.001162035062011773\n",
      "epochs 3429\n",
      "training loss 0.0011726007904746998\n",
      "testing loss 0.003057363620176515\n",
      "epochs 3430\n",
      "training loss 0.0012067748151583693\n",
      "epochs 3431\n",
      "training loss 0.001257706462549991\n",
      "epochs 3432\n",
      "training loss 0.0011592266899264647\n",
      "epochs 3433\n",
      "training loss 0.0011996727583273158\n",
      "epochs 3434\n",
      "training loss 0.0012131688688558988\n",
      "epochs 3435\n",
      "training loss 0.0011399761711015143\n",
      "epochs 3436\n",
      "training loss 0.0012125911999290022\n",
      "epochs 3437\n",
      "training loss 0.001139034670324696\n",
      "epochs 3438\n",
      "training loss 0.0011591244247277752\n",
      "epochs 3439\n",
      "training loss 0.0011543391506317834\n",
      "testing loss 0.002946764424753686\n",
      "epochs 3440\n",
      "training loss 0.00117116828277809\n",
      "epochs 3441\n",
      "training loss 0.0011932755306314983\n",
      "epochs 3442\n",
      "training loss 0.0011386073110936533\n",
      "epochs 3443\n",
      "training loss 0.001135566729718202\n",
      "epochs 3444\n",
      "training loss 0.0011435922245512007\n",
      "epochs 3445\n",
      "training loss 0.0011472029466037623\n",
      "epochs 3446\n",
      "training loss 0.001255654971402614\n",
      "epochs 3447\n",
      "training loss 0.0011683164778700534\n",
      "epochs 3448\n",
      "training loss 0.001147096482138107\n",
      "epochs 3449\n",
      "training loss 0.0011827831264828702\n",
      "testing loss 0.002769419920534979\n",
      "epochs 3450\n",
      "training loss 0.001167089875793158\n",
      "epochs 3451\n",
      "training loss 0.0011268316038833268\n",
      "epochs 3452\n",
      "training loss 0.0012188692689352417\n",
      "epochs 3453\n",
      "training loss 0.0011510197642913206\n",
      "epochs 3454\n",
      "training loss 0.0011220746088028521\n",
      "epochs 3455\n",
      "training loss 0.0011153613515467154\n",
      "epochs 3456\n",
      "training loss 0.0011432382115729063\n",
      "epochs 3457\n",
      "training loss 0.0011620773915427887\n",
      "epochs 3458\n",
      "training loss 0.001113576593732399\n",
      "epochs 3459\n",
      "training loss 0.0011863709591247006\n",
      "testing loss 0.002800866763738268\n",
      "epochs 3460\n",
      "training loss 0.0011946965636342969\n",
      "epochs 3461\n",
      "training loss 0.001174692533015752\n",
      "epochs 3462\n",
      "training loss 0.0011466134788601526\n",
      "epochs 3463\n",
      "training loss 0.0011327603342164432\n",
      "epochs 3464\n",
      "training loss 0.001126948130666632\n",
      "epochs 3465\n",
      "training loss 0.0011362981576313997\n",
      "epochs 3466\n",
      "training loss 0.0011434663900267616\n",
      "epochs 3467\n",
      "training loss 0.0011991027586578213\n",
      "epochs 3468\n",
      "training loss 0.0011602971095260584\n",
      "epochs 3469\n",
      "training loss 0.0011512395031651125\n",
      "testing loss 0.0038198467991347853\n",
      "epochs 3470\n",
      "training loss 0.0012016672949592573\n",
      "epochs 3471\n",
      "training loss 0.001111867832477094\n",
      "epochs 3472\n",
      "training loss 0.001132869369903789\n",
      "epochs 3473\n",
      "training loss 0.001156823368322693\n",
      "epochs 3474\n",
      "training loss 0.001167755390341906\n",
      "epochs 3475\n",
      "training loss 0.0011374553735226415\n",
      "epochs 3476\n",
      "training loss 0.0011067882335552305\n",
      "epochs 3477\n",
      "training loss 0.0011482108172063226\n",
      "epochs 3478\n",
      "training loss 0.001177865396342934\n",
      "epochs 3479\n",
      "training loss 0.0011492422067115825\n",
      "testing loss 0.002803910131886575\n",
      "epochs 3480\n",
      "training loss 0.0011481218286620107\n",
      "epochs 3481\n",
      "training loss 0.0011986596600044982\n",
      "epochs 3482\n",
      "training loss 0.0011680052179763925\n",
      "epochs 3483\n",
      "training loss 0.0011313109926519899\n",
      "epochs 3484\n",
      "training loss 0.0011383989890598576\n",
      "epochs 3485\n",
      "training loss 0.00118449885991903\n",
      "epochs 3486\n",
      "training loss 0.001200307285700469\n",
      "epochs 3487\n",
      "training loss 0.0011331858631792424\n",
      "epochs 3488\n",
      "training loss 0.0011698934242502987\n",
      "epochs 3489\n",
      "training loss 0.0011517695262023221\n",
      "testing loss 0.0026716814196424176\n",
      "epochs 3490\n",
      "training loss 0.0011635218909021613\n",
      "epochs 3491\n",
      "training loss 0.0011595354084984193\n",
      "epochs 3492\n",
      "training loss 0.0011733952178308417\n",
      "epochs 3493\n",
      "training loss 0.0011470307662197047\n",
      "epochs 3494\n",
      "training loss 0.0011494710679996849\n",
      "epochs 3495\n",
      "training loss 0.0011603648474972164\n",
      "epochs 3496\n",
      "training loss 0.0011764762170844935\n",
      "epochs 3497\n",
      "training loss 0.0011785174895143052\n",
      "epochs 3498\n",
      "training loss 0.0011576703881581616\n",
      "epochs 3499\n",
      "training loss 0.0011722989635836434\n",
      "testing loss 0.0030732400119238604\n",
      "epochs 3500\n",
      "training loss 0.0011659156445110936\n",
      "epochs 3501\n",
      "training loss 0.0011189258912164934\n",
      "epochs 3502\n",
      "training loss 0.0011484593801953373\n",
      "epochs 3503\n",
      "training loss 0.0011339520437198501\n",
      "epochs 3504\n",
      "training loss 0.0011178688241712364\n",
      "epochs 3505\n",
      "training loss 0.0011339755482917884\n",
      "epochs 3506\n",
      "training loss 0.001196552270925731\n",
      "epochs 3507\n",
      "training loss 0.0012031728014318798\n",
      "epochs 3508\n",
      "training loss 0.001135013692893405\n",
      "epochs 3509\n",
      "training loss 0.0011267678502177123\n",
      "testing loss 0.002833431289392583\n",
      "epochs 3510\n",
      "training loss 0.001172228581498527\n",
      "epochs 3511\n",
      "training loss 0.0011855905829917493\n",
      "epochs 3512\n",
      "training loss 0.0011763985388725429\n",
      "epochs 3513\n",
      "training loss 0.001162290937935305\n",
      "epochs 3514\n",
      "training loss 0.0011535829253704216\n",
      "epochs 3515\n",
      "training loss 0.0010901428175252215\n",
      "epochs 3516\n",
      "training loss 0.0011198226550712864\n",
      "epochs 3517\n",
      "training loss 0.0011879756148284624\n",
      "epochs 3518\n",
      "training loss 0.0011078107583345025\n",
      "epochs 3519\n",
      "training loss 0.0011584449337942576\n",
      "testing loss 0.002853934259272795\n",
      "epochs 3520\n",
      "training loss 0.0011010718955460222\n",
      "epochs 3521\n",
      "training loss 0.0011430636362709094\n",
      "epochs 3522\n",
      "training loss 0.0012048036532368975\n",
      "epochs 3523\n",
      "training loss 0.0011638780653485208\n",
      "epochs 3524\n",
      "training loss 0.0011805877375054119\n",
      "epochs 3525\n",
      "training loss 0.0011098907811434142\n",
      "epochs 3526\n",
      "training loss 0.0011206838308683508\n",
      "epochs 3527\n",
      "training loss 0.0011322039654011354\n",
      "epochs 3528\n",
      "training loss 0.0011286043528733508\n",
      "epochs 3529\n",
      "training loss 0.0011582169743710533\n",
      "testing loss 0.002696357795833247\n",
      "epochs 3530\n",
      "training loss 0.0011548546612075894\n",
      "epochs 3531\n",
      "training loss 0.0011385865635125357\n",
      "epochs 3532\n",
      "training loss 0.0011633201429725123\n",
      "epochs 3533\n",
      "training loss 0.00114178224211972\n",
      "epochs 3534\n",
      "training loss 0.0011426839844102894\n",
      "epochs 3535\n",
      "training loss 0.0011424130258025846\n",
      "epochs 3536\n",
      "training loss 0.0011586359099451011\n",
      "epochs 3537\n",
      "training loss 0.0011834638997541577\n",
      "epochs 3538\n",
      "training loss 0.0011616898757210257\n",
      "epochs 3539\n",
      "training loss 0.0011631565047838474\n",
      "testing loss 0.004288599849513448\n",
      "epochs 3540\n",
      "training loss 0.0011782790136121576\n",
      "epochs 3541\n",
      "training loss 0.0011135522988455025\n",
      "epochs 3542\n",
      "training loss 0.001154253799447048\n",
      "epochs 3543\n",
      "training loss 0.0011862413188100066\n",
      "epochs 3544\n",
      "training loss 0.001161319845020601\n",
      "epochs 3545\n",
      "training loss 0.001129352866415795\n",
      "epochs 3546\n",
      "training loss 0.0011361505005418077\n",
      "epochs 3547\n",
      "training loss 0.0011243442507633391\n",
      "epochs 3548\n",
      "training loss 0.0011096661648568682\n",
      "epochs 3549\n",
      "training loss 0.001103949233801826\n",
      "testing loss 0.002783815589475822\n",
      "epochs 3550\n",
      "training loss 0.0010777526612232205\n",
      "epochs 3551\n",
      "training loss 0.0011682678466962825\n",
      "epochs 3552\n",
      "training loss 0.0011173533618954042\n",
      "epochs 3553\n",
      "training loss 0.0011228320707558543\n",
      "epochs 3554\n",
      "training loss 0.0011603193521063696\n",
      "epochs 3555\n",
      "training loss 0.0011359645850959272\n",
      "epochs 3556\n",
      "training loss 0.0011124786291707386\n",
      "epochs 3557\n",
      "training loss 0.001130367547137759\n",
      "epochs 3558\n",
      "training loss 0.0011305023019420022\n",
      "epochs 3559\n",
      "training loss 0.001185339220727321\n",
      "testing loss 0.0028775508944044246\n",
      "epochs 3560\n",
      "training loss 0.0011587277250109653\n",
      "epochs 3561\n",
      "training loss 0.0011746018391108214\n",
      "epochs 3562\n",
      "training loss 0.0011447204910657485\n",
      "epochs 3563\n",
      "training loss 0.001097888476109149\n",
      "epochs 3564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0011235413641886825\n",
      "epochs 3565\n",
      "training loss 0.0011481616734866744\n",
      "epochs 3566\n",
      "training loss 0.0011485703255893452\n",
      "epochs 3567\n",
      "training loss 0.001133208780013036\n",
      "epochs 3568\n",
      "training loss 0.0011607597654226794\n",
      "epochs 3569\n",
      "training loss 0.0011088168112303508\n",
      "testing loss 0.0028004823190635663\n",
      "epochs 3570\n",
      "training loss 0.001125174217169544\n",
      "epochs 3571\n",
      "training loss 0.0011285928972872218\n",
      "epochs 3572\n",
      "training loss 0.0011177284617930946\n",
      "epochs 3573\n",
      "training loss 0.0011579512017297821\n",
      "epochs 3574\n",
      "training loss 0.0011300796322315071\n",
      "epochs 3575\n",
      "training loss 0.0012085506937390131\n",
      "epochs 3576\n",
      "training loss 0.0011437961123643109\n",
      "epochs 3577\n",
      "training loss 0.0011342498081054052\n",
      "epochs 3578\n",
      "training loss 0.0011243697259034453\n",
      "epochs 3579\n",
      "training loss 0.0011459258077156625\n",
      "testing loss 0.003317658548368841\n",
      "epochs 3580\n",
      "training loss 0.0011317725781645727\n",
      "epochs 3581\n",
      "training loss 0.001119825470269768\n",
      "epochs 3582\n",
      "training loss 0.0011082656029045672\n",
      "epochs 3583\n",
      "training loss 0.0011441496152736496\n",
      "epochs 3584\n",
      "training loss 0.0011358301607838133\n",
      "epochs 3585\n",
      "training loss 0.0011267186252904157\n",
      "epochs 3586\n",
      "training loss 0.0011741449325034072\n",
      "epochs 3587\n",
      "training loss 0.001150704733992001\n",
      "epochs 3588\n",
      "training loss 0.0011613156317767197\n",
      "epochs 3589\n",
      "training loss 0.0011619650259880082\n",
      "testing loss 0.0027275416803552856\n",
      "epochs 3590\n",
      "training loss 0.0011784103322182542\n",
      "epochs 3591\n",
      "training loss 0.001098123975282133\n",
      "epochs 3592\n",
      "training loss 0.0011174144940932019\n",
      "epochs 3593\n",
      "training loss 0.001156951033203837\n",
      "epochs 3594\n",
      "training loss 0.0011409221195451643\n",
      "epochs 3595\n",
      "training loss 0.0011666686664049523\n",
      "epochs 3596\n",
      "training loss 0.0010893839849498814\n",
      "epochs 3597\n",
      "training loss 0.0011477082941927423\n",
      "epochs 3598\n",
      "training loss 0.0011209641042056473\n",
      "epochs 3599\n",
      "training loss 0.0011348003032546502\n",
      "testing loss 0.0031996955880432896\n",
      "epochs 3600\n",
      "training loss 0.0011351050801594999\n",
      "epochs 3601\n",
      "training loss 0.0011193643691723085\n",
      "epochs 3602\n",
      "training loss 0.0011871735551918215\n",
      "epochs 3603\n",
      "training loss 0.0011718575218140422\n",
      "epochs 3604\n",
      "training loss 0.0011710062176116536\n",
      "epochs 3605\n",
      "training loss 0.001127370976592417\n",
      "epochs 3606\n",
      "training loss 0.0011039178982648805\n",
      "epochs 3607\n",
      "training loss 0.001117017545445757\n",
      "epochs 3608\n",
      "training loss 0.0011260388599493537\n",
      "epochs 3609\n",
      "training loss 0.0010906474211530642\n",
      "testing loss 0.0028567821169701397\n",
      "epochs 3610\n",
      "training loss 0.001119339124916693\n",
      "epochs 3611\n",
      "training loss 0.0011630825592929604\n",
      "epochs 3612\n",
      "training loss 0.001138383185945304\n",
      "epochs 3613\n",
      "training loss 0.0011239435773563648\n",
      "epochs 3614\n",
      "training loss 0.0011344497397437262\n",
      "epochs 3615\n",
      "training loss 0.0011689751410763427\n",
      "epochs 3616\n",
      "training loss 0.0011186090016335611\n",
      "epochs 3617\n",
      "training loss 0.001155294254839726\n",
      "epochs 3618\n",
      "training loss 0.0011288353498872246\n",
      "epochs 3619\n",
      "training loss 0.0011375976454305228\n",
      "testing loss 0.0027769359723658214\n",
      "epochs 3620\n",
      "training loss 0.0011211638933325068\n",
      "epochs 3621\n",
      "training loss 0.0011567591834957727\n",
      "epochs 3622\n",
      "training loss 0.001096543258496229\n",
      "epochs 3623\n",
      "training loss 0.0011537224855797655\n",
      "epochs 3624\n",
      "training loss 0.001154075646143135\n",
      "epochs 3625\n",
      "training loss 0.0011515266811871466\n",
      "epochs 3626\n",
      "training loss 0.0011276339188198302\n",
      "epochs 3627\n",
      "training loss 0.0010828395207103988\n",
      "epochs 3628\n",
      "training loss 0.0010786794781002661\n",
      "epochs 3629\n",
      "training loss 0.0011207459902809792\n",
      "testing loss 0.0027920941236851643\n",
      "epochs 3630\n",
      "training loss 0.0011137786325234296\n",
      "epochs 3631\n",
      "training loss 0.001149361897231643\n",
      "epochs 3632\n",
      "training loss 0.0011693700365325872\n",
      "epochs 3633\n",
      "training loss 0.0011186571758152707\n",
      "epochs 3634\n",
      "training loss 0.0011562829476987836\n",
      "epochs 3635\n",
      "training loss 0.0011770937154437543\n",
      "epochs 3636\n",
      "training loss 0.0011474278461440016\n",
      "epochs 3637\n",
      "training loss 0.0011287912442263974\n",
      "epochs 3638\n",
      "training loss 0.0011031081610554574\n",
      "epochs 3639\n",
      "training loss 0.001157754782897259\n",
      "testing loss 0.0027592747653041906\n",
      "epochs 3640\n",
      "training loss 0.0011102841621389368\n",
      "epochs 3641\n",
      "training loss 0.0011465614390882201\n",
      "epochs 3642\n",
      "training loss 0.001113706129131765\n",
      "epochs 3643\n",
      "training loss 0.0011671969463969481\n",
      "epochs 3644\n",
      "training loss 0.0011263540629255033\n",
      "epochs 3645\n",
      "training loss 0.001087848765854823\n",
      "epochs 3646\n",
      "training loss 0.0011338718452400282\n",
      "epochs 3647\n",
      "training loss 0.001138442621274786\n",
      "epochs 3648\n",
      "training loss 0.0011311723430652318\n",
      "epochs 3649\n",
      "training loss 0.0011401271835354504\n",
      "testing loss 0.0027955066815599904\n",
      "epochs 3650\n",
      "training loss 0.0011279179559281736\n",
      "epochs 3651\n",
      "training loss 0.0011089623763826096\n",
      "epochs 3652\n",
      "training loss 0.0011199216068563784\n",
      "epochs 3653\n",
      "training loss 0.0011100453005446897\n",
      "epochs 3654\n",
      "training loss 0.0011099686100937449\n",
      "epochs 3655\n",
      "training loss 0.0011192007748605428\n",
      "epochs 3656\n",
      "training loss 0.0011302826342797106\n",
      "epochs 3657\n",
      "training loss 0.0011548398677924915\n",
      "epochs 3658\n",
      "training loss 0.001102408243656034\n",
      "epochs 3659\n",
      "training loss 0.0011113764848036402\n",
      "testing loss 0.0027070394568541583\n",
      "epochs 3660\n",
      "training loss 0.001117429124208388\n",
      "epochs 3661\n",
      "training loss 0.001104878404373756\n",
      "epochs 3662\n",
      "training loss 0.00111340755178511\n",
      "epochs 3663\n",
      "training loss 0.0011316511624685198\n",
      "epochs 3664\n",
      "training loss 0.0010834821440791852\n",
      "epochs 3665\n",
      "training loss 0.001108825798917811\n",
      "epochs 3666\n",
      "training loss 0.0011377528026391467\n",
      "epochs 3667\n",
      "training loss 0.001138184345744856\n",
      "epochs 3668\n",
      "training loss 0.0011393842821669027\n",
      "epochs 3669\n",
      "training loss 0.0011229819213365712\n",
      "testing loss 0.003094334972705296\n",
      "epochs 3670\n",
      "training loss 0.0011135502978465816\n",
      "epochs 3671\n",
      "training loss 0.0011292028990659104\n",
      "epochs 3672\n",
      "training loss 0.001147457920573935\n",
      "epochs 3673\n",
      "training loss 0.001118195701764166\n",
      "epochs 3674\n",
      "training loss 0.0011184514457031542\n",
      "epochs 3675\n",
      "training loss 0.0011609450989688737\n",
      "epochs 3676\n",
      "training loss 0.0011474047226641656\n",
      "epochs 3677\n",
      "training loss 0.0010848992610687112\n",
      "epochs 3678\n",
      "training loss 0.0011309438716269192\n",
      "epochs 3679\n",
      "training loss 0.0010695984204211993\n",
      "testing loss 0.002857068615489976\n",
      "epochs 3680\n",
      "training loss 0.0011059106538131675\n",
      "epochs 3681\n",
      "training loss 0.0010946911307105562\n",
      "epochs 3682\n",
      "training loss 0.0011553874155817876\n",
      "epochs 3683\n",
      "training loss 0.0011075186379689802\n",
      "epochs 3684\n",
      "training loss 0.0011821412373444569\n",
      "epochs 3685\n",
      "training loss 0.001162811011531727\n",
      "epochs 3686\n",
      "training loss 0.0011650454580265813\n",
      "epochs 3687\n",
      "training loss 0.001095197092576128\n",
      "epochs 3688\n",
      "training loss 0.0011009069162811575\n",
      "epochs 3689\n",
      "training loss 0.0011280880875441056\n",
      "testing loss 0.0028840372485855054\n",
      "epochs 3690\n",
      "training loss 0.001085101213344404\n",
      "epochs 3691\n",
      "training loss 0.001133057584463814\n",
      "epochs 3692\n",
      "training loss 0.0011349164502727756\n",
      "epochs 3693\n",
      "training loss 0.0010811836350111909\n",
      "epochs 3694\n",
      "training loss 0.0011348818359106757\n",
      "epochs 3695\n",
      "training loss 0.0010900973385882119\n",
      "epochs 3696\n",
      "training loss 0.0011671181633009782\n",
      "epochs 3697\n",
      "training loss 0.0011278773970423567\n",
      "epochs 3698\n",
      "training loss 0.0011233463283631057\n",
      "epochs 3699\n",
      "training loss 0.0011542393924317417\n",
      "testing loss 0.0032695276358871597\n",
      "epochs 3700\n",
      "training loss 0.0011151132982728104\n",
      "epochs 3701\n",
      "training loss 0.0010597507559940626\n",
      "epochs 3702\n",
      "training loss 0.0011894380426781654\n",
      "epochs 3703\n",
      "training loss 0.001191179754246468\n",
      "epochs 3704\n",
      "training loss 0.0010948345895947203\n",
      "epochs 3705\n",
      "training loss 0.0010931088265716622\n",
      "epochs 3706\n",
      "training loss 0.0011003116099022351\n",
      "epochs 3707\n",
      "training loss 0.0011598739221389193\n",
      "epochs 3708\n",
      "training loss 0.0011065741291476307\n",
      "epochs 3709\n",
      "training loss 0.001116677443921498\n",
      "testing loss 0.0030093243112787604\n",
      "epochs 3710\n",
      "training loss 0.001166448835000441\n",
      "epochs 3711\n",
      "training loss 0.001134613274078601\n",
      "epochs 3712\n",
      "training loss 0.0010978076875911838\n",
      "epochs 3713\n",
      "training loss 0.0011380278301215802\n",
      "epochs 3714\n",
      "training loss 0.0010835278091391712\n",
      "epochs 3715\n",
      "training loss 0.001128689379758261\n",
      "epochs 3716\n",
      "training loss 0.001136478387834193\n",
      "epochs 3717\n",
      "training loss 0.0011780497739742593\n",
      "epochs 3718\n",
      "training loss 0.00104895691135417\n",
      "epochs 3719\n",
      "training loss 0.0011433494011206286\n",
      "testing loss 0.0027885034177551403\n",
      "epochs 3720\n",
      "training loss 0.0011449738233675756\n",
      "epochs 3721\n",
      "training loss 0.0011110721470216306\n",
      "epochs 3722\n",
      "training loss 0.0011092546154634524\n",
      "epochs 3723\n",
      "training loss 0.0010968836155857348\n",
      "epochs 3724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0011061076658684477\n",
      "epochs 3725\n",
      "training loss 0.001110262642289005\n",
      "epochs 3726\n",
      "training loss 0.0011130997121665276\n",
      "epochs 3727\n",
      "training loss 0.001105595718344566\n",
      "epochs 3728\n",
      "training loss 0.001111159695766719\n",
      "epochs 3729\n",
      "training loss 0.0012824909146209912\n",
      "testing loss 0.0032034476150606646\n",
      "epochs 3730\n",
      "training loss 0.001168774632669102\n",
      "epochs 3731\n",
      "training loss 0.0010844271909233216\n",
      "epochs 3732\n",
      "training loss 0.0011286272491840786\n",
      "epochs 3733\n",
      "training loss 0.0010871658327077773\n",
      "epochs 3734\n",
      "training loss 0.0010875873920496407\n",
      "epochs 3735\n",
      "training loss 0.0011640803051240585\n",
      "epochs 3736\n",
      "training loss 0.001091660391737489\n",
      "epochs 3737\n",
      "training loss 0.001091391088244276\n",
      "epochs 3738\n",
      "training loss 0.0011071588151938194\n",
      "epochs 3739\n",
      "training loss 0.0011022766863808353\n",
      "testing loss 0.003019488889704201\n",
      "epochs 3740\n",
      "training loss 0.0011262326550140615\n",
      "epochs 3741\n",
      "training loss 0.0010711112633590956\n",
      "epochs 3742\n",
      "training loss 0.0011128930197703145\n",
      "epochs 3743\n",
      "training loss 0.0011620001489152185\n",
      "epochs 3744\n",
      "training loss 0.0011356912902202513\n",
      "epochs 3745\n",
      "training loss 0.0010944362264480386\n",
      "epochs 3746\n",
      "training loss 0.0011196154920599427\n",
      "epochs 3747\n",
      "training loss 0.0010919892600678065\n",
      "epochs 3748\n",
      "training loss 0.0010883888811981277\n",
      "epochs 3749\n",
      "training loss 0.0011091375568496647\n",
      "testing loss 0.0027640716945554348\n",
      "epochs 3750\n",
      "training loss 0.0010789509616384572\n",
      "epochs 3751\n",
      "training loss 0.001120231025689494\n",
      "epochs 3752\n",
      "training loss 0.001096796102313704\n",
      "epochs 3753\n",
      "training loss 0.0011021497308723856\n",
      "epochs 3754\n",
      "training loss 0.0011161336987378432\n",
      "epochs 3755\n",
      "training loss 0.0010708849850580616\n",
      "epochs 3756\n",
      "training loss 0.0011008104612331804\n",
      "epochs 3757\n",
      "training loss 0.001080975831184808\n",
      "epochs 3758\n",
      "training loss 0.0011000408056598314\n",
      "epochs 3759\n",
      "training loss 0.0010710243796674948\n",
      "testing loss 0.0029739986090418875\n",
      "epochs 3760\n",
      "training loss 0.0011217632482400147\n",
      "epochs 3761\n",
      "training loss 0.0010912959272109854\n",
      "epochs 3762\n",
      "training loss 0.0010901734133471812\n",
      "epochs 3763\n",
      "training loss 0.0011123203581176403\n",
      "epochs 3764\n",
      "training loss 0.001085845366619969\n",
      "epochs 3765\n",
      "training loss 0.0011148119635814885\n",
      "epochs 3766\n",
      "training loss 0.0011178290782465194\n",
      "epochs 3767\n",
      "training loss 0.0011126938812777195\n",
      "epochs 3768\n",
      "training loss 0.0011040087142466684\n",
      "epochs 3769\n",
      "training loss 0.0011188019488833415\n",
      "testing loss 0.0026415028516038035\n",
      "epochs 3770\n",
      "training loss 0.0010842983524235797\n",
      "epochs 3771\n",
      "training loss 0.0011378699712990287\n",
      "epochs 3772\n",
      "training loss 0.0011282722058063401\n",
      "epochs 3773\n",
      "training loss 0.0011040459832496444\n",
      "epochs 3774\n",
      "training loss 0.0010708481495163905\n",
      "epochs 3775\n",
      "training loss 0.001064879879816138\n",
      "epochs 3776\n",
      "training loss 0.0011260549700252802\n",
      "epochs 3777\n",
      "training loss 0.001111463263756836\n",
      "epochs 3778\n",
      "training loss 0.0011536071976111766\n",
      "epochs 3779\n",
      "training loss 0.0011026254778311766\n",
      "testing loss 0.002756227950106461\n",
      "epochs 3780\n",
      "training loss 0.0011107040129355679\n",
      "epochs 3781\n",
      "training loss 0.0011312390633754002\n",
      "epochs 3782\n",
      "training loss 0.0010851413626535187\n",
      "epochs 3783\n",
      "training loss 0.001091507951004323\n",
      "epochs 3784\n",
      "training loss 0.0011141891637250871\n",
      "epochs 3785\n",
      "training loss 0.0011019517215022584\n",
      "epochs 3786\n",
      "training loss 0.0010722034549238697\n",
      "epochs 3787\n",
      "training loss 0.001058193665680031\n",
      "epochs 3788\n",
      "training loss 0.0011334231255666599\n",
      "epochs 3789\n",
      "training loss 0.001087473975749273\n",
      "testing loss 0.0027504702567631473\n",
      "epochs 3790\n",
      "training loss 0.0011041041762259503\n",
      "epochs 3791\n",
      "training loss 0.0010939543236310292\n",
      "epochs 3792\n",
      "training loss 0.001148745127618143\n",
      "epochs 3793\n",
      "training loss 0.0010693426662206785\n",
      "epochs 3794\n",
      "training loss 0.0011192656109409581\n",
      "epochs 3795\n",
      "training loss 0.0011121563706830099\n",
      "epochs 3796\n",
      "training loss 0.0011138957274064287\n",
      "epochs 3797\n",
      "training loss 0.0011271688521706633\n",
      "epochs 3798\n",
      "training loss 0.0011066263879502141\n",
      "epochs 3799\n",
      "training loss 0.001056817941945103\n",
      "testing loss 0.00272304826814034\n",
      "epochs 3800\n",
      "training loss 0.0011497819550281623\n",
      "epochs 3801\n",
      "training loss 0.0010535015870030053\n",
      "epochs 3802\n",
      "training loss 0.0010981652584906361\n",
      "epochs 3803\n",
      "training loss 0.0011505235312435787\n",
      "epochs 3804\n",
      "training loss 0.0011034760520493437\n",
      "epochs 3805\n",
      "training loss 0.001171158365360362\n",
      "epochs 3806\n",
      "training loss 0.0011109803558403688\n",
      "epochs 3807\n",
      "training loss 0.001115080650675034\n",
      "epochs 3808\n",
      "training loss 0.0010969808881063478\n",
      "epochs 3809\n",
      "training loss 0.0010953121270140843\n",
      "testing loss 0.0028066861976585385\n",
      "epochs 3810\n",
      "training loss 0.001073446888391687\n",
      "epochs 3811\n",
      "training loss 0.0011010657458801084\n",
      "epochs 3812\n",
      "training loss 0.0010942857086907586\n",
      "epochs 3813\n",
      "training loss 0.0011271863882458205\n",
      "epochs 3814\n",
      "training loss 0.0011352255137560827\n",
      "epochs 3815\n",
      "training loss 0.0011287436310674622\n",
      "epochs 3816\n",
      "training loss 0.001090027802548793\n",
      "epochs 3817\n",
      "training loss 0.001065438699103201\n",
      "epochs 3818\n",
      "training loss 0.0010978552786347466\n",
      "epochs 3819\n",
      "training loss 0.001092739711093821\n",
      "testing loss 0.002829398264685738\n",
      "epochs 3820\n",
      "training loss 0.001113546660133159\n",
      "epochs 3821\n",
      "training loss 0.0010930976629084524\n",
      "epochs 3822\n",
      "training loss 0.0010701246297950813\n",
      "epochs 3823\n",
      "training loss 0.0010973780962998064\n",
      "epochs 3824\n",
      "training loss 0.001113254520129186\n",
      "epochs 3825\n",
      "training loss 0.0010756210639498176\n",
      "epochs 3826\n",
      "training loss 0.0011232655612528212\n",
      "epochs 3827\n",
      "training loss 0.0011142978844895257\n",
      "epochs 3828\n",
      "training loss 0.001093515882652117\n",
      "epochs 3829\n",
      "training loss 0.0010894315584780688\n",
      "testing loss 0.002821973038685396\n",
      "epochs 3830\n",
      "training loss 0.0010827058352286558\n",
      "epochs 3831\n",
      "training loss 0.0010920678934869341\n",
      "epochs 3832\n",
      "training loss 0.0011146082238529139\n",
      "epochs 3833\n",
      "training loss 0.0010851282257682963\n",
      "epochs 3834\n",
      "training loss 0.0011069153608558955\n",
      "epochs 3835\n",
      "training loss 0.001109137725457266\n",
      "epochs 3836\n",
      "training loss 0.0010585196345965278\n",
      "epochs 3837\n",
      "training loss 0.0010869999819203056\n",
      "epochs 3838\n",
      "training loss 0.00109756031087076\n",
      "epochs 3839\n",
      "training loss 0.0011573589173723068\n",
      "testing loss 0.0027190982393015518\n",
      "epochs 3840\n",
      "training loss 0.001063334238099778\n",
      "epochs 3841\n",
      "training loss 0.0011409550946273792\n",
      "epochs 3842\n",
      "training loss 0.0010475192413988061\n",
      "epochs 3843\n",
      "training loss 0.0011326306781556039\n",
      "epochs 3844\n",
      "training loss 0.0010855547617767062\n",
      "epochs 3845\n",
      "training loss 0.001121511614575733\n",
      "epochs 3846\n",
      "training loss 0.0010971256295936663\n",
      "epochs 3847\n",
      "training loss 0.0010768625054417863\n",
      "epochs 3848\n",
      "training loss 0.0010946364016208435\n",
      "epochs 3849\n",
      "training loss 0.0011228147901574824\n",
      "testing loss 0.002637312726484255\n",
      "epochs 3850\n",
      "training loss 0.001039782451610095\n",
      "epochs 3851\n",
      "training loss 0.0011213762715025762\n",
      "epochs 3852\n",
      "training loss 0.001051399324267627\n",
      "epochs 3853\n",
      "training loss 0.001139233455148251\n",
      "epochs 3854\n",
      "training loss 0.0010896109051274922\n",
      "epochs 3855\n",
      "training loss 0.0010992665649504942\n",
      "epochs 3856\n",
      "training loss 0.0010744198547438422\n",
      "epochs 3857\n",
      "training loss 0.0011043817782916106\n",
      "epochs 3858\n",
      "training loss 0.0010952506252241783\n",
      "epochs 3859\n",
      "training loss 0.0010714093539906286\n",
      "testing loss 0.0031193696116823834\n",
      "epochs 3860\n",
      "training loss 0.0010807016815942087\n",
      "epochs 3861\n",
      "training loss 0.0011271241582409227\n",
      "epochs 3862\n",
      "training loss 0.0010828757800139192\n",
      "epochs 3863\n",
      "training loss 0.001090338995034798\n",
      "epochs 3864\n",
      "training loss 0.0010769184021347153\n",
      "epochs 3865\n",
      "training loss 0.0010702338461207911\n",
      "epochs 3866\n",
      "training loss 0.0010902642011982148\n",
      "epochs 3867\n",
      "training loss 0.0010776810109541725\n",
      "epochs 3868\n",
      "training loss 0.0010454050708621583\n",
      "epochs 3869\n",
      "training loss 0.0010673676240243765\n",
      "testing loss 0.0030969359196973503\n",
      "epochs 3870\n",
      "training loss 0.0010992822537728016\n",
      "epochs 3871\n",
      "training loss 0.0011142822659056418\n",
      "epochs 3872\n",
      "training loss 0.0010537996748037704\n",
      "epochs 3873\n",
      "training loss 0.0011196547725002952\n",
      "epochs 3874\n",
      "training loss 0.001075446032982682\n",
      "epochs 3875\n",
      "training loss 0.001113293039800718\n",
      "epochs 3876\n",
      "training loss 0.0010521592072347083\n",
      "epochs 3877\n",
      "training loss 0.0010887529233210129\n",
      "epochs 3878\n",
      "training loss 0.0010845713385686528\n",
      "epochs 3879\n",
      "training loss 0.0011018242170902587\n",
      "testing loss 0.00382904573888617\n",
      "epochs 3880\n",
      "training loss 0.0010823119570795548\n",
      "epochs 3881\n",
      "training loss 0.0011151586714098767\n",
      "epochs 3882\n",
      "training loss 0.0011205624364030557\n",
      "epochs 3883\n",
      "training loss 0.0010560881737463946\n",
      "epochs 3884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0010541578663490418\n",
      "epochs 3885\n",
      "training loss 0.001083493938295901\n",
      "epochs 3886\n",
      "training loss 0.0010812014247939036\n",
      "epochs 3887\n",
      "training loss 0.0010908181304225308\n",
      "epochs 3888\n",
      "training loss 0.0011069768815765604\n",
      "epochs 3889\n",
      "training loss 0.0010697667256611935\n",
      "testing loss 0.002776616438727905\n",
      "epochs 3890\n",
      "training loss 0.0010933491311497466\n",
      "epochs 3891\n",
      "training loss 0.0010626146425301985\n",
      "epochs 3892\n",
      "training loss 0.001121127646944364\n",
      "epochs 3893\n",
      "training loss 0.0010702787285619684\n",
      "epochs 3894\n",
      "training loss 0.0010404345943199013\n",
      "epochs 3895\n",
      "training loss 0.0011214758996642266\n",
      "epochs 3896\n",
      "training loss 0.0010839636111348912\n",
      "epochs 3897\n",
      "training loss 0.0011332193973380789\n",
      "epochs 3898\n",
      "training loss 0.001063956169618186\n",
      "epochs 3899\n",
      "training loss 0.0012405185121972125\n",
      "testing loss 0.0033824625045246054\n",
      "epochs 3900\n",
      "training loss 0.0011405993805346179\n",
      "epochs 3901\n",
      "training loss 0.001054484461572893\n",
      "epochs 3902\n",
      "training loss 0.0010854994793603689\n",
      "epochs 3903\n",
      "training loss 0.0010863277180251979\n",
      "epochs 3904\n",
      "training loss 0.0010981327508389383\n",
      "epochs 3905\n",
      "training loss 0.0010713289900878189\n",
      "epochs 3906\n",
      "training loss 0.0011017518482485213\n",
      "epochs 3907\n",
      "training loss 0.001094096819169559\n",
      "epochs 3908\n",
      "training loss 0.001074609000803365\n",
      "epochs 3909\n",
      "training loss 0.0010846002875915008\n",
      "testing loss 0.0026816471767981687\n",
      "epochs 3910\n",
      "training loss 0.0010213303561237468\n",
      "epochs 3911\n",
      "training loss 0.001092992599761272\n",
      "epochs 3912\n",
      "training loss 0.0010814648501979794\n",
      "epochs 3913\n",
      "training loss 0.0010706982801818377\n",
      "epochs 3914\n",
      "training loss 0.0010404546846321425\n",
      "epochs 3915\n",
      "training loss 0.0010486300864842887\n",
      "epochs 3916\n",
      "training loss 0.0011071904897151973\n",
      "epochs 3917\n",
      "training loss 0.0011241833207347519\n",
      "epochs 3918\n",
      "training loss 0.0010895338868382627\n",
      "epochs 3919\n",
      "training loss 0.001053088145595225\n",
      "testing loss 0.0026673692904399535\n",
      "epochs 3920\n",
      "training loss 0.0010784060308939034\n",
      "epochs 3921\n",
      "training loss 0.0010976262330813589\n",
      "epochs 3922\n",
      "training loss 0.0010480688560023724\n",
      "epochs 3923\n",
      "training loss 0.0010956052287713848\n",
      "epochs 3924\n",
      "training loss 0.0010928240260406536\n",
      "epochs 3925\n",
      "training loss 0.0010677221840485296\n",
      "epochs 3926\n",
      "training loss 0.0011201617433576304\n",
      "epochs 3927\n",
      "training loss 0.001041314777129791\n",
      "epochs 3928\n",
      "training loss 0.0011152758535159055\n",
      "epochs 3929\n",
      "training loss 0.0010804482194309132\n",
      "testing loss 0.0028550778057406397\n",
      "epochs 3930\n",
      "training loss 0.0011200949751013342\n",
      "epochs 3931\n",
      "training loss 0.0010411660581484642\n",
      "epochs 3932\n",
      "training loss 0.0010865317320728556\n",
      "epochs 3933\n",
      "training loss 0.0010586026859594265\n",
      "epochs 3934\n",
      "training loss 0.0010453214243303498\n",
      "epochs 3935\n",
      "training loss 0.0011138765452398513\n",
      "epochs 3936\n",
      "training loss 0.0010712568007344464\n",
      "epochs 3937\n",
      "training loss 0.0010726467641155408\n",
      "epochs 3938\n",
      "training loss 0.0010314519495185249\n",
      "epochs 3939\n",
      "training loss 0.0010920600563296228\n",
      "testing loss 0.0027806013397350645\n",
      "epochs 3940\n",
      "training loss 0.0010510305548511378\n",
      "epochs 3941\n",
      "training loss 0.0010408377708332178\n",
      "epochs 3942\n",
      "training loss 0.0010910508545585497\n",
      "epochs 3943\n",
      "training loss 0.0010828754458064073\n",
      "epochs 3944\n",
      "training loss 0.0011133755777331244\n",
      "epochs 3945\n",
      "training loss 0.001027457535657522\n",
      "epochs 3946\n",
      "training loss 0.001120135829802376\n",
      "epochs 3947\n",
      "training loss 0.0010634915139611052\n",
      "epochs 3948\n",
      "training loss 0.001065864116781445\n",
      "epochs 3949\n",
      "training loss 0.001053394957822054\n",
      "testing loss 0.002725131895165256\n",
      "epochs 3950\n",
      "training loss 0.0010848941483483918\n",
      "epochs 3951\n",
      "training loss 0.0010913984828287491\n",
      "epochs 3952\n",
      "training loss 0.0011152666039824328\n",
      "epochs 3953\n",
      "training loss 0.0010340065163868765\n",
      "epochs 3954\n",
      "training loss 0.0010505335663422647\n",
      "epochs 3955\n",
      "training loss 0.0010720320512317008\n",
      "epochs 3956\n",
      "training loss 0.0010526525488301637\n",
      "epochs 3957\n",
      "training loss 0.001065489088537562\n",
      "epochs 3958\n",
      "training loss 0.001081668772157225\n",
      "epochs 3959\n",
      "training loss 0.0010513286486672844\n",
      "testing loss 0.00272651229108228\n",
      "epochs 3960\n",
      "training loss 0.001089659900408759\n",
      "epochs 3961\n",
      "training loss 0.0010527691474441993\n",
      "epochs 3962\n",
      "training loss 0.0010896663991437155\n",
      "epochs 3963\n",
      "training loss 0.0010782975322562681\n",
      "epochs 3964\n",
      "training loss 0.0011169152731701482\n",
      "epochs 3965\n",
      "training loss 0.0010731873961624132\n",
      "epochs 3966\n",
      "training loss 0.0010616083818383085\n",
      "epochs 3967\n",
      "training loss 0.001056000167478476\n",
      "epochs 3968\n",
      "training loss 0.001095501129942915\n",
      "epochs 3969\n",
      "training loss 0.0010434260179987885\n",
      "testing loss 0.002777316433324723\n",
      "epochs 3970\n",
      "training loss 0.0010669096369483144\n",
      "epochs 3971\n",
      "training loss 0.0010474456927461668\n",
      "epochs 3972\n",
      "training loss 0.0010170979467064912\n",
      "epochs 3973\n",
      "training loss 0.0010322306942446638\n",
      "epochs 3974\n",
      "training loss 0.0010764314738204166\n",
      "epochs 3975\n",
      "training loss 0.0010568077725890493\n",
      "epochs 3976\n",
      "training loss 0.0010658601807758755\n",
      "epochs 3977\n",
      "training loss 0.0010564980667868578\n",
      "epochs 3978\n",
      "training loss 0.0010441454814953623\n",
      "epochs 3979\n",
      "training loss 0.0010829604549990191\n",
      "testing loss 0.002772855254943674\n",
      "epochs 3980\n",
      "training loss 0.001069382820129791\n",
      "epochs 3981\n",
      "training loss 0.0010670189497855103\n",
      "epochs 3982\n",
      "training loss 0.001013038075942331\n",
      "epochs 3983\n",
      "training loss 0.0010927826331399019\n",
      "epochs 3984\n",
      "training loss 0.001083816184575944\n",
      "epochs 3985\n",
      "training loss 0.0010926139970539326\n",
      "epochs 3986\n",
      "training loss 0.0010367583990872676\n",
      "epochs 3987\n",
      "training loss 0.001078916850003009\n",
      "epochs 3988\n",
      "training loss 0.0010469710083868026\n",
      "epochs 3989\n",
      "training loss 0.0010512549232685865\n",
      "testing loss 0.0026008367669729\n",
      "epochs 3990\n",
      "training loss 0.0010541088740754657\n",
      "epochs 3991\n",
      "training loss 0.001107712440110607\n",
      "epochs 3992\n",
      "training loss 0.0011309869732498263\n",
      "epochs 3993\n",
      "training loss 0.0011047830863213195\n",
      "epochs 3994\n",
      "training loss 0.0011180886937911115\n",
      "epochs 3995\n",
      "training loss 0.0010569060625052675\n",
      "epochs 3996\n",
      "training loss 0.0010598755286267668\n",
      "epochs 3997\n",
      "training loss 0.0010749980450627651\n",
      "epochs 3998\n",
      "training loss 0.0010599860628517656\n",
      "epochs 3999\n",
      "training loss 0.001034524980338013\n",
      "testing loss 0.0027636833611136\n",
      "epochs 4000\n",
      "training loss 0.0010467153789169837\n",
      "epochs 4001\n",
      "training loss 0.0010356783948620742\n",
      "epochs 4002\n",
      "training loss 0.0010411238915635143\n",
      "epochs 4003\n",
      "training loss 0.0010362639071385866\n",
      "epochs 4004\n",
      "training loss 0.0010329062558956852\n",
      "epochs 4005\n",
      "training loss 0.0010509050863803475\n",
      "epochs 4006\n",
      "training loss 0.001046327653529949\n",
      "epochs 4007\n",
      "training loss 0.0010321964193093038\n",
      "epochs 4008\n",
      "training loss 0.0010667582685453393\n",
      "epochs 4009\n",
      "training loss 0.0010548686480579903\n",
      "testing loss 0.002780552526625494\n",
      "epochs 4010\n",
      "training loss 0.0010339137905215035\n",
      "epochs 4011\n",
      "training loss 0.0010925771846891717\n",
      "epochs 4012\n",
      "training loss 0.0010276352385155244\n",
      "epochs 4013\n",
      "training loss 0.0010560658911814896\n",
      "epochs 4014\n",
      "training loss 0.0010767266016997012\n",
      "epochs 4015\n",
      "training loss 0.00106629052788223\n",
      "epochs 4016\n",
      "training loss 0.001057142394277802\n",
      "epochs 4017\n",
      "training loss 0.0011381761800415679\n",
      "epochs 4018\n",
      "training loss 0.0011151228869676385\n",
      "epochs 4019\n",
      "training loss 0.001047736159589504\n",
      "testing loss 0.0026902363363963864\n",
      "epochs 4020\n",
      "training loss 0.0010567545010793196\n",
      "epochs 4021\n",
      "training loss 0.0010325568661802818\n",
      "epochs 4022\n",
      "training loss 0.0010853697474880318\n",
      "epochs 4023\n",
      "training loss 0.001017725792937094\n",
      "epochs 4024\n",
      "training loss 0.0010993702071407487\n",
      "epochs 4025\n",
      "training loss 0.0010518794843932687\n",
      "epochs 4026\n",
      "training loss 0.0010355031354873693\n",
      "epochs 4027\n",
      "training loss 0.001052562429571077\n",
      "epochs 4028\n",
      "training loss 0.001063889370223445\n",
      "epochs 4029\n",
      "training loss 0.0010294028961044485\n",
      "testing loss 0.0029869654867302705\n",
      "epochs 4030\n",
      "training loss 0.001074627294108883\n",
      "epochs 4031\n",
      "training loss 0.001045621060737972\n",
      "epochs 4032\n",
      "training loss 0.0010467686339728762\n",
      "epochs 4033\n",
      "training loss 0.0010699833452058984\n",
      "epochs 4034\n",
      "training loss 0.001047911063879785\n",
      "epochs 4035\n",
      "training loss 0.0010556797549166749\n",
      "epochs 4036\n",
      "training loss 0.0010391011532313987\n",
      "epochs 4037\n",
      "training loss 0.0010361699427238816\n",
      "epochs 4038\n",
      "training loss 0.0010567328308448535\n",
      "epochs 4039\n",
      "training loss 0.001064921910347315\n",
      "testing loss 0.0026203420798373506\n",
      "epochs 4040\n",
      "training loss 0.0010729430021865344\n",
      "epochs 4041\n",
      "training loss 0.0010560706008712571\n",
      "epochs 4042\n",
      "training loss 0.0010731572587478985\n",
      "epochs 4043\n",
      "training loss 0.0010647625256525097\n",
      "epochs 4044\n",
      "training loss 0.001027093956630541\n",
      "epochs 4045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0010850881495283729\n",
      "epochs 4046\n",
      "training loss 0.001193349430653179\n",
      "epochs 4047\n",
      "training loss 0.001168591362221795\n",
      "epochs 4048\n",
      "training loss 0.0010428703768677035\n",
      "epochs 4049\n",
      "training loss 0.0010594301106900792\n",
      "testing loss 0.002864815150668303\n",
      "epochs 4050\n",
      "training loss 0.0010427032189732446\n",
      "epochs 4051\n",
      "training loss 0.0010087838864214378\n",
      "epochs 4052\n",
      "training loss 0.0010602467314419436\n",
      "epochs 4053\n",
      "training loss 0.0010884105203826104\n",
      "epochs 4054\n",
      "training loss 0.0010251229275782285\n",
      "epochs 4055\n",
      "training loss 0.00105560519240689\n",
      "epochs 4056\n",
      "training loss 0.00104397870734685\n",
      "epochs 4057\n",
      "training loss 0.0010070848522147327\n",
      "epochs 4058\n",
      "training loss 0.001107622604635983\n",
      "epochs 4059\n",
      "training loss 0.0010385306169545011\n",
      "testing loss 0.002616605458034744\n",
      "epochs 4060\n",
      "training loss 0.001029376103772745\n",
      "epochs 4061\n",
      "training loss 0.0010205459408391278\n",
      "epochs 4062\n",
      "training loss 0.0010496115085164984\n",
      "epochs 4063\n",
      "training loss 0.001096564537907829\n",
      "epochs 4064\n",
      "training loss 0.0010349486319710555\n",
      "epochs 4065\n",
      "training loss 0.001057278902211869\n",
      "epochs 4066\n",
      "training loss 0.0010930928994340985\n",
      "epochs 4067\n",
      "training loss 0.001061132484848829\n",
      "epochs 4068\n",
      "training loss 0.0010947712127750565\n",
      "epochs 4069\n",
      "training loss 0.0010419765798343871\n",
      "testing loss 0.0027404255134628173\n",
      "epochs 4070\n",
      "training loss 0.0010571458833759204\n",
      "epochs 4071\n",
      "training loss 0.0010331784533181066\n",
      "epochs 4072\n",
      "training loss 0.0010721547255578441\n",
      "epochs 4073\n",
      "training loss 0.0010245599390126447\n",
      "epochs 4074\n",
      "training loss 0.0010207227281207008\n",
      "epochs 4075\n",
      "training loss 0.001061036843646567\n",
      "epochs 4076\n",
      "training loss 0.0010545701628882087\n",
      "epochs 4077\n",
      "training loss 0.001056107162713076\n",
      "epochs 4078\n",
      "training loss 0.0010655386225493077\n",
      "epochs 4079\n",
      "training loss 0.0010798688403664775\n",
      "testing loss 0.002899349672911385\n",
      "epochs 4080\n",
      "training loss 0.0010426862531208567\n",
      "epochs 4081\n",
      "training loss 0.0010198625922769218\n",
      "epochs 4082\n",
      "training loss 0.0010818246407731575\n",
      "epochs 4083\n",
      "training loss 0.0010294764437840114\n",
      "epochs 4084\n",
      "training loss 0.0010364229955758593\n",
      "epochs 4085\n",
      "training loss 0.0010586363899643253\n",
      "epochs 4086\n",
      "training loss 0.001075005725289392\n",
      "epochs 4087\n",
      "training loss 0.0010785655200617292\n",
      "epochs 4088\n",
      "training loss 0.001048258852884513\n",
      "epochs 4089\n",
      "training loss 0.001046600195332954\n",
      "testing loss 0.0027687170688089325\n",
      "epochs 4090\n",
      "training loss 0.0010346082943430777\n",
      "epochs 4091\n",
      "training loss 0.001038411229857354\n",
      "epochs 4092\n",
      "training loss 0.0010431451780656437\n",
      "epochs 4093\n",
      "training loss 0.0010545441129253357\n",
      "epochs 4094\n",
      "training loss 0.0010711877984716628\n",
      "epochs 4095\n",
      "training loss 0.0010988296104784726\n",
      "epochs 4096\n",
      "training loss 0.0010377511029672386\n",
      "epochs 4097\n",
      "training loss 0.0010553174788389909\n",
      "epochs 4098\n",
      "training loss 0.0010545595230939473\n",
      "epochs 4099\n",
      "training loss 0.0011230141936807035\n",
      "testing loss 0.0028151615166950787\n",
      "epochs 4100\n",
      "training loss 0.0010269559735126544\n",
      "epochs 4101\n",
      "training loss 0.0010295193316839568\n",
      "epochs 4102\n",
      "training loss 0.0010291949094281643\n",
      "epochs 4103\n",
      "training loss 0.0010816403943302228\n",
      "epochs 4104\n",
      "training loss 0.0010773930038710472\n",
      "epochs 4105\n",
      "training loss 0.0011106722386296285\n",
      "epochs 4106\n",
      "training loss 0.0010978246281435854\n",
      "epochs 4107\n",
      "training loss 0.0010507972061769941\n",
      "epochs 4108\n",
      "training loss 0.0010253758260649049\n",
      "epochs 4109\n",
      "training loss 0.0010474037924303042\n",
      "testing loss 0.002750389027765932\n",
      "epochs 4110\n",
      "training loss 0.001088837220929393\n",
      "epochs 4111\n",
      "training loss 0.0010460956879512133\n",
      "epochs 4112\n",
      "training loss 0.0010236507914337615\n",
      "epochs 4113\n",
      "training loss 0.001001593936199328\n",
      "epochs 4114\n",
      "training loss 0.0010384025201158977\n",
      "epochs 4115\n",
      "training loss 0.0010259905458487413\n",
      "epochs 4116\n",
      "training loss 0.0010536611209323455\n",
      "epochs 4117\n",
      "training loss 0.001067378904633677\n",
      "epochs 4118\n",
      "training loss 0.0010139582279537404\n",
      "epochs 4119\n",
      "training loss 0.0010480947479730228\n",
      "testing loss 0.0034475850250162447\n",
      "epochs 4120\n",
      "training loss 0.0010356976608901446\n",
      "epochs 4121\n",
      "training loss 0.0010472795587050019\n",
      "epochs 4122\n",
      "training loss 0.001052240347121715\n",
      "epochs 4123\n",
      "training loss 0.0010655538848103067\n",
      "epochs 4124\n",
      "training loss 0.0010647862805704669\n",
      "epochs 4125\n",
      "training loss 0.0011590532139354134\n",
      "epochs 4126\n",
      "training loss 0.0010834264835784364\n",
      "epochs 4127\n",
      "training loss 0.0010688173818886053\n",
      "epochs 4128\n",
      "training loss 0.0010322278424231251\n",
      "epochs 4129\n",
      "training loss 0.001060319476156555\n",
      "testing loss 0.0027134174981372472\n",
      "epochs 4130\n",
      "training loss 0.001011131936401174\n",
      "epochs 4131\n",
      "training loss 0.0010646546132491738\n",
      "epochs 4132\n",
      "training loss 0.0010501997965376678\n",
      "epochs 4133\n",
      "training loss 0.0010084319227420383\n",
      "epochs 4134\n",
      "training loss 0.0010705025651489347\n",
      "epochs 4135\n",
      "training loss 0.0011113957145624995\n",
      "epochs 4136\n",
      "training loss 0.0010455902908237985\n",
      "epochs 4137\n",
      "training loss 0.0010380033783311743\n",
      "epochs 4138\n",
      "training loss 0.0010328266218001742\n",
      "epochs 4139\n",
      "training loss 0.0010488237769875144\n",
      "testing loss 0.002878987265863053\n",
      "epochs 4140\n",
      "training loss 0.0010414492405418838\n",
      "epochs 4141\n",
      "training loss 0.0010187919775312795\n",
      "epochs 4142\n",
      "training loss 0.0010180468476408567\n",
      "epochs 4143\n",
      "training loss 0.001027401908595217\n",
      "epochs 4144\n",
      "training loss 0.0010487285331542686\n",
      "epochs 4145\n",
      "training loss 0.001011340771511616\n",
      "epochs 4146\n",
      "training loss 0.0010518678065918748\n",
      "epochs 4147\n",
      "training loss 0.0009870192989293702\n",
      "epochs 4148\n",
      "training loss 0.0010285377606825965\n",
      "epochs 4149\n",
      "training loss 0.0010486435271462704\n",
      "testing loss 0.0027294738006824297\n",
      "epochs 4150\n",
      "training loss 0.0010556925001828775\n",
      "epochs 4151\n",
      "training loss 0.0010360557928123712\n",
      "epochs 4152\n",
      "training loss 0.0011193275092152657\n",
      "epochs 4153\n",
      "training loss 0.0010842838475698645\n",
      "epochs 4154\n",
      "training loss 0.001089777080922481\n",
      "epochs 4155\n",
      "training loss 0.0010192833814662414\n",
      "epochs 4156\n",
      "training loss 0.0010160244635085498\n",
      "epochs 4157\n",
      "training loss 0.0010216087598274362\n",
      "epochs 4158\n",
      "training loss 0.0009957361689663994\n",
      "epochs 4159\n",
      "training loss 0.0010547584537905004\n",
      "testing loss 0.002850265690895022\n",
      "epochs 4160\n",
      "training loss 0.0010554108734732342\n",
      "epochs 4161\n",
      "training loss 0.0010355382789376455\n",
      "epochs 4162\n",
      "training loss 0.0010161313287047585\n",
      "epochs 4163\n",
      "training loss 0.0009802162874947191\n",
      "epochs 4164\n",
      "training loss 0.0010405626317123828\n",
      "epochs 4165\n",
      "training loss 0.001053171892265441\n",
      "epochs 4166\n",
      "training loss 0.0010179765508326172\n",
      "epochs 4167\n",
      "training loss 0.0010379151569249103\n",
      "epochs 4168\n",
      "training loss 0.0010518246782535976\n",
      "epochs 4169\n",
      "training loss 0.0010590254041468188\n",
      "testing loss 0.0026822400408143057\n",
      "epochs 4170\n",
      "training loss 0.0010457238027440968\n",
      "epochs 4171\n",
      "training loss 0.0010071254068543983\n",
      "epochs 4172\n",
      "training loss 0.0010564025207691597\n",
      "epochs 4173\n",
      "training loss 0.0009941104781816751\n",
      "epochs 4174\n",
      "training loss 0.0010597111155154428\n",
      "epochs 4175\n",
      "training loss 0.0010167815792232774\n",
      "epochs 4176\n",
      "training loss 0.001016696680253683\n",
      "epochs 4177\n",
      "training loss 0.0010325556985770654\n",
      "epochs 4178\n",
      "training loss 0.001109176036536603\n",
      "epochs 4179\n",
      "training loss 0.0010610863739429297\n",
      "testing loss 0.0029351244406470803\n",
      "epochs 4180\n",
      "training loss 0.0010177022811113136\n",
      "epochs 4181\n",
      "training loss 0.00105718772477643\n",
      "epochs 4182\n",
      "training loss 0.0010165052248184826\n",
      "epochs 4183\n",
      "training loss 0.0010232813293637997\n",
      "epochs 4184\n",
      "training loss 0.001070653366336831\n",
      "epochs 4185\n",
      "training loss 0.0010383801891625574\n",
      "epochs 4186\n",
      "training loss 0.0010046809235882115\n",
      "epochs 4187\n",
      "training loss 0.0010147441366452463\n",
      "epochs 4188\n",
      "training loss 0.001018359718595187\n",
      "epochs 4189\n",
      "training loss 0.0010502431651373538\n",
      "testing loss 0.0030158016292365095\n",
      "epochs 4190\n",
      "training loss 0.001029305005061092\n",
      "epochs 4191\n",
      "training loss 0.0010059288820382039\n",
      "epochs 4192\n",
      "training loss 0.001042233816295647\n",
      "epochs 4193\n",
      "training loss 0.0010598063181256339\n",
      "epochs 4194\n",
      "training loss 0.001044314678604106\n",
      "epochs 4195\n",
      "training loss 0.0010346844645519953\n",
      "epochs 4196\n",
      "training loss 0.0010300990088636163\n",
      "epochs 4197\n",
      "training loss 0.0010114325253621286\n",
      "epochs 4198\n",
      "training loss 0.0010440661194189352\n",
      "epochs 4199\n",
      "training loss 0.001051849492409445\n",
      "testing loss 0.0029168987323873485\n",
      "epochs 4200\n",
      "training loss 0.001035790469909319\n",
      "epochs 4201\n",
      "training loss 0.0010499455007132834\n",
      "epochs 4202\n",
      "training loss 0.0010191241985520966\n",
      "epochs 4203\n",
      "training loss 0.001005206913842981\n",
      "epochs 4204\n",
      "training loss 0.0009996074166553702\n",
      "epochs 4205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0010070291037832621\n",
      "epochs 4206\n",
      "training loss 0.0010232838275162984\n",
      "epochs 4207\n",
      "training loss 0.0010168032023962304\n",
      "epochs 4208\n",
      "training loss 0.0010090439614350128\n",
      "epochs 4209\n",
      "training loss 0.0010586078059335881\n",
      "testing loss 0.003404637743349035\n",
      "epochs 4210\n",
      "training loss 0.0010501063905803537\n",
      "epochs 4211\n",
      "training loss 0.001019056662892368\n",
      "epochs 4212\n",
      "training loss 0.0009975447910370752\n",
      "epochs 4213\n",
      "training loss 0.0009867517804121174\n",
      "epochs 4214\n",
      "training loss 0.0010180278432818759\n",
      "epochs 4215\n",
      "training loss 0.0010536185134553257\n",
      "epochs 4216\n",
      "training loss 0.001048028145847406\n",
      "epochs 4217\n",
      "training loss 0.0010567779320512975\n",
      "epochs 4218\n",
      "training loss 0.001051482985484043\n",
      "epochs 4219\n",
      "training loss 0.00100840904836976\n",
      "testing loss 0.002865453309326677\n",
      "epochs 4220\n",
      "training loss 0.0010589733002403054\n",
      "epochs 4221\n",
      "training loss 0.0010249182950962889\n",
      "epochs 4222\n",
      "training loss 0.0010354377875687684\n",
      "epochs 4223\n",
      "training loss 0.0010455471033932453\n",
      "epochs 4224\n",
      "training loss 0.001044556335050692\n",
      "epochs 4225\n",
      "training loss 0.0010399912808820785\n",
      "epochs 4226\n",
      "training loss 0.0010468539066038074\n",
      "epochs 4227\n",
      "training loss 0.0010392264035561528\n",
      "epochs 4228\n",
      "training loss 0.0010352716634352904\n",
      "epochs 4229\n",
      "training loss 0.0009986181811735284\n",
      "testing loss 0.0026348519868858106\n",
      "epochs 4230\n",
      "training loss 0.0010055356662887456\n",
      "epochs 4231\n",
      "training loss 0.0010078256708459426\n",
      "epochs 4232\n",
      "training loss 0.0009743704185976808\n",
      "epochs 4233\n",
      "training loss 0.0010157808306553375\n",
      "epochs 4234\n",
      "training loss 0.0010128163061696345\n",
      "epochs 4235\n",
      "training loss 0.0010144805725335532\n",
      "epochs 4236\n",
      "training loss 0.0009935985610885461\n",
      "epochs 4237\n",
      "training loss 0.0009990632010335075\n",
      "epochs 4238\n",
      "training loss 0.0010360401041669867\n",
      "epochs 4239\n",
      "training loss 0.0010705487774317025\n",
      "testing loss 0.003171859427143866\n",
      "epochs 4240\n",
      "training loss 0.0010357360215079163\n",
      "epochs 4241\n",
      "training loss 0.0010276485439198867\n",
      "epochs 4242\n",
      "training loss 0.000986783314987417\n",
      "epochs 4243\n",
      "training loss 0.0011311606626099933\n",
      "epochs 4244\n",
      "training loss 0.001001279235861369\n",
      "epochs 4245\n",
      "training loss 0.001072639947715371\n",
      "epochs 4246\n",
      "training loss 0.0010282795847603048\n",
      "epochs 4247\n",
      "training loss 0.000988291126501104\n",
      "epochs 4248\n",
      "training loss 0.0010427984879295537\n",
      "epochs 4249\n",
      "training loss 0.0010041235928426534\n",
      "testing loss 0.002624419386538261\n",
      "epochs 4250\n",
      "training loss 0.0010336524708547535\n",
      "epochs 4251\n",
      "training loss 0.0010514622474567664\n",
      "epochs 4252\n",
      "training loss 0.0010415107396779452\n",
      "epochs 4253\n",
      "training loss 0.001023488199567609\n",
      "epochs 4254\n",
      "training loss 0.0010508438547518343\n",
      "epochs 4255\n",
      "training loss 0.0009973637165578144\n",
      "epochs 4256\n",
      "training loss 0.0010419135814529806\n",
      "epochs 4257\n",
      "training loss 0.0009983115248158462\n",
      "epochs 4258\n",
      "training loss 0.0010380603489466012\n",
      "epochs 4259\n",
      "training loss 0.0009977834396160527\n",
      "testing loss 0.0028374686148612107\n",
      "epochs 4260\n",
      "training loss 0.0009965766062938798\n",
      "epochs 4261\n",
      "training loss 0.0010298586786316997\n",
      "epochs 4262\n",
      "training loss 0.0010840049498972105\n",
      "epochs 4263\n",
      "training loss 0.0010652945909116948\n",
      "epochs 4264\n",
      "training loss 0.0010385206174445098\n",
      "epochs 4265\n",
      "training loss 0.0010557252523421359\n",
      "epochs 4266\n",
      "training loss 0.0010006612688331301\n",
      "epochs 4267\n",
      "training loss 0.001022843162792432\n",
      "epochs 4268\n",
      "training loss 0.0010205200341838697\n",
      "epochs 4269\n",
      "training loss 0.001044811255059355\n",
      "testing loss 0.003072739101976077\n",
      "epochs 4270\n",
      "training loss 0.001009825822128702\n",
      "epochs 4271\n",
      "training loss 0.001057322740718994\n",
      "epochs 4272\n",
      "training loss 0.0010913214830279713\n",
      "epochs 4273\n",
      "training loss 0.001079538555060001\n",
      "epochs 4274\n",
      "training loss 0.0010409605101785568\n",
      "epochs 4275\n",
      "training loss 0.0010634661900901482\n",
      "epochs 4276\n",
      "training loss 0.0010090547631138025\n",
      "epochs 4277\n",
      "training loss 0.001042940682522092\n",
      "epochs 4278\n",
      "training loss 0.0009775149928247357\n",
      "epochs 4279\n",
      "training loss 0.001061977453970019\n",
      "testing loss 0.0028653789496590905\n",
      "epochs 4280\n",
      "training loss 0.0010610520015230068\n",
      "epochs 4281\n",
      "training loss 0.0010542201635846727\n",
      "epochs 4282\n",
      "training loss 0.0010401200013741918\n",
      "epochs 4283\n",
      "training loss 0.0010022364045835827\n",
      "epochs 4284\n",
      "training loss 0.001011424470235704\n",
      "epochs 4285\n",
      "training loss 0.0010664384409793763\n",
      "epochs 4286\n",
      "training loss 0.001031896463378777\n",
      "epochs 4287\n",
      "training loss 0.0010407574385996217\n",
      "epochs 4288\n",
      "training loss 0.0010597633019565269\n",
      "epochs 4289\n",
      "training loss 0.0010317862913151211\n",
      "testing loss 0.002735164982071873\n",
      "epochs 4290\n",
      "training loss 0.0010432145308128537\n",
      "epochs 4291\n",
      "training loss 0.0009782654608353565\n",
      "epochs 4292\n",
      "training loss 0.0010453157270564994\n",
      "epochs 4293\n",
      "training loss 0.0009880668568951402\n",
      "epochs 4294\n",
      "training loss 0.0010328917365342854\n",
      "epochs 4295\n",
      "training loss 0.0010492823981555423\n",
      "epochs 4296\n",
      "training loss 0.0010037694480567378\n",
      "epochs 4297\n",
      "training loss 0.0010342657593316695\n",
      "epochs 4298\n",
      "training loss 0.001033879940908991\n",
      "epochs 4299\n",
      "training loss 0.0010050283741392762\n",
      "testing loss 0.0029475248801845606\n",
      "epochs 4300\n",
      "training loss 0.0010145607897672111\n",
      "epochs 4301\n",
      "training loss 0.0010146303411989294\n",
      "epochs 4302\n",
      "training loss 0.001005040924170969\n",
      "epochs 4303\n",
      "training loss 0.001027829083118667\n",
      "epochs 4304\n",
      "training loss 0.0010212401349570229\n",
      "epochs 4305\n",
      "training loss 0.000987393938208078\n",
      "epochs 4306\n",
      "training loss 0.001003867519649997\n",
      "epochs 4307\n",
      "training loss 0.001060040272760989\n",
      "epochs 4308\n",
      "training loss 0.0010354706217318287\n",
      "epochs 4309\n",
      "training loss 0.000960557138417086\n",
      "testing loss 0.0029787367910760988\n",
      "epochs 4310\n",
      "training loss 0.0010063780533959437\n",
      "epochs 4311\n",
      "training loss 0.0010061389576441306\n",
      "epochs 4312\n",
      "training loss 0.0010473215130708353\n",
      "epochs 4313\n",
      "training loss 0.0010256135760520594\n",
      "epochs 4314\n",
      "training loss 0.00100593829911774\n",
      "epochs 4315\n",
      "training loss 0.0010510060358028235\n",
      "epochs 4316\n",
      "training loss 0.0010146269856776623\n",
      "epochs 4317\n",
      "training loss 0.0009942799230710543\n",
      "epochs 4318\n",
      "training loss 0.0009767583686905122\n",
      "epochs 4319\n",
      "training loss 0.0010037330562379042\n",
      "testing loss 0.0027891494125682624\n",
      "epochs 4320\n",
      "training loss 0.0010204542312193606\n",
      "epochs 4321\n",
      "training loss 0.0009526175549069821\n",
      "epochs 4322\n",
      "training loss 0.0010329491567110081\n",
      "epochs 4323\n",
      "training loss 0.0009938208678224915\n",
      "epochs 4324\n",
      "training loss 0.0010723713127256437\n",
      "epochs 4325\n",
      "training loss 0.0009923403239440977\n",
      "epochs 4326\n",
      "training loss 0.0010019246917670893\n",
      "epochs 4327\n",
      "training loss 0.0010325960078245554\n",
      "epochs 4328\n",
      "training loss 0.0010277359982765644\n",
      "epochs 4329\n",
      "training loss 0.0010078281481215295\n",
      "testing loss 0.003173276207802804\n",
      "epochs 4330\n",
      "training loss 0.0010155187634889908\n",
      "epochs 4331\n",
      "training loss 0.000993228909887917\n",
      "epochs 4332\n",
      "training loss 0.0010219331742986177\n",
      "epochs 4333\n",
      "training loss 0.001001596078559711\n",
      "epochs 4334\n",
      "training loss 0.001055687709197\n",
      "epochs 4335\n",
      "training loss 0.000981740166976663\n",
      "epochs 4336\n",
      "training loss 0.0010584703665000236\n",
      "epochs 4337\n",
      "training loss 0.0010474603967266978\n",
      "epochs 4338\n",
      "training loss 0.0010046532298339174\n",
      "epochs 4339\n",
      "training loss 0.001027276111401527\n",
      "testing loss 0.002616499305425331\n",
      "epochs 4340\n",
      "training loss 0.0010108806336713292\n",
      "epochs 4341\n",
      "training loss 0.0010204361687983332\n",
      "epochs 4342\n",
      "training loss 0.0010212763942605433\n",
      "epochs 4343\n",
      "training loss 0.0010466689327420344\n",
      "epochs 4344\n",
      "training loss 0.0010591522554476323\n",
      "epochs 4345\n",
      "training loss 0.001031729773622527\n",
      "epochs 4346\n",
      "training loss 0.0010230497385964002\n",
      "epochs 4347\n",
      "training loss 0.0010094184836791688\n",
      "epochs 4348\n",
      "training loss 0.0010016100202675762\n",
      "epochs 4349\n",
      "training loss 0.001040759903136754\n",
      "testing loss 0.0027323897283174203\n",
      "epochs 4350\n",
      "training loss 0.001042698329883574\n",
      "epochs 4351\n",
      "training loss 0.0010370032778321157\n",
      "epochs 4352\n",
      "training loss 0.001048137810142105\n",
      "epochs 4353\n",
      "training loss 0.0010175917130940252\n",
      "epochs 4354\n",
      "training loss 0.0010204170789855523\n",
      "epochs 4355\n",
      "training loss 0.0010056786131578013\n",
      "epochs 4356\n",
      "training loss 0.0009799137921001773\n",
      "epochs 4357\n",
      "training loss 0.0010454016118409469\n",
      "epochs 4358\n",
      "training loss 0.0010346879414424278\n",
      "epochs 4359\n",
      "training loss 0.000997810941939681\n",
      "testing loss 0.003066035354153273\n",
      "epochs 4360\n",
      "training loss 0.0010288345067879315\n",
      "epochs 4361\n",
      "training loss 0.001037570669656863\n",
      "epochs 4362\n",
      "training loss 0.0009931915856849708\n",
      "epochs 4363\n",
      "training loss 0.0010094248758180289\n",
      "epochs 4364\n",
      "training loss 0.0010149235025792747\n",
      "epochs 4365\n",
      "training loss 0.0009933485169887916\n",
      "epochs 4366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0010000102138458143\n",
      "epochs 4367\n",
      "training loss 0.000978640587802263\n",
      "epochs 4368\n",
      "training loss 0.001027601809802785\n",
      "epochs 4369\n",
      "training loss 0.0009896136277339684\n",
      "testing loss 0.0026867621147291776\n",
      "epochs 4370\n",
      "training loss 0.0010445643392232979\n",
      "epochs 4371\n",
      "training loss 0.0010266371236230872\n",
      "epochs 4372\n",
      "training loss 0.0010059321756364275\n",
      "epochs 4373\n",
      "training loss 0.0010028688054309596\n",
      "epochs 4374\n",
      "training loss 0.0009728913318569627\n",
      "epochs 4375\n",
      "training loss 0.0010303473353066048\n",
      "epochs 4376\n",
      "training loss 0.001015633455404055\n",
      "epochs 4377\n",
      "training loss 0.001010856633361197\n",
      "epochs 4378\n",
      "training loss 0.001024592845802589\n",
      "epochs 4379\n",
      "training loss 0.0009652831578219599\n",
      "testing loss 0.00288767302623481\n",
      "epochs 4380\n",
      "training loss 0.0010351225401849507\n",
      "epochs 4381\n",
      "training loss 0.0010124729848240352\n",
      "epochs 4382\n",
      "training loss 0.0010240381648325245\n",
      "epochs 4383\n",
      "training loss 0.0009871840664174261\n",
      "epochs 4384\n",
      "training loss 0.0009996063135405803\n",
      "epochs 4385\n",
      "training loss 0.001041470736684136\n",
      "epochs 4386\n",
      "training loss 0.0010169745240845975\n",
      "epochs 4387\n",
      "training loss 0.0010068900110044355\n",
      "epochs 4388\n",
      "training loss 0.000989881707362457\n",
      "epochs 4389\n",
      "training loss 0.0009692856554709104\n",
      "testing loss 0.0031139375629557417\n",
      "epochs 4390\n",
      "training loss 0.0009987295257057826\n",
      "epochs 4391\n",
      "training loss 0.0010210677295269326\n",
      "epochs 4392\n",
      "training loss 0.0009924111803597275\n",
      "epochs 4393\n",
      "training loss 0.0010267549407056967\n",
      "epochs 4394\n",
      "training loss 0.0010407642616344032\n",
      "epochs 4395\n",
      "training loss 0.0010652765635214179\n",
      "epochs 4396\n",
      "training loss 0.0010266880175533299\n",
      "epochs 4397\n",
      "training loss 0.0010153656469100356\n",
      "epochs 4398\n",
      "training loss 0.000958539462028394\n",
      "epochs 4399\n",
      "training loss 0.0009782149397702972\n",
      "testing loss 0.0032393809635826248\n",
      "epochs 4400\n",
      "training loss 0.0010178421044051307\n",
      "epochs 4401\n",
      "training loss 0.0009961472933908138\n",
      "epochs 4402\n",
      "training loss 0.0010046499139433982\n",
      "epochs 4403\n",
      "training loss 0.0010100884109524597\n",
      "epochs 4404\n",
      "training loss 0.0010095496554391905\n",
      "epochs 4405\n",
      "training loss 0.0010402427240003094\n",
      "epochs 4406\n",
      "training loss 0.000984192396019441\n",
      "epochs 4407\n",
      "training loss 0.0010532616171632018\n",
      "epochs 4408\n",
      "training loss 0.001009444861038019\n",
      "epochs 4409\n",
      "training loss 0.0009929639894846476\n",
      "testing loss 0.0027770620464439774\n",
      "epochs 4410\n",
      "training loss 0.0009788149289466938\n",
      "epochs 4411\n",
      "training loss 0.0009879730864764845\n",
      "epochs 4412\n",
      "training loss 0.0010335719583366395\n",
      "epochs 4413\n",
      "training loss 0.000984568049420679\n",
      "epochs 4414\n",
      "training loss 0.0009798032702597872\n",
      "epochs 4415\n",
      "training loss 0.0010444442076342447\n",
      "epochs 4416\n",
      "training loss 0.0009970602223445806\n",
      "epochs 4417\n",
      "training loss 0.0010379596511970663\n",
      "epochs 4418\n",
      "training loss 0.0010053520838377607\n",
      "epochs 4419\n",
      "training loss 0.001012464972689895\n",
      "testing loss 0.0027265743837949444\n",
      "epochs 4420\n",
      "training loss 0.0010233214822113741\n",
      "epochs 4421\n",
      "training loss 0.0009928889486635823\n",
      "epochs 4422\n",
      "training loss 0.0009732863766362035\n",
      "epochs 4423\n",
      "training loss 0.0010225808919877334\n",
      "epochs 4424\n",
      "training loss 0.0010103609491285434\n",
      "epochs 4425\n",
      "training loss 0.0010344631737982714\n",
      "epochs 4426\n",
      "training loss 0.000993506142715315\n",
      "epochs 4427\n",
      "training loss 0.0009864841950179776\n",
      "epochs 4428\n",
      "training loss 0.0010098889958407831\n",
      "epochs 4429\n",
      "training loss 0.0009788132027976249\n",
      "testing loss 0.0026519982741174705\n",
      "epochs 4430\n",
      "training loss 0.0010119198625454379\n",
      "epochs 4431\n",
      "training loss 0.0009979263687928102\n",
      "epochs 4432\n",
      "training loss 0.001029973531546677\n",
      "epochs 4433\n",
      "training loss 0.000969489999391467\n",
      "epochs 4434\n",
      "training loss 0.0009650475815990176\n",
      "epochs 4435\n",
      "training loss 0.001018901295676165\n",
      "epochs 4436\n",
      "training loss 0.001020557042402371\n",
      "epochs 4437\n",
      "training loss 0.0009760258508432871\n",
      "epochs 4438\n",
      "training loss 0.0009808178383702735\n",
      "epochs 4439\n",
      "training loss 0.0009816616473190124\n",
      "testing loss 0.003591202869879227\n",
      "epochs 4440\n",
      "training loss 0.0010279910982158998\n",
      "epochs 4441\n",
      "training loss 0.0009767650946830384\n",
      "epochs 4442\n",
      "training loss 0.0009895639690799827\n",
      "epochs 4443\n",
      "training loss 0.0010132978227637753\n",
      "epochs 4444\n",
      "training loss 0.000972155716353716\n",
      "epochs 4445\n",
      "training loss 0.0009811447243670448\n",
      "epochs 4446\n",
      "training loss 0.000998334515426523\n",
      "epochs 4447\n",
      "training loss 0.0010308990758167167\n",
      "epochs 4448\n",
      "training loss 0.0010218143205357982\n",
      "epochs 4449\n",
      "training loss 0.0010147768148506982\n",
      "testing loss 0.002842537556355489\n",
      "epochs 4450\n",
      "training loss 0.001038976137506518\n",
      "epochs 4451\n",
      "training loss 0.0011604816837188163\n",
      "epochs 4452\n",
      "training loss 0.0010466390230273969\n",
      "epochs 4453\n",
      "training loss 0.0009870842178097408\n",
      "epochs 4454\n",
      "training loss 0.0009772147103828466\n",
      "epochs 4455\n",
      "training loss 0.0011335658666742881\n",
      "epochs 4456\n",
      "training loss 0.00105914762201167\n",
      "epochs 4457\n",
      "training loss 0.0010074080407865753\n",
      "epochs 4458\n",
      "training loss 0.000980698353346256\n",
      "epochs 4459\n",
      "training loss 0.0010097231953878998\n",
      "testing loss 0.0028201513003115377\n",
      "epochs 4460\n",
      "training loss 0.0010328386184167808\n",
      "epochs 4461\n",
      "training loss 0.0010187531271906949\n",
      "epochs 4462\n",
      "training loss 0.0009845744941941262\n",
      "epochs 4463\n",
      "training loss 0.0009607247384420758\n",
      "epochs 4464\n",
      "training loss 0.0010146064208579847\n",
      "epochs 4465\n",
      "training loss 0.0009805003935145639\n",
      "epochs 4466\n",
      "training loss 0.001010438282694526\n",
      "epochs 4467\n",
      "training loss 0.0009884842486392513\n",
      "epochs 4468\n",
      "training loss 0.0010197839321424238\n",
      "epochs 4469\n",
      "training loss 0.0010415749055741066\n",
      "testing loss 0.0026958167164071287\n",
      "epochs 4470\n",
      "training loss 0.000997465011245489\n",
      "epochs 4471\n",
      "training loss 0.0009741207303785036\n",
      "epochs 4472\n",
      "training loss 0.0010245416189917564\n",
      "epochs 4473\n",
      "training loss 0.00099128860103442\n",
      "epochs 4474\n",
      "training loss 0.0010043998856782506\n",
      "epochs 4475\n",
      "training loss 0.0010267865846193987\n",
      "epochs 4476\n",
      "training loss 0.0009816043867268188\n",
      "epochs 4477\n",
      "training loss 0.0010155105754934071\n",
      "epochs 4478\n",
      "training loss 0.0009989102048506412\n",
      "epochs 4479\n",
      "training loss 0.001046601103832464\n",
      "testing loss 0.002994224689207337\n",
      "epochs 4480\n",
      "training loss 0.0010401674561870587\n",
      "epochs 4481\n",
      "training loss 0.000992664365284711\n",
      "epochs 4482\n",
      "training loss 0.0009781019910773698\n",
      "epochs 4483\n",
      "training loss 0.0010292480477148889\n",
      "epochs 4484\n",
      "training loss 0.0009840744047561565\n",
      "epochs 4485\n",
      "training loss 0.0010220008250467457\n",
      "epochs 4486\n",
      "training loss 0.0009763563847265526\n",
      "epochs 4487\n",
      "training loss 0.001005541247678043\n",
      "epochs 4488\n",
      "training loss 0.0010165310872429954\n",
      "epochs 4489\n",
      "training loss 0.0010093840005054595\n",
      "testing loss 0.0030697261314370163\n",
      "epochs 4490\n",
      "training loss 0.0009547405617734797\n",
      "epochs 4491\n",
      "training loss 0.0010226514292343046\n",
      "epochs 4492\n",
      "training loss 0.0009658308310496499\n",
      "epochs 4493\n",
      "training loss 0.0009755791740995863\n",
      "epochs 4494\n",
      "training loss 0.0010388456322153673\n",
      "epochs 4495\n",
      "training loss 0.0009648010020050868\n",
      "epochs 4496\n",
      "training loss 0.001007200031021842\n",
      "epochs 4497\n",
      "training loss 0.0009632711785301951\n",
      "epochs 4498\n",
      "training loss 0.0009997189039644703\n",
      "epochs 4499\n",
      "training loss 0.0010774029260656958\n",
      "testing loss 0.002810171893252892\n",
      "epochs 4500\n",
      "training loss 0.0010634129644919322\n",
      "epochs 4501\n",
      "training loss 0.0009906781585405844\n",
      "epochs 4502\n",
      "training loss 0.0010275279789579903\n",
      "epochs 4503\n",
      "training loss 0.001010561655954908\n",
      "epochs 4504\n",
      "training loss 0.0011818664572003515\n",
      "epochs 4505\n",
      "training loss 0.0010460305427863492\n",
      "epochs 4506\n",
      "training loss 0.0009730035243810673\n",
      "epochs 4507\n",
      "training loss 0.00098342214987718\n",
      "epochs 4508\n",
      "training loss 0.0009777616162071044\n",
      "epochs 4509\n",
      "training loss 0.001039736672257863\n",
      "testing loss 0.002707685872331156\n",
      "epochs 4510\n",
      "training loss 0.0009934180766474287\n",
      "epochs 4511\n",
      "training loss 0.0009923960516755794\n",
      "epochs 4512\n",
      "training loss 0.0009734250467460274\n",
      "epochs 4513\n",
      "training loss 0.0009723436090908378\n",
      "epochs 4514\n",
      "training loss 0.0010090744217339687\n",
      "epochs 4515\n",
      "training loss 0.0010065627473655602\n",
      "epochs 4516\n",
      "training loss 0.0010455959531553598\n",
      "epochs 4517\n",
      "training loss 0.0009934807121598\n",
      "epochs 4518\n",
      "training loss 0.000973494184354892\n",
      "epochs 4519\n",
      "training loss 0.000962732328009285\n",
      "testing loss 0.0027198816083226327\n",
      "epochs 4520\n",
      "training loss 0.0010272392827598519\n",
      "epochs 4521\n",
      "training loss 0.0009827357104704871\n",
      "epochs 4522\n",
      "training loss 0.0010010959994717356\n",
      "epochs 4523\n",
      "training loss 0.0010016492939848554\n",
      "epochs 4524\n",
      "training loss 0.000996632546244478\n",
      "epochs 4525\n",
      "training loss 0.001049080936841123\n",
      "epochs 4526\n",
      "training loss 0.0009941709831875034\n",
      "epochs 4527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.000946751605870774\n",
      "epochs 4528\n",
      "training loss 0.0009919900495253253\n",
      "epochs 4529\n",
      "training loss 0.0010246166815743485\n",
      "testing loss 0.0027066029493485622\n",
      "epochs 4530\n",
      "training loss 0.001102267391908925\n",
      "epochs 4531\n",
      "training loss 0.0009837062922546974\n",
      "epochs 4532\n",
      "training loss 0.0009932666036829715\n",
      "epochs 4533\n",
      "training loss 0.0010068335318810513\n",
      "epochs 4534\n",
      "training loss 0.0010004124431134877\n",
      "epochs 4535\n",
      "training loss 0.0010787332389789624\n",
      "epochs 4536\n",
      "training loss 0.0009843804530295511\n",
      "epochs 4537\n",
      "training loss 0.0010217354265975804\n",
      "epochs 4538\n",
      "training loss 0.0009756592740333076\n",
      "epochs 4539\n",
      "training loss 0.001016331675581317\n",
      "testing loss 0.002663451426081254\n",
      "epochs 4540\n",
      "training loss 0.0009591114807429534\n",
      "epochs 4541\n",
      "training loss 0.001014196521271371\n",
      "epochs 4542\n",
      "training loss 0.0009823564245750544\n",
      "epochs 4543\n",
      "training loss 0.0009749327732482217\n",
      "epochs 4544\n",
      "training loss 0.0009861031230850468\n",
      "epochs 4545\n",
      "training loss 0.0010248860079790877\n",
      "epochs 4546\n",
      "training loss 0.0010003923317474118\n",
      "epochs 4547\n",
      "training loss 0.0009734519778738099\n",
      "epochs 4548\n",
      "training loss 0.0009845005933762922\n",
      "epochs 4549\n",
      "training loss 0.0009624698782469907\n",
      "testing loss 0.002714616663469966\n",
      "epochs 4550\n",
      "training loss 0.0009875577120082085\n",
      "epochs 4551\n",
      "training loss 0.000997013128277673\n",
      "epochs 4552\n",
      "training loss 0.0010236715632533275\n",
      "epochs 4553\n",
      "training loss 0.000978192354136325\n",
      "epochs 4554\n",
      "training loss 0.000997479504510749\n",
      "epochs 4555\n",
      "training loss 0.0009878432872849532\n",
      "epochs 4556\n",
      "training loss 0.0009845711694574579\n",
      "epochs 4557\n",
      "training loss 0.001007676333695718\n",
      "epochs 4558\n",
      "training loss 0.001004136697704816\n",
      "epochs 4559\n",
      "training loss 0.0009944219454290702\n",
      "testing loss 0.002871692113756658\n",
      "epochs 4560\n",
      "training loss 0.0010065358458723772\n",
      "epochs 4561\n",
      "training loss 0.0009768957897240869\n",
      "epochs 4562\n",
      "training loss 0.0009825937973624123\n",
      "epochs 4563\n",
      "training loss 0.0010431258929298913\n",
      "epochs 4564\n",
      "training loss 0.001052202425803706\n",
      "epochs 4565\n",
      "training loss 0.0009610308992384868\n",
      "epochs 4566\n",
      "training loss 0.0009669007969302575\n",
      "epochs 4567\n",
      "training loss 0.001014930243698716\n",
      "epochs 4568\n",
      "training loss 0.0009919233777805154\n",
      "epochs 4569\n",
      "training loss 0.0009919156157270096\n",
      "testing loss 0.0027017868594563705\n",
      "epochs 4570\n",
      "training loss 0.0009923047144786883\n",
      "epochs 4571\n",
      "training loss 0.000989244595852236\n",
      "epochs 4572\n",
      "training loss 0.0010087487982595935\n",
      "epochs 4573\n",
      "training loss 0.0010002507698314554\n",
      "epochs 4574\n",
      "training loss 0.000992235458862195\n",
      "epochs 4575\n",
      "training loss 0.000998848652018455\n",
      "epochs 4576\n",
      "training loss 0.0009981607128356478\n",
      "epochs 4577\n",
      "training loss 0.0010073537767389194\n",
      "epochs 4578\n",
      "training loss 0.000996871982661492\n",
      "epochs 4579\n",
      "training loss 0.001003214675263646\n",
      "testing loss 0.002869009860157174\n",
      "epochs 4580\n",
      "training loss 0.0010214984656869233\n",
      "epochs 4581\n",
      "training loss 0.0009640077448175445\n",
      "epochs 4582\n",
      "training loss 0.0010168439093665828\n",
      "epochs 4583\n",
      "training loss 0.000982909141700024\n",
      "epochs 4584\n",
      "training loss 0.0009539229372490797\n",
      "epochs 4585\n",
      "training loss 0.0009514341107145019\n",
      "epochs 4586\n",
      "training loss 0.0009809610271815064\n",
      "epochs 4587\n",
      "training loss 0.000985382471108345\n",
      "epochs 4588\n",
      "training loss 0.000997976569627273\n",
      "epochs 4589\n",
      "training loss 0.0009785513867734503\n",
      "testing loss 0.0027643733122055783\n",
      "epochs 4590\n",
      "training loss 0.0009727844986838138\n",
      "epochs 4591\n",
      "training loss 0.0010164180274424355\n",
      "epochs 4592\n",
      "training loss 0.001005517473652404\n",
      "epochs 4593\n",
      "training loss 0.0009748366547077554\n",
      "epochs 4594\n",
      "training loss 0.0009905589642010256\n",
      "epochs 4595\n",
      "training loss 0.000945459654282971\n",
      "epochs 4596\n",
      "training loss 0.0009822862583359754\n",
      "epochs 4597\n",
      "training loss 0.0009760952177443358\n",
      "epochs 4598\n",
      "training loss 0.0009953283544171228\n",
      "epochs 4599\n",
      "training loss 0.0009852562446110403\n",
      "testing loss 0.002849739844537611\n",
      "epochs 4600\n",
      "training loss 0.0010019333654162574\n",
      "epochs 4601\n",
      "training loss 0.0009715762313124144\n",
      "epochs 4602\n",
      "training loss 0.0009883010931918822\n",
      "epochs 4603\n",
      "training loss 0.001001439243855838\n",
      "epochs 4604\n",
      "training loss 0.0009699761122677441\n",
      "epochs 4605\n",
      "training loss 0.001007192734364239\n",
      "epochs 4606\n",
      "training loss 0.0009880324254714031\n",
      "epochs 4607\n",
      "training loss 0.0010061842395774\n",
      "epochs 4608\n",
      "training loss 0.0010043624420522919\n",
      "epochs 4609\n",
      "training loss 0.0009511531429545031\n",
      "testing loss 0.0031738040476693963\n",
      "epochs 4610\n",
      "training loss 0.000995808568150428\n",
      "epochs 4611\n",
      "training loss 0.0009857390608814032\n",
      "epochs 4612\n",
      "training loss 0.000983005455032693\n",
      "epochs 4613\n",
      "training loss 0.0009994621064491275\n",
      "epochs 4614\n",
      "training loss 0.000967376622531314\n",
      "epochs 4615\n",
      "training loss 0.0009736622254488747\n",
      "epochs 4616\n",
      "training loss 0.0009725552790382124\n",
      "epochs 4617\n",
      "training loss 0.00103239168177322\n",
      "epochs 4618\n",
      "training loss 0.0009693745700614159\n",
      "epochs 4619\n",
      "training loss 0.000983568814428846\n",
      "testing loss 0.0028432425885983074\n",
      "epochs 4620\n",
      "training loss 0.0010002986669517759\n",
      "epochs 4621\n",
      "training loss 0.0009818679226192965\n",
      "epochs 4622\n",
      "training loss 0.0009566094537193414\n",
      "epochs 4623\n",
      "training loss 0.0010031543172808154\n",
      "epochs 4624\n",
      "training loss 0.0010626270153735252\n",
      "epochs 4625\n",
      "training loss 0.0009986654764018658\n",
      "epochs 4626\n",
      "training loss 0.0009625166050217887\n",
      "epochs 4627\n",
      "training loss 0.00097830326494189\n",
      "epochs 4628\n",
      "training loss 0.0009688128342874643\n",
      "epochs 4629\n",
      "training loss 0.0009617601271144095\n",
      "testing loss 0.0027802984171599837\n",
      "epochs 4630\n",
      "training loss 0.0009743166913343509\n",
      "epochs 4631\n",
      "training loss 0.000988238846379301\n",
      "epochs 4632\n",
      "training loss 0.0010476563315864051\n",
      "epochs 4633\n",
      "training loss 0.0009858270138955044\n",
      "epochs 4634\n",
      "training loss 0.001004777739559071\n",
      "epochs 4635\n",
      "training loss 0.0009992209168138283\n",
      "epochs 4636\n",
      "training loss 0.0009913250763609008\n",
      "epochs 4637\n",
      "training loss 0.0009756389078981382\n",
      "epochs 4638\n",
      "training loss 0.0009535799021648645\n",
      "epochs 4639\n",
      "training loss 0.0009756435717648534\n",
      "testing loss 0.002686520178095563\n",
      "epochs 4640\n",
      "training loss 0.0009707939860766249\n",
      "epochs 4641\n",
      "training loss 0.0009419359648911605\n",
      "epochs 4642\n",
      "training loss 0.000990085694252437\n",
      "epochs 4643\n",
      "training loss 0.0009788913124362658\n",
      "epochs 4644\n",
      "training loss 0.0009649588979814643\n",
      "epochs 4645\n",
      "training loss 0.000973013816875516\n",
      "epochs 4646\n",
      "training loss 0.0009836283711274934\n",
      "epochs 4647\n",
      "training loss 0.0009892962831935894\n",
      "epochs 4648\n",
      "training loss 0.000986479773535747\n",
      "epochs 4649\n",
      "training loss 0.0009653123328639166\n",
      "testing loss 0.0026834896358608512\n",
      "epochs 4650\n",
      "training loss 0.0009598706586719533\n",
      "epochs 4651\n",
      "training loss 0.0009886754413438242\n",
      "epochs 4652\n",
      "training loss 0.0009689928387365329\n",
      "epochs 4653\n",
      "training loss 0.0010217695063868917\n",
      "epochs 4654\n",
      "training loss 0.0009455605504516297\n",
      "epochs 4655\n",
      "training loss 0.0009741774739132837\n",
      "epochs 4656\n",
      "training loss 0.0009820744891348083\n",
      "epochs 4657\n",
      "training loss 0.0009726885192048111\n",
      "epochs 4658\n",
      "training loss 0.0009596007904405833\n",
      "epochs 4659\n",
      "training loss 0.0009838187933324815\n",
      "testing loss 0.0027921809455716705\n",
      "epochs 4660\n",
      "training loss 0.0010048106321951765\n",
      "epochs 4661\n",
      "training loss 0.0009612124181482771\n",
      "epochs 4662\n",
      "training loss 0.0009976256152934829\n",
      "epochs 4663\n",
      "training loss 0.0009871478052562142\n",
      "epochs 4664\n",
      "training loss 0.0009881386053564429\n",
      "epochs 4665\n",
      "training loss 0.0009703604908911993\n",
      "epochs 4666\n",
      "training loss 0.0009871672888995518\n",
      "epochs 4667\n",
      "training loss 0.0009817652042323898\n",
      "epochs 4668\n",
      "training loss 0.0009222355160627254\n",
      "epochs 4669\n",
      "training loss 0.000945194945921895\n",
      "testing loss 0.002713706056217511\n",
      "epochs 4670\n",
      "training loss 0.0009687425804715094\n",
      "epochs 4671\n",
      "training loss 0.000981001157913708\n",
      "epochs 4672\n",
      "training loss 0.0009319549617513244\n",
      "epochs 4673\n",
      "training loss 0.0009803681554397324\n",
      "epochs 4674\n",
      "training loss 0.000991393100062674\n",
      "epochs 4675\n",
      "training loss 0.0010004548145752501\n",
      "epochs 4676\n",
      "training loss 0.0009841268864816965\n",
      "epochs 4677\n",
      "training loss 0.0010059237850640298\n",
      "epochs 4678\n",
      "training loss 0.0009826592231271252\n",
      "epochs 4679\n",
      "training loss 0.0009986160754362026\n",
      "testing loss 0.002597247010116723\n",
      "epochs 4680\n",
      "training loss 0.0010050580231579004\n",
      "epochs 4681\n",
      "training loss 0.0009795635978275157\n",
      "epochs 4682\n",
      "training loss 0.0010005408421365437\n",
      "epochs 4683\n",
      "training loss 0.0009918382184687538\n",
      "epochs 4684\n",
      "training loss 0.000987881740964213\n",
      "epochs 4685\n",
      "training loss 0.0009702494115664944\n",
      "epochs 4686\n",
      "training loss 0.0009990537497193742\n",
      "epochs 4687\n",
      "training loss 0.0009718779676189041\n",
      "epochs 4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0009425413335877326\n",
      "epochs 4689\n",
      "training loss 0.0009566040226261168\n",
      "testing loss 0.0027161718404799013\n",
      "epochs 4690\n",
      "training loss 0.0009944164981473928\n",
      "epochs 4691\n",
      "training loss 0.0009878116003789665\n",
      "epochs 4692\n",
      "training loss 0.0009623255690709775\n",
      "epochs 4693\n",
      "training loss 0.0009471548685455825\n",
      "epochs 4694\n",
      "training loss 0.0009684988027183752\n",
      "epochs 4695\n",
      "training loss 0.0009477013089739375\n",
      "epochs 4696\n",
      "training loss 0.001021709641930718\n",
      "epochs 4697\n",
      "training loss 0.0009792907520708286\n",
      "epochs 4698\n",
      "training loss 0.0010212911460987409\n",
      "epochs 4699\n",
      "training loss 0.0009713340483430354\n",
      "testing loss 0.0026931655696421494\n",
      "epochs 4700\n",
      "training loss 0.000979360550840875\n",
      "epochs 4701\n",
      "training loss 0.000977135679860087\n",
      "epochs 4702\n",
      "training loss 0.0009691488110293325\n",
      "epochs 4703\n",
      "training loss 0.0010289478856690885\n",
      "epochs 4704\n",
      "training loss 0.0009618898309444538\n",
      "epochs 4705\n",
      "training loss 0.0009680196892690921\n",
      "epochs 4706\n",
      "training loss 0.0009758114531858453\n",
      "epochs 4707\n",
      "training loss 0.0009733056502719622\n",
      "epochs 4708\n",
      "training loss 0.0009669159819643752\n",
      "epochs 4709\n",
      "training loss 0.0009586114995818483\n",
      "testing loss 0.0026499004320386823\n",
      "epochs 4710\n",
      "training loss 0.0009625160175490287\n",
      "epochs 4711\n",
      "training loss 0.0009605704785868141\n",
      "epochs 4712\n",
      "training loss 0.0009712710358077903\n",
      "epochs 4713\n",
      "training loss 0.0009789279642434207\n",
      "epochs 4714\n",
      "training loss 0.000987490472340628\n",
      "epochs 4715\n",
      "training loss 0.0009674214043917762\n",
      "epochs 4716\n",
      "training loss 0.0010135379390958073\n",
      "epochs 4717\n",
      "training loss 0.0009721950141325208\n",
      "epochs 4718\n",
      "training loss 0.0009692975090452863\n",
      "epochs 4719\n",
      "training loss 0.0010321133000079803\n",
      "testing loss 0.002760546025159564\n",
      "epochs 4720\n",
      "training loss 0.000994005061453916\n",
      "epochs 4721\n",
      "training loss 0.000990884984248264\n",
      "epochs 4722\n",
      "training loss 0.0009888423276615066\n",
      "epochs 4723\n",
      "training loss 0.0010272137426005502\n",
      "epochs 4724\n",
      "training loss 0.0009553489678837981\n",
      "epochs 4725\n",
      "training loss 0.0010011695607743682\n",
      "epochs 4726\n",
      "training loss 0.0009559028191735408\n",
      "epochs 4727\n",
      "training loss 0.000996109429218928\n",
      "epochs 4728\n",
      "training loss 0.0009929374913758625\n",
      "epochs 4729\n",
      "training loss 0.0009496639867307545\n",
      "testing loss 0.00271738469163088\n",
      "epochs 4730\n",
      "training loss 0.0009850025733247807\n",
      "epochs 4731\n",
      "training loss 0.0010097691280438954\n",
      "epochs 4732\n",
      "training loss 0.0009628768977043885\n",
      "epochs 4733\n",
      "training loss 0.0010356287665503796\n",
      "epochs 4734\n",
      "training loss 0.0009618226301999292\n",
      "epochs 4735\n",
      "training loss 0.0009609535364802123\n",
      "epochs 4736\n",
      "training loss 0.0009626391969018228\n",
      "epochs 4737\n",
      "training loss 0.0009605223105974088\n",
      "epochs 4738\n",
      "training loss 0.0010179306179112372\n",
      "epochs 4739\n",
      "training loss 0.0009778388925385025\n",
      "testing loss 0.0026535261178831735\n",
      "epochs 4740\n",
      "training loss 0.0009749581630229894\n",
      "epochs 4741\n",
      "training loss 0.000978107459943651\n",
      "epochs 4742\n",
      "training loss 0.0009967562975008097\n",
      "epochs 4743\n",
      "training loss 0.0009321057675392183\n",
      "epochs 4744\n",
      "training loss 0.0009685975096420608\n",
      "epochs 4745\n",
      "training loss 0.0010019328983395862\n",
      "epochs 4746\n",
      "training loss 0.0009456367936412773\n",
      "epochs 4747\n",
      "training loss 0.0009242657644215598\n",
      "epochs 4748\n",
      "training loss 0.0009692016119239314\n",
      "epochs 4749\n",
      "training loss 0.0009746780482971458\n",
      "testing loss 0.0026304531852377857\n",
      "epochs 4750\n",
      "training loss 0.0009489609937022205\n",
      "epochs 4751\n",
      "training loss 0.0009743674632761979\n",
      "epochs 4752\n",
      "training loss 0.0009815690468035717\n",
      "epochs 4753\n",
      "training loss 0.0009628388739479734\n",
      "epochs 4754\n",
      "training loss 0.0010138850406662908\n",
      "epochs 4755\n",
      "training loss 0.0009647401666840521\n",
      "epochs 4756\n",
      "training loss 0.0009572974905636343\n",
      "epochs 4757\n",
      "training loss 0.0009708235540645236\n",
      "epochs 4758\n",
      "training loss 0.0009844055363737148\n",
      "epochs 4759\n",
      "training loss 0.0009549621626731903\n",
      "testing loss 0.0027049568247261412\n",
      "epochs 4760\n",
      "training loss 0.000962385244927253\n",
      "epochs 4761\n",
      "training loss 0.0009998316841382578\n",
      "epochs 4762\n",
      "training loss 0.0009595785147756746\n",
      "epochs 4763\n",
      "training loss 0.0009304964060379945\n",
      "epochs 4764\n",
      "training loss 0.0009868440154931397\n",
      "epochs 4765\n",
      "training loss 0.000996115100308177\n",
      "epochs 4766\n",
      "training loss 0.0009689711060482545\n",
      "epochs 4767\n",
      "training loss 0.0009605376284122241\n",
      "epochs 4768\n",
      "training loss 0.0009641052677726379\n",
      "epochs 4769\n",
      "training loss 0.0009983323907584382\n",
      "testing loss 0.0027567032218501234\n",
      "epochs 4770\n",
      "training loss 0.0009948047927841204\n",
      "epochs 4771\n",
      "training loss 0.0009510122748420186\n",
      "epochs 4772\n",
      "training loss 0.000984313577381005\n",
      "epochs 4773\n",
      "training loss 0.0009405143586579019\n",
      "epochs 4774\n",
      "training loss 0.0009679953966821943\n",
      "epochs 4775\n",
      "training loss 0.0009417277776726767\n",
      "epochs 4776\n",
      "training loss 0.0009694829012414491\n",
      "epochs 4777\n",
      "training loss 0.0009774648341864041\n",
      "epochs 4778\n",
      "training loss 0.0009472254346305997\n",
      "epochs 4779\n",
      "training loss 0.0009780806742465593\n",
      "testing loss 0.0029236023613179087\n",
      "epochs 4780\n",
      "training loss 0.0009647369623434727\n",
      "epochs 4781\n",
      "training loss 0.0009587652398143134\n",
      "epochs 4782\n",
      "training loss 0.0009698348102516475\n",
      "epochs 4783\n",
      "training loss 0.0009933540669742598\n",
      "epochs 4784\n",
      "training loss 0.0009485710247548574\n",
      "epochs 4785\n",
      "training loss 0.0009688997856521056\n",
      "epochs 4786\n",
      "training loss 0.0009449360935345836\n",
      "epochs 4787\n",
      "training loss 0.000980894526786604\n",
      "epochs 4788\n",
      "training loss 0.0009782501302820865\n",
      "epochs 4789\n",
      "training loss 0.000976589133631789\n",
      "testing loss 0.003098133235595169\n",
      "epochs 4790\n",
      "training loss 0.0009770239333588189\n",
      "epochs 4791\n",
      "training loss 0.0009467110077080552\n",
      "epochs 4792\n",
      "training loss 0.0009597240344587272\n",
      "epochs 4793\n",
      "training loss 0.0010100409909049586\n",
      "epochs 4794\n",
      "training loss 0.000979847062413091\n",
      "epochs 4795\n",
      "training loss 0.0009694982481600949\n",
      "epochs 4796\n",
      "training loss 0.0009569473713063167\n",
      "epochs 4797\n",
      "training loss 0.0009649622122796768\n",
      "epochs 4798\n",
      "training loss 0.000962816710275312\n",
      "epochs 4799\n",
      "training loss 0.0009638506819714868\n",
      "testing loss 0.002896746988358729\n",
      "epochs 4800\n",
      "training loss 0.0009668411724701083\n",
      "epochs 4801\n",
      "training loss 0.0009568245940994596\n",
      "epochs 4802\n",
      "training loss 0.0009427080405939733\n",
      "epochs 4803\n",
      "training loss 0.0009880346715971148\n",
      "epochs 4804\n",
      "training loss 0.0009719657631715517\n",
      "epochs 4805\n",
      "training loss 0.000971793141364655\n",
      "epochs 4806\n",
      "training loss 0.0009617337601055536\n",
      "epochs 4807\n",
      "training loss 0.000975590365539704\n",
      "epochs 4808\n",
      "training loss 0.0009448492427506571\n",
      "epochs 4809\n",
      "training loss 0.0009686090275050795\n",
      "testing loss 0.002733130887702292\n",
      "epochs 4810\n",
      "training loss 0.0009646863217669395\n",
      "epochs 4811\n",
      "training loss 0.0009641229068169705\n",
      "epochs 4812\n",
      "training loss 0.0009756028784175707\n",
      "epochs 4813\n",
      "training loss 0.0009345046050300239\n",
      "epochs 4814\n",
      "training loss 0.0009604007185091432\n",
      "epochs 4815\n",
      "training loss 0.0009420678071892823\n",
      "epochs 4816\n",
      "training loss 0.0009634080729525151\n",
      "epochs 4817\n",
      "training loss 0.0009856561951107122\n",
      "epochs 4818\n",
      "training loss 0.0010433879896423759\n",
      "epochs 4819\n",
      "training loss 0.0009967776507777545\n",
      "testing loss 0.0027593079477991143\n",
      "epochs 4820\n",
      "training loss 0.0009599420971641923\n",
      "epochs 4821\n",
      "training loss 0.0009715658455795544\n",
      "epochs 4822\n",
      "training loss 0.0009369346267886256\n",
      "epochs 4823\n",
      "training loss 0.000940478127573597\n",
      "epochs 4824\n",
      "training loss 0.0009541855028074653\n",
      "epochs 4825\n",
      "training loss 0.0010142628781817356\n",
      "epochs 4826\n",
      "training loss 0.0009454944882949525\n",
      "epochs 4827\n",
      "training loss 0.000995085653063408\n",
      "epochs 4828\n",
      "training loss 0.0009663327675635466\n",
      "epochs 4829\n",
      "training loss 0.0009685425407332468\n",
      "testing loss 0.003044267893844806\n",
      "epochs 4830\n",
      "training loss 0.0009574595459108453\n",
      "epochs 4831\n",
      "training loss 0.0009307663899769945\n",
      "epochs 4832\n",
      "training loss 0.0009670085632836723\n",
      "epochs 4833\n",
      "training loss 0.0009583561819387843\n",
      "epochs 4834\n",
      "training loss 0.0009841169711870444\n",
      "epochs 4835\n",
      "training loss 0.0009641309681356995\n",
      "epochs 4836\n",
      "training loss 0.0009653728489274312\n",
      "epochs 4837\n",
      "training loss 0.0010155968118238186\n",
      "epochs 4838\n",
      "training loss 0.0009548063371380038\n",
      "epochs 4839\n",
      "training loss 0.0009548115315082792\n",
      "testing loss 0.0030127219110231925\n",
      "epochs 4840\n",
      "training loss 0.0009521441510276966\n",
      "epochs 4841\n",
      "training loss 0.000990862429841198\n",
      "epochs 4842\n",
      "training loss 0.0009667011448302473\n",
      "epochs 4843\n",
      "training loss 0.0009528919854512761\n",
      "epochs 4844\n",
      "training loss 0.000978807238989303\n",
      "epochs 4845\n",
      "training loss 0.0009976886653364\n",
      "epochs 4846\n",
      "training loss 0.0009542159634602672\n",
      "epochs 4847\n",
      "training loss 0.0009697054275368145\n",
      "epochs 4848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0009509700758801631\n",
      "epochs 4849\n",
      "training loss 0.0009475675559076155\n",
      "testing loss 0.0027821690573997743\n",
      "epochs 4850\n",
      "training loss 0.0009591683443199764\n",
      "epochs 4851\n",
      "training loss 0.0009278465036018313\n",
      "epochs 4852\n",
      "training loss 0.0009761459527977411\n",
      "epochs 4853\n",
      "training loss 0.0009586729695256177\n",
      "epochs 4854\n",
      "training loss 0.0010048687951830372\n",
      "epochs 4855\n",
      "training loss 0.0010221762830174973\n",
      "epochs 4856\n",
      "training loss 0.0009618637537354417\n",
      "epochs 4857\n",
      "training loss 0.0010031737174165058\n",
      "epochs 4858\n",
      "training loss 0.0009482525305689446\n",
      "epochs 4859\n",
      "training loss 0.0009983663022939942\n",
      "testing loss 0.002704567655736039\n",
      "epochs 4860\n",
      "training loss 0.0009545729785561732\n",
      "epochs 4861\n",
      "training loss 0.0009419699745305097\n",
      "epochs 4862\n",
      "training loss 0.0009834005205119228\n",
      "epochs 4863\n",
      "training loss 0.0009482986558940669\n",
      "epochs 4864\n",
      "training loss 0.0009704743946143808\n",
      "epochs 4865\n",
      "training loss 0.0009374258637967545\n",
      "epochs 4866\n",
      "training loss 0.0009520184142532048\n",
      "epochs 4867\n",
      "training loss 0.0009640097971241299\n",
      "epochs 4868\n",
      "training loss 0.000969622901458248\n",
      "epochs 4869\n",
      "training loss 0.0009489064561316353\n",
      "testing loss 0.0027742676729177857\n",
      "epochs 4870\n",
      "training loss 0.0009768524101551757\n",
      "epochs 4871\n",
      "training loss 0.0009411797718297815\n",
      "epochs 4872\n",
      "training loss 0.0009682057454572045\n",
      "epochs 4873\n",
      "training loss 0.00096669997598857\n",
      "epochs 4874\n",
      "training loss 0.0009649850948788735\n",
      "epochs 4875\n",
      "training loss 0.0009634053234809212\n",
      "epochs 4876\n",
      "training loss 0.0009292955186549223\n",
      "epochs 4877\n",
      "training loss 0.0009382702840143945\n",
      "epochs 4878\n",
      "training loss 0.0009930455907867898\n",
      "epochs 4879\n",
      "training loss 0.0009546550899272711\n",
      "testing loss 0.0030326346771012173\n",
      "epochs 4880\n",
      "training loss 0.0009722455104283626\n",
      "epochs 4881\n",
      "training loss 0.0009464784681027058\n",
      "epochs 4882\n",
      "training loss 0.00100785905019717\n",
      "epochs 4883\n",
      "training loss 0.000979023734510998\n",
      "epochs 4884\n",
      "training loss 0.0009257673759347445\n",
      "epochs 4885\n",
      "training loss 0.0009656994434435905\n",
      "epochs 4886\n",
      "training loss 0.0009799792615648665\n",
      "epochs 4887\n",
      "training loss 0.0009393214188320817\n",
      "epochs 4888\n",
      "training loss 0.0009757310752176586\n",
      "epochs 4889\n",
      "training loss 0.0009821414757140638\n",
      "testing loss 0.0025939244119754287\n",
      "epochs 4890\n",
      "training loss 0.0009567398218067199\n",
      "epochs 4891\n",
      "training loss 0.0009327659345908504\n",
      "epochs 4892\n",
      "training loss 0.0009714867728606636\n",
      "epochs 4893\n",
      "training loss 0.000992530396106967\n",
      "epochs 4894\n",
      "training loss 0.0009684114761885516\n",
      "epochs 4895\n",
      "training loss 0.0009562781714518642\n",
      "epochs 4896\n",
      "training loss 0.0009562772485331312\n",
      "epochs 4897\n",
      "training loss 0.0009333426445951872\n",
      "epochs 4898\n",
      "training loss 0.0009643720081826179\n",
      "epochs 4899\n",
      "training loss 0.0009704188949366222\n",
      "testing loss 0.0027793289326521046\n",
      "epochs 4900\n",
      "training loss 0.0009441250773489272\n",
      "epochs 4901\n",
      "training loss 0.0009956519270887582\n",
      "epochs 4902\n",
      "training loss 0.0009511338754225873\n",
      "epochs 4903\n",
      "training loss 0.0009519432685168114\n",
      "epochs 4904\n",
      "training loss 0.0009266099872033602\n",
      "epochs 4905\n",
      "training loss 0.0009396512596195672\n",
      "epochs 4906\n",
      "training loss 0.000949396250686696\n",
      "epochs 4907\n",
      "training loss 0.000941190750696937\n",
      "epochs 4908\n",
      "training loss 0.0009650789572090182\n",
      "epochs 4909\n",
      "training loss 0.001006617538466778\n",
      "testing loss 0.0028368432292754345\n",
      "epochs 4910\n",
      "training loss 0.0009354453291149533\n",
      "epochs 4911\n",
      "training loss 0.0009228434112194733\n",
      "epochs 4912\n",
      "training loss 0.0009463621149693069\n",
      "epochs 4913\n",
      "training loss 0.0009461411037538937\n",
      "epochs 4914\n",
      "training loss 0.000947220779875418\n",
      "epochs 4915\n",
      "training loss 0.0009408228976530304\n",
      "epochs 4916\n",
      "training loss 0.000975071697242524\n",
      "epochs 4917\n",
      "training loss 0.0009730528662544547\n",
      "epochs 4918\n",
      "training loss 0.000913377740494001\n",
      "epochs 4919\n",
      "training loss 0.0010004565395743197\n",
      "testing loss 0.0027805836970343236\n",
      "epochs 4920\n",
      "training loss 0.000982633298602646\n",
      "epochs 4921\n",
      "training loss 0.0009457877972290645\n",
      "epochs 4922\n",
      "training loss 0.0009982445241712135\n",
      "epochs 4923\n",
      "training loss 0.0009870915011096588\n",
      "epochs 4924\n",
      "training loss 0.0009727792886558546\n",
      "epochs 4925\n",
      "training loss 0.000947668946841392\n",
      "epochs 4926\n",
      "training loss 0.0009370940561564837\n",
      "epochs 4927\n",
      "training loss 0.0009620673469717877\n",
      "epochs 4928\n",
      "training loss 0.0009443609919370715\n",
      "epochs 4929\n",
      "training loss 0.0009905657413242933\n",
      "testing loss 0.0026833916958982906\n",
      "epochs 4930\n",
      "training loss 0.0009485158136384816\n",
      "epochs 4931\n",
      "training loss 0.000987472130204367\n",
      "epochs 4932\n",
      "training loss 0.0009286398872008439\n",
      "epochs 4933\n",
      "training loss 0.00097811973026011\n",
      "epochs 4934\n",
      "training loss 0.0009640114134924888\n",
      "epochs 4935\n",
      "training loss 0.0009485854758239864\n",
      "epochs 4936\n",
      "training loss 0.0009223975791060861\n",
      "epochs 4937\n",
      "training loss 0.0009646130673372184\n",
      "epochs 4938\n",
      "training loss 0.0009427878792124508\n",
      "epochs 4939\n",
      "training loss 0.0009246217745064693\n",
      "testing loss 0.0026667609328655064\n",
      "epochs 4940\n",
      "training loss 0.0009916111323373862\n",
      "epochs 4941\n",
      "training loss 0.000938199453075674\n",
      "epochs 4942\n",
      "training loss 0.0009614768976594765\n",
      "epochs 4943\n",
      "training loss 0.0009617110776947103\n",
      "epochs 4944\n",
      "training loss 0.0009207819115390327\n",
      "epochs 4945\n",
      "training loss 0.0009369212798958242\n",
      "epochs 4946\n",
      "training loss 0.0009367792553262712\n",
      "epochs 4947\n",
      "training loss 0.0009218332937835096\n",
      "epochs 4948\n",
      "training loss 0.000974913853989195\n",
      "epochs 4949\n",
      "training loss 0.0009914052555561178\n",
      "testing loss 0.0028639828851479534\n",
      "epochs 4950\n",
      "training loss 0.000962510596009645\n",
      "epochs 4951\n",
      "training loss 0.0009374633340496219\n",
      "epochs 4952\n",
      "training loss 0.000955324596831558\n",
      "epochs 4953\n",
      "training loss 0.0009220549143216712\n",
      "epochs 4954\n",
      "training loss 0.000923378306804055\n",
      "epochs 4955\n",
      "training loss 0.000908831384872995\n",
      "epochs 4956\n",
      "training loss 0.0009707386885333727\n",
      "epochs 4957\n",
      "training loss 0.0009877070269545987\n",
      "epochs 4958\n",
      "training loss 0.0009418448415056924\n",
      "epochs 4959\n",
      "training loss 0.0009372962328589784\n",
      "testing loss 0.0029033795787609374\n",
      "epochs 4960\n",
      "training loss 0.0009250325138580144\n",
      "epochs 4961\n",
      "training loss 0.0009419806126440028\n",
      "epochs 4962\n",
      "training loss 0.0009705338061477219\n",
      "epochs 4963\n",
      "training loss 0.0009387250993037058\n",
      "epochs 4964\n",
      "training loss 0.0008944255763581334\n",
      "epochs 4965\n",
      "training loss 0.0009437264852639073\n",
      "epochs 4966\n",
      "training loss 0.000938024080470261\n",
      "epochs 4967\n",
      "training loss 0.0009328037072935546\n",
      "epochs 4968\n",
      "training loss 0.0009472082380706439\n",
      "epochs 4969\n",
      "training loss 0.0009799745346251083\n",
      "testing loss 0.002801803127674267\n",
      "epochs 4970\n",
      "training loss 0.0009077062807682799\n",
      "epochs 4971\n",
      "training loss 0.0009165192556342444\n",
      "epochs 4972\n",
      "training loss 0.0009366479679470809\n",
      "epochs 4973\n",
      "training loss 0.0009480025329114727\n",
      "epochs 4974\n",
      "training loss 0.0009362156778725436\n",
      "epochs 4975\n",
      "training loss 0.0009567368587890872\n",
      "epochs 4976\n",
      "training loss 0.0009293320550198318\n",
      "epochs 4977\n",
      "training loss 0.0009455212516997486\n",
      "epochs 4978\n",
      "training loss 0.0009696701942981252\n",
      "epochs 4979\n",
      "training loss 0.000938511423403567\n",
      "testing loss 0.0029351061320672116\n",
      "epochs 4980\n",
      "training loss 0.0009598413705762113\n",
      "epochs 4981\n",
      "training loss 0.0009556464680348804\n",
      "epochs 4982\n",
      "training loss 0.0009408069372539462\n",
      "epochs 4983\n",
      "training loss 0.0009277093783834819\n",
      "epochs 4984\n",
      "training loss 0.0009283269978465236\n",
      "epochs 4985\n",
      "training loss 0.0009438681470528867\n",
      "epochs 4986\n",
      "training loss 0.0009436370082352435\n",
      "epochs 4987\n",
      "training loss 0.000925600882739927\n",
      "epochs 4988\n",
      "training loss 0.0009515390341809886\n",
      "epochs 4989\n",
      "training loss 0.000977607692593969\n",
      "testing loss 0.0028363351593725383\n",
      "epochs 4990\n",
      "training loss 0.0009637876092824281\n",
      "epochs 4991\n",
      "training loss 0.0009656274405670406\n",
      "epochs 4992\n",
      "training loss 0.0009184500881390063\n",
      "epochs 4993\n",
      "training loss 0.000943981379264892\n",
      "epochs 4994\n",
      "training loss 0.0009511915338261047\n",
      "epochs 4995\n",
      "training loss 0.0009653191499717782\n",
      "epochs 4996\n",
      "training loss 0.000937081315401817\n",
      "epochs 4997\n",
      "training loss 0.0009421932487677794\n",
      "epochs 4998\n",
      "training loss 0.0009451032562059631\n",
      "epochs 4999\n",
      "training loss 0.0009495980167509506\n",
      "testing loss 0.0033975263548922455\n",
      "epochs 5000\n",
      "training loss 0.0009710937818033921\n",
      "epochs 5001\n",
      "training loss 0.0009407299893800633\n",
      "epochs 5002\n",
      "training loss 0.000954121824511041\n",
      "epochs 5003\n",
      "training loss 0.0009279673383712746\n",
      "epochs 5004\n",
      "training loss 0.0009700466937450608\n",
      "epochs 5005\n",
      "training loss 0.0009468779724026183\n",
      "epochs 5006\n",
      "training loss 0.0009474353909019754\n",
      "epochs 5007\n",
      "training loss 0.0009200209780917489\n",
      "epochs 5008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.000951410603488719\n",
      "epochs 5009\n",
      "training loss 0.0009398396233256658\n",
      "testing loss 0.002606444034381961\n",
      "epochs 5010\n",
      "training loss 0.0009423992163105372\n",
      "epochs 5011\n",
      "training loss 0.0009309862951583889\n",
      "epochs 5012\n",
      "training loss 0.0009506516497211366\n",
      "epochs 5013\n",
      "training loss 0.0008890793508762284\n",
      "epochs 5014\n",
      "training loss 0.0009842740478330945\n",
      "epochs 5015\n",
      "training loss 0.0009560294712590773\n",
      "epochs 5016\n",
      "training loss 0.0009318190872015312\n",
      "epochs 5017\n",
      "training loss 0.0009287698239499935\n",
      "epochs 5018\n",
      "training loss 0.000928624576816794\n",
      "epochs 5019\n",
      "training loss 0.0009471020980817365\n",
      "testing loss 0.0027465430535838433\n",
      "epochs 5020\n",
      "training loss 0.0009455475841201616\n",
      "epochs 5021\n",
      "training loss 0.0009521650550971912\n",
      "epochs 5022\n",
      "training loss 0.0009476822711765132\n",
      "epochs 5023\n",
      "training loss 0.0009859598514738587\n",
      "epochs 5024\n",
      "training loss 0.0009225445854729523\n",
      "epochs 5025\n",
      "training loss 0.0009162833772268497\n",
      "epochs 5026\n",
      "training loss 0.0009739343910251594\n",
      "epochs 5027\n",
      "training loss 0.000982913848205178\n",
      "epochs 5028\n",
      "training loss 0.0009288349177187314\n",
      "epochs 5029\n",
      "training loss 0.0009261801866120956\n",
      "testing loss 0.0026842076252941174\n",
      "epochs 5030\n",
      "training loss 0.0009361179773752382\n",
      "epochs 5031\n",
      "training loss 0.000983510448068018\n",
      "epochs 5032\n",
      "training loss 0.0009406691468051864\n",
      "epochs 5033\n",
      "training loss 0.0009411078230916639\n",
      "epochs 5034\n",
      "training loss 0.0009400196163632021\n",
      "epochs 5035\n",
      "training loss 0.0009307547820601782\n",
      "epochs 5036\n",
      "training loss 0.0009148859151421075\n",
      "epochs 5037\n",
      "training loss 0.0009887364417340005\n",
      "epochs 5038\n",
      "training loss 0.0009048365920429659\n",
      "epochs 5039\n",
      "training loss 0.0009760533239746234\n",
      "testing loss 0.002785266900487931\n",
      "epochs 5040\n",
      "training loss 0.0009891485445425027\n",
      "epochs 5041\n",
      "training loss 0.000942790613380207\n",
      "epochs 5042\n",
      "training loss 0.0009275414065548465\n",
      "epochs 5043\n",
      "training loss 0.0009223913618555932\n",
      "epochs 5044\n",
      "training loss 0.0009559783302558909\n",
      "epochs 5045\n",
      "training loss 0.0009692622524527635\n",
      "epochs 5046\n",
      "training loss 0.0009241259119354506\n",
      "epochs 5047\n",
      "training loss 0.0009888790715570734\n",
      "epochs 5048\n",
      "training loss 0.0009459207305227517\n",
      "epochs 5049\n",
      "training loss 0.0009356924656624297\n",
      "testing loss 0.0026011885777908438\n",
      "epochs 5050\n",
      "training loss 0.0009350056273828751\n",
      "epochs 5051\n",
      "training loss 0.0009481351565014808\n",
      "epochs 5052\n",
      "training loss 0.0009461396931469622\n",
      "epochs 5053\n",
      "training loss 0.0009377418888455376\n",
      "epochs 5054\n",
      "training loss 0.0011236051888004125\n",
      "epochs 5055\n",
      "training loss 0.0009833852814278342\n",
      "epochs 5056\n",
      "training loss 0.0009450302575184117\n",
      "epochs 5057\n",
      "training loss 0.0009427867841476569\n",
      "epochs 5058\n",
      "training loss 0.0009378337658344453\n",
      "epochs 5059\n",
      "training loss 0.0009096258187752874\n",
      "testing loss 0.002948266010259192\n",
      "epochs 5060\n",
      "training loss 0.0009596722231828254\n",
      "epochs 5061\n",
      "training loss 0.0009506499861143416\n",
      "epochs 5062\n",
      "training loss 0.0009549416032496636\n",
      "epochs 5063\n",
      "training loss 0.0009579646748230201\n",
      "epochs 5064\n",
      "training loss 0.0010119570065523274\n",
      "epochs 5065\n",
      "training loss 0.000988829460937677\n",
      "epochs 5066\n",
      "training loss 0.000925824617419256\n",
      "epochs 5067\n",
      "training loss 0.0009247339877305626\n",
      "epochs 5068\n",
      "training loss 0.0009313194635133103\n",
      "epochs 5069\n",
      "training loss 0.0009467440047287198\n",
      "testing loss 0.0026401477219398194\n",
      "epochs 5070\n",
      "training loss 0.0009331869164561021\n",
      "epochs 5071\n",
      "training loss 0.0009245695617038609\n",
      "epochs 5072\n",
      "training loss 0.0009424048317575086\n",
      "epochs 5073\n",
      "training loss 0.0009330909434347881\n",
      "epochs 5074\n",
      "training loss 0.0009488427727044444\n",
      "epochs 5075\n",
      "training loss 0.0008932721948302082\n",
      "epochs 5076\n",
      "training loss 0.00090946529699648\n",
      "epochs 5077\n",
      "training loss 0.0009116775289533156\n",
      "epochs 5078\n",
      "training loss 0.0009743875777384261\n",
      "epochs 5079\n",
      "training loss 0.0009641671516277225\n",
      "testing loss 0.002720256803758733\n",
      "epochs 5080\n",
      "training loss 0.0009627645836341956\n",
      "epochs 5081\n",
      "training loss 0.000961701598780592\n",
      "epochs 5082\n",
      "training loss 0.0009445023006762415\n",
      "epochs 5083\n",
      "training loss 0.0009462331824350004\n",
      "epochs 5084\n",
      "training loss 0.0009352322914644702\n",
      "epochs 5085\n",
      "training loss 0.0009845235356869951\n",
      "epochs 5086\n",
      "training loss 0.0009516686939572106\n",
      "epochs 5087\n",
      "training loss 0.0009304674656843727\n",
      "epochs 5088\n",
      "training loss 0.0009514916596026938\n",
      "epochs 5089\n",
      "training loss 0.0009469794662171086\n",
      "testing loss 0.002857225893828225\n",
      "epochs 5090\n",
      "training loss 0.0009394562504208989\n",
      "epochs 5091\n",
      "training loss 0.000923336887718702\n",
      "epochs 5092\n",
      "training loss 0.0009137098420033441\n",
      "epochs 5093\n",
      "training loss 0.0009352427103703892\n",
      "epochs 5094\n",
      "training loss 0.0009468884129816027\n",
      "epochs 5095\n",
      "training loss 0.000923171915619218\n",
      "epochs 5096\n",
      "training loss 0.0009241488903384418\n",
      "epochs 5097\n",
      "training loss 0.0009081105157233333\n",
      "epochs 5098\n",
      "training loss 0.0009515109305846078\n",
      "epochs 5099\n",
      "training loss 0.0009540721432109134\n",
      "testing loss 0.002749330501636847\n",
      "epochs 5100\n",
      "training loss 0.0009327015685063354\n",
      "epochs 5101\n",
      "training loss 0.0009094169936609939\n",
      "epochs 5102\n",
      "training loss 0.0009696042392683134\n",
      "epochs 5103\n",
      "training loss 0.0009264321558992732\n",
      "epochs 5104\n",
      "training loss 0.0009129156263355494\n",
      "epochs 5105\n",
      "training loss 0.0009078368592170834\n",
      "epochs 5106\n",
      "training loss 0.0009212884259350757\n",
      "epochs 5107\n",
      "training loss 0.0009304580937201966\n",
      "epochs 5108\n",
      "training loss 0.0009206806091552085\n",
      "epochs 5109\n",
      "training loss 0.000932931852785978\n",
      "testing loss 0.003410587043514647\n",
      "epochs 5110\n",
      "training loss 0.0009405848054968297\n",
      "epochs 5111\n",
      "training loss 0.0008882986992741949\n",
      "epochs 5112\n",
      "training loss 0.0009548269985576294\n",
      "epochs 5113\n",
      "training loss 0.0009771745624891158\n",
      "epochs 5114\n",
      "training loss 0.0009251499504677521\n",
      "epochs 5115\n",
      "training loss 0.0009431092499006585\n",
      "epochs 5116\n",
      "training loss 0.0009096753437639607\n",
      "epochs 5117\n",
      "training loss 0.0009071909419846888\n",
      "epochs 5118\n",
      "training loss 0.0009189971880477748\n",
      "epochs 5119\n",
      "training loss 0.0009587558374193845\n",
      "testing loss 0.00259549059518712\n",
      "epochs 5120\n",
      "training loss 0.0009247375843978535\n",
      "epochs 5121\n",
      "training loss 0.0009106701402004956\n",
      "epochs 5122\n",
      "training loss 0.0009134600289650035\n",
      "epochs 5123\n",
      "training loss 0.0009315626836052068\n",
      "epochs 5124\n",
      "training loss 0.0009796264789089855\n",
      "epochs 5125\n",
      "training loss 0.0009585395324437406\n",
      "epochs 5126\n",
      "training loss 0.0009699712479472908\n",
      "epochs 5127\n",
      "training loss 0.0008844005162112902\n",
      "epochs 5128\n",
      "training loss 0.0009610998721320556\n",
      "epochs 5129\n",
      "training loss 0.000955650171563652\n",
      "testing loss 0.0026474844516545587\n",
      "epochs 5130\n",
      "training loss 0.0009568190303139988\n",
      "epochs 5131\n",
      "training loss 0.000927706188019818\n",
      "epochs 5132\n",
      "training loss 0.0009536873665992114\n",
      "epochs 5133\n",
      "training loss 0.0009486897514869762\n",
      "epochs 5134\n",
      "training loss 0.0009348531152612861\n",
      "epochs 5135\n",
      "training loss 0.000900928656277629\n",
      "epochs 5136\n",
      "training loss 0.0009204632256570694\n",
      "epochs 5137\n",
      "training loss 0.0009234489272928891\n",
      "epochs 5138\n",
      "training loss 0.0009243867244524739\n",
      "epochs 5139\n",
      "training loss 0.0009311079595196925\n",
      "testing loss 0.0026696212470022867\n",
      "epochs 5140\n",
      "training loss 0.0009611779221476517\n",
      "epochs 5141\n",
      "training loss 0.000902111542543487\n",
      "epochs 5142\n",
      "training loss 0.000968604891222695\n",
      "epochs 5143\n",
      "training loss 0.0009022038597167727\n",
      "epochs 5144\n",
      "training loss 0.0009534016974648251\n",
      "epochs 5145\n",
      "training loss 0.0009777162112239012\n",
      "epochs 5146\n",
      "training loss 0.0009369075105994045\n",
      "epochs 5147\n",
      "training loss 0.0009380385859432067\n",
      "epochs 5148\n",
      "training loss 0.0009431124085951087\n",
      "epochs 5149\n",
      "training loss 0.0009268741440070414\n",
      "testing loss 0.002723960298384335\n",
      "epochs 5150\n",
      "training loss 0.0009414787966146567\n",
      "epochs 5151\n",
      "training loss 0.000935544646776771\n",
      "epochs 5152\n",
      "training loss 0.0009215451236658568\n",
      "epochs 5153\n",
      "training loss 0.000941139978578167\n",
      "epochs 5154\n",
      "training loss 0.0009390672399537881\n",
      "epochs 5155\n",
      "training loss 0.0009095309039303257\n",
      "epochs 5156\n",
      "training loss 0.0009291485067149822\n",
      "epochs 5157\n",
      "training loss 0.0008941232194942861\n",
      "epochs 5158\n",
      "training loss 0.0009505826601968077\n",
      "epochs 5159\n",
      "training loss 0.0009544068571650013\n",
      "testing loss 0.002635066465190645\n",
      "epochs 5160\n",
      "training loss 0.0009289951034323355\n",
      "epochs 5161\n",
      "training loss 0.0009445550004593564\n",
      "epochs 5162\n",
      "training loss 0.0008947687567845779\n",
      "epochs 5163\n",
      "training loss 0.0008917273655441791\n",
      "epochs 5164\n",
      "training loss 0.0009177454415882873\n",
      "epochs 5165\n",
      "training loss 0.0008989781986796057\n",
      "epochs 5166\n",
      "training loss 0.0009466560391530871\n",
      "epochs 5167\n",
      "training loss 0.0009643500375329293\n",
      "epochs 5168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.000907241028084598\n",
      "epochs 5169\n",
      "training loss 0.0009483472761870744\n",
      "testing loss 0.0029819826752356922\n",
      "epochs 5170\n",
      "training loss 0.0009349045992296721\n",
      "epochs 5171\n",
      "training loss 0.0009301753707072984\n",
      "epochs 5172\n",
      "training loss 0.0009691743275694164\n",
      "epochs 5173\n",
      "training loss 0.0009057941560111204\n",
      "epochs 5174\n",
      "training loss 0.0009238257192819744\n",
      "epochs 5175\n",
      "training loss 0.0009148789615381656\n",
      "epochs 5176\n",
      "training loss 0.0009295718336058922\n",
      "epochs 5177\n",
      "training loss 0.0009112321668358288\n",
      "epochs 5178\n",
      "training loss 0.000924080031914042\n",
      "epochs 5179\n",
      "training loss 0.000949326897054871\n",
      "testing loss 0.0027736787595918255\n",
      "epochs 5180\n",
      "training loss 0.0009185055518129382\n",
      "epochs 5181\n",
      "training loss 0.0009449999648539808\n",
      "epochs 5182\n",
      "training loss 0.0009076257645347833\n",
      "epochs 5183\n",
      "training loss 0.0009480033776302479\n",
      "epochs 5184\n",
      "training loss 0.0009175743843997775\n",
      "epochs 5185\n",
      "training loss 0.0009155766866849254\n",
      "epochs 5186\n",
      "training loss 0.0009545938544949137\n",
      "epochs 5187\n",
      "training loss 0.0009390607560803621\n",
      "epochs 5188\n",
      "training loss 0.0009069879482518655\n",
      "epochs 5189\n",
      "training loss 0.0009525538210567416\n",
      "testing loss 0.002764082354813788\n",
      "epochs 5190\n",
      "training loss 0.0009221802170156278\n",
      "epochs 5191\n",
      "training loss 0.000913461114918264\n",
      "epochs 5192\n",
      "training loss 0.0009319221449267164\n",
      "epochs 5193\n",
      "training loss 0.0009540694318662219\n",
      "epochs 5194\n",
      "training loss 0.0008774710350102352\n",
      "epochs 5195\n",
      "training loss 0.0009292493560875124\n",
      "epochs 5196\n",
      "training loss 0.0009268517336152823\n",
      "epochs 5197\n",
      "training loss 0.0009080794317708677\n",
      "epochs 5198\n",
      "training loss 0.0009368201528426747\n",
      "epochs 5199\n",
      "training loss 0.0009380996320679739\n",
      "testing loss 0.003055748336119511\n",
      "epochs 5200\n",
      "training loss 0.0009526369408003726\n",
      "epochs 5201\n",
      "training loss 0.000934664555701447\n",
      "epochs 5202\n",
      "training loss 0.0009120503142561005\n",
      "epochs 5203\n",
      "training loss 0.0009034316359430348\n",
      "epochs 5204\n",
      "training loss 0.0009329598022824419\n",
      "epochs 5205\n",
      "training loss 0.0009203076422409831\n",
      "epochs 5206\n",
      "training loss 0.0008994101503889408\n",
      "epochs 5207\n",
      "training loss 0.0009313147470120081\n",
      "epochs 5208\n",
      "training loss 0.0009611118229256099\n",
      "epochs 5209\n",
      "training loss 0.000923548194885503\n",
      "testing loss 0.0027693628625785733\n",
      "epochs 5210\n",
      "training loss 0.0009262653055854175\n",
      "epochs 5211\n",
      "training loss 0.0009223182121642771\n",
      "epochs 5212\n",
      "training loss 0.0009399506047235004\n",
      "epochs 5213\n",
      "training loss 0.0009107021284063196\n",
      "epochs 5214\n",
      "training loss 0.0009252137199679735\n",
      "epochs 5215\n",
      "training loss 0.0008977251221131431\n",
      "epochs 5216\n",
      "training loss 0.0009283099345980343\n",
      "epochs 5217\n",
      "training loss 0.0009219621019409601\n",
      "epochs 5218\n",
      "training loss 0.0009181048828290385\n",
      "epochs 5219\n",
      "training loss 0.0009806883922285517\n",
      "testing loss 0.0028070156926500566\n",
      "epochs 5220\n",
      "training loss 0.0009158027480783841\n",
      "epochs 5221\n",
      "training loss 0.0008874957436111142\n",
      "epochs 5222\n",
      "training loss 0.0008839121665862194\n",
      "epochs 5223\n",
      "training loss 0.0008833075880276826\n",
      "epochs 5224\n",
      "training loss 0.0009038211195021981\n",
      "epochs 5225\n",
      "training loss 0.000957511143552247\n",
      "epochs 5226\n",
      "training loss 0.0009403468631059328\n",
      "epochs 5227\n",
      "training loss 0.0009049898613064541\n",
      "epochs 5228\n",
      "training loss 0.0009006881541650357\n",
      "epochs 5229\n",
      "training loss 0.000901924272398337\n",
      "testing loss 0.002733262278905786\n",
      "epochs 5230\n",
      "training loss 0.0009342807740161315\n",
      "epochs 5231\n",
      "training loss 0.0009184453408531052\n",
      "epochs 5232\n",
      "training loss 0.0009395573212125403\n",
      "epochs 5233\n",
      "training loss 0.0009054047441057646\n",
      "epochs 5234\n",
      "training loss 0.0009041946898122519\n",
      "epochs 5235\n",
      "training loss 0.0012573302023029267\n",
      "epochs 5236\n",
      "training loss 0.0009452474599358793\n",
      "epochs 5237\n",
      "training loss 0.0009129802552391535\n",
      "epochs 5238\n",
      "training loss 0.0009149031514212727\n",
      "epochs 5239\n",
      "training loss 0.0009640085181478966\n",
      "testing loss 0.002832596554794078\n",
      "epochs 5240\n",
      "training loss 0.0009497746566556802\n",
      "epochs 5241\n",
      "training loss 0.0009051911512709655\n",
      "epochs 5242\n",
      "training loss 0.0009500672624094021\n",
      "epochs 5243\n",
      "training loss 0.0008988868091135119\n",
      "epochs 5244\n",
      "training loss 0.0009260108914880201\n",
      "epochs 5245\n",
      "training loss 0.0009528708593434397\n",
      "epochs 5246\n",
      "training loss 0.0009321984689892199\n",
      "epochs 5247\n",
      "training loss 0.0009442371203731123\n",
      "epochs 5248\n",
      "training loss 0.0008786771918673883\n",
      "epochs 5249\n",
      "training loss 0.0009573480709863537\n",
      "testing loss 0.0026576429913447944\n",
      "epochs 5250\n",
      "training loss 0.0009575242975989985\n",
      "epochs 5251\n",
      "training loss 0.0009408175487405306\n",
      "epochs 5252\n",
      "training loss 0.0009212500436442386\n",
      "epochs 5253\n",
      "training loss 0.0008903041740810063\n",
      "epochs 5254\n",
      "training loss 0.0009342604999693739\n",
      "epochs 5255\n",
      "training loss 0.0009301649394167706\n",
      "epochs 5256\n",
      "training loss 0.0009325796113374974\n",
      "epochs 5257\n",
      "training loss 0.0009038455046198152\n",
      "epochs 5258\n",
      "training loss 0.0009392914811636132\n",
      "epochs 5259\n",
      "training loss 0.0009248246584585826\n",
      "testing loss 0.0029187330919159377\n",
      "epochs 5260\n",
      "training loss 0.0009431504726899635\n",
      "epochs 5261\n",
      "training loss 0.000924142624964898\n",
      "epochs 5262\n",
      "training loss 0.000971245428329294\n",
      "epochs 5263\n",
      "training loss 0.0008976650686147637\n",
      "epochs 5264\n",
      "training loss 0.0008967436930924756\n",
      "epochs 5265\n",
      "training loss 0.0008808017600791976\n",
      "epochs 5266\n",
      "training loss 0.0008860161682975827\n",
      "epochs 5267\n",
      "training loss 0.0009254462333002601\n",
      "epochs 5268\n",
      "training loss 0.0009005915466094486\n",
      "epochs 5269\n",
      "training loss 0.0009309633973438157\n",
      "testing loss 0.002577971236892573\n",
      "epochs 5270\n",
      "training loss 0.0009396013361118779\n",
      "epochs 5271\n",
      "training loss 0.0009274431294655444\n",
      "epochs 5272\n",
      "training loss 0.0009411089093103088\n",
      "epochs 5273\n",
      "training loss 0.0008873631535480111\n",
      "epochs 5274\n",
      "training loss 0.0009624795240879421\n",
      "epochs 5275\n",
      "training loss 0.0009456505370184802\n",
      "epochs 5276\n",
      "training loss 0.0009368210007460635\n",
      "epochs 5277\n",
      "training loss 0.000939015164178029\n",
      "epochs 5278\n",
      "training loss 0.0008919353424896993\n",
      "epochs 5279\n",
      "training loss 0.0009288378584440684\n",
      "testing loss 0.002676488003934608\n",
      "epochs 5280\n",
      "training loss 0.0009341398467229096\n",
      "epochs 5281\n",
      "training loss 0.0009062843596121143\n",
      "epochs 5282\n",
      "training loss 0.0009037208580447355\n",
      "epochs 5283\n",
      "training loss 0.000894267278766588\n",
      "epochs 5284\n",
      "training loss 0.0009151847670732298\n",
      "epochs 5285\n",
      "training loss 0.0009448066306736399\n",
      "epochs 5286\n",
      "training loss 0.000900935775569943\n",
      "epochs 5287\n",
      "training loss 0.0009267657283462844\n",
      "epochs 5288\n",
      "training loss 0.0009025382775902511\n",
      "epochs 5289\n",
      "training loss 0.0009335959277124255\n",
      "testing loss 0.002822047364088898\n",
      "epochs 5290\n",
      "training loss 0.0009406651935496262\n",
      "epochs 5291\n",
      "training loss 0.000938626944291261\n",
      "epochs 5292\n",
      "training loss 0.0009345152163396852\n",
      "epochs 5293\n",
      "training loss 0.0009483207454359991\n",
      "epochs 5294\n",
      "training loss 0.0009174961465804364\n",
      "epochs 5295\n",
      "training loss 0.0009210033092426991\n",
      "epochs 5296\n",
      "training loss 0.0009075404377654195\n",
      "epochs 5297\n",
      "training loss 0.0009302379088235685\n",
      "epochs 5298\n",
      "training loss 0.0009505074525373707\n",
      "epochs 5299\n",
      "training loss 0.0008901305165669759\n",
      "testing loss 0.0025884975032742195\n",
      "epochs 5300\n",
      "training loss 0.0008636735782903368\n",
      "epochs 5301\n",
      "training loss 0.000951570938259935\n",
      "epochs 5302\n",
      "training loss 0.0009235010242109444\n",
      "epochs 5303\n",
      "training loss 0.0009393234897155802\n",
      "epochs 5304\n",
      "training loss 0.0009230344455779776\n",
      "epochs 5305\n",
      "training loss 0.0009020898996436216\n",
      "epochs 5306\n",
      "training loss 0.0008800834869204435\n",
      "epochs 5307\n",
      "training loss 0.000872765995928661\n",
      "epochs 5308\n",
      "training loss 0.0009394355013359361\n",
      "epochs 5309\n",
      "training loss 0.0009564782967323884\n",
      "testing loss 0.002792342057359134\n",
      "epochs 5310\n",
      "training loss 0.0009454531521864779\n",
      "epochs 5311\n",
      "training loss 0.0008804934969954589\n",
      "epochs 5312\n",
      "training loss 0.0009124402610880909\n",
      "epochs 5313\n",
      "training loss 0.0009728575976866957\n",
      "epochs 5314\n",
      "training loss 0.0008752501241850036\n",
      "epochs 5315\n",
      "training loss 0.0008893495792343273\n",
      "epochs 5316\n",
      "training loss 0.000913525160506798\n",
      "epochs 5317\n",
      "training loss 0.00090940088314276\n",
      "epochs 5318\n",
      "training loss 0.0009372879334903777\n",
      "epochs 5319\n",
      "training loss 0.000932624755005457\n",
      "testing loss 0.0027882298652796033\n",
      "epochs 5320\n",
      "training loss 0.0009109906130930336\n",
      "epochs 5321\n",
      "training loss 0.0009395769722250185\n",
      "epochs 5322\n",
      "training loss 0.0009214483518372814\n",
      "epochs 5323\n",
      "training loss 0.0009110613033779838\n",
      "epochs 5324\n",
      "training loss 0.0009089880476961914\n",
      "epochs 5325\n",
      "training loss 0.0009329344107384445\n",
      "epochs 5326\n",
      "training loss 0.0009389660517737484\n",
      "epochs 5327\n",
      "training loss 0.0008721187822931195\n",
      "epochs 5328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0009216093789081239\n",
      "epochs 5329\n",
      "training loss 0.0009376300177904905\n",
      "testing loss 0.0026804411553460706\n",
      "epochs 5330\n",
      "training loss 0.000912304120357672\n",
      "epochs 5331\n",
      "training loss 0.0008872328012952394\n",
      "epochs 5332\n",
      "training loss 0.0009068636649881283\n",
      "epochs 5333\n",
      "training loss 0.0009272400777019832\n",
      "epochs 5334\n",
      "training loss 0.0009272278452470423\n",
      "epochs 5335\n",
      "training loss 0.0009130496725632517\n",
      "epochs 5336\n",
      "training loss 0.000896868175748413\n",
      "epochs 5337\n",
      "training loss 0.0009087269768716151\n",
      "epochs 5338\n",
      "training loss 0.0009091171836919268\n",
      "epochs 5339\n",
      "training loss 0.0009037544996842832\n",
      "testing loss 0.0026690989244788728\n",
      "epochs 5340\n",
      "training loss 0.0009155518346675601\n",
      "epochs 5341\n",
      "training loss 0.000899440066915113\n",
      "epochs 5342\n",
      "training loss 0.0009840970153362691\n",
      "epochs 5343\n",
      "training loss 0.0009019841927621729\n",
      "epochs 5344\n",
      "training loss 0.0009045662112657184\n",
      "epochs 5345\n",
      "training loss 0.0009363262045783158\n",
      "epochs 5346\n",
      "training loss 0.0009387399869302916\n",
      "epochs 5347\n",
      "training loss 0.0008614595579075426\n",
      "epochs 5348\n",
      "training loss 0.0009515387150119299\n",
      "epochs 5349\n",
      "training loss 0.0009729638025178758\n",
      "testing loss 0.002659653057922206\n",
      "epochs 5350\n",
      "training loss 0.000908340110534117\n",
      "epochs 5351\n",
      "training loss 0.0009412361689493637\n",
      "epochs 5352\n",
      "training loss 0.0009177768388713532\n",
      "epochs 5353\n",
      "training loss 0.0009023840569234297\n",
      "epochs 5354\n",
      "training loss 0.0009094898483332383\n",
      "epochs 5355\n",
      "training loss 0.0009112755510931988\n",
      "epochs 5356\n",
      "training loss 0.0008929080369996932\n",
      "epochs 5357\n",
      "training loss 0.0009447556027242387\n",
      "epochs 5358\n",
      "training loss 0.000943668528922089\n",
      "epochs 5359\n",
      "training loss 0.0009199017666791223\n",
      "testing loss 0.0029531878502610147\n",
      "epochs 5360\n",
      "training loss 0.0009479747569764537\n",
      "epochs 5361\n",
      "training loss 0.0009551617263117097\n",
      "epochs 5362\n",
      "training loss 0.0009004677724415907\n",
      "epochs 5363\n",
      "training loss 0.0009197404523738192\n",
      "epochs 5364\n",
      "training loss 0.0009260302431468136\n",
      "epochs 5365\n",
      "training loss 0.0009550256357389561\n",
      "epochs 5366\n",
      "training loss 0.0009112959560629625\n",
      "epochs 5367\n",
      "training loss 0.0009035952462663302\n",
      "epochs 5368\n",
      "training loss 0.0009691860295207973\n",
      "epochs 5369\n",
      "training loss 0.0009172174906730368\n",
      "testing loss 0.002760850509020982\n",
      "epochs 5370\n",
      "training loss 0.0009384132910372639\n",
      "epochs 5371\n",
      "training loss 0.0009371517360561433\n",
      "epochs 5372\n",
      "training loss 0.0009720240978631659\n",
      "epochs 5373\n",
      "training loss 0.000898942648837241\n",
      "epochs 5374\n",
      "training loss 0.0008966443187268423\n",
      "epochs 5375\n",
      "training loss 0.0009465798706249376\n",
      "epochs 5376\n",
      "training loss 0.0009025267883887554\n",
      "epochs 5377\n",
      "training loss 0.0008973232976452512\n",
      "epochs 5378\n",
      "training loss 0.0009457040243742472\n",
      "epochs 5379\n",
      "training loss 0.000958148397662391\n",
      "testing loss 0.0028341827259385785\n",
      "epochs 5380\n",
      "training loss 0.0009035233949243139\n",
      "epochs 5381\n",
      "training loss 0.0009053116777521137\n",
      "epochs 5382\n",
      "training loss 0.0009112503653990903\n",
      "epochs 5383\n",
      "training loss 0.0009015820746905932\n",
      "epochs 5384\n",
      "training loss 0.0009148520590719062\n",
      "epochs 5385\n",
      "training loss 0.0008914997842053863\n",
      "epochs 5386\n",
      "training loss 0.0009148582514647202\n",
      "epochs 5387\n",
      "training loss 0.0009391990902134441\n",
      "epochs 5388\n",
      "training loss 0.0008969910401783\n",
      "epochs 5389\n",
      "training loss 0.0009510565004566271\n",
      "testing loss 0.00264799880303099\n",
      "epochs 5390\n",
      "training loss 0.0009463272976765471\n",
      "epochs 5391\n",
      "training loss 0.0009091937944390683\n",
      "epochs 5392\n",
      "training loss 0.000885824917385349\n",
      "epochs 5393\n",
      "training loss 0.0009166863586825792\n",
      "epochs 5394\n",
      "training loss 0.0009061265401549264\n",
      "epochs 5395\n",
      "training loss 0.0009043624192564026\n",
      "epochs 5396\n",
      "training loss 0.000914546449302121\n",
      "epochs 5397\n",
      "training loss 0.000907355028142343\n",
      "epochs 5398\n",
      "training loss 0.0009323346397080841\n",
      "epochs 5399\n",
      "training loss 0.0009111058904462435\n",
      "testing loss 0.00269534412551843\n",
      "epochs 5400\n",
      "training loss 0.000918204785398233\n",
      "epochs 5401\n",
      "training loss 0.0008870064705365413\n",
      "epochs 5402\n",
      "training loss 0.0009196205815692407\n",
      "epochs 5403\n",
      "training loss 0.0009207170969547596\n",
      "epochs 5404\n",
      "training loss 0.0009368062021117374\n",
      "epochs 5405\n",
      "training loss 0.0008725682013430335\n",
      "epochs 5406\n",
      "training loss 0.0009082855235135585\n",
      "epochs 5407\n",
      "training loss 0.0008961369729697093\n",
      "epochs 5408\n",
      "training loss 0.0009384521438547702\n",
      "epochs 5409\n",
      "training loss 0.0010015052071125686\n",
      "testing loss 0.0026797712297535483\n",
      "epochs 5410\n",
      "training loss 0.0008884186480133468\n",
      "epochs 5411\n",
      "training loss 0.0009380780252603967\n",
      "epochs 5412\n",
      "training loss 0.0009217736609195186\n",
      "epochs 5413\n",
      "training loss 0.0009319210870157486\n",
      "epochs 5414\n",
      "training loss 0.000897854549147183\n",
      "epochs 5415\n",
      "training loss 0.0008994586214474134\n",
      "epochs 5416\n",
      "training loss 0.0009236614011782915\n",
      "epochs 5417\n",
      "training loss 0.0009483849915667326\n",
      "epochs 5418\n",
      "training loss 0.0009126956354349704\n",
      "epochs 5419\n",
      "training loss 0.0009049014767772011\n",
      "testing loss 0.002935661758454715\n",
      "epochs 5420\n",
      "training loss 0.0009609553667484559\n",
      "epochs 5421\n",
      "training loss 0.0008960496688206429\n",
      "epochs 5422\n",
      "training loss 0.0009162770992917742\n",
      "epochs 5423\n",
      "training loss 0.0009371247494630063\n",
      "epochs 5424\n",
      "training loss 0.0009162573005486065\n",
      "epochs 5425\n",
      "training loss 0.0008950332274496284\n",
      "epochs 5426\n",
      "training loss 0.0009471941841936084\n",
      "epochs 5427\n",
      "training loss 0.0009041689757376592\n",
      "epochs 5428\n",
      "training loss 0.0008895085907985646\n",
      "epochs 5429\n",
      "training loss 0.00089506104151155\n",
      "testing loss 0.0028102484090808543\n",
      "epochs 5430\n",
      "training loss 0.0009201881306785184\n",
      "epochs 5431\n",
      "training loss 0.000904994926522953\n",
      "epochs 5432\n",
      "training loss 0.0009021403954971559\n",
      "epochs 5433\n",
      "training loss 0.0009429222269169137\n",
      "epochs 5434\n",
      "training loss 0.0009380925187910411\n",
      "epochs 5435\n",
      "training loss 0.0008905193896533916\n",
      "epochs 5436\n",
      "training loss 0.0008912877695235823\n",
      "epochs 5437\n",
      "training loss 0.0009445229299843459\n",
      "epochs 5438\n",
      "training loss 0.0009475397820072107\n",
      "epochs 5439\n",
      "training loss 0.0008811093698748271\n",
      "testing loss 0.003135093416794078\n",
      "epochs 5440\n",
      "training loss 0.0009428201315642864\n",
      "epochs 5441\n",
      "training loss 0.0009123403917804168\n",
      "epochs 5442\n",
      "training loss 0.0009360185274634918\n",
      "epochs 5443\n",
      "training loss 0.0009042583589086813\n",
      "epochs 5444\n",
      "training loss 0.0009163286167870652\n",
      "epochs 5445\n",
      "training loss 0.0009370850107921316\n",
      "epochs 5446\n",
      "training loss 0.0009026879191364652\n",
      "epochs 5447\n",
      "training loss 0.0009004786848741665\n",
      "epochs 5448\n",
      "training loss 0.0009207164418089591\n",
      "epochs 5449\n",
      "training loss 0.0009290816162049057\n",
      "testing loss 0.002640767387487655\n",
      "epochs 5450\n",
      "training loss 0.0009111975001929877\n",
      "epochs 5451\n",
      "training loss 0.0009196949848483422\n",
      "epochs 5452\n",
      "training loss 0.0008944369004020258\n",
      "epochs 5453\n",
      "training loss 0.000913625202491302\n",
      "epochs 5454\n",
      "training loss 0.0009063964198862902\n",
      "epochs 5455\n",
      "training loss 0.0009038065231793014\n",
      "epochs 5456\n",
      "training loss 0.0009145884047294925\n",
      "epochs 5457\n",
      "training loss 0.000901177479431888\n",
      "epochs 5458\n",
      "training loss 0.000928713926637834\n",
      "epochs 5459\n",
      "training loss 0.0008893890623399552\n",
      "testing loss 0.002617420677379672\n",
      "epochs 5460\n",
      "training loss 0.0009721503170181666\n",
      "epochs 5461\n",
      "training loss 0.0009168263925147444\n",
      "epochs 5462\n",
      "training loss 0.0009059266399204346\n",
      "epochs 5463\n",
      "training loss 0.0008996902828070825\n",
      "epochs 5464\n",
      "training loss 0.0009463910286075586\n",
      "epochs 5465\n",
      "training loss 0.0008694379525697907\n",
      "epochs 5466\n",
      "training loss 0.0008951746221733673\n",
      "epochs 5467\n",
      "training loss 0.0008959523181000719\n",
      "epochs 5468\n",
      "training loss 0.0009321087163145509\n",
      "epochs 5469\n",
      "training loss 0.0009081353650868539\n",
      "testing loss 0.002620239105118531\n",
      "epochs 5470\n",
      "training loss 0.0009003638307207284\n",
      "epochs 5471\n",
      "training loss 0.0009153159085794219\n",
      "epochs 5472\n",
      "training loss 0.0008915686436913239\n",
      "epochs 5473\n",
      "training loss 0.0009254064203217331\n",
      "epochs 5474\n",
      "training loss 0.0008979690730738762\n",
      "epochs 5475\n",
      "training loss 0.0009132780038785631\n",
      "epochs 5476\n",
      "training loss 0.0009182014173154343\n",
      "epochs 5477\n",
      "training loss 0.0009340464990617181\n",
      "epochs 5478\n",
      "training loss 0.0008706050803056865\n",
      "epochs 5479\n",
      "training loss 0.0009234653152263579\n",
      "testing loss 0.0025956122099805015\n",
      "epochs 5480\n",
      "training loss 0.0009185474547826456\n",
      "epochs 5481\n",
      "training loss 0.0009147315544407464\n",
      "epochs 5482\n",
      "training loss 0.0009131175093458949\n",
      "epochs 5483\n",
      "training loss 0.0009450222092035218\n",
      "epochs 5484\n",
      "training loss 0.0009117068137759824\n",
      "epochs 5485\n",
      "training loss 0.0009094387464300308\n",
      "epochs 5486\n",
      "training loss 0.0009597119667190821\n",
      "epochs 5487\n",
      "training loss 0.0008973094510334887\n",
      "epochs 5488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0009203504130179147\n",
      "epochs 5489\n",
      "training loss 0.0009366272246121148\n",
      "testing loss 0.002710041156573657\n",
      "epochs 5490\n",
      "training loss 0.0009095864914503447\n",
      "epochs 5491\n",
      "training loss 0.0008940821325818309\n",
      "epochs 5492\n",
      "training loss 0.0009193480750623706\n",
      "epochs 5493\n",
      "training loss 0.0008412600194329062\n",
      "epochs 5494\n",
      "training loss 0.0009128280617558542\n",
      "epochs 5495\n",
      "training loss 0.0009054807127109448\n",
      "epochs 5496\n",
      "training loss 0.0008899077806178775\n",
      "epochs 5497\n",
      "training loss 0.0009005730409078911\n",
      "epochs 5498\n",
      "training loss 0.0008897425464069964\n",
      "epochs 5499\n",
      "training loss 0.000933728624017779\n",
      "testing loss 0.0028789371478285457\n",
      "epochs 5500\n",
      "training loss 0.0009385305794740045\n",
      "epochs 5501\n",
      "training loss 0.0008997169266119429\n",
      "epochs 5502\n",
      "training loss 0.0008692373210241548\n",
      "epochs 5503\n",
      "training loss 0.0009280472035281994\n",
      "epochs 5504\n",
      "training loss 0.0008979813685133986\n",
      "epochs 5505\n",
      "training loss 0.0009125202423949531\n",
      "epochs 5506\n",
      "training loss 0.0008949934995710555\n",
      "epochs 5507\n",
      "training loss 0.00089478187703904\n",
      "epochs 5508\n",
      "training loss 0.0008714084289147091\n",
      "epochs 5509\n",
      "training loss 0.0008823254728112313\n",
      "testing loss 0.0026301514012210653\n",
      "epochs 5510\n",
      "training loss 0.0009308755328796668\n",
      "epochs 5511\n",
      "training loss 0.0008925190856248582\n",
      "epochs 5512\n",
      "training loss 0.0008946627194109998\n",
      "epochs 5513\n",
      "training loss 0.0009255969644266557\n",
      "epochs 5514\n",
      "training loss 0.0008796906696900012\n",
      "epochs 5515\n",
      "training loss 0.000907729294998175\n",
      "epochs 5516\n",
      "training loss 0.0009528572341507884\n",
      "epochs 5517\n",
      "training loss 0.0009032585299863993\n",
      "epochs 5518\n",
      "training loss 0.0009059747564252522\n",
      "epochs 5519\n",
      "training loss 0.0008925034790717371\n",
      "testing loss 0.0028554860664307016\n",
      "epochs 5520\n",
      "training loss 0.0009247127498958633\n",
      "epochs 5521\n",
      "training loss 0.0008932605979710784\n",
      "epochs 5522\n",
      "training loss 0.0008643267467995896\n",
      "epochs 5523\n",
      "training loss 0.0009161305395669748\n",
      "epochs 5524\n",
      "training loss 0.0009132586875159045\n",
      "epochs 5525\n",
      "training loss 0.0009050540484333734\n",
      "epochs 5526\n",
      "training loss 0.0009390911242909061\n",
      "epochs 5527\n",
      "training loss 0.0009044919852634445\n",
      "epochs 5528\n",
      "training loss 0.0008674115019512156\n",
      "epochs 5529\n",
      "training loss 0.0009041040388111444\n",
      "testing loss 0.0026293772339642525\n",
      "epochs 5530\n",
      "training loss 0.0008977350678385481\n",
      "epochs 5531\n",
      "training loss 0.0009161323753198308\n",
      "epochs 5532\n",
      "training loss 0.0009157646064912635\n",
      "epochs 5533\n",
      "training loss 0.0009334081296290987\n",
      "epochs 5534\n",
      "training loss 0.0008774151396442284\n",
      "epochs 5535\n",
      "training loss 0.0009031754789773465\n",
      "epochs 5536\n",
      "training loss 0.0008718638970498222\n",
      "epochs 5537\n",
      "training loss 0.0008969782777505681\n",
      "epochs 5538\n",
      "training loss 0.0009159015040981971\n",
      "epochs 5539\n",
      "training loss 0.0009076051010805434\n",
      "testing loss 0.0027669888705589827\n",
      "epochs 5540\n",
      "training loss 0.0008676883926980287\n",
      "epochs 5541\n",
      "training loss 0.0008996848165946461\n",
      "epochs 5542\n",
      "training loss 0.0008661254357063922\n",
      "epochs 5543\n",
      "training loss 0.0009200836227156539\n",
      "epochs 5544\n",
      "training loss 0.0009512091358935341\n",
      "epochs 5545\n",
      "training loss 0.000905701854053211\n",
      "epochs 5546\n",
      "training loss 0.0008825210017211577\n",
      "epochs 5547\n",
      "training loss 0.0009198937487065237\n",
      "epochs 5548\n",
      "training loss 0.0009022140752497246\n",
      "epochs 5549\n",
      "training loss 0.0008820402618189056\n",
      "testing loss 0.0026766275194414118\n",
      "epochs 5550\n",
      "training loss 0.0009152354408228221\n",
      "epochs 5551\n",
      "training loss 0.000899608692120319\n",
      "epochs 5552\n",
      "training loss 0.0008947260300614687\n",
      "epochs 5553\n",
      "training loss 0.0008969435138885487\n",
      "epochs 5554\n",
      "training loss 0.0008944809818244043\n",
      "epochs 5555\n",
      "training loss 0.0009067695468273524\n",
      "epochs 5556\n",
      "training loss 0.0009195538171167884\n",
      "epochs 5557\n",
      "training loss 0.0009148994340040469\n",
      "epochs 5558\n",
      "training loss 0.0008799318892252844\n",
      "epochs 5559\n",
      "training loss 0.0009020022117486082\n",
      "testing loss 0.002657629308416277\n",
      "epochs 5560\n",
      "training loss 0.0009033189736884144\n",
      "epochs 5561\n",
      "training loss 0.000964762258879829\n",
      "epochs 5562\n",
      "training loss 0.0008923528035876191\n",
      "epochs 5563\n",
      "training loss 0.0008817376636321116\n",
      "epochs 5564\n",
      "training loss 0.0008937979707427855\n",
      "epochs 5565\n",
      "training loss 0.000898594489267328\n",
      "epochs 5566\n",
      "training loss 0.0008837457597298168\n",
      "epochs 5567\n",
      "training loss 0.0009124536546000979\n",
      "epochs 5568\n",
      "training loss 0.0008798485793779097\n",
      "epochs 5569\n",
      "training loss 0.0009341048893071485\n",
      "testing loss 0.002604019268356422\n",
      "epochs 5570\n",
      "training loss 0.0009002820947801971\n",
      "epochs 5571\n",
      "training loss 0.000896699749581561\n",
      "epochs 5572\n",
      "training loss 0.0008893066427690234\n",
      "epochs 5573\n",
      "training loss 0.0008987181310967958\n",
      "epochs 5574\n",
      "training loss 0.0008852597509863246\n",
      "epochs 5575\n",
      "training loss 0.000942299156279889\n",
      "epochs 5576\n",
      "training loss 0.0009141760107057627\n",
      "epochs 5577\n",
      "training loss 0.0008962668055112228\n",
      "epochs 5578\n",
      "training loss 0.000899874735099906\n",
      "epochs 5579\n",
      "training loss 0.000882325222376751\n",
      "testing loss 0.003718909355877826\n",
      "epochs 5580\n",
      "training loss 0.0009010952217454844\n",
      "epochs 5581\n",
      "training loss 0.0008819024105086402\n",
      "epochs 5582\n",
      "training loss 0.0009079271100184068\n",
      "epochs 5583\n",
      "training loss 0.0009161566546375051\n",
      "epochs 5584\n",
      "training loss 0.0008892767347351543\n",
      "epochs 5585\n",
      "training loss 0.0009209261977150584\n",
      "epochs 5586\n",
      "training loss 0.0008938159170138755\n",
      "epochs 5587\n",
      "training loss 0.0008829228107082887\n",
      "epochs 5588\n",
      "training loss 0.0009114084730638152\n",
      "epochs 5589\n",
      "training loss 0.0009003089486810981\n",
      "testing loss 0.0026440209641405347\n",
      "epochs 5590\n",
      "training loss 0.0008909053180337031\n",
      "epochs 5591\n",
      "training loss 0.0009056607402484627\n",
      "epochs 5592\n",
      "training loss 0.0009024227783756223\n",
      "epochs 5593\n",
      "training loss 0.0008882487573665157\n",
      "epochs 5594\n",
      "training loss 0.0008672553146969934\n",
      "epochs 5595\n",
      "training loss 0.0008835412433978148\n",
      "epochs 5596\n",
      "training loss 0.0009016901096130939\n",
      "epochs 5597\n",
      "training loss 0.0008831282538513293\n",
      "epochs 5598\n",
      "training loss 0.0009162935970059531\n",
      "epochs 5599\n",
      "training loss 0.0008728342371572398\n",
      "testing loss 0.0026398483946166455\n",
      "epochs 5600\n",
      "training loss 0.0009119562317453054\n",
      "epochs 5601\n",
      "training loss 0.0008889472593820872\n",
      "epochs 5602\n",
      "training loss 0.000935082794641255\n",
      "epochs 5603\n",
      "training loss 0.0009037825451614647\n",
      "epochs 5604\n",
      "training loss 0.0009234430495575976\n",
      "epochs 5605\n",
      "training loss 0.0009323484443006387\n",
      "epochs 5606\n",
      "training loss 0.000904079502424378\n",
      "epochs 5607\n",
      "training loss 0.0009081132183103373\n",
      "epochs 5608\n",
      "training loss 0.0009292590887976476\n",
      "epochs 5609\n",
      "training loss 0.000876042845649758\n",
      "testing loss 0.0027836266540136215\n",
      "epochs 5610\n",
      "training loss 0.0009066701463655792\n",
      "epochs 5611\n",
      "training loss 0.0008862308405394917\n",
      "epochs 5612\n",
      "training loss 0.0009051459860184023\n",
      "epochs 5613\n",
      "training loss 0.0009152096652674933\n",
      "epochs 5614\n",
      "training loss 0.0008940863149526521\n",
      "epochs 5615\n",
      "training loss 0.0008922041481216424\n",
      "epochs 5616\n",
      "training loss 0.0008812977827655146\n",
      "epochs 5617\n",
      "training loss 0.0008781337274605052\n",
      "epochs 5618\n",
      "training loss 0.0008826269496601686\n",
      "epochs 5619\n",
      "training loss 0.000871510672528795\n",
      "testing loss 0.0027161989173627335\n",
      "epochs 5620\n",
      "training loss 0.0009127547960030623\n",
      "epochs 5621\n",
      "training loss 0.0008730053163726608\n",
      "epochs 5622\n",
      "training loss 0.0008940255036931429\n",
      "epochs 5623\n",
      "training loss 0.0008723390261051005\n",
      "epochs 5624\n",
      "training loss 0.000932811895023754\n",
      "epochs 5625\n",
      "training loss 0.0008704221608735746\n",
      "epochs 5626\n",
      "training loss 0.000907595260358928\n",
      "epochs 5627\n",
      "training loss 0.0009062463174557093\n",
      "epochs 5628\n",
      "training loss 0.0009347400416607338\n",
      "epochs 5629\n",
      "training loss 0.0008697722724279372\n",
      "testing loss 0.0026314885901280592\n",
      "epochs 5630\n",
      "training loss 0.0009102657691032079\n",
      "epochs 5631\n",
      "training loss 0.0009003237805769447\n",
      "epochs 5632\n",
      "training loss 0.0009180299525848366\n",
      "epochs 5633\n",
      "training loss 0.0008949232139974255\n",
      "epochs 5634\n",
      "training loss 0.0008877516910999923\n",
      "epochs 5635\n",
      "training loss 0.000913908287046313\n",
      "epochs 5636\n",
      "training loss 0.0009145474387438951\n",
      "epochs 5637\n",
      "training loss 0.0008695340063564457\n",
      "epochs 5638\n",
      "training loss 0.0009024790381145095\n",
      "epochs 5639\n",
      "training loss 0.0009093448897614214\n",
      "testing loss 0.002695622171542825\n",
      "epochs 5640\n",
      "training loss 0.0009230127312896893\n",
      "epochs 5641\n",
      "training loss 0.0009069182515663794\n",
      "epochs 5642\n",
      "training loss 0.0009077330278961616\n",
      "epochs 5643\n",
      "training loss 0.0009120344516954251\n",
      "epochs 5644\n",
      "training loss 0.0008465856495759193\n",
      "epochs 5645\n",
      "training loss 0.0008987114341196386\n",
      "epochs 5646\n",
      "training loss 0.0008927937449305672\n",
      "epochs 5647\n",
      "training loss 0.0008988150503906764\n",
      "epochs 5648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0008553690193886203\n",
      "epochs 5649\n",
      "training loss 0.0009082120127224372\n",
      "testing loss 0.0027179370439406616\n",
      "epochs 5650\n",
      "training loss 0.0009098325413368687\n",
      "epochs 5651\n",
      "training loss 0.0009056920186392849\n",
      "epochs 5652\n",
      "training loss 0.000896987124076566\n",
      "epochs 5653\n",
      "training loss 0.0009111552866349863\n",
      "epochs 5654\n",
      "training loss 0.000874402969888069\n",
      "epochs 5655\n",
      "training loss 0.0008749020096419895\n",
      "epochs 5656\n",
      "training loss 0.0009002526228614774\n",
      "epochs 5657\n",
      "training loss 0.0008919685698640859\n",
      "epochs 5658\n",
      "training loss 0.0008790525604226261\n",
      "epochs 5659\n",
      "training loss 0.0008785918202480488\n",
      "testing loss 0.002884149486991636\n",
      "epochs 5660\n",
      "training loss 0.0009021067228076249\n",
      "epochs 5661\n",
      "training loss 0.0009683665280204866\n",
      "epochs 5662\n",
      "training loss 0.0009035022572280222\n",
      "epochs 5663\n",
      "training loss 0.0008901421463337803\n",
      "epochs 5664\n",
      "training loss 0.0008928389558292597\n",
      "epochs 5665\n",
      "training loss 0.0008943076184448307\n",
      "epochs 5666\n",
      "training loss 0.0008673464778901324\n",
      "epochs 5667\n",
      "training loss 0.0008652241371386603\n",
      "epochs 5668\n",
      "training loss 0.00088453220113642\n",
      "epochs 5669\n",
      "training loss 0.0009263813061997758\n",
      "testing loss 0.0026616945686083\n",
      "epochs 5670\n",
      "training loss 0.0008711093215068016\n",
      "epochs 5671\n",
      "training loss 0.0008809469072509127\n",
      "epochs 5672\n",
      "training loss 0.0009146224013650026\n",
      "epochs 5673\n",
      "training loss 0.0008785817032611977\n",
      "epochs 5674\n",
      "training loss 0.00090477254990443\n",
      "epochs 5675\n",
      "training loss 0.0009148381906986169\n",
      "epochs 5676\n",
      "training loss 0.0008824851212097405\n",
      "epochs 5677\n",
      "training loss 0.0008588084825604735\n",
      "epochs 5678\n",
      "training loss 0.000850908665232768\n",
      "epochs 5679\n",
      "training loss 0.0008777620376648217\n",
      "testing loss 0.002728309735394882\n",
      "epochs 5680\n",
      "training loss 0.0009106010030339385\n",
      "epochs 5681\n",
      "training loss 0.0008851084375179353\n",
      "epochs 5682\n",
      "training loss 0.0009079617739189416\n",
      "epochs 5683\n",
      "training loss 0.0009017626405162821\n",
      "epochs 5684\n",
      "training loss 0.0008722274046883802\n",
      "epochs 5685\n",
      "training loss 0.0008654312160231263\n",
      "epochs 5686\n",
      "training loss 0.0008904358299023056\n",
      "epochs 5687\n",
      "training loss 0.0009063177284364246\n",
      "epochs 5688\n",
      "training loss 0.0008797364708922215\n",
      "epochs 5689\n",
      "training loss 0.000888412540189718\n",
      "testing loss 0.002638993386826511\n",
      "epochs 5690\n",
      "training loss 0.0009110128255964379\n",
      "epochs 5691\n",
      "training loss 0.0009570220615544942\n",
      "epochs 5692\n",
      "training loss 0.0009179164283614503\n",
      "epochs 5693\n",
      "training loss 0.0008801890108635292\n",
      "epochs 5694\n",
      "training loss 0.000873081270912464\n",
      "epochs 5695\n",
      "training loss 0.0008924326949291452\n",
      "epochs 5696\n",
      "training loss 0.0008907550230224569\n",
      "epochs 5697\n",
      "training loss 0.0009382198192993048\n",
      "epochs 5698\n",
      "training loss 0.00091621219385755\n",
      "epochs 5699\n",
      "training loss 0.0009025358126108113\n",
      "testing loss 0.0028636380035185803\n",
      "epochs 5700\n",
      "training loss 0.0008762148727300523\n",
      "epochs 5701\n",
      "training loss 0.0008829024295346658\n",
      "epochs 5702\n",
      "training loss 0.0010151506579759895\n",
      "epochs 5703\n",
      "training loss 0.0009704946187688576\n",
      "epochs 5704\n",
      "training loss 0.0009043952778348342\n",
      "epochs 5705\n",
      "training loss 0.0008549192106658395\n",
      "epochs 5706\n",
      "training loss 0.0008835116680676123\n",
      "epochs 5707\n",
      "training loss 0.000893930982678738\n",
      "epochs 5708\n",
      "training loss 0.0009053369252808041\n",
      "epochs 5709\n",
      "training loss 0.0008669391081252314\n",
      "testing loss 0.002780377893735067\n",
      "epochs 5710\n",
      "training loss 0.0008944817854085864\n",
      "epochs 5711\n",
      "training loss 0.0008829664035578539\n",
      "epochs 5712\n",
      "training loss 0.0008604246649734031\n",
      "epochs 5713\n",
      "training loss 0.0009027411925038861\n",
      "epochs 5714\n",
      "training loss 0.0009227424869200605\n",
      "epochs 5715\n",
      "training loss 0.0008803710911499398\n",
      "epochs 5716\n",
      "training loss 0.0008551763666271427\n",
      "epochs 5717\n",
      "training loss 0.0009252065444141513\n",
      "epochs 5718\n",
      "training loss 0.0008675741054942848\n",
      "epochs 5719\n",
      "training loss 0.0008509250104393369\n",
      "testing loss 0.0027110872464915013\n",
      "epochs 5720\n",
      "training loss 0.0009036401327767362\n",
      "epochs 5721\n",
      "training loss 0.0009099913209550774\n",
      "epochs 5722\n",
      "training loss 0.0008867749136498928\n",
      "epochs 5723\n",
      "training loss 0.0008850854816726242\n",
      "epochs 5724\n",
      "training loss 0.0008587729562488653\n",
      "epochs 5725\n",
      "training loss 0.0008950629928835744\n",
      "epochs 5726\n",
      "training loss 0.000881731047331834\n",
      "epochs 5727\n",
      "training loss 0.0008851911007118125\n",
      "epochs 5728\n",
      "training loss 0.0008774290779020955\n",
      "epochs 5729\n",
      "training loss 0.0008968860128580233\n",
      "testing loss 0.0026492745876031537\n",
      "epochs 5730\n",
      "training loss 0.0008948720400865645\n",
      "epochs 5731\n",
      "training loss 0.0008702178790529844\n",
      "epochs 5732\n",
      "training loss 0.000885259316640405\n",
      "epochs 5733\n",
      "training loss 0.0008674420673424225\n",
      "epochs 5734\n",
      "training loss 0.0009088386958613596\n",
      "epochs 5735\n",
      "training loss 0.000877763201994963\n",
      "epochs 5736\n",
      "training loss 0.0009032363716214273\n",
      "epochs 5737\n",
      "training loss 0.0008979203716616818\n",
      "epochs 5738\n",
      "training loss 0.000930089260434511\n",
      "epochs 5739\n",
      "training loss 0.0008893622470082642\n",
      "testing loss 0.0026155617996284092\n",
      "epochs 5740\n",
      "training loss 0.0008886550724050624\n",
      "epochs 5741\n",
      "training loss 0.0008521073289593472\n",
      "epochs 5742\n",
      "training loss 0.0009199565118688264\n",
      "epochs 5743\n",
      "training loss 0.0008755151190728481\n",
      "epochs 5744\n",
      "training loss 0.0008570025171652877\n",
      "epochs 5745\n",
      "training loss 0.0008883379665337854\n",
      "epochs 5746\n",
      "training loss 0.000882939156534088\n",
      "epochs 5747\n",
      "training loss 0.0008803354292268664\n",
      "epochs 5748\n",
      "training loss 0.0008855041508006617\n",
      "epochs 5749\n",
      "training loss 0.000896497618967503\n",
      "testing loss 0.0026758162033123276\n",
      "epochs 5750\n",
      "training loss 0.0008554173678394667\n",
      "epochs 5751\n",
      "training loss 0.0008967780587893251\n",
      "epochs 5752\n",
      "training loss 0.0008770248747931788\n",
      "epochs 5753\n",
      "training loss 0.0008780750761652156\n",
      "epochs 5754\n",
      "training loss 0.0009274482773050756\n",
      "epochs 5755\n",
      "training loss 0.0008633551914082121\n",
      "epochs 5756\n",
      "training loss 0.0008887846716736248\n",
      "epochs 5757\n",
      "training loss 0.0008862790753173226\n",
      "epochs 5758\n",
      "training loss 0.0008711460454100715\n",
      "epochs 5759\n",
      "training loss 0.0009174262037066216\n",
      "testing loss 0.0026445232058465743\n",
      "epochs 5760\n",
      "training loss 0.0008765670424362639\n",
      "epochs 5761\n",
      "training loss 0.0008988411470612166\n",
      "epochs 5762\n",
      "training loss 0.0008889488148889405\n",
      "epochs 5763\n",
      "training loss 0.000906360051598213\n",
      "epochs 5764\n",
      "training loss 0.0008852928506223184\n",
      "epochs 5765\n",
      "training loss 0.0008846551859623959\n",
      "epochs 5766\n",
      "training loss 0.0008934964264861922\n",
      "epochs 5767\n",
      "training loss 0.0008792879052303087\n",
      "epochs 5768\n",
      "training loss 0.0008862414407030054\n",
      "epochs 5769\n",
      "training loss 0.0008943817967663613\n",
      "testing loss 0.002727854541991672\n",
      "epochs 5770\n",
      "training loss 0.0009293869792555932\n",
      "epochs 5771\n",
      "training loss 0.0008785455068537971\n",
      "epochs 5772\n",
      "training loss 0.0008630609883938132\n",
      "epochs 5773\n",
      "training loss 0.0009032719950637523\n",
      "epochs 5774\n",
      "training loss 0.0008466305451093972\n",
      "epochs 5775\n",
      "training loss 0.0008735871844780617\n",
      "epochs 5776\n",
      "training loss 0.0008916926349436802\n",
      "epochs 5777\n",
      "training loss 0.0008952886756618527\n",
      "epochs 5778\n",
      "training loss 0.0008908026368006226\n",
      "epochs 5779\n",
      "training loss 0.0009773344374375027\n",
      "testing loss 0.0036460718739421125\n",
      "epochs 5780\n",
      "training loss 0.0009169210285290872\n",
      "epochs 5781\n",
      "training loss 0.0008992010782017035\n",
      "epochs 5782\n",
      "training loss 0.0008501985217734753\n",
      "epochs 5783\n",
      "training loss 0.000921747732148888\n",
      "epochs 5784\n",
      "training loss 0.0008778059453488325\n",
      "epochs 5785\n",
      "training loss 0.0008857897730504578\n",
      "epochs 5786\n",
      "training loss 0.0008841879104298049\n",
      "epochs 5787\n",
      "training loss 0.0009029389066780184\n",
      "epochs 5788\n",
      "training loss 0.0009299056121681769\n",
      "epochs 5789\n",
      "training loss 0.00086246578639945\n",
      "testing loss 0.0026990841058205073\n",
      "epochs 5790\n",
      "training loss 0.0008623843209741575\n",
      "epochs 5791\n",
      "training loss 0.0009126188909340547\n",
      "epochs 5792\n",
      "training loss 0.0008470996240179386\n",
      "epochs 5793\n",
      "training loss 0.0009313176119258474\n",
      "epochs 5794\n",
      "training loss 0.0008868656535547985\n",
      "epochs 5795\n",
      "training loss 0.0009050771275055411\n",
      "epochs 5796\n",
      "training loss 0.0008853076429758787\n",
      "epochs 5797\n",
      "training loss 0.0008964478996289345\n",
      "epochs 5798\n",
      "training loss 0.0008578204705010488\n",
      "epochs 5799\n",
      "training loss 0.0008928805412222152\n",
      "testing loss 0.0026964207682659493\n",
      "epochs 5800\n",
      "training loss 0.0008838336333054993\n",
      "epochs 5801\n",
      "training loss 0.0008802887507520424\n",
      "epochs 5802\n",
      "training loss 0.0008954007438975627\n",
      "epochs 5803\n",
      "training loss 0.0008747253713295667\n",
      "epochs 5804\n",
      "training loss 0.0008475210081175887\n",
      "epochs 5805\n",
      "training loss 0.0009245327794160071\n",
      "epochs 5806\n",
      "training loss 0.0009178234163231548\n",
      "epochs 5807\n",
      "training loss 0.0008640077590870198\n",
      "epochs 5808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0008433766951105108\n",
      "epochs 5809\n",
      "training loss 0.0009037600583389775\n",
      "testing loss 0.0025424267650839497\n",
      "epochs 5810\n",
      "training loss 0.0008905712130485176\n",
      "epochs 5811\n",
      "training loss 0.0009062096664447077\n",
      "epochs 5812\n",
      "training loss 0.0008926380408530076\n",
      "epochs 5813\n",
      "training loss 0.000870677139355283\n",
      "epochs 5814\n",
      "training loss 0.0009067317610323474\n",
      "epochs 5815\n",
      "training loss 0.0008722417729575538\n",
      "epochs 5816\n",
      "training loss 0.0008967833507326248\n",
      "epochs 5817\n",
      "training loss 0.0008813004481986924\n",
      "epochs 5818\n",
      "training loss 0.0008783557957022714\n",
      "epochs 5819\n",
      "training loss 0.0008671188317090669\n",
      "testing loss 0.0026054479002602503\n",
      "epochs 5820\n",
      "training loss 0.0008715820988133482\n",
      "epochs 5821\n",
      "training loss 0.0008921079249312326\n",
      "epochs 5822\n",
      "training loss 0.0008755145333693179\n",
      "epochs 5823\n",
      "training loss 0.0008581990146468815\n",
      "epochs 5824\n",
      "training loss 0.0008439878759924096\n",
      "epochs 5825\n",
      "training loss 0.0008405399822635649\n",
      "epochs 5826\n",
      "training loss 0.0009079290394519815\n",
      "epochs 5827\n",
      "training loss 0.0009470014560629407\n",
      "epochs 5828\n",
      "training loss 0.0008854952869592885\n",
      "epochs 5829\n",
      "training loss 0.0008408437996266245\n",
      "testing loss 0.0026201934979713346\n",
      "epochs 5830\n",
      "training loss 0.0008667156106996205\n",
      "epochs 5831\n",
      "training loss 0.0008682532094972417\n",
      "epochs 5832\n",
      "training loss 0.0008631739984790152\n",
      "epochs 5833\n",
      "training loss 0.0009054972071520486\n",
      "epochs 5834\n",
      "training loss 0.0008943007518869958\n",
      "epochs 5835\n",
      "training loss 0.0008534808285070136\n",
      "epochs 5836\n",
      "training loss 0.0008645196472532414\n",
      "epochs 5837\n",
      "training loss 0.0008488466598179849\n",
      "epochs 5838\n",
      "training loss 0.0008874858522010644\n",
      "epochs 5839\n",
      "training loss 0.0009018845402736127\n",
      "testing loss 0.0026775026014437967\n",
      "epochs 5840\n",
      "training loss 0.0008930227965877977\n",
      "epochs 5841\n",
      "training loss 0.0008916513078582776\n",
      "epochs 5842\n",
      "training loss 0.0009064776828232303\n",
      "epochs 5843\n",
      "training loss 0.0008715013399299823\n",
      "epochs 5844\n",
      "training loss 0.0009057861570577424\n",
      "epochs 5845\n",
      "training loss 0.0008334633861533946\n",
      "epochs 5846\n",
      "training loss 0.0009033820607966961\n",
      "epochs 5847\n",
      "training loss 0.0008891726904874243\n",
      "epochs 5848\n",
      "training loss 0.0008808026010825901\n",
      "epochs 5849\n",
      "training loss 0.0008708667117107297\n",
      "testing loss 0.002983223044574393\n",
      "epochs 5850\n",
      "training loss 0.0009078885937968727\n",
      "epochs 5851\n",
      "training loss 0.0008462010927910217\n",
      "epochs 5852\n",
      "training loss 0.0009902848198296542\n",
      "epochs 5853\n",
      "training loss 0.0009280268706545505\n",
      "epochs 5854\n",
      "training loss 0.0008781184575033564\n",
      "epochs 5855\n",
      "training loss 0.0008761423714614635\n",
      "epochs 5856\n",
      "training loss 0.0009130969448800631\n",
      "epochs 5857\n",
      "training loss 0.0008877674147761271\n",
      "epochs 5858\n",
      "training loss 0.0008637555383922855\n",
      "epochs 5859\n",
      "training loss 0.0008916856394974532\n",
      "testing loss 0.002803684966490069\n",
      "epochs 5860\n",
      "training loss 0.0008457489507194993\n",
      "epochs 5861\n",
      "training loss 0.0008754519493415341\n",
      "epochs 5862\n",
      "training loss 0.0008838778650239506\n",
      "epochs 5863\n",
      "training loss 0.000883389183579828\n",
      "epochs 5864\n",
      "training loss 0.0008800263567204874\n",
      "epochs 5865\n",
      "training loss 0.0008709552201745312\n",
      "epochs 5866\n",
      "training loss 0.0009009427947238495\n",
      "epochs 5867\n",
      "training loss 0.0009086243559692712\n",
      "epochs 5868\n",
      "training loss 0.0008665610733406624\n",
      "epochs 5869\n",
      "training loss 0.0008857094461245513\n",
      "testing loss 0.0026315258372515598\n",
      "epochs 5870\n",
      "training loss 0.000855224247647472\n",
      "epochs 5871\n",
      "training loss 0.0008681978238463447\n",
      "epochs 5872\n",
      "training loss 0.0009124473733034857\n",
      "epochs 5873\n",
      "training loss 0.0010052540368367961\n",
      "epochs 5874\n",
      "training loss 0.0008544927548920017\n",
      "epochs 5875\n",
      "training loss 0.000884856809515189\n",
      "epochs 5876\n",
      "training loss 0.0008756743945177123\n",
      "epochs 5877\n",
      "training loss 0.0008683161879748129\n",
      "epochs 5878\n",
      "training loss 0.0008690444020820516\n",
      "epochs 5879\n",
      "training loss 0.0008645434546288624\n",
      "testing loss 0.002656902471418552\n",
      "epochs 5880\n",
      "training loss 0.0008622492360316307\n",
      "epochs 5881\n",
      "training loss 0.0009252701844067502\n",
      "epochs 5882\n",
      "training loss 0.0008842288705769439\n",
      "epochs 5883\n",
      "training loss 0.0008621585874189835\n",
      "epochs 5884\n",
      "training loss 0.0008863636933332225\n",
      "epochs 5885\n",
      "training loss 0.0008828624265408568\n",
      "epochs 5886\n",
      "training loss 0.0008816326407349094\n",
      "epochs 5887\n",
      "training loss 0.0008905474946651563\n",
      "epochs 5888\n",
      "training loss 0.0008549983665385314\n",
      "epochs 5889\n",
      "training loss 0.0008547076336914917\n",
      "testing loss 0.0027612113156058687\n",
      "epochs 5890\n",
      "training loss 0.0008516778106486997\n",
      "epochs 5891\n",
      "training loss 0.0008747791417621042\n",
      "epochs 5892\n",
      "training loss 0.0008925197356398922\n",
      "epochs 5893\n",
      "training loss 0.0008810906520426148\n",
      "epochs 5894\n",
      "training loss 0.0008772461937661183\n",
      "epochs 5895\n",
      "training loss 0.0008697889934189188\n",
      "epochs 5896\n",
      "training loss 0.0008939935569757579\n",
      "epochs 5897\n",
      "training loss 0.0008928547804399555\n",
      "epochs 5898\n",
      "training loss 0.0008877854973663743\n",
      "epochs 5899\n",
      "training loss 0.0008732113543307652\n",
      "testing loss 0.002779592195169089\n",
      "epochs 5900\n",
      "training loss 0.0008793701808743949\n",
      "epochs 5901\n",
      "training loss 0.0008655099959344827\n",
      "epochs 5902\n",
      "training loss 0.0008659026457957845\n",
      "epochs 5903\n",
      "training loss 0.0008678000304801248\n",
      "epochs 5904\n",
      "training loss 0.0008711482090896737\n",
      "epochs 5905\n",
      "training loss 0.0008788439641582091\n",
      "epochs 5906\n",
      "training loss 0.000866240595494281\n",
      "epochs 5907\n",
      "training loss 0.0008979906117661055\n",
      "epochs 5908\n",
      "training loss 0.0008778651110476908\n",
      "epochs 5909\n",
      "training loss 0.0008537721390075879\n",
      "testing loss 0.0026766287335458854\n",
      "epochs 5910\n",
      "training loss 0.0008765077358182508\n",
      "epochs 5911\n",
      "training loss 0.000873532741649733\n",
      "epochs 5912\n",
      "training loss 0.0008569335218909618\n",
      "epochs 5913\n",
      "training loss 0.0008574735573535406\n",
      "epochs 5914\n",
      "training loss 0.0008465425743145364\n",
      "epochs 5915\n",
      "training loss 0.0008790098650148846\n",
      "epochs 5916\n",
      "training loss 0.0008748502158814628\n",
      "epochs 5917\n",
      "training loss 0.0008905513349553927\n",
      "epochs 5918\n",
      "training loss 0.0008738130557677768\n",
      "epochs 5919\n",
      "training loss 0.0008954490590868886\n",
      "testing loss 0.002769056460341267\n",
      "epochs 5920\n",
      "training loss 0.0008669842703701039\n",
      "epochs 5921\n",
      "training loss 0.0008521279800289784\n",
      "epochs 5922\n",
      "training loss 0.0008719773044155791\n",
      "epochs 5923\n",
      "training loss 0.000856729648054767\n",
      "epochs 5924\n",
      "training loss 0.0008652930355476538\n",
      "epochs 5925\n",
      "training loss 0.000873080315351441\n",
      "epochs 5926\n",
      "training loss 0.0008507564870533066\n",
      "epochs 5927\n",
      "training loss 0.0008892320987476903\n",
      "epochs 5928\n",
      "training loss 0.0009092177690953685\n",
      "epochs 5929\n",
      "training loss 0.0008872489581672945\n",
      "testing loss 0.002635944986398867\n",
      "epochs 5930\n",
      "training loss 0.0008636631985728581\n",
      "epochs 5931\n",
      "training loss 0.0008800430455999477\n",
      "epochs 5932\n",
      "training loss 0.0008541588361182542\n",
      "epochs 5933\n",
      "training loss 0.0008630490874040782\n",
      "epochs 5934\n",
      "training loss 0.000842831600358353\n",
      "epochs 5935\n",
      "training loss 0.0008687014182170395\n",
      "epochs 5936\n",
      "training loss 0.0008581737347412854\n",
      "epochs 5937\n",
      "training loss 0.0008590190771699665\n",
      "epochs 5938\n",
      "training loss 0.0008652319543036749\n",
      "epochs 5939\n",
      "training loss 0.0008712577152152272\n",
      "testing loss 0.0028419502806876196\n",
      "epochs 5940\n",
      "training loss 0.000874188430515319\n",
      "epochs 5941\n",
      "training loss 0.000830822428412407\n",
      "epochs 5942\n",
      "training loss 0.0008779407343876726\n",
      "epochs 5943\n",
      "training loss 0.000907580497993813\n",
      "epochs 5944\n",
      "training loss 0.0008547148206568462\n",
      "epochs 5945\n",
      "training loss 0.0008821881242276178\n",
      "epochs 5946\n",
      "training loss 0.0008851748790859463\n",
      "epochs 5947\n",
      "training loss 0.0008958054438062115\n",
      "epochs 5948\n",
      "training loss 0.0008661215925853881\n",
      "epochs 5949\n",
      "training loss 0.0008577967533561483\n",
      "testing loss 0.0027435829112444944\n",
      "epochs 5950\n",
      "training loss 0.0009711066151772397\n",
      "epochs 5951\n",
      "training loss 0.0009281290500455941\n",
      "epochs 5952\n",
      "training loss 0.0008654112983877151\n",
      "epochs 5953\n",
      "training loss 0.0008754420555430241\n",
      "epochs 5954\n",
      "training loss 0.0008700745667495105\n",
      "epochs 5955\n",
      "training loss 0.0008613279511823707\n",
      "epochs 5956\n",
      "training loss 0.0009333032045703053\n",
      "epochs 5957\n",
      "training loss 0.0008480898345223313\n",
      "epochs 5958\n",
      "training loss 0.0008625671750332277\n",
      "epochs 5959\n",
      "training loss 0.0008745003493586243\n",
      "testing loss 0.0027213196310624218\n",
      "epochs 5960\n",
      "training loss 0.0009037450360740028\n",
      "epochs 5961\n",
      "training loss 0.0008552295921368971\n",
      "epochs 5962\n",
      "training loss 0.0008760076400110537\n",
      "epochs 5963\n",
      "training loss 0.0008652925231786994\n",
      "epochs 5964\n",
      "training loss 0.0009076380429012378\n",
      "epochs 5965\n",
      "training loss 0.0008817136682758227\n",
      "epochs 5966\n",
      "training loss 0.0008595457364589904\n",
      "epochs 5967\n",
      "training loss 0.0008691190823340805\n",
      "epochs 5968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0008539660452683895\n",
      "epochs 5969\n",
      "training loss 0.0008660709282127139\n",
      "testing loss 0.002786348382727712\n",
      "epochs 5970\n",
      "training loss 0.0008732216014444692\n",
      "epochs 5971\n",
      "training loss 0.0008417645577761687\n",
      "epochs 5972\n",
      "training loss 0.0008444876978343698\n",
      "epochs 5973\n",
      "training loss 0.0008830059592903656\n",
      "epochs 5974\n",
      "training loss 0.0008745327654526792\n",
      "epochs 5975\n",
      "training loss 0.0008380366080599104\n",
      "epochs 5976\n",
      "training loss 0.0008618164834804199\n",
      "epochs 5977\n",
      "training loss 0.0008973388083061164\n",
      "epochs 5978\n",
      "training loss 0.0009132476611449284\n",
      "epochs 5979\n",
      "training loss 0.0008468671658856222\n",
      "testing loss 0.0026139673927191194\n",
      "epochs 5980\n",
      "training loss 0.0008718280916422107\n",
      "epochs 5981\n",
      "training loss 0.0008998540689918215\n",
      "epochs 5982\n",
      "training loss 0.0008855566991377045\n",
      "epochs 5983\n",
      "training loss 0.0008507543759198299\n",
      "epochs 5984\n",
      "training loss 0.0008650991506945331\n",
      "epochs 5985\n",
      "training loss 0.000877396648184971\n",
      "epochs 5986\n",
      "training loss 0.0009632640526917308\n",
      "epochs 5987\n",
      "training loss 0.0008930620966666013\n",
      "epochs 5988\n",
      "training loss 0.0008450443092110853\n",
      "epochs 5989\n",
      "training loss 0.0008597841947573014\n",
      "testing loss 0.0032152942157224985\n",
      "epochs 5990\n",
      "training loss 0.0008662175016490442\n",
      "epochs 5991\n",
      "training loss 0.0008660295788350155\n",
      "epochs 5992\n",
      "training loss 0.0008958201290329063\n",
      "epochs 5993\n",
      "training loss 0.0008928551721474365\n",
      "epochs 5994\n",
      "training loss 0.0008920308748843279\n",
      "epochs 5995\n",
      "training loss 0.0008604190047649174\n",
      "epochs 5996\n",
      "training loss 0.0008817272326954298\n",
      "epochs 5997\n",
      "training loss 0.0008431346316526058\n",
      "epochs 5998\n",
      "training loss 0.0008382678317121233\n",
      "epochs 5999\n",
      "training loss 0.000892318558906089\n",
      "testing loss 0.002713684636211115\n",
      "epochs 6000\n",
      "training loss 0.0008850954580941665\n",
      "epochs 6001\n",
      "training loss 0.0008735017956087315\n",
      "epochs 6002\n",
      "training loss 0.0008923499467237753\n",
      "epochs 6003\n",
      "training loss 0.0008842085392956019\n",
      "epochs 6004\n",
      "training loss 0.0008855793577524066\n",
      "epochs 6005\n",
      "training loss 0.0008565992013251233\n",
      "epochs 6006\n",
      "training loss 0.0008543085717874944\n",
      "epochs 6007\n",
      "training loss 0.0008903758152385206\n",
      "epochs 6008\n",
      "training loss 0.0008603312786545401\n",
      "epochs 6009\n",
      "training loss 0.0008572586056617822\n",
      "testing loss 0.0025610759815511606\n",
      "epochs 6010\n",
      "training loss 0.0008977294274454365\n",
      "epochs 6011\n",
      "training loss 0.0009132423072785853\n",
      "epochs 6012\n",
      "training loss 0.0008920457664032193\n",
      "epochs 6013\n",
      "training loss 0.0008602501243483105\n",
      "epochs 6014\n",
      "training loss 0.0008639765312077087\n",
      "epochs 6015\n",
      "training loss 0.0008637606948125813\n",
      "epochs 6016\n",
      "training loss 0.000844825912828877\n",
      "epochs 6017\n",
      "training loss 0.0008949608990347925\n",
      "epochs 6018\n",
      "training loss 0.0008463659597097933\n",
      "epochs 6019\n",
      "training loss 0.0008529118751600317\n",
      "testing loss 0.0025970708124309857\n",
      "epochs 6020\n",
      "training loss 0.0008713401339015439\n",
      "epochs 6021\n",
      "training loss 0.0008580157135918985\n",
      "epochs 6022\n",
      "training loss 0.0008835911771670369\n",
      "epochs 6023\n",
      "training loss 0.0008933593642524243\n",
      "epochs 6024\n",
      "training loss 0.0008472032345389794\n",
      "epochs 6025\n",
      "training loss 0.000865993468235261\n",
      "epochs 6026\n",
      "training loss 0.0008544486032311994\n",
      "epochs 6027\n",
      "training loss 0.0008854147111912085\n",
      "epochs 6028\n",
      "training loss 0.000887278676274343\n",
      "epochs 6029\n",
      "training loss 0.0008676922486459491\n",
      "testing loss 0.002709386453316514\n",
      "epochs 6030\n",
      "training loss 0.0008622394022100115\n",
      "epochs 6031\n",
      "training loss 0.0008435036866227873\n",
      "epochs 6032\n",
      "training loss 0.0009046035125571385\n",
      "epochs 6033\n",
      "training loss 0.0008796244352988015\n",
      "epochs 6034\n",
      "training loss 0.0008446823273563197\n",
      "epochs 6035\n",
      "training loss 0.0008478533968613921\n",
      "epochs 6036\n",
      "training loss 0.0008928866087958658\n",
      "epochs 6037\n",
      "training loss 0.0009019254989169062\n",
      "epochs 6038\n",
      "training loss 0.0008743370491813155\n",
      "epochs 6039\n",
      "training loss 0.0008414663537370047\n",
      "testing loss 0.0027866918095783826\n",
      "epochs 6040\n",
      "training loss 0.0008820676811656555\n",
      "epochs 6041\n",
      "training loss 0.0008534069146853404\n",
      "epochs 6042\n",
      "training loss 0.0008805252970436922\n",
      "epochs 6043\n",
      "training loss 0.000831440092990809\n",
      "epochs 6044\n",
      "training loss 0.000848562622090411\n",
      "epochs 6045\n",
      "training loss 0.0008698051677178875\n",
      "epochs 6046\n",
      "training loss 0.0008454965906979171\n",
      "epochs 6047\n",
      "training loss 0.0008757868610955152\n",
      "epochs 6048\n",
      "training loss 0.0008429288134328444\n",
      "epochs 6049\n",
      "training loss 0.0008783481028256514\n",
      "testing loss 0.002601479087272619\n",
      "epochs 6050\n",
      "training loss 0.0008511773623224619\n",
      "epochs 6051\n",
      "training loss 0.0008613594841653634\n",
      "epochs 6052\n",
      "training loss 0.0008614511797197537\n",
      "epochs 6053\n",
      "training loss 0.0008800835829011609\n",
      "epochs 6054\n",
      "training loss 0.0008590895100319786\n",
      "epochs 6055\n",
      "training loss 0.000856120663583222\n",
      "epochs 6056\n",
      "training loss 0.0009186180763330174\n",
      "epochs 6057\n",
      "training loss 0.0008706752882101275\n",
      "epochs 6058\n",
      "training loss 0.0008331022985558391\n",
      "epochs 6059\n",
      "training loss 0.0008725979694308243\n",
      "testing loss 0.0026479404545983578\n",
      "epochs 6060\n",
      "training loss 0.0008902155691941798\n",
      "epochs 6061\n",
      "training loss 0.0008216475418416393\n",
      "epochs 6062\n",
      "training loss 0.0008544434844070371\n",
      "epochs 6063\n",
      "training loss 0.0008429135052603316\n",
      "epochs 6064\n",
      "training loss 0.0008553637448722344\n",
      "epochs 6065\n",
      "training loss 0.000862694355941556\n",
      "epochs 6066\n",
      "training loss 0.000927535329604278\n",
      "epochs 6067\n",
      "training loss 0.0008396254479106641\n",
      "epochs 6068\n",
      "training loss 0.0008691288852826394\n",
      "epochs 6069\n",
      "training loss 0.0008484370281812267\n",
      "testing loss 0.0027720816086660672\n",
      "epochs 6070\n",
      "training loss 0.0008999017165622768\n",
      "epochs 6071\n",
      "training loss 0.0008489283062354874\n",
      "epochs 6072\n",
      "training loss 0.0008728268691996752\n",
      "epochs 6073\n",
      "training loss 0.0008926545441402604\n",
      "epochs 6074\n",
      "training loss 0.0008606080069860563\n",
      "epochs 6075\n",
      "training loss 0.0008224478915176641\n",
      "epochs 6076\n",
      "training loss 0.0009094651899580762\n",
      "epochs 6077\n",
      "training loss 0.0008511423696990275\n",
      "epochs 6078\n",
      "training loss 0.0008622432906232989\n",
      "epochs 6079\n",
      "training loss 0.0008773818139891257\n",
      "testing loss 0.0029146981702998596\n",
      "epochs 6080\n",
      "training loss 0.0008296858172369753\n",
      "epochs 6081\n",
      "training loss 0.0008335193229193791\n",
      "epochs 6082\n",
      "training loss 0.0008513143878447113\n",
      "epochs 6083\n",
      "training loss 0.0008823449115161316\n",
      "epochs 6084\n",
      "training loss 0.0009078242869815566\n",
      "epochs 6085\n",
      "training loss 0.000904156551055899\n",
      "epochs 6086\n",
      "training loss 0.0008590144599224568\n",
      "epochs 6087\n",
      "training loss 0.0008704063390936465\n",
      "epochs 6088\n",
      "training loss 0.000851390729491998\n",
      "epochs 6089\n",
      "training loss 0.0008549142270992965\n",
      "testing loss 0.002634211686102951\n",
      "epochs 6090\n",
      "training loss 0.0009095223747387721\n",
      "epochs 6091\n",
      "training loss 0.0008891181739706739\n",
      "epochs 6092\n",
      "training loss 0.0008698525036385107\n",
      "epochs 6093\n",
      "training loss 0.0008661275729360087\n",
      "epochs 6094\n",
      "training loss 0.0008530013455540837\n",
      "epochs 6095\n",
      "training loss 0.0008619516393690622\n",
      "epochs 6096\n",
      "training loss 0.0008161201950843449\n",
      "epochs 6097\n",
      "training loss 0.0009022612454819756\n",
      "epochs 6098\n",
      "training loss 0.000807812700234212\n",
      "epochs 6099\n",
      "training loss 0.0008674600185673559\n",
      "testing loss 0.002714911706131348\n",
      "epochs 6100\n",
      "training loss 0.0008354036559953742\n",
      "epochs 6101\n",
      "training loss 0.0008458426059612942\n",
      "epochs 6102\n",
      "training loss 0.0008931372140068561\n",
      "epochs 6103\n",
      "training loss 0.0008890038225438876\n",
      "epochs 6104\n",
      "training loss 0.0008691707200485376\n",
      "epochs 6105\n",
      "training loss 0.0009144346295547385\n",
      "epochs 6106\n",
      "training loss 0.0008381341762188256\n",
      "epochs 6107\n",
      "training loss 0.0008573375269346007\n",
      "epochs 6108\n",
      "training loss 0.0008714570569923278\n",
      "epochs 6109\n",
      "training loss 0.0009058936489151514\n",
      "testing loss 0.0029441397533923755\n",
      "epochs 6110\n",
      "training loss 0.0009029771361073995\n",
      "epochs 6111\n",
      "training loss 0.0008521540935072017\n",
      "epochs 6112\n",
      "training loss 0.0008662462155412498\n",
      "epochs 6113\n",
      "training loss 0.0008410909717356203\n",
      "epochs 6114\n",
      "training loss 0.0008719278817768506\n",
      "epochs 6115\n",
      "training loss 0.0008424129309649311\n",
      "epochs 6116\n",
      "training loss 0.0008608141921440814\n",
      "epochs 6117\n",
      "training loss 0.0008935886349403059\n",
      "epochs 6118\n",
      "training loss 0.000852093995247308\n",
      "epochs 6119\n",
      "training loss 0.0008633012302527006\n",
      "testing loss 0.002696908108810795\n",
      "epochs 6120\n",
      "training loss 0.0008652584676278365\n",
      "epochs 6121\n",
      "training loss 0.0008890960555902956\n",
      "epochs 6122\n",
      "training loss 0.0009029369561906089\n",
      "epochs 6123\n",
      "training loss 0.0008635528273823866\n",
      "epochs 6124\n",
      "training loss 0.0008344359600903767\n",
      "epochs 6125\n",
      "training loss 0.0008325156840491519\n",
      "epochs 6126\n",
      "training loss 0.0008810239964862578\n",
      "epochs 6127\n",
      "training loss 0.0008246950764671863\n",
      "epochs 6128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0008998338604950287\n",
      "epochs 6129\n",
      "training loss 0.0008563627578257257\n",
      "testing loss 0.0028449794469923044\n",
      "epochs 6130\n",
      "training loss 0.0008727286042295974\n",
      "epochs 6131\n",
      "training loss 0.0008642899853886476\n",
      "epochs 6132\n",
      "training loss 0.0008568584748775922\n",
      "epochs 6133\n",
      "training loss 0.0008845070552499823\n",
      "epochs 6134\n",
      "training loss 0.000819170702369827\n",
      "epochs 6135\n",
      "training loss 0.0008737851577559004\n",
      "epochs 6136\n",
      "training loss 0.0008727553842651536\n",
      "epochs 6137\n",
      "training loss 0.0008553652049291392\n",
      "epochs 6138\n",
      "training loss 0.0008660693806673947\n",
      "epochs 6139\n",
      "training loss 0.0008419243023323184\n",
      "testing loss 0.0032620093544139294\n",
      "epochs 6140\n",
      "training loss 0.0008608319821921786\n",
      "epochs 6141\n",
      "training loss 0.0008707763751902219\n",
      "epochs 6142\n",
      "training loss 0.000876918245700689\n",
      "epochs 6143\n",
      "training loss 0.0008639659060095933\n",
      "epochs 6144\n",
      "training loss 0.0008209168501934291\n",
      "epochs 6145\n",
      "training loss 0.0008517981596760973\n",
      "epochs 6146\n",
      "training loss 0.0008856033554971558\n",
      "epochs 6147\n",
      "training loss 0.0008534021471417579\n",
      "epochs 6148\n",
      "training loss 0.00083599272769689\n",
      "epochs 6149\n",
      "training loss 0.0008479342413754811\n",
      "testing loss 0.0026232163361537913\n",
      "epochs 6150\n",
      "training loss 0.0009083573760055742\n",
      "epochs 6151\n",
      "training loss 0.0008672933161611128\n",
      "epochs 6152\n",
      "training loss 0.0008780554754873256\n",
      "epochs 6153\n",
      "training loss 0.0008839737675414565\n",
      "epochs 6154\n",
      "training loss 0.0008433023481813789\n",
      "epochs 6155\n",
      "training loss 0.0008497350319503809\n",
      "epochs 6156\n",
      "training loss 0.0008607717204554499\n",
      "epochs 6157\n",
      "training loss 0.0008518463321770385\n",
      "epochs 6158\n",
      "training loss 0.0008472481166263108\n",
      "epochs 6159\n",
      "training loss 0.0008395442330967749\n",
      "testing loss 0.0027047963301050112\n",
      "epochs 6160\n",
      "training loss 0.0008801685456514902\n",
      "epochs 6161\n",
      "training loss 0.000859453649904811\n",
      "epochs 6162\n",
      "training loss 0.0008625417242200315\n",
      "epochs 6163\n",
      "training loss 0.0008380279273338231\n",
      "epochs 6164\n",
      "training loss 0.0008439032597457396\n",
      "epochs 6165\n",
      "training loss 0.0008630889114402914\n",
      "epochs 6166\n",
      "training loss 0.0008621754753367981\n",
      "epochs 6167\n",
      "training loss 0.0008713397973055717\n",
      "epochs 6168\n",
      "training loss 0.0008840455445758206\n",
      "epochs 6169\n",
      "training loss 0.0008932539414208224\n",
      "testing loss 0.0026611966507819793\n",
      "epochs 6170\n",
      "training loss 0.0008496034848482538\n",
      "epochs 6171\n",
      "training loss 0.00087684998191443\n",
      "epochs 6172\n",
      "training loss 0.0008525863349279191\n",
      "epochs 6173\n",
      "training loss 0.0008492807054992675\n",
      "epochs 6174\n",
      "training loss 0.000843620304344505\n",
      "epochs 6175\n",
      "training loss 0.0008668189637977234\n",
      "epochs 6176\n",
      "training loss 0.0008633165430252109\n",
      "epochs 6177\n",
      "training loss 0.0008404380785185266\n",
      "epochs 6178\n",
      "training loss 0.0008648646999671375\n",
      "epochs 6179\n",
      "training loss 0.0008529191742060949\n",
      "testing loss 0.002787830020343977\n",
      "epochs 6180\n",
      "training loss 0.0008538029893332566\n",
      "epochs 6181\n",
      "training loss 0.0008377223534794031\n",
      "epochs 6182\n",
      "training loss 0.0008797134283546495\n",
      "epochs 6183\n",
      "training loss 0.0008605145972248982\n",
      "epochs 6184\n",
      "training loss 0.0008907331328727249\n",
      "epochs 6185\n",
      "training loss 0.0008507606354549154\n",
      "epochs 6186\n",
      "training loss 0.0008724121318310555\n",
      "epochs 6187\n",
      "training loss 0.0008175799973085409\n",
      "epochs 6188\n",
      "training loss 0.0008458600218172856\n",
      "epochs 6189\n",
      "training loss 0.0008794484913206198\n",
      "testing loss 0.0026508298572912444\n",
      "epochs 6190\n",
      "training loss 0.0008733086837321169\n",
      "epochs 6191\n",
      "training loss 0.0008666144353464971\n",
      "epochs 6192\n",
      "training loss 0.0008923836454209846\n",
      "epochs 6193\n",
      "training loss 0.0008686692258420947\n",
      "epochs 6194\n",
      "training loss 0.0008236484692855186\n",
      "epochs 6195\n",
      "training loss 0.0008303732175123807\n",
      "epochs 6196\n",
      "training loss 0.0008358212111278587\n",
      "epochs 6197\n",
      "training loss 0.0008317954258145453\n",
      "epochs 6198\n",
      "training loss 0.0008843016481107684\n",
      "epochs 6199\n",
      "training loss 0.000880633092766322\n",
      "testing loss 0.0025947593003494917\n",
      "epochs 6200\n",
      "training loss 0.0008328161462447904\n",
      "epochs 6201\n",
      "training loss 0.0008347488211370201\n",
      "epochs 6202\n",
      "training loss 0.000855209056067204\n",
      "epochs 6203\n",
      "training loss 0.0008165492381798641\n",
      "epochs 6204\n",
      "training loss 0.0009001090319043076\n",
      "epochs 6205\n",
      "training loss 0.0008332403077698655\n",
      "epochs 6206\n",
      "training loss 0.0008578731954956885\n",
      "epochs 6207\n",
      "training loss 0.0008818047719343783\n",
      "epochs 6208\n",
      "training loss 0.0008405817632448766\n",
      "epochs 6209\n",
      "training loss 0.0008404574945772853\n",
      "testing loss 0.0026818005036206003\n",
      "epochs 6210\n",
      "training loss 0.0008142711773393025\n",
      "epochs 6211\n",
      "training loss 0.0008662399844907643\n",
      "epochs 6212\n",
      "training loss 0.0008865928456596294\n",
      "epochs 6213\n",
      "training loss 0.0008963474929177041\n",
      "epochs 6214\n",
      "training loss 0.0008175173416269496\n",
      "epochs 6215\n",
      "training loss 0.0008197356219612923\n",
      "epochs 6216\n",
      "training loss 0.0008539748284328831\n",
      "epochs 6217\n",
      "training loss 0.0008669992166466581\n",
      "epochs 6218\n",
      "training loss 0.0008727412092843374\n",
      "epochs 6219\n",
      "training loss 0.000827349668777743\n",
      "testing loss 0.0026247295483299497\n",
      "epochs 6220\n",
      "training loss 0.0008576611719677355\n",
      "epochs 6221\n",
      "training loss 0.0008649274163333116\n",
      "epochs 6222\n",
      "training loss 0.0008472799547129851\n",
      "epochs 6223\n",
      "training loss 0.0008504543721316365\n",
      "epochs 6224\n",
      "training loss 0.0008392183810379073\n",
      "epochs 6225\n",
      "training loss 0.000879094272403975\n",
      "epochs 6226\n",
      "training loss 0.0008455008315417837\n",
      "epochs 6227\n",
      "training loss 0.0008579342448050694\n",
      "epochs 6228\n",
      "training loss 0.0008686869139825546\n",
      "epochs 6229\n",
      "training loss 0.0008595489845880078\n",
      "testing loss 0.002711743111424941\n",
      "epochs 6230\n",
      "training loss 0.0008669682097441508\n",
      "epochs 6231\n",
      "training loss 0.0008615415449017846\n",
      "epochs 6232\n",
      "training loss 0.0008637920857264199\n",
      "epochs 6233\n",
      "training loss 0.0008291256520083305\n",
      "epochs 6234\n",
      "training loss 0.0008414807070561863\n",
      "epochs 6235\n",
      "training loss 0.0008560533726079769\n",
      "epochs 6236\n",
      "training loss 0.0008178734645002598\n",
      "epochs 6237\n",
      "training loss 0.0008599269503726139\n",
      "epochs 6238\n",
      "training loss 0.0008562728564472908\n",
      "epochs 6239\n",
      "training loss 0.000868822656813188\n",
      "testing loss 0.0026554763346005573\n",
      "epochs 6240\n",
      "training loss 0.0008582247227945543\n",
      "epochs 6241\n",
      "training loss 0.0008431523841045694\n",
      "epochs 6242\n",
      "training loss 0.0008238057155122465\n",
      "epochs 6243\n",
      "training loss 0.0008678038518396022\n",
      "epochs 6244\n",
      "training loss 0.0008217766426297013\n",
      "epochs 6245\n",
      "training loss 0.0008516931305865907\n",
      "epochs 6246\n",
      "training loss 0.000839672294373859\n",
      "epochs 6247\n",
      "training loss 0.0009022659679101978\n",
      "epochs 6248\n",
      "training loss 0.000898000735122184\n",
      "epochs 6249\n",
      "training loss 0.0008657868543928517\n",
      "testing loss 0.0025973654325708007\n",
      "epochs 6250\n",
      "training loss 0.0008451612691018493\n",
      "epochs 6251\n",
      "training loss 0.0008239135577656204\n",
      "epochs 6252\n",
      "training loss 0.0008522079233858609\n",
      "epochs 6253\n",
      "training loss 0.0008513471132885993\n",
      "epochs 6254\n",
      "training loss 0.0008215314034042783\n",
      "epochs 6255\n",
      "training loss 0.0008615144625933144\n",
      "epochs 6256\n",
      "training loss 0.0008523512616970132\n",
      "epochs 6257\n",
      "training loss 0.0008482429062513108\n",
      "epochs 6258\n",
      "training loss 0.0008378693513539662\n",
      "epochs 6259\n",
      "training loss 0.000867378658322776\n",
      "testing loss 0.0025722206869915605\n",
      "epochs 6260\n",
      "training loss 0.0008618237362611235\n",
      "epochs 6261\n",
      "training loss 0.0008501673550210542\n",
      "epochs 6262\n",
      "training loss 0.000833114593199208\n",
      "epochs 6263\n",
      "training loss 0.0008563435738899187\n",
      "epochs 6264\n",
      "training loss 0.0008809463830281185\n",
      "epochs 6265\n",
      "training loss 0.0008282812524906792\n",
      "epochs 6266\n",
      "training loss 0.0008435937037973135\n",
      "epochs 6267\n",
      "training loss 0.0008221808641386079\n",
      "epochs 6268\n",
      "training loss 0.0008567146432167061\n",
      "epochs 6269\n",
      "training loss 0.0008710259135556337\n",
      "testing loss 0.0029015438594023123\n",
      "epochs 6270\n",
      "training loss 0.0008829491399440881\n",
      "epochs 6271\n",
      "training loss 0.0008277185240018784\n",
      "epochs 6272\n",
      "training loss 0.0008433179083806788\n",
      "epochs 6273\n",
      "training loss 0.0008581020776607029\n",
      "epochs 6274\n",
      "training loss 0.0008342854274715662\n",
      "epochs 6275\n",
      "training loss 0.0008455982772699956\n",
      "epochs 6276\n",
      "training loss 0.000861257561843412\n",
      "epochs 6277\n",
      "training loss 0.0009207669338586494\n",
      "epochs 6278\n",
      "training loss 0.0008594171112399027\n",
      "epochs 6279\n",
      "training loss 0.0008543237031254872\n",
      "testing loss 0.002681148794549022\n",
      "epochs 6280\n",
      "training loss 0.0008317396635830821\n",
      "epochs 6281\n",
      "training loss 0.0008496888727445077\n",
      "epochs 6282\n",
      "training loss 0.0008633342301041329\n",
      "epochs 6283\n",
      "training loss 0.0008332458974745431\n",
      "epochs 6284\n",
      "training loss 0.0008643237169935315\n",
      "epochs 6285\n",
      "training loss 0.0008660613190832811\n",
      "epochs 6286\n",
      "training loss 0.0008818834992111477\n",
      "epochs 6287\n",
      "training loss 0.000820127798111311\n",
      "epochs 6288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0008312894915489588\n",
      "epochs 6289\n",
      "training loss 0.0008474291143209977\n",
      "testing loss 0.00274290931237703\n",
      "epochs 6290\n",
      "training loss 0.0008263099370000595\n",
      "epochs 6291\n",
      "training loss 0.000873264031733123\n",
      "epochs 6292\n",
      "training loss 0.0008396681880799198\n",
      "epochs 6293\n",
      "training loss 0.000845726566070034\n",
      "epochs 6294\n",
      "training loss 0.0009007427078356122\n",
      "epochs 6295\n",
      "training loss 0.0008399609099835791\n",
      "epochs 6296\n",
      "training loss 0.0008823390754462025\n",
      "epochs 6297\n",
      "training loss 0.0008609265654834729\n",
      "epochs 6298\n",
      "training loss 0.0008745043765679908\n",
      "epochs 6299\n",
      "training loss 0.0008585143739344853\n",
      "testing loss 0.0025511208128230316\n",
      "epochs 6300\n",
      "training loss 0.0008610255241685612\n",
      "epochs 6301\n",
      "training loss 0.0008616040201219348\n",
      "epochs 6302\n",
      "training loss 0.0008428316116814239\n",
      "epochs 6303\n",
      "training loss 0.0008821322925538843\n",
      "epochs 6304\n",
      "training loss 0.0008731779378257114\n",
      "epochs 6305\n",
      "training loss 0.0008558476695650764\n",
      "epochs 6306\n",
      "training loss 0.0008348451289850766\n",
      "epochs 6307\n",
      "training loss 0.0008854043612852523\n",
      "epochs 6308\n",
      "training loss 0.0008306493314788436\n",
      "epochs 6309\n",
      "training loss 0.0008154093526908137\n",
      "testing loss 0.0031242211715067267\n",
      "epochs 6310\n",
      "training loss 0.000878118833995461\n",
      "epochs 6311\n",
      "training loss 0.0008986984367381871\n",
      "epochs 6312\n",
      "training loss 0.0008438094358963434\n",
      "epochs 6313\n",
      "training loss 0.0008131435355398023\n",
      "epochs 6314\n",
      "training loss 0.0008374054624810871\n",
      "epochs 6315\n",
      "training loss 0.00087270878770567\n",
      "epochs 6316\n",
      "training loss 0.0008153558771888864\n",
      "epochs 6317\n",
      "training loss 0.0008606765534067781\n",
      "epochs 6318\n",
      "training loss 0.0008450748384215425\n",
      "epochs 6319\n",
      "training loss 0.0008227162518344627\n",
      "testing loss 0.002594981984076823\n",
      "epochs 6320\n",
      "training loss 0.0008345846362563422\n",
      "epochs 6321\n",
      "training loss 0.0008494561139315843\n",
      "epochs 6322\n",
      "training loss 0.0008775859488751796\n",
      "epochs 6323\n",
      "training loss 0.0008568014265929765\n",
      "epochs 6324\n",
      "training loss 0.0008544188699087745\n",
      "epochs 6325\n",
      "training loss 0.0008220419056443243\n",
      "epochs 6326\n",
      "training loss 0.0008262483083563757\n",
      "epochs 6327\n",
      "training loss 0.0008605565109869075\n",
      "epochs 6328\n",
      "training loss 0.0008665895049522509\n",
      "epochs 6329\n",
      "training loss 0.0008574800252154367\n",
      "testing loss 0.0026623111830013707\n",
      "epochs 6330\n",
      "training loss 0.0008534321413371191\n",
      "epochs 6331\n",
      "training loss 0.0008218840090487164\n",
      "epochs 6332\n",
      "training loss 0.0008332431014722049\n",
      "epochs 6333\n",
      "training loss 0.0008471403442575147\n",
      "epochs 6334\n",
      "training loss 0.0008290834365041762\n",
      "epochs 6335\n",
      "training loss 0.0008362853163775442\n",
      "epochs 6336\n",
      "training loss 0.000838035163395305\n",
      "epochs 6337\n",
      "training loss 0.0008556705735529006\n",
      "epochs 6338\n",
      "training loss 0.0008930054800740605\n",
      "epochs 6339\n",
      "training loss 0.000862822997083712\n",
      "testing loss 0.002803539263632756\n",
      "epochs 6340\n",
      "training loss 0.0008309906868562725\n",
      "epochs 6341\n",
      "training loss 0.000843662273394946\n",
      "epochs 6342\n",
      "training loss 0.0008573459842069624\n",
      "epochs 6343\n",
      "training loss 0.0008579712930966259\n",
      "epochs 6344\n",
      "training loss 0.0008540792101612003\n",
      "epochs 6345\n",
      "training loss 0.0008477124852258542\n",
      "epochs 6346\n",
      "training loss 0.0008726722191407778\n",
      "epochs 6347\n",
      "training loss 0.0008650360401439562\n",
      "epochs 6348\n",
      "training loss 0.0008474753577422102\n",
      "epochs 6349\n",
      "training loss 0.0008736937184744489\n",
      "testing loss 0.00269655383469418\n",
      "epochs 6350\n",
      "training loss 0.0008345053364568047\n",
      "epochs 6351\n",
      "training loss 0.0008478849018905538\n",
      "epochs 6352\n",
      "training loss 0.0008393100211269429\n",
      "epochs 6353\n",
      "training loss 0.0008599825451464752\n",
      "epochs 6354\n",
      "training loss 0.0008671049144165732\n",
      "epochs 6355\n",
      "training loss 0.0008442462475915188\n",
      "epochs 6356\n",
      "training loss 0.0008310298377005412\n",
      "epochs 6357\n",
      "training loss 0.0008213509080823975\n",
      "epochs 6358\n",
      "training loss 0.000839599813097567\n",
      "epochs 6359\n",
      "training loss 0.0008321901834477871\n",
      "testing loss 0.002769567152065826\n",
      "epochs 6360\n",
      "training loss 0.0008482223367431742\n",
      "epochs 6361\n",
      "training loss 0.0008434288788977501\n",
      "epochs 6362\n",
      "training loss 0.0008666968799520306\n",
      "epochs 6363\n",
      "training loss 0.0008415297116259096\n",
      "epochs 6364\n",
      "training loss 0.0008617388900145775\n",
      "epochs 6365\n",
      "training loss 0.0008634937203334967\n",
      "epochs 6366\n",
      "training loss 0.0008288937346411071\n",
      "epochs 6367\n",
      "training loss 0.0008405255923213262\n",
      "epochs 6368\n",
      "training loss 0.0008183702545015476\n",
      "epochs 6369\n",
      "training loss 0.000820474369089773\n",
      "testing loss 0.0030415860691656036\n",
      "epochs 6370\n",
      "training loss 0.0008810071646530284\n",
      "epochs 6371\n",
      "training loss 0.0008604927440520693\n",
      "epochs 6372\n",
      "training loss 0.0008323437562226502\n",
      "epochs 6373\n",
      "training loss 0.000827628541946564\n",
      "epochs 6374\n",
      "training loss 0.0008274442919651697\n",
      "epochs 6375\n",
      "training loss 0.0008610051856333769\n",
      "epochs 6376\n",
      "training loss 0.0008647518257587853\n",
      "epochs 6377\n",
      "training loss 0.0008555678873659766\n",
      "epochs 6378\n",
      "training loss 0.0008395945095658121\n",
      "epochs 6379\n",
      "training loss 0.0008477643055248281\n",
      "testing loss 0.0027179243249475216\n",
      "epochs 6380\n",
      "training loss 0.0008372977293891929\n",
      "epochs 6381\n",
      "training loss 0.0008094661943770175\n",
      "epochs 6382\n",
      "training loss 0.0009092528802572005\n",
      "epochs 6383\n",
      "training loss 0.0009017785358961705\n",
      "epochs 6384\n",
      "training loss 0.0008640000843450053\n",
      "epochs 6385\n",
      "training loss 0.0008040842461105334\n",
      "epochs 6386\n",
      "training loss 0.0008566769550831869\n",
      "epochs 6387\n",
      "training loss 0.0008587719862686193\n",
      "epochs 6388\n",
      "training loss 0.0008414468602744416\n",
      "epochs 6389\n",
      "training loss 0.0008409620151397883\n",
      "testing loss 0.002662516234967699\n",
      "epochs 6390\n",
      "training loss 0.0008907471476497814\n",
      "epochs 6391\n",
      "training loss 0.0008005536435416824\n",
      "epochs 6392\n",
      "training loss 0.0008152045323166679\n",
      "epochs 6393\n",
      "training loss 0.0009032052112382335\n",
      "epochs 6394\n",
      "training loss 0.0008507570282607071\n",
      "epochs 6395\n",
      "training loss 0.000849881030121637\n",
      "epochs 6396\n",
      "training loss 0.000848110255680548\n",
      "epochs 6397\n",
      "training loss 0.0008427000394562332\n",
      "epochs 6398\n",
      "training loss 0.0008468936584213336\n",
      "epochs 6399\n",
      "training loss 0.0008143906394517938\n",
      "testing loss 0.002651743631700965\n",
      "epochs 6400\n",
      "training loss 0.0008599096044896617\n",
      "epochs 6401\n",
      "training loss 0.0008499844347043275\n",
      "epochs 6402\n",
      "training loss 0.0008303084173473306\n",
      "epochs 6403\n",
      "training loss 0.0008358918752282077\n",
      "epochs 6404\n",
      "training loss 0.000814090242452274\n",
      "epochs 6405\n",
      "training loss 0.0008383606774428163\n",
      "epochs 6406\n",
      "training loss 0.0008381284123334728\n",
      "epochs 6407\n",
      "training loss 0.000820299681441683\n",
      "epochs 6408\n",
      "training loss 0.000825976658333659\n",
      "epochs 6409\n",
      "training loss 0.0008198597808481735\n",
      "testing loss 0.0027846050186987625\n",
      "epochs 6410\n",
      "training loss 0.0008295676301199502\n",
      "epochs 6411\n",
      "training loss 0.000844807981861625\n",
      "epochs 6412\n",
      "training loss 0.0008462466915933512\n",
      "epochs 6413\n",
      "training loss 0.0008436142211131707\n",
      "epochs 6414\n",
      "training loss 0.0008674284917766675\n",
      "epochs 6415\n",
      "training loss 0.0008294405313038853\n",
      "epochs 6416\n",
      "training loss 0.0008767815067936697\n",
      "epochs 6417\n",
      "training loss 0.0008439357466089871\n",
      "epochs 6418\n",
      "training loss 0.0008222778004170947\n",
      "epochs 6419\n",
      "training loss 0.0008511814488894886\n",
      "testing loss 0.0036536311038855647\n",
      "epochs 6420\n",
      "training loss 0.0008518695165183803\n",
      "epochs 6421\n",
      "training loss 0.0008244339369080316\n",
      "epochs 6422\n",
      "training loss 0.0008446917719473797\n",
      "epochs 6423\n",
      "training loss 0.0008730249035159426\n",
      "epochs 6424\n",
      "training loss 0.0008534786749120213\n",
      "epochs 6425\n",
      "training loss 0.0008361609713676864\n",
      "epochs 6426\n",
      "training loss 0.0008356532634492103\n",
      "epochs 6427\n",
      "training loss 0.0008241564432076974\n",
      "epochs 6428\n",
      "training loss 0.0008382955137010392\n",
      "epochs 6429\n",
      "training loss 0.0008296746787852905\n",
      "testing loss 0.002696970348900303\n",
      "epochs 6430\n",
      "training loss 0.0008435056364025048\n",
      "epochs 6431\n",
      "training loss 0.0008244152400411925\n",
      "epochs 6432\n",
      "training loss 0.0008549814997630132\n",
      "epochs 6433\n",
      "training loss 0.0008906387090774983\n",
      "epochs 6434\n",
      "training loss 0.0008439925762167976\n",
      "epochs 6435\n",
      "training loss 0.0008686439715018694\n",
      "epochs 6436\n",
      "training loss 0.0008751366274731409\n",
      "epochs 6437\n",
      "training loss 0.0008349498464247512\n",
      "epochs 6438\n",
      "training loss 0.0008894158283101344\n",
      "epochs 6439\n",
      "training loss 0.0008538075127231246\n",
      "testing loss 0.002669597732758559\n",
      "epochs 6440\n",
      "training loss 0.0008198094050368821\n",
      "epochs 6441\n",
      "training loss 0.0008174872492393337\n",
      "epochs 6442\n",
      "training loss 0.0008266018447041579\n",
      "epochs 6443\n",
      "training loss 0.0008302705647639002\n",
      "epochs 6444\n",
      "training loss 0.0008205240186322254\n",
      "epochs 6445\n",
      "training loss 0.0008401768513824961\n",
      "epochs 6446\n",
      "training loss 0.0008099691758865044\n",
      "epochs 6447\n",
      "training loss 0.0008724930158874309\n",
      "epochs 6448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.000852736197981706\n",
      "epochs 6449\n",
      "training loss 0.000845518243063162\n",
      "testing loss 0.0026327984874856703\n",
      "epochs 6450\n",
      "training loss 0.0008520702528024212\n",
      "epochs 6451\n",
      "training loss 0.0009151787597418545\n",
      "epochs 6452\n",
      "training loss 0.0008555902482193008\n",
      "epochs 6453\n",
      "training loss 0.0008276833643631494\n",
      "epochs 6454\n",
      "training loss 0.0008648800609511604\n",
      "epochs 6455\n",
      "training loss 0.0008311762919792435\n",
      "epochs 6456\n",
      "training loss 0.0008566409710718257\n",
      "epochs 6457\n",
      "training loss 0.0008326919600233084\n",
      "epochs 6458\n",
      "training loss 0.00083893624673623\n",
      "epochs 6459\n",
      "training loss 0.0008266860262510624\n",
      "testing loss 0.002641300549433428\n",
      "epochs 6460\n",
      "training loss 0.0008518051245146485\n",
      "epochs 6461\n",
      "training loss 0.0008279923854734024\n",
      "epochs 6462\n",
      "training loss 0.0008360901516635752\n",
      "epochs 6463\n",
      "training loss 0.0008095711074050482\n",
      "epochs 6464\n",
      "training loss 0.0008861756071308203\n",
      "epochs 6465\n",
      "training loss 0.0008031090009019152\n",
      "epochs 6466\n",
      "training loss 0.0008510761350424434\n",
      "epochs 6467\n",
      "training loss 0.0008421238644669925\n",
      "epochs 6468\n",
      "training loss 0.0008737279916405791\n",
      "epochs 6469\n",
      "training loss 0.0007842073810080174\n",
      "testing loss 0.002636330911859949\n",
      "epochs 6470\n",
      "training loss 0.0008397962393608555\n",
      "epochs 6471\n",
      "training loss 0.0008316139867854812\n",
      "epochs 6472\n",
      "training loss 0.000863065327341473\n",
      "epochs 6473\n",
      "training loss 0.0008562842475449932\n",
      "epochs 6474\n",
      "training loss 0.0008171525490852385\n",
      "epochs 6475\n",
      "training loss 0.0008288262237505959\n",
      "epochs 6476\n",
      "training loss 0.000832394003410934\n",
      "epochs 6477\n",
      "training loss 0.0008455451029794444\n",
      "epochs 6478\n",
      "training loss 0.0008077803011247634\n",
      "epochs 6479\n",
      "training loss 0.0008410555740470197\n",
      "testing loss 0.0026504637121143653\n",
      "epochs 6480\n",
      "training loss 0.000800232169088146\n",
      "epochs 6481\n",
      "training loss 0.00081914131962029\n",
      "epochs 6482\n",
      "training loss 0.000837265346291274\n",
      "epochs 6483\n",
      "training loss 0.0008095693135829388\n",
      "epochs 6484\n",
      "training loss 0.0008054743489953987\n",
      "epochs 6485\n",
      "training loss 0.000789268650797459\n",
      "epochs 6486\n",
      "training loss 0.0008655860568049978\n",
      "epochs 6487\n",
      "training loss 0.0008158493121112145\n",
      "epochs 6488\n",
      "training loss 0.000853739727955838\n",
      "epochs 6489\n",
      "training loss 0.0008642903609961374\n",
      "testing loss 0.002758108046407818\n",
      "epochs 6490\n",
      "training loss 0.0008308101217382755\n",
      "epochs 6491\n",
      "training loss 0.0008086715401171517\n",
      "epochs 6492\n",
      "training loss 0.0008924711611699369\n",
      "epochs 6493\n",
      "training loss 0.0008594127208076554\n",
      "epochs 6494\n",
      "training loss 0.0008710602371448136\n",
      "epochs 6495\n",
      "training loss 0.0008338931269446915\n",
      "epochs 6496\n",
      "training loss 0.0008607709611020133\n",
      "epochs 6497\n",
      "training loss 0.0008125978462379652\n",
      "epochs 6498\n",
      "training loss 0.0008518977919069761\n",
      "epochs 6499\n",
      "training loss 0.0008582739766487586\n",
      "testing loss 0.002589051952298255\n",
      "epochs 6500\n",
      "training loss 0.0008358342546862082\n",
      "epochs 6501\n",
      "training loss 0.0008291817102319416\n",
      "epochs 6502\n",
      "training loss 0.0008061006662575952\n",
      "epochs 6503\n",
      "training loss 0.0008841419484045945\n",
      "epochs 6504\n",
      "training loss 0.0008496927558501056\n",
      "epochs 6505\n",
      "training loss 0.0008178344069828639\n",
      "epochs 6506\n",
      "training loss 0.0008197641685728187\n",
      "epochs 6507\n",
      "training loss 0.000838649717844615\n",
      "epochs 6508\n",
      "training loss 0.0008371886363788012\n",
      "epochs 6509\n",
      "training loss 0.0008196895337015345\n",
      "testing loss 0.002597189178947914\n",
      "epochs 6510\n",
      "training loss 0.0008378282374607563\n",
      "epochs 6511\n",
      "training loss 0.0008060730491109521\n",
      "epochs 6512\n",
      "training loss 0.0008554549435384312\n",
      "epochs 6513\n",
      "training loss 0.0008331663506905234\n",
      "epochs 6514\n",
      "training loss 0.0008131765491912272\n",
      "epochs 6515\n",
      "training loss 0.0008290743090475606\n",
      "epochs 6516\n",
      "training loss 0.0008137564710553702\n",
      "epochs 6517\n",
      "training loss 0.0008334133100496339\n",
      "epochs 6518\n",
      "training loss 0.0008809572902414664\n",
      "epochs 6519\n",
      "training loss 0.0008198652246798528\n",
      "testing loss 0.002716756785777232\n",
      "epochs 6520\n",
      "training loss 0.0008373860864069223\n",
      "epochs 6521\n",
      "training loss 0.0008414306306870411\n",
      "epochs 6522\n",
      "training loss 0.0008360644319274471\n",
      "epochs 6523\n",
      "training loss 0.0008291246998973055\n",
      "epochs 6524\n",
      "training loss 0.0008716003231189035\n",
      "epochs 6525\n",
      "training loss 0.0008274272595012792\n",
      "epochs 6526\n",
      "training loss 0.0008461045144277265\n",
      "epochs 6527\n",
      "training loss 0.0008511104778277668\n",
      "epochs 6528\n",
      "training loss 0.0008108627811353088\n",
      "epochs 6529\n",
      "training loss 0.000824869461223132\n",
      "testing loss 0.0028876506592186682\n",
      "epochs 6530\n",
      "training loss 0.0008323628741661851\n",
      "epochs 6531\n",
      "training loss 0.0008457778435693006\n",
      "epochs 6532\n",
      "training loss 0.0008416874656215944\n",
      "epochs 6533\n",
      "training loss 0.0008126467498731276\n",
      "epochs 6534\n",
      "training loss 0.0008073293127680033\n",
      "epochs 6535\n",
      "training loss 0.000841929097829732\n",
      "epochs 6536\n",
      "training loss 0.0008587585496758663\n",
      "epochs 6537\n",
      "training loss 0.0008529066976859011\n",
      "epochs 6538\n",
      "training loss 0.000835846168768244\n",
      "epochs 6539\n",
      "training loss 0.0008160386154552679\n",
      "testing loss 0.0026715070633415846\n",
      "epochs 6540\n",
      "training loss 0.0008367754129401343\n",
      "epochs 6541\n",
      "training loss 0.0008113140705265218\n",
      "epochs 6542\n",
      "training loss 0.0008519143128865271\n",
      "epochs 6543\n",
      "training loss 0.0008333696147616624\n",
      "epochs 6544\n",
      "training loss 0.0008173631231716656\n",
      "epochs 6545\n",
      "training loss 0.0008368926904076501\n",
      "epochs 6546\n",
      "training loss 0.0008270640097279668\n",
      "epochs 6547\n",
      "training loss 0.0008456771022967122\n",
      "epochs 6548\n",
      "training loss 0.0008262227378547825\n",
      "epochs 6549\n",
      "training loss 0.0007962698784252023\n",
      "testing loss 0.002646524622187626\n",
      "epochs 6550\n",
      "training loss 0.000833232423197195\n",
      "epochs 6551\n",
      "training loss 0.0008376127735769665\n",
      "epochs 6552\n",
      "training loss 0.0008140175042838129\n",
      "epochs 6553\n",
      "training loss 0.0008073705216688542\n",
      "epochs 6554\n",
      "training loss 0.0008413498527844547\n",
      "epochs 6555\n",
      "training loss 0.0008491690414441087\n",
      "epochs 6556\n",
      "training loss 0.0008610389163536458\n",
      "epochs 6557\n",
      "training loss 0.0008263055184370581\n",
      "epochs 6558\n",
      "training loss 0.0008233943446341188\n",
      "epochs 6559\n",
      "training loss 0.0008237945945759369\n",
      "testing loss 0.0025800598189994325\n",
      "epochs 6560\n",
      "training loss 0.0008128257590419638\n",
      "epochs 6561\n",
      "training loss 0.0008378425158530553\n",
      "epochs 6562\n",
      "training loss 0.0008452108999789271\n",
      "epochs 6563\n",
      "training loss 0.0008350714718629988\n",
      "epochs 6564\n",
      "training loss 0.0008237125188548197\n",
      "epochs 6565\n",
      "training loss 0.0008460969510587289\n",
      "epochs 6566\n",
      "training loss 0.0008328866143144757\n",
      "epochs 6567\n",
      "training loss 0.0008410605341712676\n",
      "epochs 6568\n",
      "training loss 0.0008835053417441014\n",
      "epochs 6569\n",
      "training loss 0.0008380749209084771\n",
      "testing loss 0.0029044149927332573\n",
      "epochs 6570\n",
      "training loss 0.0008342748657003396\n",
      "epochs 6571\n",
      "training loss 0.0008209862682252194\n",
      "epochs 6572\n",
      "training loss 0.0008817263060613142\n",
      "epochs 6573\n",
      "training loss 0.0008252221671829006\n",
      "epochs 6574\n",
      "training loss 0.0008022521795332715\n",
      "epochs 6575\n",
      "training loss 0.0011598244835771224\n",
      "epochs 6576\n",
      "training loss 0.0008328970482588483\n",
      "epochs 6577\n",
      "training loss 0.0008400646524007028\n",
      "epochs 6578\n",
      "training loss 0.0008495505139306697\n",
      "epochs 6579\n",
      "training loss 0.0008070922375651003\n",
      "testing loss 0.002580206960122636\n",
      "epochs 6580\n",
      "training loss 0.0008490704489895921\n",
      "epochs 6581\n",
      "training loss 0.0008026521041137266\n",
      "epochs 6582\n",
      "training loss 0.0008026426645649716\n",
      "epochs 6583\n",
      "training loss 0.0008379612069351933\n",
      "epochs 6584\n",
      "training loss 0.0008213833738033487\n",
      "epochs 6585\n",
      "training loss 0.0008439104558226276\n",
      "epochs 6586\n",
      "training loss 0.0008328104412747904\n",
      "epochs 6587\n",
      "training loss 0.0008424738701397559\n",
      "epochs 6588\n",
      "training loss 0.000807704758815507\n",
      "epochs 6589\n",
      "training loss 0.0008536391546716206\n",
      "testing loss 0.0026496697187502966\n",
      "epochs 6590\n",
      "training loss 0.0008855493822224201\n",
      "epochs 6591\n",
      "training loss 0.0008172970509303784\n",
      "epochs 6592\n",
      "training loss 0.0008017668904103961\n",
      "epochs 6593\n",
      "training loss 0.0007992066578489465\n",
      "epochs 6594\n",
      "training loss 0.0007928234201609681\n",
      "epochs 6595\n",
      "training loss 0.0008337689258616791\n",
      "epochs 6596\n",
      "training loss 0.0008421717110758019\n",
      "epochs 6597\n",
      "training loss 0.0009437230037734773\n",
      "epochs 6598\n",
      "training loss 0.0008806937767297459\n",
      "epochs 6599\n",
      "training loss 0.000826320786448054\n",
      "testing loss 0.002842169709946194\n",
      "epochs 6600\n",
      "training loss 0.0008518347178025335\n",
      "epochs 6601\n",
      "training loss 0.0008336449076285683\n",
      "epochs 6602\n",
      "training loss 0.0008210223704211324\n",
      "epochs 6603\n",
      "training loss 0.0008449412858089585\n",
      "epochs 6604\n",
      "training loss 0.000798054265143565\n",
      "epochs 6605\n",
      "training loss 0.0008362781418852598\n",
      "epochs 6606\n",
      "training loss 0.0008087788653131122\n",
      "epochs 6607\n",
      "training loss 0.0008242767614504962\n",
      "epochs 6608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0008190559170394286\n",
      "epochs 6609\n",
      "training loss 0.0008456402894896441\n",
      "testing loss 0.002647760374131381\n",
      "epochs 6610\n",
      "training loss 0.0008393765963602664\n",
      "epochs 6611\n",
      "training loss 0.0008182324873181598\n",
      "epochs 6612\n",
      "training loss 0.0008380436302215076\n",
      "epochs 6613\n",
      "training loss 0.0008288165806519507\n",
      "epochs 6614\n",
      "training loss 0.0008443280258166425\n",
      "epochs 6615\n",
      "training loss 0.0008245009074757572\n",
      "epochs 6616\n",
      "training loss 0.000801208621442267\n",
      "epochs 6617\n",
      "training loss 0.0008208323587659225\n",
      "epochs 6618\n",
      "training loss 0.0008141491118051383\n",
      "epochs 6619\n",
      "training loss 0.0008437821822379657\n",
      "testing loss 0.002924451031490949\n",
      "epochs 6620\n",
      "training loss 0.0008163648697485338\n",
      "epochs 6621\n",
      "training loss 0.0008329221331953188\n",
      "epochs 6622\n",
      "training loss 0.0007929891422369707\n",
      "epochs 6623\n",
      "training loss 0.0008220163559311814\n",
      "epochs 6624\n",
      "training loss 0.000820437574328849\n",
      "epochs 6625\n",
      "training loss 0.0008136296126776372\n",
      "epochs 6626\n",
      "training loss 0.000832383187755229\n",
      "epochs 6627\n",
      "training loss 0.0008313119897829782\n",
      "epochs 6628\n",
      "training loss 0.0008397441958735405\n",
      "epochs 6629\n",
      "training loss 0.0008413452414638649\n",
      "testing loss 0.0026549813098188306\n",
      "epochs 6630\n",
      "training loss 0.0008181750642222078\n",
      "epochs 6631\n",
      "training loss 0.000826725632052778\n",
      "epochs 6632\n",
      "training loss 0.0008646932171903958\n",
      "epochs 6633\n",
      "training loss 0.0008612107168840624\n",
      "epochs 6634\n",
      "training loss 0.0008513277348259741\n",
      "epochs 6635\n",
      "training loss 0.0008132110229880184\n",
      "epochs 6636\n",
      "training loss 0.0008099470355676764\n",
      "epochs 6637\n",
      "training loss 0.0008336700961534444\n",
      "epochs 6638\n",
      "training loss 0.00083243007729917\n",
      "epochs 6639\n",
      "training loss 0.0007996329619113277\n",
      "testing loss 0.002764772756705523\n",
      "epochs 6640\n",
      "training loss 0.0007950455337970894\n",
      "epochs 6641\n",
      "training loss 0.0008261351023289718\n",
      "epochs 6642\n",
      "training loss 0.0008189140070275061\n",
      "epochs 6643\n",
      "training loss 0.0008288245608514928\n",
      "epochs 6644\n",
      "training loss 0.0008614857557780282\n",
      "epochs 6645\n",
      "training loss 0.0008007495968898413\n",
      "epochs 6646\n",
      "training loss 0.0008227623146173111\n",
      "epochs 6647\n",
      "training loss 0.0008254709522987186\n",
      "epochs 6648\n",
      "training loss 0.0008121709632835886\n",
      "epochs 6649\n",
      "training loss 0.0008177814853383301\n",
      "testing loss 0.002883630273005007\n",
      "epochs 6650\n",
      "training loss 0.0008214232743587515\n",
      "epochs 6651\n",
      "training loss 0.0008396004341856936\n",
      "epochs 6652\n",
      "training loss 0.0007934996040501153\n",
      "epochs 6653\n",
      "training loss 0.0008433254897958206\n",
      "epochs 6654\n",
      "training loss 0.0008455227592876493\n",
      "epochs 6655\n",
      "training loss 0.0007897497226072243\n",
      "epochs 6656\n",
      "training loss 0.00081097449755967\n",
      "epochs 6657\n",
      "training loss 0.0007994813604123245\n",
      "epochs 6658\n",
      "training loss 0.0008211129073953783\n",
      "epochs 6659\n",
      "training loss 0.0007930177137946376\n",
      "testing loss 0.00265513760688325\n",
      "epochs 6660\n",
      "training loss 0.0008217989324484484\n",
      "epochs 6661\n",
      "training loss 0.0008091209786285938\n",
      "epochs 6662\n",
      "training loss 0.0008464571328990806\n",
      "epochs 6663\n",
      "training loss 0.0007889869772032254\n",
      "epochs 6664\n",
      "training loss 0.0008120485018842533\n",
      "epochs 6665\n",
      "training loss 0.0008293617258271582\n",
      "epochs 6666\n",
      "training loss 0.0008296242552047941\n",
      "epochs 6667\n",
      "training loss 0.0008172004293978758\n",
      "epochs 6668\n",
      "training loss 0.0008782107130189831\n",
      "epochs 6669\n",
      "training loss 0.0008076465005546208\n",
      "testing loss 0.003024121329472367\n",
      "epochs 6670\n",
      "training loss 0.0007984719504913638\n",
      "epochs 6671\n",
      "training loss 0.0008244908421504167\n",
      "epochs 6672\n",
      "training loss 0.0008157537673319772\n",
      "epochs 6673\n",
      "training loss 0.0008545261514047585\n",
      "epochs 6674\n",
      "training loss 0.0008298067471025211\n",
      "epochs 6675\n",
      "training loss 0.0008511827463541735\n",
      "epochs 6676\n",
      "training loss 0.000855995928988959\n",
      "epochs 6677\n",
      "training loss 0.0008757670824331059\n",
      "epochs 6678\n",
      "training loss 0.0008481950391194356\n",
      "epochs 6679\n",
      "training loss 0.0008033050660653985\n",
      "testing loss 0.002635172945166548\n",
      "epochs 6680\n",
      "training loss 0.0008259880663275061\n",
      "epochs 6681\n",
      "training loss 0.000789752231375102\n",
      "epochs 6682\n",
      "training loss 0.0008047893191340993\n",
      "epochs 6683\n",
      "training loss 0.0008471412850454687\n",
      "epochs 6684\n",
      "training loss 0.0008205470026082906\n",
      "epochs 6685\n",
      "training loss 0.0008121309157936497\n",
      "epochs 6686\n",
      "training loss 0.0008221311538231115\n",
      "epochs 6687\n",
      "training loss 0.0008068055582004897\n",
      "epochs 6688\n",
      "training loss 0.0008130412466334332\n",
      "epochs 6689\n",
      "training loss 0.0008351155399276923\n",
      "testing loss 0.0026452592194265295\n",
      "epochs 6690\n",
      "training loss 0.0007978679268517585\n",
      "epochs 6691\n",
      "training loss 0.0008283167144023222\n",
      "epochs 6692\n",
      "training loss 0.0008207916729378664\n",
      "epochs 6693\n",
      "training loss 0.0008436141191170718\n",
      "epochs 6694\n",
      "training loss 0.0008077195781498219\n",
      "epochs 6695\n",
      "training loss 0.0007854780323073087\n",
      "epochs 6696\n",
      "training loss 0.000807196456081967\n",
      "epochs 6697\n",
      "training loss 0.0007761121658703148\n",
      "epochs 6698\n",
      "training loss 0.0008065153723993087\n",
      "epochs 6699\n",
      "training loss 0.0008100895211985193\n",
      "testing loss 0.0026285496807875153\n",
      "epochs 6700\n",
      "training loss 0.0008369931850496022\n",
      "epochs 6701\n",
      "training loss 0.0008208168060858494\n",
      "epochs 6702\n",
      "training loss 0.0008071903300352712\n",
      "epochs 6703\n",
      "training loss 0.0008515756847768581\n",
      "epochs 6704\n",
      "training loss 0.0007945793612908266\n",
      "epochs 6705\n",
      "training loss 0.000823864419795969\n",
      "epochs 6706\n",
      "training loss 0.0008118062929956577\n",
      "epochs 6707\n",
      "training loss 0.0008283524059599951\n",
      "epochs 6708\n",
      "training loss 0.0008392879675888375\n",
      "epochs 6709\n",
      "training loss 0.0008258151463169242\n",
      "testing loss 0.002608105706848561\n",
      "epochs 6710\n",
      "training loss 0.0008586922252308691\n",
      "epochs 6711\n",
      "training loss 0.0008108782802077186\n",
      "epochs 6712\n",
      "training loss 0.0008099102941490314\n",
      "epochs 6713\n",
      "training loss 0.000807570465161015\n",
      "epochs 6714\n",
      "training loss 0.0007789711054629649\n",
      "epochs 6715\n",
      "training loss 0.0008595754550083464\n",
      "epochs 6716\n",
      "training loss 0.0008258128110220299\n",
      "epochs 6717\n",
      "training loss 0.0008049612756221241\n",
      "epochs 6718\n",
      "training loss 0.0008201105043321769\n",
      "epochs 6719\n",
      "training loss 0.0008497086653838325\n",
      "testing loss 0.0026153608989829182\n",
      "epochs 6720\n",
      "training loss 0.0008259949661468617\n",
      "epochs 6721\n",
      "training loss 0.0008199030854017403\n",
      "epochs 6722\n",
      "training loss 0.0008092719499279368\n",
      "epochs 6723\n",
      "training loss 0.0008251523308167206\n",
      "epochs 6724\n",
      "training loss 0.0008500439848568245\n",
      "epochs 6725\n",
      "training loss 0.0008276580225345098\n",
      "epochs 6726\n",
      "training loss 0.0008128561264563544\n",
      "epochs 6727\n",
      "training loss 0.0008556078268444352\n",
      "epochs 6728\n",
      "training loss 0.0008425963398544712\n",
      "epochs 6729\n",
      "training loss 0.0008324349979695929\n",
      "testing loss 0.0026680759718998317\n",
      "epochs 6730\n",
      "training loss 0.0008035360383985162\n",
      "epochs 6731\n",
      "training loss 0.0008130591346084007\n",
      "epochs 6732\n",
      "training loss 0.0008110428630766433\n",
      "epochs 6733\n",
      "training loss 0.000821055568072458\n",
      "epochs 6734\n",
      "training loss 0.0008232041574713114\n",
      "epochs 6735\n",
      "training loss 0.0008088052347988898\n",
      "epochs 6736\n",
      "training loss 0.000829032341500965\n",
      "epochs 6737\n",
      "training loss 0.0008429680153193931\n",
      "epochs 6738\n",
      "training loss 0.0008304216611477912\n",
      "epochs 6739\n",
      "training loss 0.0008006878684615959\n",
      "testing loss 0.0025821420911481562\n",
      "epochs 6740\n",
      "training loss 0.0008723808622863823\n",
      "epochs 6741\n",
      "training loss 0.0008336181956198984\n",
      "epochs 6742\n",
      "training loss 0.0008068615450356778\n",
      "epochs 6743\n",
      "training loss 0.0007945974778502862\n",
      "epochs 6744\n",
      "training loss 0.0008283893101477831\n",
      "epochs 6745\n",
      "training loss 0.0007974628424010095\n",
      "epochs 6746\n",
      "training loss 0.0008216625553489264\n",
      "epochs 6747\n",
      "training loss 0.0007797124927246371\n",
      "epochs 6748\n",
      "training loss 0.0008382114065502484\n",
      "epochs 6749\n",
      "training loss 0.0008195986726043865\n",
      "testing loss 0.0027811994639073386\n",
      "epochs 6750\n",
      "training loss 0.0008246329149314822\n",
      "epochs 6751\n",
      "training loss 0.0008302347885485806\n",
      "epochs 6752\n",
      "training loss 0.0008171091108663591\n",
      "epochs 6753\n",
      "training loss 0.0008227647226275508\n",
      "epochs 6754\n",
      "training loss 0.0007952945597050853\n",
      "epochs 6755\n",
      "training loss 0.0007990001195526503\n",
      "epochs 6756\n",
      "training loss 0.000817858653127479\n",
      "epochs 6757\n",
      "training loss 0.0008228596922301752\n",
      "epochs 6758\n",
      "training loss 0.0008668968161903492\n",
      "epochs 6759\n",
      "training loss 0.000810556690135014\n",
      "testing loss 0.0025488325705956536\n",
      "epochs 6760\n",
      "training loss 0.0008055231374537\n",
      "epochs 6761\n",
      "training loss 0.0008343470322306474\n",
      "epochs 6762\n",
      "training loss 0.0008485104076954958\n",
      "epochs 6763\n",
      "training loss 0.0007870050313837238\n",
      "epochs 6764\n",
      "training loss 0.0008248776497494848\n",
      "epochs 6765\n",
      "training loss 0.0008250239876128654\n",
      "epochs 6766\n",
      "training loss 0.0008265111326646218\n",
      "epochs 6767\n",
      "training loss 0.0008084438121976688\n",
      "epochs 6768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.0008526595021346102\n",
      "epochs 6769\n",
      "training loss 0.0008163224851944707\n",
      "testing loss 0.00261011490216875\n",
      "epochs 6770\n",
      "training loss 0.0008104909246781768\n",
      "epochs 6771\n",
      "training loss 0.0008113965268974337\n",
      "epochs 6772\n",
      "training loss 0.000806994841729169\n",
      "epochs 6773\n",
      "training loss 0.000814763340627701\n",
      "epochs 6774\n",
      "training loss 0.0008265039464954208\n",
      "epochs 6775\n",
      "training loss 0.0008111300284296308\n",
      "epochs 6776\n",
      "training loss 0.0008724512955022404\n",
      "epochs 6777\n",
      "training loss 0.0008600512184209748\n",
      "epochs 6778\n",
      "training loss 0.0009315937980768868\n",
      "epochs 6779\n",
      "training loss 0.0008084825915036762\n",
      "testing loss 0.0027328411014816653\n",
      "epochs 6780\n",
      "training loss 0.0007735300212206834\n",
      "epochs 6781\n",
      "training loss 0.0008315670766300128\n",
      "epochs 6782\n",
      "training loss 0.0009396279971667291\n",
      "epochs 6783\n",
      "training loss 0.0008443650393428332\n",
      "epochs 6784\n",
      "training loss 0.0008290331573812942\n",
      "epochs 6785\n",
      "training loss 0.0008421621100848264\n",
      "epochs 6786\n",
      "training loss 0.000825076273484665\n",
      "epochs 6787\n",
      "training loss 0.0007824071282789813\n",
      "epochs 6788\n",
      "training loss 0.0008198287339610725\n",
      "epochs 6789\n",
      "training loss 0.0007996572593636075\n",
      "testing loss 0.002643083159461723\n",
      "epochs 6790\n",
      "training loss 0.0008199737485290129\n",
      "epochs 6791\n",
      "training loss 0.0008098253352910077\n",
      "epochs 6792\n",
      "training loss 0.0008243778896536452\n",
      "epochs 6793\n",
      "training loss 0.0008016616038979516\n",
      "epochs 6794\n",
      "training loss 0.000815236046988132\n",
      "epochs 6795\n",
      "training loss 0.0008432270379950742\n",
      "epochs 6796\n",
      "training loss 0.0008129677539769175\n",
      "epochs 6797\n",
      "training loss 0.0008202547330082335\n",
      "epochs 6798\n",
      "training loss 0.0007816137471568904\n",
      "epochs 6799\n",
      "training loss 0.0008141033762413442\n",
      "testing loss 0.0027324564524609197\n",
      "epochs 6800\n",
      "training loss 0.0008559870742591195\n",
      "epochs 6801\n",
      "training loss 0.0007897151361104938\n",
      "epochs 6802\n",
      "training loss 0.0007744720242818461\n",
      "epochs 6803\n",
      "training loss 0.0007785959779652895\n",
      "epochs 6804\n",
      "training loss 0.0008071344899576961\n",
      "epochs 6805\n",
      "training loss 0.000828082261021082\n",
      "epochs 6806\n",
      "training loss 0.0008188701282704028\n",
      "epochs 6807\n",
      "training loss 0.0008602616238998006\n",
      "epochs 6808\n",
      "training loss 0.0008051399834026613\n",
      "epochs 6809\n",
      "training loss 0.0008113245257901136\n",
      "testing loss 0.0026730724502229365\n",
      "epochs 6810\n",
      "training loss 0.0007979337879354672\n",
      "epochs 6811\n",
      "training loss 0.000838554079384659\n",
      "epochs 6812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-70c91e694486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-1414834edee8>\u001b[0m in \u001b[0;36mtrain_func\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train_losses = []\n",
    "#eval_losses = []\n",
    "#accs = []\n",
    "for t in range(2000):\n",
    "    print('epochs', t)\n",
    "    train_loss = train_func(model, train_loader)\n",
    "    if (t+1) % 10 == 0:\n",
    "        eval_loss = eval_func(model, eval_loader)\n",
    "        #acc = accuracy(model)\n",
    "        \n",
    "        eval_losses.append(eval_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        #accs.append(acc)\n",
    "        #print('accuracy: ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(eval_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6053820b38>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(eval_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "label = y_train.cpu()\n",
    "label = y_scaler.inverse_transform(label)\n",
    "with torch.no_grad():\n",
    "    pred = model(X_train)\n",
    "    pred = pred.cpu().numpy()\n",
    "    pred = y_scaler.inverse_transform(pred)\n",
    "    plot(label, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test_func(model, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>total_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X5gsdTWGS3W7JJQB</td>\n",
       "      <td>1.132997e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BTshNOJyKHnT2YIT</td>\n",
       "      <td>3.951944e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dhdymr0lV8N5kZOT</td>\n",
       "      <td>1.026775e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VEwyGGMcD56w5BOc</td>\n",
       "      <td>1.128428e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wmUeMoJZfsqaSX9b</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EtBjGAHmHCe9t7TZ</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hPNH34vmaZtvBtqc</td>\n",
       "      <td>1.395539e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wXjeI38bYDMJJwZC</td>\n",
       "      <td>1.176495e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fxZSGX6aPAFKU8W4</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ewr0Fx6ign87OwaV</td>\n",
       "      <td>3.435798e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gHKurnEP4AowzsLg</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PmLfTgY2FElLrTl0</td>\n",
       "      <td>8.128618e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>eM2NppIOwzW0o8iy</td>\n",
       "      <td>1.010574e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dxxwNun97NH4WTrZ</td>\n",
       "      <td>2.883770e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jykBfhh3vdeFUi3H</td>\n",
       "      <td>5.404822e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NlXbvdFfmJZf3L18</td>\n",
       "      <td>2.682031e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D7jaFWHCzSqLBwdt</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>L10dBBdqGmemweSl</td>\n",
       "      <td>6.942950e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>OgB0AdiPKlElakKN</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>StiWNN1GQrpPBOYt</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>a016eMAVQKnfwMnt</td>\n",
       "      <td>1.912465e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gsCFcQHnOH3AKMcZ</td>\n",
       "      <td>5.360608e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>IbNsDXfsPwSuFpow</td>\n",
       "      <td>4.896964e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>EgAVWOVxD1Jy5YkE</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BrKghvR76XdbQPnx</td>\n",
       "      <td>7.949838e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>a7fxkXTnUGWHUmKG</td>\n",
       "      <td>2.744803e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>WgzXa170DfpzpURE</td>\n",
       "      <td>8.933723e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>JPWqZbLq0VNC0yKI</td>\n",
       "      <td>1.197265e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>JQgTtbVstqFZwEK1</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bCSDbEthlS3nSIor</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>QL412tWF5RDIX7IO</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>d3c2ceGtckONZzsr</td>\n",
       "      <td>9.530539e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>P1j8YRbxDAovumaI</td>\n",
       "      <td>1.051121e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>IxcBhEoFLcrI9TPr</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>rKiV0KDbAl2myBQI</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>GSdIXmKr0g5jQQcF</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>Am6Wcg3TO64qvzd8</td>\n",
       "      <td>3.161117e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>RZqACAhkL4Tgw4Jr</td>\n",
       "      <td>1.196686e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>u7NKZfWoMUlZy9rJ</td>\n",
       "      <td>5.153093e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>C1BqV4MWH15rjAgz</td>\n",
       "      <td>4.666322e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>wz8A2UbwsgR0lXGJ</td>\n",
       "      <td>4.487565e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>MGJ8ABBTmC2yIaSm</td>\n",
       "      <td>1.528512e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>MjHL2HP1PGIp8aBt</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>FMz7nnURFn85LaGt</td>\n",
       "      <td>9.156485e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>kydULx0r0G7OklRD</td>\n",
       "      <td>6.582546e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>nVNYRuk2fRbtlV00</td>\n",
       "      <td>3.026997e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>F8SGEOGPxrPfiRv2</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>w7VMfiMvRb765ejK</td>\n",
       "      <td>3.094947e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>lgZWdUKliWt2y5sM</td>\n",
       "      <td>1.850801e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>TER8YrP9mw7UwWwr</td>\n",
       "      <td>5.383668e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>TXHk3oUpVsm5Cmag</td>\n",
       "      <td>2.899991e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>JtgDm9aQcGE9zELB</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>wTQmcqbN0OCuSF1t</td>\n",
       "      <td>2.051906e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>WgsI1cBtzSfiWA1j</td>\n",
       "      <td>2.023272e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>qNgt1ajb5uVMKbqm</td>\n",
       "      <td>5.464222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>UEeCDaAJzPwdKKKA</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>i0fgbPaQsDWs7Q87</td>\n",
       "      <td>4.317041e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>YunNwAhcqkf6YclI</td>\n",
       "      <td>1.960572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>A2NotxtRY9MYoWMl</td>\n",
       "      <td>4.181549e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>kKvgBXiA50gRmQhP</td>\n",
       "      <td>7.026610e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           building_id   total_price\n",
       "0     X5gsdTWGS3W7JJQB  1.132997e+07\n",
       "1     BTshNOJyKHnT2YIT  3.951944e+06\n",
       "2     dhdymr0lV8N5kZOT  1.026775e+07\n",
       "3     VEwyGGMcD56w5BOc  1.128428e+07\n",
       "4     wmUeMoJZfsqaSX9b  1.960572e+06\n",
       "5     EtBjGAHmHCe9t7TZ  1.960572e+06\n",
       "6     hPNH34vmaZtvBtqc  1.395539e+07\n",
       "7     wXjeI38bYDMJJwZC  1.176495e+07\n",
       "8     fxZSGX6aPAFKU8W4  1.960572e+06\n",
       "9     ewr0Fx6ign87OwaV  3.435798e+06\n",
       "10    gHKurnEP4AowzsLg  1.960572e+06\n",
       "11    PmLfTgY2FElLrTl0  8.128618e+06\n",
       "12    eM2NppIOwzW0o8iy  1.010574e+07\n",
       "13    dxxwNun97NH4WTrZ  2.883770e+06\n",
       "14    jykBfhh3vdeFUi3H  5.404822e+06\n",
       "15    NlXbvdFfmJZf3L18  2.682031e+07\n",
       "16    D7jaFWHCzSqLBwdt  1.960572e+06\n",
       "17    L10dBBdqGmemweSl  6.942950e+06\n",
       "18    OgB0AdiPKlElakKN  1.960572e+06\n",
       "19    StiWNN1GQrpPBOYt  1.960572e+06\n",
       "20    a016eMAVQKnfwMnt  1.912465e+07\n",
       "21    gsCFcQHnOH3AKMcZ  5.360608e+06\n",
       "22    IbNsDXfsPwSuFpow  4.896964e+06\n",
       "23    EgAVWOVxD1Jy5YkE  1.960572e+06\n",
       "24    BrKghvR76XdbQPnx  7.949838e+06\n",
       "25    a7fxkXTnUGWHUmKG  2.744803e+07\n",
       "26    WgzXa170DfpzpURE  8.933723e+06\n",
       "27    JPWqZbLq0VNC0yKI  1.197265e+07\n",
       "28    JQgTtbVstqFZwEK1  1.960572e+06\n",
       "29    bCSDbEthlS3nSIor  1.960572e+06\n",
       "...                ...           ...\n",
       "9970  QL412tWF5RDIX7IO  1.960572e+06\n",
       "9971  d3c2ceGtckONZzsr  9.530539e+06\n",
       "9972  P1j8YRbxDAovumaI  1.051121e+07\n",
       "9973  IxcBhEoFLcrI9TPr  1.960572e+06\n",
       "9974  rKiV0KDbAl2myBQI  1.960572e+06\n",
       "9975  GSdIXmKr0g5jQQcF  1.960572e+06\n",
       "9976  Am6Wcg3TO64qvzd8  3.161117e+07\n",
       "9977  RZqACAhkL4Tgw4Jr  1.196686e+08\n",
       "9978  u7NKZfWoMUlZy9rJ  5.153093e+06\n",
       "9979  C1BqV4MWH15rjAgz  4.666322e+06\n",
       "9980  wz8A2UbwsgR0lXGJ  4.487565e+06\n",
       "9981  MGJ8ABBTmC2yIaSm  1.528512e+07\n",
       "9982  MjHL2HP1PGIp8aBt  1.960572e+06\n",
       "9983  FMz7nnURFn85LaGt  9.156485e+07\n",
       "9984  kydULx0r0G7OklRD  6.582546e+06\n",
       "9985  nVNYRuk2fRbtlV00  3.026997e+07\n",
       "9986  F8SGEOGPxrPfiRv2  1.960572e+06\n",
       "9987  w7VMfiMvRb765ejK  3.094947e+07\n",
       "9988  lgZWdUKliWt2y5sM  1.850801e+07\n",
       "9989  TER8YrP9mw7UwWwr  5.383668e+06\n",
       "9990  TXHk3oUpVsm5Cmag  2.899991e+06\n",
       "9991  JtgDm9aQcGE9zELB  1.960572e+06\n",
       "9992  wTQmcqbN0OCuSF1t  2.051906e+07\n",
       "9993  WgsI1cBtzSfiWA1j  2.023272e+07\n",
       "9994  qNgt1ajb5uVMKbqm  5.464222e+06\n",
       "9995  UEeCDaAJzPwdKKKA  1.960572e+06\n",
       "9996  i0fgbPaQsDWs7Q87  4.317041e+07\n",
       "9997  YunNwAhcqkf6YclI  1.960572e+06\n",
       "9998  A2NotxtRY9MYoWMl  4.181549e+06\n",
       "9999  kKvgBXiA50gRmQhP  7.026610e+06\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('./dataset-0510/submit_test.csv')\n",
    "submission['total_price'] = pred\n",
    "submission.to_csv('submission/DNN2_result.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size use 128 or 32 , learning rate use 0.003 which find loss will stock in 0.6\n",
    "\n",
    "Result 1 DNN 233->256->128->1, lr=0.001, batch_size=128, predict score : 13\n",
    "change: \n",
    "- replacing Standard to MinMax \n",
    "- adding DropOut 0.3 layer\n",
    "- batch size change to 512\n",
    "\n",
    "Result 2 lr=0.001, batch_size=64, DNN 233->256->128->64->1\n",
    "after 1k loss : 0.00011785521522113447, can't decrease...\n",
    "- x_scale false\n",
    "- y_scale true\n",
    "\n",
    "Result 3 lr=0.001 batch_size=128 DNN 211->256->512->512->256->128->1\n",
    "after 1w loss : 0.0003, test loss : 0.0007 score: 1670\n",
    "\n",
    "Result 4 lr=0.001 batch_size=128 DNN 211->256->512->512->256->128->64->32->1\n",
    "train_loss: 0.0004, test loss: 0.0002 score: 1600\n",
    "\n",
    "Result 5 lr=0.001 batch_size=128 DNN 211->256->512->512->256->128->64->32->1 + batch_noram\n",
    "train_loss: 0.0005, test loss: 0.003 score: 1000\n",
    "\n",
    "Result 6 lr=0.001 batch_size=128 DNN 211->256->512->512->256->128->64->1 + batch_noram + weight_decay\n",
    "after 2k\n",
    "train_loss: 0.004, test loss 0.006 ,look like L2 regular not work which will increase loss\n",
    "\n",
    "Result 6 lr=0.0015 batch_size=128 DNN 211->256->512->512->256->128->64->32->1 + batch_noram\n",
    "after 3k\n",
    "train_loss 0.0004 test loss 0.002 score : 1400\n",
    "I think we should reduce lr or add L2 regularzation\n",
    "\n",
    "Result 7 lr=0.0015 batch_size= 128 DNN 211->256->512->512->256->128->64->32->1 + batch_noram + weight_decay(0.0005)\n",
    "after 3.5k \n",
    "train loss 0.0007, test loss 0.002, score: 2217\n",
    "regularzation is work, but test loss is not reduce, next tuning lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why output is negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
