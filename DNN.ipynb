{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building_id\n",
    "#columns = X.columns\n",
    "data_train = pd.read_csv('./dataset-0510/train.csv')\n",
    "X_test = pd.read_csv('./dataset-0510/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "data = data_train.append(X_test, ignore_index=True, sort=False)\n",
    "data = data.drop(['building_id'], axis=1)\n",
    "print(data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(data.median(), inplace=True)\n",
    "columns = data.columns\n",
    "sale_price = data['total_price']\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/islab/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_material</th>\n",
       "      <th>city</th>\n",
       "      <th>txn_dt</th>\n",
       "      <th>total_floor</th>\n",
       "      <th>building_type</th>\n",
       "      <th>building_use</th>\n",
       "      <th>building_complete_dt</th>\n",
       "      <th>parking_way</th>\n",
       "      <th>parking_area</th>\n",
       "      <th>parking_price</th>\n",
       "      <th>...</th>\n",
       "      <th>XIV_500</th>\n",
       "      <th>XIV_index_500</th>\n",
       "      <th>XIV_1000</th>\n",
       "      <th>XIV_index_1000</th>\n",
       "      <th>XIV_5000</th>\n",
       "      <th>XIV_index_5000</th>\n",
       "      <th>XIV_10000</th>\n",
       "      <th>XIV_index_10000</th>\n",
       "      <th>XIV_MIN</th>\n",
       "      <th>total_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.220133</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.300577</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034503</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.045336</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.135021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031126</td>\n",
       "      <td>6.476038e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.269487</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.379486</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016657</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033208</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.868705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.979105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036191</td>\n",
       "      <td>3.321452e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.461026</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.288697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.061219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.862568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.979317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049197</td>\n",
       "      <td>9.570885e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890325</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.889899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.079516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011898</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.139679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090022</td>\n",
       "      <td>1.421501e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996866</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.330351</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013572</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.140720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112169</td>\n",
       "      <td>7.627120e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 234 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_material      city    txn_dt  total_floor  building_type  \\\n",
       "0                0.7  1.000000  0.220133     0.107143           0.75   \n",
       "1                0.7  0.222222  0.269487     0.142857           0.25   \n",
       "2                0.7  0.222222  0.461026     0.107143           0.25   \n",
       "3                0.7  1.000000  0.890325     0.821429           0.00   \n",
       "4                0.0  1.000000  0.996866     0.035714           1.00   \n",
       "\n",
       "   building_use  building_complete_dt  parking_way  parking_area  \\\n",
       "0           0.2              0.300577          1.0      0.005952   \n",
       "1           0.2              0.379486          1.0      0.005952   \n",
       "2           0.2              0.288697          1.0      0.005952   \n",
       "3           0.2              0.889899          0.0      0.005952   \n",
       "4           0.2              0.330351          1.0      0.005952   \n",
       "\n",
       "   parking_price  ...   XIV_500  XIV_index_500  XIV_1000  XIV_index_1000  \\\n",
       "0       0.042916  ...  0.034503            1.0  0.045336             1.0   \n",
       "1       0.042916  ...  0.016657            1.0  0.033208             1.0   \n",
       "2       0.042916  ...  0.046401            1.0  0.061219             1.0   \n",
       "3       0.079516  ...  0.011898            1.0  0.036096             1.0   \n",
       "4       0.042916  ...  0.010708            1.0  0.013572             1.0   \n",
       "\n",
       "   XIV_5000  XIV_index_5000  XIV_10000  XIV_index_10000   XIV_MIN  \\\n",
       "0  0.135021             0.0   0.181915              0.0  0.031126   \n",
       "1  0.868705             0.0   0.979105              0.0  0.036191   \n",
       "2  0.862568             0.0   0.979317              0.0  0.049197   \n",
       "3  0.139679             0.0   0.220238              0.0  0.090022   \n",
       "4  0.140720             0.0   0.225440              0.0  0.112169   \n",
       "\n",
       "    total_price  \n",
       "0  6.476038e+05  \n",
       "1  3.321452e+06  \n",
       "2  9.570885e+06  \n",
       "3  1.421501e+07  \n",
       "4  7.627120e+05  \n",
       "\n",
       "[5 rows x 234 columns]"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(data), columns = columns)\n",
    "data['total_price'] = sale_price\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_material</th>\n",
       "      <th>city</th>\n",
       "      <th>txn_dt</th>\n",
       "      <th>total_floor</th>\n",
       "      <th>building_type</th>\n",
       "      <th>building_use</th>\n",
       "      <th>building_complete_dt</th>\n",
       "      <th>parking_way</th>\n",
       "      <th>parking_area</th>\n",
       "      <th>parking_price</th>\n",
       "      <th>...</th>\n",
       "      <th>XIV_500</th>\n",
       "      <th>XIV_index_500</th>\n",
       "      <th>XIV_1000</th>\n",
       "      <th>XIV_index_1000</th>\n",
       "      <th>XIV_5000</th>\n",
       "      <th>XIV_index_5000</th>\n",
       "      <th>XIV_10000</th>\n",
       "      <th>XIV_index_10000</th>\n",
       "      <th>XIV_MIN</th>\n",
       "      <th>total_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.220133</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.300577</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034503</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.045336</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.135021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031126</td>\n",
       "      <td>6.476038e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.269487</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.379486</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016657</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033208</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.868705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.979105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036191</td>\n",
       "      <td>3.321452e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.461026</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.288697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.061219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.862568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.979317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049197</td>\n",
       "      <td>9.570885e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890325</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.889899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.079516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011898</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.139679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090022</td>\n",
       "      <td>1.421501e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996866</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.330351</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013572</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.140720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112169</td>\n",
       "      <td>7.627120e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 234 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_material      city    txn_dt  total_floor  building_type  \\\n",
       "0                0.7  1.000000  0.220133     0.107143           0.75   \n",
       "1                0.7  0.222222  0.269487     0.142857           0.25   \n",
       "2                0.7  0.222222  0.461026     0.107143           0.25   \n",
       "3                0.7  1.000000  0.890325     0.821429           0.00   \n",
       "4                0.0  1.000000  0.996866     0.035714           1.00   \n",
       "\n",
       "   building_use  building_complete_dt  parking_way  parking_area  \\\n",
       "0           0.2              0.300577          1.0      0.005952   \n",
       "1           0.2              0.379486          1.0      0.005952   \n",
       "2           0.2              0.288697          1.0      0.005952   \n",
       "3           0.2              0.889899          0.0      0.005952   \n",
       "4           0.2              0.330351          1.0      0.005952   \n",
       "\n",
       "   parking_price  ...   XIV_500  XIV_index_500  XIV_1000  XIV_index_1000  \\\n",
       "0       0.042916  ...  0.034503            1.0  0.045336             1.0   \n",
       "1       0.042916  ...  0.016657            1.0  0.033208             1.0   \n",
       "2       0.042916  ...  0.046401            1.0  0.061219             1.0   \n",
       "3       0.079516  ...  0.011898            1.0  0.036096             1.0   \n",
       "4       0.042916  ...  0.010708            1.0  0.013572             1.0   \n",
       "\n",
       "   XIV_5000  XIV_index_5000  XIV_10000  XIV_index_10000   XIV_MIN  \\\n",
       "0  0.135021             0.0   0.181915              0.0  0.031126   \n",
       "1  0.868705             0.0   0.979105              0.0  0.036191   \n",
       "2  0.862568             0.0   0.979317              0.0  0.049197   \n",
       "3  0.139679             0.0   0.220238              0.0  0.090022   \n",
       "4  0.140720             0.0   0.225440              0.0  0.112169   \n",
       "\n",
       "    total_price  \n",
       "0  6.476038e+05  \n",
       "1  3.321452e+06  \n",
       "2  9.570885e+06  \n",
       "3  1.421501e+07  \n",
       "4  7.627120e+05  \n",
       "\n",
       "[5 rows x 234 columns]"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(data), columns = columns)\n",
    "data['total_price'] = sale_price\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/islab/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:3930: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "train = data.iloc[:60000]\n",
    "test  = data.iloc[60000:]\n",
    "test.drop('total_price', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train.drop('total_price', axis=1), train['total_price'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(233, 144)\n",
    "        self.fc2 = nn.Linear(144, 72)\n",
    "        self.fc3 = nn.Linear(72, 18)\n",
    "        self.fc4 = nn.Linear(18, 1)\n",
    "\n",
    "        #self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = self.dropout(F.relu(self.fc1(x)))\n",
    "        #x = self.dropout(F.relu(self.fc2(x)))\n",
    "        #x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = np.array_split(X_train, 50)\n",
    "label_batch = np.array_split(y_train, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_batch)):\n",
    "    train_batch[i] = torch.from_numpy(train_batch[i].values).float().to(device)\n",
    "for i in range(len(label_batch)):\n",
    "    label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1).to(device)\n",
    "\n",
    "X_val = torch.from_numpy(X_val.values).float().to(device)\n",
    "y_val = torch.from_numpy(y_val.values).float().view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5626e+06],\n",
      "        [1.7368e+06],\n",
      "        [4.0604e+06],\n",
      "        [3.6722e+05],\n",
      "        [9.5709e+06],\n",
      "        [1.4838e+07],\n",
      "        [1.3601e+07],\n",
      "        [1.0117e+07],\n",
      "        [8.7190e+06],\n",
      "        [4.2459e+06],\n",
      "        [6.5210e+06],\n",
      "        [3.3215e+06],\n",
      "        [9.1416e+06],\n",
      "        [4.1609e+07],\n",
      "        [7.0013e+06],\n",
      "        [2.9901e+06],\n",
      "        [1.1469e+07],\n",
      "        [7.4930e+06],\n",
      "        [3.1358e+06],\n",
      "        [1.5693e+08],\n",
      "        [3.7429e+06],\n",
      "        [2.7480e+07],\n",
      "        [1.2401e+07],\n",
      "        [7.9961e+06],\n",
      "        [9.5481e+07],\n",
      "        [3.9005e+06],\n",
      "        [8.3549e+06],\n",
      "        [2.1056e+06],\n",
      "        [3.8214e+06],\n",
      "        [2.7261e+06],\n",
      "        [1.4465e+08],\n",
      "        [2.0135e+07],\n",
      "        [1.0117e+07],\n",
      "        [7.9961e+06],\n",
      "        [5.5972e+06],\n",
      "        [8.2211e+06],\n",
      "        [4.0202e+06],\n",
      "        [2.2019e+06],\n",
      "        [3.6400e+07],\n",
      "        [1.0673e+07],\n",
      "        [2.9541e+06],\n",
      "        [5.8772e+06],\n",
      "        [4.4698e+06],\n",
      "        [2.7434e+06],\n",
      "        [2.5196e+07],\n",
      "        [3.1381e+06],\n",
      "        [4.7596e+05],\n",
      "        [6.0526e+06],\n",
      "        [8.3033e+06],\n",
      "        [1.1438e+06],\n",
      "        [9.6793e+06],\n",
      "        [4.9290e+05],\n",
      "        [6.8078e+06],\n",
      "        [1.1012e+07],\n",
      "        [4.4282e+06],\n",
      "        [4.0604e+06],\n",
      "        [2.1059e+06],\n",
      "        [4.7219e+06],\n",
      "        [6.6638e+06],\n",
      "        [5.5965e+06],\n",
      "        [8.8240e+06],\n",
      "        [1.5060e+06],\n",
      "        [2.8469e+06],\n",
      "        [9.0353e+06],\n",
      "        [7.6400e+05],\n",
      "        [1.2357e+06],\n",
      "        [5.0656e+06],\n",
      "        [1.1235e+08],\n",
      "        [8.5103e+06],\n",
      "        [4.8071e+06],\n",
      "        [9.8702e+05],\n",
      "        [5.1016e+05],\n",
      "        [1.2401e+07],\n",
      "        [7.4931e+06],\n",
      "        [5.6388e+05],\n",
      "        [2.0834e+07],\n",
      "        [1.3683e+06],\n",
      "        [1.1335e+06],\n",
      "        [6.7885e+06],\n",
      "        [1.2519e+07],\n",
      "        [9.5709e+06],\n",
      "        [8.0509e+05],\n",
      "        [3.8111e+07],\n",
      "        [1.1638e+06],\n",
      "        [2.0834e+07],\n",
      "        [1.4838e+07],\n",
      "        [2.2473e+06],\n",
      "        [2.1057e+06],\n",
      "        [5.7792e+06],\n",
      "        [1.2486e+07],\n",
      "        [7.2949e+06],\n",
      "        [2.2019e+06],\n",
      "        [2.4325e+06],\n",
      "        [3.2468e+06],\n",
      "        [7.0012e+06],\n",
      "        [8.8240e+06],\n",
      "        [1.5470e+07],\n",
      "        [3.3060e+07],\n",
      "        [1.1012e+07],\n",
      "        [1.5060e+06],\n",
      "        [2.7762e+06],\n",
      "        [6.5210e+06],\n",
      "        [1.0349e+06],\n",
      "        [6.0526e+06],\n",
      "        [7.4930e+06],\n",
      "        [5.4709e+06],\n",
      "        [3.9005e+06],\n",
      "        [3.0263e+06],\n",
      "        [1.0109e+06],\n",
      "        [5.5965e+06],\n",
      "        [3.5108e+06],\n",
      "        [1.9798e+06],\n",
      "        [2.4325e+06],\n",
      "        [2.0038e+07],\n",
      "        [5.5965e+06],\n",
      "        [4.9290e+05],\n",
      "        [8.5103e+06],\n",
      "        [2.4154e+07],\n",
      "        [1.9798e+06],\n",
      "        [6.7117e+06],\n",
      "        [7.4930e+06],\n",
      "        [1.8873e+06],\n",
      "        [5.5965e+06],\n",
      "        [4.7602e+07],\n",
      "        [5.1745e+07],\n",
      "        [2.1056e+06],\n",
      "        [3.8214e+06],\n",
      "        [5.2405e+06],\n",
      "        [6.7945e+05],\n",
      "        [2.2999e+06],\n",
      "        [4.2880e+06],\n",
      "        [1.0673e+07],\n",
      "        [3.1358e+06],\n",
      "        [1.1815e+07],\n",
      "        [3.9847e+07],\n",
      "        [2.2019e+06],\n",
      "        [3.1359e+06],\n",
      "        [3.7273e+06],\n",
      "        [1.4964e+07],\n",
      "        [2.1696e+06],\n",
      "        [1.8085e+07],\n",
      "        [1.5626e+06],\n",
      "        [6.0526e+06],\n",
      "        [1.1068e+07],\n",
      "        [2.5680e+06],\n",
      "        [1.2997e+07],\n",
      "        [1.1012e+07],\n",
      "        [3.1727e+06],\n",
      "        [1.9179e+06],\n",
      "        [1.3601e+07],\n",
      "        [1.9179e+06],\n",
      "        [1.5512e+06],\n",
      "        [4.1412e+06],\n",
      "        [8.5107e+06],\n",
      "        [4.0202e+06],\n",
      "        [8.6265e+07],\n",
      "        [3.5657e+06],\n",
      "        [4.3044e+06],\n",
      "        [4.7220e+06],\n",
      "        [5.0221e+06],\n",
      "        [1.8085e+07],\n",
      "        [2.2343e+06],\n",
      "        [3.5876e+06],\n",
      "        [2.4325e+06],\n",
      "        [1.7977e+06],\n",
      "        [2.4353e+06],\n",
      "        [4.9789e+06],\n",
      "        [1.6780e+06],\n",
      "        [2.7762e+06],\n",
      "        [2.6711e+07],\n",
      "        [2.1973e+06],\n",
      "        [1.2401e+07],\n",
      "        [1.1842e+06],\n",
      "        [1.5626e+06],\n",
      "        [4.0604e+06],\n",
      "        [1.2494e+08],\n",
      "        [4.7219e+06],\n",
      "        [2.0834e+07],\n",
      "        [1.8489e+07],\n",
      "        [1.8874e+06],\n",
      "        [7.4930e+06],\n",
      "        [7.5192e+07],\n",
      "        [3.4346e+06],\n",
      "        [3.6650e+06],\n",
      "        [2.8477e+06],\n",
      "        [1.7952e+07],\n",
      "        [1.6314e+06],\n",
      "        [2.3682e+07],\n",
      "        [1.1239e+07],\n",
      "        [1.0117e+07],\n",
      "        [1.2381e+06],\n",
      "        [4.3044e+06],\n",
      "        [8.5105e+06],\n",
      "        [6.8077e+06],\n",
      "        [1.3358e+07],\n",
      "        [2.2257e+07],\n",
      "        [1.0350e+06],\n",
      "        [4.3044e+06],\n",
      "        [2.1698e+06],\n",
      "        [1.3010e+08],\n",
      "        [7.4930e+06],\n",
      "        [1.6240e+07],\n",
      "        [3.2096e+06],\n",
      "        [6.5210e+06],\n",
      "        [1.2996e+07],\n",
      "        [2.4325e+06],\n",
      "        [2.1057e+06],\n",
      "        [5.8687e+06],\n",
      "        [1.2996e+07],\n",
      "        [3.9005e+06],\n",
      "        [1.6760e+07],\n",
      "        [4.7974e+07],\n",
      "        [2.9541e+06],\n",
      "        [4.6124e+07],\n",
      "        [4.2719e+05],\n",
      "        [4.3044e+06],\n",
      "        [1.1815e+07],\n",
      "        [4.7219e+06],\n",
      "        [2.4325e+06],\n",
      "        [1.5798e+06],\n",
      "        [1.1085e+06],\n",
      "        [6.1454e+06],\n",
      "        [1.8489e+07],\n",
      "        [3.7429e+06],\n",
      "        [1.8085e+07],\n",
      "        [7.6214e+05],\n",
      "        [1.4463e+07],\n",
      "        [9.9520e+06],\n",
      "        [1.4503e+06],\n",
      "        [2.0110e+06],\n",
      "        [2.8469e+06],\n",
      "        [1.2881e+06],\n",
      "        [5.5965e+06],\n",
      "        [1.2166e+07],\n",
      "        [2.4325e+06],\n",
      "        [1.4781e+06],\n",
      "        [3.4346e+06],\n",
      "        [3.9005e+06],\n",
      "        [3.1358e+06],\n",
      "        [2.1258e+07],\n",
      "        [1.2357e+06],\n",
      "        [2.5680e+06],\n",
      "        [1.7980e+06],\n",
      "        [9.5709e+06],\n",
      "        [2.6368e+06],\n",
      "        [2.5196e+07],\n",
      "        [2.0110e+06],\n",
      "        [1.6240e+07],\n",
      "        [6.2572e+06],\n",
      "        [8.7190e+06],\n",
      "        [5.1528e+06],\n",
      "        [9.4199e+06],\n",
      "        [8.4066e+06],\n",
      "        [2.4999e+06],\n",
      "        [1.5626e+06],\n",
      "        [2.4325e+06],\n",
      "        [1.1239e+07],\n",
      "        [6.4075e+06],\n",
      "        [3.3060e+07],\n",
      "        [2.1056e+06],\n",
      "        [2.0834e+07],\n",
      "        [6.5210e+06],\n",
      "        [7.6214e+05],\n",
      "        [7.8946e+06],\n",
      "        [7.4930e+06],\n",
      "        [4.3044e+06],\n",
      "        [1.6780e+06],\n",
      "        [9.1416e+06],\n",
      "        [1.2401e+07],\n",
      "        [6.0526e+06],\n",
      "        [1.9180e+06],\n",
      "        [2.6783e+06],\n",
      "        [6.5210e+06],\n",
      "        [2.6023e+06],\n",
      "        [2.6368e+06],\n",
      "        [1.2357e+06],\n",
      "        [5.0917e+06],\n",
      "        [1.1815e+07],\n",
      "        [8.5103e+06],\n",
      "        [2.3712e+07],\n",
      "        [5.3287e+06],\n",
      "        [1.0673e+07],\n",
      "        [7.0012e+06],\n",
      "        [1.6199e+06],\n",
      "        [7.4930e+06],\n",
      "        [6.0526e+06],\n",
      "        [4.1412e+06],\n",
      "        [4.3044e+06],\n",
      "        [3.5108e+06],\n",
      "        [3.7429e+06],\n",
      "        [2.3327e+06],\n",
      "        [1.7966e+06],\n",
      "        [1.5470e+07],\n",
      "        [1.2996e+07],\n",
      "        [6.9043e+06],\n",
      "        [1.0117e+07],\n",
      "        [5.0656e+06],\n",
      "        [7.9961e+06],\n",
      "        [2.7762e+06],\n",
      "        [4.7219e+06],\n",
      "        [2.1542e+07],\n",
      "        [7.9961e+06],\n",
      "        [2.2997e+06],\n",
      "        [3.9005e+06],\n",
      "        [2.3859e+07],\n",
      "        [6.0430e+05],\n",
      "        [8.5103e+06],\n",
      "        [1.1335e+06],\n",
      "        [2.7271e+06],\n",
      "        [2.8469e+06],\n",
      "        [4.7219e+06],\n",
      "        [3.2096e+06],\n",
      "        [4.8498e+06],\n",
      "        [1.6111e+07],\n",
      "        [1.8876e+06],\n",
      "        [2.6930e+06],\n",
      "        [1.7966e+06],\n",
      "        [8.5103e+06],\n",
      "        [2.4999e+06],\n",
      "        [7.4930e+06],\n",
      "        [3.5108e+06],\n",
      "        [9.1416e+06],\n",
      "        [1.2996e+07],\n",
      "        [4.5115e+06],\n",
      "        [2.9039e+07],\n",
      "        [3.9005e+06],\n",
      "        [3.8221e+05],\n",
      "        [1.0349e+06],\n",
      "        [7.9962e+06],\n",
      "        [9.5709e+06],\n",
      "        [4.3044e+06],\n",
      "        [5.5965e+06],\n",
      "        [2.0448e+06],\n",
      "        [7.4930e+06],\n",
      "        [2.6368e+06],\n",
      "        [8.0529e+05],\n",
      "        [1.3601e+07],\n",
      "        [1.6111e+07],\n",
      "        [1.1239e+07],\n",
      "        [5.8687e+06],\n",
      "        [2.9829e+07],\n",
      "        [4.7219e+06],\n",
      "        [4.3044e+06],\n",
      "        [5.8687e+06],\n",
      "        [6.7597e+06],\n",
      "        [6.0526e+06],\n",
      "        [3.9005e+06],\n",
      "        [2.6716e+05],\n",
      "        [1.0227e+07],\n",
      "        [2.9325e+06],\n",
      "        [4.3044e+06],\n",
      "        [2.2981e+07],\n",
      "        [7.0907e+07],\n",
      "        [1.2853e+07],\n",
      "        [3.4195e+06],\n",
      "        [2.6711e+07],\n",
      "        [1.1733e+08],\n",
      "        [1.1411e+07],\n",
      "        [3.9005e+06],\n",
      "        [2.4325e+06],\n",
      "        [2.7062e+06],\n",
      "        [2.8256e+07],\n",
      "        [5.5965e+06],\n",
      "        [2.7762e+06],\n",
      "        [5.2702e+07],\n",
      "        [8.5104e+06],\n",
      "        [4.2634e+06],\n",
      "        [1.8760e+07],\n",
      "        [1.1012e+07],\n",
      "        [6.9849e+07],\n",
      "        [1.5626e+06],\n",
      "        [4.3396e+07],\n",
      "        [6.3976e+05],\n",
      "        [4.1412e+06],\n",
      "        [7.0012e+06],\n",
      "        [4.7219e+06],\n",
      "        [4.9358e+06],\n",
      "        [2.0110e+06],\n",
      "        [1.6760e+07],\n",
      "        [9.8702e+05],\n",
      "        [6.5211e+06],\n",
      "        [9.8702e+05],\n",
      "        [6.5212e+06],\n",
      "        [4.3044e+06],\n",
      "        [3.5108e+06],\n",
      "        [5.2155e+06],\n",
      "        [3.6650e+06],\n",
      "        [6.7944e+05],\n",
      "        [2.1056e+06],\n",
      "        [2.1696e+06],\n",
      "        [6.5210e+06],\n",
      "        [5.4175e+06],\n",
      "        [4.7219e+06],\n",
      "        [1.7861e+06],\n",
      "        [2.4327e+06],\n",
      "        [9.7881e+06],\n",
      "        [1.7965e+06],\n",
      "        [7.2457e+06],\n",
      "        [2.9829e+07],\n",
      "        [1.1344e+06],\n",
      "        [1.0227e+07],\n",
      "        [3.5108e+06],\n",
      "        [4.6021e+06],\n",
      "        [3.7039e+06],\n",
      "        [1.3683e+06],\n",
      "        [7.4930e+06],\n",
      "        [1.6760e+07],\n",
      "        [5.1528e+06],\n",
      "        [1.4274e+06],\n",
      "        [3.5108e+06],\n",
      "        [1.3601e+07],\n",
      "        [1.2996e+07],\n",
      "        [1.6240e+07],\n",
      "        [6.5210e+06],\n",
      "        [2.1542e+07],\n",
      "        [4.1611e+07],\n",
      "        [3.7429e+06],\n",
      "        [1.2277e+08],\n",
      "        [4.7219e+06],\n",
      "        [9.0353e+06],\n",
      "        [1.5626e+06],\n",
      "        [8.6667e+06],\n",
      "        [6.0526e+06],\n",
      "        [7.6214e+05],\n",
      "        [9.0353e+06],\n",
      "        [4.7219e+06],\n",
      "        [3.3060e+07],\n",
      "        [8.6265e+07],\n",
      "        [6.5211e+06],\n",
      "        [5.1016e+05],\n",
      "        [2.9901e+06],\n",
      "        [3.2098e+06],\n",
      "        [9.3554e+06],\n",
      "        [4.2459e+06],\n",
      "        [9.0353e+06],\n",
      "        [9.3018e+06],\n",
      "        [3.6262e+06],\n",
      "        [1.4838e+07],\n",
      "        [6.2572e+06],\n",
      "        [3.7039e+06],\n",
      "        [1.0898e+07],\n",
      "        [8.8240e+06],\n",
      "        [5.0656e+06],\n",
      "        [5.7182e+07],\n",
      "        [1.7073e+06],\n",
      "        [9.5709e+06],\n",
      "        [1.4503e+06],\n",
      "        [3.8609e+06],\n",
      "        [4.3044e+06],\n",
      "        [7.6215e+05],\n",
      "        [3.8113e+07],\n",
      "        [6.0597e+06],\n",
      "        [4.7219e+06],\n",
      "        [1.4838e+07],\n",
      "        [4.1899e+06],\n",
      "        [2.4326e+06],\n",
      "        [3.7595e+07],\n",
      "        [7.9961e+06],\n",
      "        [1.1815e+07],\n",
      "        [1.1469e+07],\n",
      "        [4.5936e+05],\n",
      "        [4.3044e+06],\n",
      "        [6.5210e+06],\n",
      "        [1.1239e+07],\n",
      "        [4.3044e+06],\n",
      "        [3.1358e+06],\n",
      "        [5.1569e+06],\n",
      "        [3.7429e+06],\n",
      "        [3.3956e+05],\n",
      "        [4.0202e+06],\n",
      "        [2.2257e+07],\n",
      "        [7.9961e+06],\n",
      "        [1.5647e+06],\n",
      "        [8.9408e+05],\n",
      "        [9.8728e+05],\n",
      "        [2.9901e+06],\n",
      "        [6.2853e+06],\n",
      "        [1.8898e+06],\n",
      "        [3.6262e+06],\n",
      "        [2.3658e+06],\n",
      "        [4.5484e+07],\n",
      "        [3.1358e+06],\n",
      "        [6.0527e+06],\n",
      "        [1.8760e+07],\n",
      "        [7.6217e+05],\n",
      "        [1.1239e+07],\n",
      "        [6.0526e+06],\n",
      "        [1.2357e+06],\n",
      "        [3.6722e+05],\n",
      "        [3.5108e+06],\n",
      "        [9.8704e+05],\n",
      "        [8.4908e+05],\n",
      "        [6.6457e+05],\n",
      "        [3.1359e+06],\n",
      "        [7.4930e+06],\n",
      "        [6.5210e+06],\n",
      "        [2.9325e+06],\n",
      "        [1.0117e+07],\n",
      "        [1.9423e+08],\n",
      "        [8.7145e+05],\n",
      "        [5.5965e+06],\n",
      "        [5.5965e+06],\n",
      "        [1.2937e+07],\n",
      "        [7.7935e+06],\n",
      "        [1.0838e+06],\n",
      "        [9.5709e+06],\n",
      "        [8.0982e+06],\n",
      "        [8.6602e+06],\n",
      "        [2.4325e+06],\n",
      "        [1.6240e+07],\n",
      "        [2.0424e+06],\n",
      "        [1.5060e+06],\n",
      "        [6.5210e+06],\n",
      "        [1.0622e+08],\n",
      "        [6.9043e+06],\n",
      "        [9.8702e+05],\n",
      "        [9.0353e+06],\n",
      "        [1.9443e+07],\n",
      "        [1.2401e+07],\n",
      "        [4.3044e+06],\n",
      "        [4.4698e+06],\n",
      "        [3.3590e+06],\n",
      "        [7.6214e+05],\n",
      "        [1.0117e+07],\n",
      "        [1.7418e+07],\n",
      "        [1.6760e+07],\n",
      "        [1.8568e+06],\n",
      "        [9.1695e+05],\n",
      "        [1.6784e+06],\n",
      "        [1.6199e+06],\n",
      "        [2.9901e+06],\n",
      "        [8.3033e+06],\n",
      "        [8.2890e+07],\n",
      "        [1.0501e+08],\n",
      "        [9.4199e+06],\n",
      "        [1.3954e+06],\n",
      "        [1.8873e+06],\n",
      "        [6.8796e+07],\n",
      "        [6.1920e+06],\n",
      "        [1.3954e+06],\n",
      "        [4.5245e+08],\n",
      "        [2.2981e+07],\n",
      "        [5.6388e+05],\n",
      "        [2.9541e+06],\n",
      "        [1.5060e+06],\n",
      "        [1.0349e+06],\n",
      "        [9.0917e+05],\n",
      "        [2.9541e+06],\n",
      "        [6.1461e+06],\n",
      "        [1.7978e+06],\n",
      "        [2.0135e+07],\n",
      "        [1.3954e+06],\n",
      "        [6.2572e+06],\n",
      "        [5.3666e+07],\n",
      "        [6.7944e+05],\n",
      "        [2.2997e+06],\n",
      "        [2.0834e+07],\n",
      "        [2.6722e+07],\n",
      "        [1.0673e+07],\n",
      "        [1.0673e+07],\n",
      "        [1.2166e+07],\n",
      "        [4.5533e+06],\n",
      "        [1.0955e+07],\n",
      "        [1.7965e+06],\n",
      "        [3.0626e+07],\n",
      "        [6.6707e+07],\n",
      "        [8.5103e+06],\n",
      "        [3.3956e+05],\n",
      "        [9.7880e+06],\n",
      "        [7.4930e+06],\n",
      "        [2.4326e+06],\n",
      "        [2.2997e+06],\n",
      "        [3.4683e+07],\n",
      "        [7.0012e+06],\n",
      "        [2.7062e+06],\n",
      "        [4.5483e+07],\n",
      "        [2.2257e+07],\n",
      "        [1.7418e+07],\n",
      "        [2.5680e+06],\n",
      "        [1.4216e+07],\n",
      "        [3.3591e+06],\n",
      "        [2.8469e+06],\n",
      "        [1.2996e+07],\n",
      "        [1.2996e+07],\n",
      "        [7.6214e+05],\n",
      "        [1.2494e+08],\n",
      "        [2.5347e+07],\n",
      "        [4.8499e+06],\n",
      "        [2.5339e+06],\n",
      "        [7.0013e+06],\n",
      "        [1.7989e+06],\n",
      "        [4.1412e+06],\n",
      "        [9.8702e+05],\n",
      "        [6.7946e+05],\n",
      "        [7.9969e+06],\n",
      "        [2.3712e+07],\n",
      "        [6.0147e+05],\n",
      "        [1.0253e+06],\n",
      "        [9.8702e+05],\n",
      "        [1.5060e+06],\n",
      "        [6.3976e+05],\n",
      "        [9.5709e+06],\n",
      "        [1.1239e+07],\n",
      "        [5.5610e+07],\n",
      "        [4.9293e+05],\n",
      "        [2.8941e+05],\n",
      "        [9.6342e+05],\n",
      "        [2.5196e+07],\n",
      "        [9.5709e+06],\n",
      "        [1.5626e+06],\n",
      "        [2.7061e+06],\n",
      "        [2.2257e+07],\n",
      "        [4.1423e+06],\n",
      "        [7.9961e+06],\n",
      "        [1.1239e+07],\n",
      "        [1.5087e+06],\n",
      "        [1.6489e+06],\n",
      "        [9.5405e+05],\n",
      "        [1.3417e+06],\n",
      "        [7.4930e+06],\n",
      "        [1.6314e+06],\n",
      "        [3.5108e+06],\n",
      "        [1.7428e+06],\n",
      "        [6.6638e+06],\n",
      "        [5.2775e+05],\n",
      "        [2.5197e+07],\n",
      "        [7.6214e+05],\n",
      "        [7.2949e+06],\n",
      "        [2.4154e+07],\n",
      "        [5.1531e+06],\n",
      "        [5.1528e+06],\n",
      "        [1.3954e+06],\n",
      "        [7.0381e+05],\n",
      "        [4.5115e+06],\n",
      "        [1.5342e+06],\n",
      "        [5.5965e+06],\n",
      "        [5.7775e+06],\n",
      "        [5.4175e+06],\n",
      "        [2.5196e+07],\n",
      "        [6.5210e+06],\n",
      "        [2.1056e+06],\n",
      "        [2.8568e+07],\n",
      "        [2.2997e+06],\n",
      "        [5.3730e+06],\n",
      "        [2.1056e+06],\n",
      "        [7.9961e+06],\n",
      "        [2.7762e+06],\n",
      "        [7.2949e+06],\n",
      "        [2.6409e+06],\n",
      "        [4.7219e+06],\n",
      "        [9.2623e+05],\n",
      "        [4.3044e+06],\n",
      "        [5.6958e+06],\n",
      "        [1.2357e+06],\n",
      "        [3.0263e+06],\n",
      "        [1.0936e+06],\n",
      "        [1.6488e+06],\n",
      "        [4.7219e+06],\n",
      "        [7.9961e+06],\n",
      "        [9.4211e+06],\n",
      "        [1.2888e+06],\n",
      "        [5.5965e+06],\n",
      "        [1.2996e+07],\n",
      "        [1.9798e+06],\n",
      "        [8.6667e+06],\n",
      "        [8.9684e+07],\n",
      "        [4.3044e+06],\n",
      "        [4.7644e+06],\n",
      "        [3.8629e+07],\n",
      "        [3.5108e+06],\n",
      "        [1.1234e+08],\n",
      "        [5.1528e+06],\n",
      "        [1.3177e+07],\n",
      "        [2.6368e+06],\n",
      "        [1.6760e+07],\n",
      "        [3.4717e+07],\n",
      "        [6.8849e+06],\n",
      "        [1.1354e+07],\n",
      "        [9.6250e+06],\n",
      "        [3.6400e+07],\n",
      "        [3.5108e+06],\n",
      "        [6.9971e+05],\n",
      "        [2.7762e+06],\n",
      "        [2.9901e+06],\n",
      "        [3.2096e+06],\n",
      "        [2.1046e+07],\n",
      "        [1.7965e+06],\n",
      "        [1.5470e+07],\n",
      "        [3.5108e+06],\n",
      "        [1.1638e+06],\n",
      "        [1.2937e+07],\n",
      "        [3.5108e+06],\n",
      "        [2.4329e+06],\n",
      "        [3.5108e+06],\n",
      "        [1.6214e+07],\n",
      "        [1.0840e+06],\n",
      "        [5.2406e+06],\n",
      "        [6.4265e+06],\n",
      "        [3.3215e+06],\n",
      "        [1.4215e+07],\n",
      "        [3.9364e+08],\n",
      "        [8.0509e+05],\n",
      "        [7.4930e+06],\n",
      "        [6.5210e+06],\n",
      "        [3.1358e+06],\n",
      "        [1.3146e+06],\n",
      "        [2.7762e+06],\n",
      "        [5.8687e+06],\n",
      "        [6.5210e+06],\n",
      "        [2.6368e+06],\n",
      "        [7.0012e+06],\n",
      "        [1.1012e+07],\n",
      "        [5.9145e+06],\n",
      "        [3.5108e+06],\n",
      "        [1.1526e+07],\n",
      "        [5.5965e+06],\n",
      "        [7.4931e+06],\n",
      "        [2.0739e+06],\n",
      "        [9.8702e+05],\n",
      "        [7.0012e+06],\n",
      "        [2.7762e+06],\n",
      "        [9.4199e+06],\n",
      "        [2.5950e+07],\n",
      "        [1.5470e+07],\n",
      "        [1.9798e+06],\n",
      "        [1.9798e+06],\n",
      "        [7.4931e+06],\n",
      "        [2.8477e+06],\n",
      "        [7.4930e+06],\n",
      "        [7.0987e+06],\n",
      "        [3.1358e+06],\n",
      "        [8.1492e+06],\n",
      "        [1.7965e+06],\n",
      "        [5.6388e+05],\n",
      "        [4.3044e+06],\n",
      "        [1.1815e+07],\n",
      "        [3.1359e+06],\n",
      "        [6.6708e+07],\n",
      "        [1.9798e+06],\n",
      "        [1.3954e+06],\n",
      "        [1.4215e+07],\n",
      "        [1.6484e+07],\n",
      "        [3.1358e+06],\n",
      "        [1.9798e+06],\n",
      "        [1.2166e+07],\n",
      "        [2.8469e+06],\n",
      "        [3.6417e+06],\n",
      "        [2.2257e+07],\n",
      "        [8.2696e+05],\n",
      "        [9.3554e+06],\n",
      "        [1.1234e+08],\n",
      "        [1.6206e+07],\n",
      "        [1.3954e+06],\n",
      "        [1.9443e+07],\n",
      "        [3.8618e+08],\n",
      "        [5.8687e+06],\n",
      "        [4.2634e+06],\n",
      "        [3.3590e+06],\n",
      "        [8.5103e+06],\n",
      "        [6.3322e+06],\n",
      "        [4.5936e+05],\n",
      "        [1.5060e+06],\n",
      "        [2.4325e+06],\n",
      "        [9.0353e+06],\n",
      "        [6.2385e+06],\n",
      "        [1.0842e+07],\n",
      "        [7.9961e+06],\n",
      "        [2.1056e+06],\n",
      "        [8.6667e+06],\n",
      "        [3.9005e+06],\n",
      "        [1.1336e+06],\n",
      "        [2.9901e+06],\n",
      "        [2.6023e+06],\n",
      "        [5.1966e+06],\n",
      "        [1.1239e+07],\n",
      "        [4.4698e+06],\n",
      "        [8.0510e+05],\n",
      "        [2.5950e+07],\n",
      "        [2.5339e+06],\n",
      "        [7.0379e+05],\n",
      "        [9.0353e+06],\n",
      "        [3.5555e+07],\n",
      "        [9.0353e+06],\n",
      "        [6.5175e+07],\n",
      "        [2.6711e+07],\n",
      "        [7.4930e+06],\n",
      "        [2.5680e+06],\n",
      "        [2.4325e+06],\n",
      "        [6.5210e+06],\n",
      "        [1.1085e+06],\n",
      "        [3.1358e+06],\n",
      "        [1.1638e+06],\n",
      "        [8.5103e+06],\n",
      "        [2.2981e+07],\n",
      "        [1.1587e+06],\n",
      "        [7.3687e+05],\n",
      "        [1.1012e+07],\n",
      "        [7.4930e+06],\n",
      "        [1.1815e+07],\n",
      "        [4.7219e+06],\n",
      "        [3.9005e+06],\n",
      "        [1.0839e+06],\n",
      "        [2.4325e+06],\n",
      "        [3.6262e+06],\n",
      "        [1.2401e+07],\n",
      "        [1.5060e+06],\n",
      "        [4.7298e+06],\n",
      "        [5.7320e+06],\n",
      "        [4.0604e+06],\n",
      "        [8.5103e+06],\n",
      "        [5.6590e+07],\n",
      "        [7.6929e+06],\n",
      "        [7.0987e+06],\n",
      "        [5.1528e+06],\n",
      "        [3.9005e+06],\n",
      "        [8.2250e+06],\n",
      "        [7.4930e+06],\n",
      "        [2.0947e+07],\n",
      "        [2.4325e+06],\n",
      "        [9.0140e+06],\n",
      "        [7.4931e+06],\n",
      "        [1.7370e+06],\n",
      "        [1.0996e+07],\n",
      "        [3.3060e+07],\n",
      "        [4.4698e+06],\n",
      "        [8.5103e+06],\n",
      "        [2.1696e+06],\n",
      "        [1.3146e+06],\n",
      "        [1.2357e+06],\n",
      "        [6.5210e+06],\n",
      "        [1.1012e+07],\n",
      "        [3.7429e+06],\n",
      "        [1.8569e+06],\n",
      "        [4.9290e+05],\n",
      "        [1.5060e+06],\n",
      "        [1.4215e+07],\n",
      "        [1.3870e+07],\n",
      "        [3.9802e+06],\n",
      "        [8.4908e+05],\n",
      "        [1.1815e+07],\n",
      "        [9.4007e+05],\n",
      "        [8.0981e+06],\n",
      "        [2.7061e+06],\n",
      "        [8.5103e+06],\n",
      "        [4.0725e+07],\n",
      "        [6.9043e+06],\n",
      "        [1.2357e+06],\n",
      "        [3.6400e+07],\n",
      "        [4.9099e+06],\n",
      "        [2.6026e+06],\n",
      "        [1.0117e+07],\n",
      "        [3.5108e+06],\n",
      "        [5.9604e+06],\n",
      "        [1.9443e+07],\n",
      "        [2.6368e+06],\n",
      "        [6.5210e+06],\n",
      "        [6.0526e+06],\n",
      "        [2.2257e+07],\n",
      "        [3.9005e+06],\n",
      "        [2.8256e+07],\n",
      "        [6.1527e+06],\n",
      "        [3.4717e+07],\n",
      "        [3.3215e+06],\n",
      "        [2.5339e+06],\n",
      "        [4.0383e+08],\n",
      "        [1.2049e+07],\n",
      "        [1.8979e+08],\n",
      "        [1.3821e+07],\n",
      "        [3.9847e+07],\n",
      "        [1.0117e+07],\n",
      "        [1.6780e+06],\n",
      "        [2.6368e+06],\n",
      "        [9.5494e+05],\n",
      "        [1.3358e+07],\n",
      "        [7.9961e+06],\n",
      "        [5.8698e+06],\n",
      "        [4.3044e+06],\n",
      "        [2.2257e+07],\n",
      "        [3.3215e+06],\n",
      "        [3.5051e+07],\n",
      "        [3.1358e+06],\n",
      "        [1.0117e+07],\n",
      "        [1.0673e+07],\n",
      "        [3.9645e+05],\n",
      "        [4.7219e+06],\n",
      "        [1.6780e+06],\n",
      "        [2.1056e+06],\n",
      "        [7.9961e+06],\n",
      "        [1.7286e+07],\n",
      "        [7.9961e+06],\n",
      "        [1.6323e+06],\n",
      "        [3.5876e+06],\n",
      "        [2.1056e+06],\n",
      "        [7.0012e+06],\n",
      "        [8.5103e+06],\n",
      "        [1.8489e+07],\n",
      "        [1.8568e+06],\n",
      "        [2.2981e+07],\n",
      "        [1.3413e+06],\n",
      "        [3.4726e+06],\n",
      "        [9.4006e+05],\n",
      "        [1.6240e+07],\n",
      "        [5.2229e+06],\n",
      "        [3.9005e+06],\n",
      "        [7.0987e+06],\n",
      "        [2.7061e+06],\n",
      "        [7.7366e+07],\n",
      "        [1.1335e+06],\n",
      "        [8.2696e+05],\n",
      "        [1.7965e+06],\n",
      "        [4.3044e+06],\n",
      "        [2.0839e+07],\n",
      "        [2.4325e+06],\n",
      "        [2.6714e+06],\n",
      "        [1.6488e+06],\n",
      "        [1.7965e+06],\n",
      "        [2.7762e+06],\n",
      "        [1.7965e+06],\n",
      "        [2.1056e+06],\n",
      "        [9.5405e+05],\n",
      "        [3.8111e+07],\n",
      "        [5.1528e+06],\n",
      "        [2.4334e+06],\n",
      "        [2.5950e+07],\n",
      "        [3.9005e+06],\n",
      "        [1.6314e+06],\n",
      "        [1.4838e+07],\n",
      "        [1.2996e+07],\n",
      "        [5.5965e+06],\n",
      "        [6.9527e+06],\n",
      "        [1.1815e+07],\n",
      "        [4.1412e+06],\n",
      "        [1.1335e+06],\n",
      "        [1.0230e+07],\n",
      "        [7.0012e+06],\n",
      "        [3.7429e+06],\n",
      "        [2.2680e+06],\n",
      "        [1.6240e+07],\n",
      "        [3.7252e+07],\n",
      "        [4.5938e+05],\n",
      "        [1.1842e+06],\n",
      "        [1.1842e+06],\n",
      "        [3.9011e+06],\n",
      "        [3.0626e+07],\n",
      "        [4.4864e+06],\n",
      "        [1.5470e+07],\n",
      "        [2.5680e+06],\n",
      "        [2.8256e+07],\n",
      "        [1.3907e+07],\n",
      "        [1.1239e+07],\n",
      "        [3.7429e+06],\n",
      "        [2.0835e+07],\n",
      "        [1.5470e+07],\n",
      "        [2.7762e+06],\n",
      "        [5.5610e+07],\n",
      "        [1.1239e+07],\n",
      "        [5.5965e+06],\n",
      "        [1.9858e+07],\n",
      "        [1.5470e+07],\n",
      "        [2.1258e+07]], device='cuda:0')\n",
      "tensor([[-0.2742],\n",
      "        [-0.2794],\n",
      "        [-0.2751],\n",
      "        [-0.2712],\n",
      "        [-0.2792],\n",
      "        [-0.2819],\n",
      "        [-0.2872],\n",
      "        [-0.2811],\n",
      "        [-0.2768],\n",
      "        [-0.2804],\n",
      "        [-0.2791],\n",
      "        [-0.2740],\n",
      "        [-0.2738],\n",
      "        [-0.2775],\n",
      "        [-0.2803],\n",
      "        [-0.2714],\n",
      "        [-0.2722],\n",
      "        [-0.2799],\n",
      "        [-0.2703],\n",
      "        [-0.2783],\n",
      "        [-0.2756],\n",
      "        [-0.2831],\n",
      "        [-0.2746],\n",
      "        [-0.2723],\n",
      "        [-0.2816],\n",
      "        [-0.2793],\n",
      "        [-0.2787],\n",
      "        [-0.2835],\n",
      "        [-0.2851],\n",
      "        [-0.2686],\n",
      "        [-0.2849],\n",
      "        [-0.2794],\n",
      "        [-0.2734],\n",
      "        [-0.2813],\n",
      "        [-0.2734],\n",
      "        [-0.2860],\n",
      "        [-0.2778],\n",
      "        [-0.2822],\n",
      "        [-0.2771],\n",
      "        [-0.2831],\n",
      "        [-0.2703],\n",
      "        [-0.2757],\n",
      "        [-0.2767],\n",
      "        [-0.2676],\n",
      "        [-0.2775],\n",
      "        [-0.2758],\n",
      "        [-0.2808],\n",
      "        [-0.2818],\n",
      "        [-0.2784],\n",
      "        [-0.2699],\n",
      "        [-0.2776],\n",
      "        [-0.2767],\n",
      "        [-0.2786],\n",
      "        [-0.2800],\n",
      "        [-0.2751],\n",
      "        [-0.2827],\n",
      "        [-0.2885],\n",
      "        [-0.2744],\n",
      "        [-0.2845],\n",
      "        [-0.2687],\n",
      "        [-0.2698],\n",
      "        [-0.2705],\n",
      "        [-0.2761],\n",
      "        [-0.2702],\n",
      "        [-0.2748],\n",
      "        [-0.2672],\n",
      "        [-0.2841],\n",
      "        [-0.2762],\n",
      "        [-0.2921],\n",
      "        [-0.2816],\n",
      "        [-0.2706],\n",
      "        [-0.2774],\n",
      "        [-0.2752],\n",
      "        [-0.2765],\n",
      "        [-0.2753],\n",
      "        [-0.2847],\n",
      "        [-0.2759],\n",
      "        [-0.2734],\n",
      "        [-0.2775],\n",
      "        [-0.2676],\n",
      "        [-0.2750],\n",
      "        [-0.2622],\n",
      "        [-0.2790],\n",
      "        [-0.2685],\n",
      "        [-0.2698],\n",
      "        [-0.2719],\n",
      "        [-0.2843],\n",
      "        [-0.2787],\n",
      "        [-0.2758],\n",
      "        [-0.2769],\n",
      "        [-0.2835],\n",
      "        [-0.2687],\n",
      "        [-0.2657],\n",
      "        [-0.2796],\n",
      "        [-0.2780],\n",
      "        [-0.2711],\n",
      "        [-0.2796],\n",
      "        [-0.2814],\n",
      "        [-0.2773],\n",
      "        [-0.2817],\n",
      "        [-0.2721],\n",
      "        [-0.2766],\n",
      "        [-0.2766],\n",
      "        [-0.2835],\n",
      "        [-0.2787],\n",
      "        [-0.2722],\n",
      "        [-0.2819],\n",
      "        [-0.2790],\n",
      "        [-0.2739],\n",
      "        [-0.2788],\n",
      "        [-0.2764],\n",
      "        [-0.2812],\n",
      "        [-0.2743],\n",
      "        [-0.2809],\n",
      "        [-0.2869],\n",
      "        [-0.2754],\n",
      "        [-0.2812],\n",
      "        [-0.2844],\n",
      "        [-0.2730],\n",
      "        [-0.2838],\n",
      "        [-0.2819],\n",
      "        [-0.2806],\n",
      "        [-0.2810],\n",
      "        [-0.2817],\n",
      "        [-0.2832],\n",
      "        [-0.2595],\n",
      "        [-0.2845],\n",
      "        [-0.2770],\n",
      "        [-0.2779],\n",
      "        [-0.2714],\n",
      "        [-0.2863],\n",
      "        [-0.2860],\n",
      "        [-0.2737],\n",
      "        [-0.2833],\n",
      "        [-0.2803],\n",
      "        [-0.2805],\n",
      "        [-0.2758],\n",
      "        [-0.2732],\n",
      "        [-0.2706],\n",
      "        [-0.2785],\n",
      "        [-0.2869],\n",
      "        [-0.2767],\n",
      "        [-0.2790],\n",
      "        [-0.2749],\n",
      "        [-0.2782],\n",
      "        [-0.2835],\n",
      "        [-0.2796],\n",
      "        [-0.2758],\n",
      "        [-0.2780],\n",
      "        [-0.2785],\n",
      "        [-0.2687],\n",
      "        [-0.2765],\n",
      "        [-0.2757],\n",
      "        [-0.2810],\n",
      "        [-0.2833],\n",
      "        [-0.2880],\n",
      "        [-0.2639],\n",
      "        [-0.2832],\n",
      "        [-0.2717],\n",
      "        [-0.2709],\n",
      "        [-0.2765],\n",
      "        [-0.2879],\n",
      "        [-0.2723],\n",
      "        [-0.2764],\n",
      "        [-0.2734],\n",
      "        [-0.2825],\n",
      "        [-0.2793],\n",
      "        [-0.2800],\n",
      "        [-0.2734],\n",
      "        [-0.2838],\n",
      "        [-0.2649],\n",
      "        [-0.2749],\n",
      "        [-0.2742],\n",
      "        [-0.2694],\n",
      "        [-0.2829],\n",
      "        [-0.2789],\n",
      "        [-0.2783],\n",
      "        [-0.2796],\n",
      "        [-0.2673],\n",
      "        [-0.2730],\n",
      "        [-0.2714],\n",
      "        [-0.2782],\n",
      "        [-0.2782],\n",
      "        [-0.2769],\n",
      "        [-0.2747],\n",
      "        [-0.2719],\n",
      "        [-0.2809],\n",
      "        [-0.2707],\n",
      "        [-0.2752],\n",
      "        [-0.2737],\n",
      "        [-0.2752],\n",
      "        [-0.2844],\n",
      "        [-0.2764],\n",
      "        [-0.2750],\n",
      "        [-0.2735],\n",
      "        [-0.2810],\n",
      "        [-0.2698],\n",
      "        [-0.2799],\n",
      "        [-0.2788],\n",
      "        [-0.2809],\n",
      "        [-0.2814],\n",
      "        [-0.2849],\n",
      "        [-0.2738],\n",
      "        [-0.2810],\n",
      "        [-0.2730],\n",
      "        [-0.2790],\n",
      "        [-0.2759],\n",
      "        [-0.2847],\n",
      "        [-0.2810],\n",
      "        [-0.2818],\n",
      "        [-0.2748],\n",
      "        [-0.2755],\n",
      "        [-0.2804],\n",
      "        [-0.2798],\n",
      "        [-0.2804],\n",
      "        [-0.2713],\n",
      "        [-0.2700],\n",
      "        [-0.2776],\n",
      "        [-0.2750],\n",
      "        [-0.2797],\n",
      "        [-0.2758],\n",
      "        [-0.2776],\n",
      "        [-0.2756],\n",
      "        [-0.2729],\n",
      "        [-0.2782],\n",
      "        [-0.2725],\n",
      "        [-0.2796],\n",
      "        [-0.2762],\n",
      "        [-0.2716],\n",
      "        [-0.2748],\n",
      "        [-0.2774],\n",
      "        [-0.2729],\n",
      "        [-0.2777],\n",
      "        [-0.2909],\n",
      "        [-0.2855],\n",
      "        [-0.2780],\n",
      "        [-0.2771],\n",
      "        [-0.2743],\n",
      "        [-0.2803],\n",
      "        [-0.2720],\n",
      "        [-0.2771],\n",
      "        [-0.2762],\n",
      "        [-0.2797],\n",
      "        [-0.2834],\n",
      "        [-0.2840],\n",
      "        [-0.2761],\n",
      "        [-0.2702],\n",
      "        [-0.2757],\n",
      "        [-0.2855],\n",
      "        [-0.2791],\n",
      "        [-0.2802],\n",
      "        [-0.2724],\n",
      "        [-0.2663],\n",
      "        [-0.2753],\n",
      "        [-0.2740],\n",
      "        [-0.2735],\n",
      "        [-0.2833],\n",
      "        [-0.2674],\n",
      "        [-0.2767],\n",
      "        [-0.2766],\n",
      "        [-0.2744],\n",
      "        [-0.2794],\n",
      "        [-0.2737],\n",
      "        [-0.2752],\n",
      "        [-0.2666],\n",
      "        [-0.2783],\n",
      "        [-0.2765],\n",
      "        [-0.2840],\n",
      "        [-0.2794],\n",
      "        [-0.2746],\n",
      "        [-0.2739],\n",
      "        [-0.2725],\n",
      "        [-0.2799],\n",
      "        [-0.2850],\n",
      "        [-0.2781],\n",
      "        [-0.2617],\n",
      "        [-0.2698],\n",
      "        [-0.2823],\n",
      "        [-0.2867],\n",
      "        [-0.2830],\n",
      "        [-0.2682],\n",
      "        [-0.2854],\n",
      "        [-0.2715],\n",
      "        [-0.2754],\n",
      "        [-0.2824],\n",
      "        [-0.2634],\n",
      "        [-0.2780],\n",
      "        [-0.2674],\n",
      "        [-0.2736],\n",
      "        [-0.2707],\n",
      "        [-0.2675],\n",
      "        [-0.2865],\n",
      "        [-0.2712],\n",
      "        [-0.2817],\n",
      "        [-0.2762],\n",
      "        [-0.2721],\n",
      "        [-0.2793],\n",
      "        [-0.2819],\n",
      "        [-0.2760],\n",
      "        [-0.2828],\n",
      "        [-0.2732],\n",
      "        [-0.2746],\n",
      "        [-0.2723],\n",
      "        [-0.2763],\n",
      "        [-0.2745],\n",
      "        [-0.2686],\n",
      "        [-0.2698],\n",
      "        [-0.2700],\n",
      "        [-0.2695],\n",
      "        [-0.2711],\n",
      "        [-0.2828],\n",
      "        [-0.2757],\n",
      "        [-0.2741],\n",
      "        [-0.2803],\n",
      "        [-0.2747],\n",
      "        [-0.2746],\n",
      "        [-0.2730],\n",
      "        [-0.2799],\n",
      "        [-0.2788],\n",
      "        [-0.2802],\n",
      "        [-0.2909],\n",
      "        [-0.2823],\n",
      "        [-0.2750],\n",
      "        [-0.2727],\n",
      "        [-0.2671],\n",
      "        [-0.2781],\n",
      "        [-0.2825],\n",
      "        [-0.2573],\n",
      "        [-0.2787],\n",
      "        [-0.2876],\n",
      "        [-0.2647],\n",
      "        [-0.2724],\n",
      "        [-0.2706],\n",
      "        [-0.2805],\n",
      "        [-0.2744],\n",
      "        [-0.2690],\n",
      "        [-0.2746],\n",
      "        [-0.2772],\n",
      "        [-0.2717],\n",
      "        [-0.2684],\n",
      "        [-0.2900],\n",
      "        [-0.2754],\n",
      "        [-0.2565],\n",
      "        [-0.2672],\n",
      "        [-0.2732],\n",
      "        [-0.2718],\n",
      "        [-0.2748],\n",
      "        [-0.2796],\n",
      "        [-0.2749],\n",
      "        [-0.2719],\n",
      "        [-0.2770],\n",
      "        [-0.2807],\n",
      "        [-0.2784],\n",
      "        [-0.2804],\n",
      "        [-0.2869],\n",
      "        [-0.2737],\n",
      "        [-0.2808],\n",
      "        [-0.2766],\n",
      "        [-0.2744],\n",
      "        [-0.2770],\n",
      "        [-0.2773],\n",
      "        [-0.2763],\n",
      "        [-0.2677],\n",
      "        [-0.2767],\n",
      "        [-0.2741],\n",
      "        [-0.2750],\n",
      "        [-0.2788],\n",
      "        [-0.2755],\n",
      "        [-0.2675],\n",
      "        [-0.2765],\n",
      "        [-0.2805],\n",
      "        [-0.2799],\n",
      "        [-0.2832],\n",
      "        [-0.2785],\n",
      "        [-0.2711],\n",
      "        [-0.2688],\n",
      "        [-0.2799],\n",
      "        [-0.2870],\n",
      "        [-0.2738],\n",
      "        [-0.2783],\n",
      "        [-0.2736],\n",
      "        [-0.2719],\n",
      "        [-0.2814],\n",
      "        [-0.2700],\n",
      "        [-0.2841],\n",
      "        [-0.2755],\n",
      "        [-0.2729],\n",
      "        [-0.2785],\n",
      "        [-0.2752],\n",
      "        [-0.2701],\n",
      "        [-0.2835],\n",
      "        [-0.2789],\n",
      "        [-0.2812],\n",
      "        [-0.2836],\n",
      "        [-0.2757],\n",
      "        [-0.2761],\n",
      "        [-0.2714],\n",
      "        [-0.2755],\n",
      "        [-0.2821],\n",
      "        [-0.2751],\n",
      "        [-0.2825],\n",
      "        [-0.2788],\n",
      "        [-0.2801],\n",
      "        [-0.2815],\n",
      "        [-0.2778],\n",
      "        [-0.2813],\n",
      "        [-0.2754],\n",
      "        [-0.2914],\n",
      "        [-0.2837],\n",
      "        [-0.2755],\n",
      "        [-0.2701],\n",
      "        [-0.2737],\n",
      "        [-0.2776],\n",
      "        [-0.2740],\n",
      "        [-0.2898],\n",
      "        [-0.2836],\n",
      "        [-0.2801],\n",
      "        [-0.2800],\n",
      "        [-0.2853],\n",
      "        [-0.2753],\n",
      "        [-0.2680],\n",
      "        [-0.2702],\n",
      "        [-0.2807],\n",
      "        [-0.2746],\n",
      "        [-0.2801],\n",
      "        [-0.2746],\n",
      "        [-0.2805],\n",
      "        [-0.2838],\n",
      "        [-0.2749],\n",
      "        [-0.2794],\n",
      "        [-0.2745],\n",
      "        [-0.2812],\n",
      "        [-0.2855],\n",
      "        [-0.2728],\n",
      "        [-0.2690],\n",
      "        [-0.2789],\n",
      "        [-0.2747],\n",
      "        [-0.2817],\n",
      "        [-0.2779],\n",
      "        [-0.2771],\n",
      "        [-0.2866],\n",
      "        [-0.2815],\n",
      "        [-0.2874],\n",
      "        [-0.2743],\n",
      "        [-0.2712],\n",
      "        [-0.2746],\n",
      "        [-0.2806],\n",
      "        [-0.2798],\n",
      "        [-0.2726],\n",
      "        [-0.2671],\n",
      "        [-0.2809],\n",
      "        [-0.2630],\n",
      "        [-0.2796],\n",
      "        [-0.2877],\n",
      "        [-0.2793],\n",
      "        [-0.2719],\n",
      "        [-0.2742],\n",
      "        [-0.2713],\n",
      "        [-0.2885],\n",
      "        [-0.2741],\n",
      "        [-0.2773],\n",
      "        [-0.2759],\n",
      "        [-0.2768],\n",
      "        [-0.2788],\n",
      "        [-0.2778],\n",
      "        [-0.2766],\n",
      "        [-0.2637],\n",
      "        [-0.2661],\n",
      "        [-0.2771],\n",
      "        [-0.2756],\n",
      "        [-0.2816],\n",
      "        [-0.2761],\n",
      "        [-0.2694],\n",
      "        [-0.2705],\n",
      "        [-0.2717],\n",
      "        [-0.2747],\n",
      "        [-0.2782],\n",
      "        [-0.2785],\n",
      "        [-0.2779],\n",
      "        [-0.2727],\n",
      "        [-0.2895],\n",
      "        [-0.2728],\n",
      "        [-0.2844],\n",
      "        [-0.2850],\n",
      "        [-0.2778],\n",
      "        [-0.2827],\n",
      "        [-0.2781],\n",
      "        [-0.2726],\n",
      "        [-0.2822],\n",
      "        [-0.2801],\n",
      "        [-0.2707],\n",
      "        [-0.2795],\n",
      "        [-0.2723],\n",
      "        [-0.2785],\n",
      "        [-0.2694],\n",
      "        [-0.2780],\n",
      "        [-0.2799],\n",
      "        [-0.2704],\n",
      "        [-0.2760],\n",
      "        [-0.2788],\n",
      "        [-0.2780],\n",
      "        [-0.2795],\n",
      "        [-0.2831],\n",
      "        [-0.2776],\n",
      "        [-0.2788],\n",
      "        [-0.2826],\n",
      "        [-0.2842],\n",
      "        [-0.2814],\n",
      "        [-0.2747],\n",
      "        [-0.2751],\n",
      "        [-0.2754],\n",
      "        [-0.2740],\n",
      "        [-0.2760],\n",
      "        [-0.2801],\n",
      "        [-0.2782],\n",
      "        [-0.2709],\n",
      "        [-0.2723],\n",
      "        [-0.2759],\n",
      "        [-0.2783],\n",
      "        [-0.2799],\n",
      "        [-0.2828],\n",
      "        [-0.2792],\n",
      "        [-0.2766],\n",
      "        [-0.2897],\n",
      "        [-0.2722],\n",
      "        [-0.2778],\n",
      "        [-0.2772],\n",
      "        [-0.2795],\n",
      "        [-0.2760],\n",
      "        [-0.2730],\n",
      "        [-0.2917],\n",
      "        [-0.2751],\n",
      "        [-0.2793],\n",
      "        [-0.2771],\n",
      "        [-0.2827],\n",
      "        [-0.2660],\n",
      "        [-0.2789],\n",
      "        [-0.2809],\n",
      "        [-0.2714],\n",
      "        [-0.2788],\n",
      "        [-0.2766],\n",
      "        [-0.2830],\n",
      "        [-0.2721],\n",
      "        [-0.2711],\n",
      "        [-0.2822],\n",
      "        [-0.2852],\n",
      "        [-0.2802],\n",
      "        [-0.2770],\n",
      "        [-0.2778],\n",
      "        [-0.2728],\n",
      "        [-0.2727],\n",
      "        [-0.2708],\n",
      "        [-0.2693],\n",
      "        [-0.2792],\n",
      "        [-0.2718],\n",
      "        [-0.2735],\n",
      "        [-0.2736],\n",
      "        [-0.2695],\n",
      "        [-0.2688],\n",
      "        [-0.2722],\n",
      "        [-0.2740],\n",
      "        [-0.2727],\n",
      "        [-0.2713],\n",
      "        [-0.2802],\n",
      "        [-0.2785],\n",
      "        [-0.2778],\n",
      "        [-0.2727],\n",
      "        [-0.2801],\n",
      "        [-0.2860],\n",
      "        [-0.2766],\n",
      "        [-0.2794],\n",
      "        [-0.2776],\n",
      "        [-0.2796],\n",
      "        [-0.2736],\n",
      "        [-0.2705],\n",
      "        [-0.2884],\n",
      "        [-0.2732],\n",
      "        [-0.2770],\n",
      "        [-0.2803],\n",
      "        [-0.2678],\n",
      "        [-0.2691],\n",
      "        [-0.2794],\n",
      "        [-0.2813],\n",
      "        [-0.2758],\n",
      "        [-0.2818],\n",
      "        [-0.2772],\n",
      "        [-0.2800],\n",
      "        [-0.2842],\n",
      "        [-0.2822],\n",
      "        [-0.2822],\n",
      "        [-0.2738],\n",
      "        [-0.2782],\n",
      "        [-0.2829],\n",
      "        [-0.2795],\n",
      "        [-0.2797],\n",
      "        [-0.2791],\n",
      "        [-0.2778],\n",
      "        [-0.2842],\n",
      "        [-0.2774],\n",
      "        [-0.2771],\n",
      "        [-0.2763],\n",
      "        [-0.2710],\n",
      "        [-0.2687],\n",
      "        [-0.2826],\n",
      "        [-0.2811],\n",
      "        [-0.2668],\n",
      "        [-0.2771],\n",
      "        [-0.2774],\n",
      "        [-0.2753],\n",
      "        [-0.2859],\n",
      "        [-0.2867],\n",
      "        [-0.2792],\n",
      "        [-0.2787],\n",
      "        [-0.2813],\n",
      "        [-0.2793],\n",
      "        [-0.2709],\n",
      "        [-0.2736],\n",
      "        [-0.2866],\n",
      "        [-0.2722],\n",
      "        [-0.2824],\n",
      "        [-0.2736],\n",
      "        [-0.2782],\n",
      "        [-0.2817],\n",
      "        [-0.2689],\n",
      "        [-0.2770],\n",
      "        [-0.2761],\n",
      "        [-0.2777],\n",
      "        [-0.2789],\n",
      "        [-0.2825],\n",
      "        [-0.2814],\n",
      "        [-0.2717],\n",
      "        [-0.2823],\n",
      "        [-0.2770],\n",
      "        [-0.2793],\n",
      "        [-0.2770],\n",
      "        [-0.2731],\n",
      "        [-0.2816],\n",
      "        [-0.2707],\n",
      "        [-0.2671],\n",
      "        [-0.2735],\n",
      "        [-0.2670],\n",
      "        [-0.2734],\n",
      "        [-0.2655],\n",
      "        [-0.2738],\n",
      "        [-0.2724],\n",
      "        [-0.2708],\n",
      "        [-0.2720],\n",
      "        [-0.2840],\n",
      "        [-0.2728],\n",
      "        [-0.2715],\n",
      "        [-0.2749],\n",
      "        [-0.2844],\n",
      "        [-0.2808],\n",
      "        [-0.2669],\n",
      "        [-0.2797],\n",
      "        [-0.2679],\n",
      "        [-0.2883],\n",
      "        [-0.2722],\n",
      "        [-0.2736],\n",
      "        [-0.2700],\n",
      "        [-0.2648],\n",
      "        [-0.2841],\n",
      "        [-0.2911],\n",
      "        [-0.2721],\n",
      "        [-0.2708],\n",
      "        [-0.2755],\n",
      "        [-0.2743],\n",
      "        [-0.2707],\n",
      "        [-0.2799],\n",
      "        [-0.2751],\n",
      "        [-0.2904],\n",
      "        [-0.2789],\n",
      "        [-0.2746],\n",
      "        [-0.2725],\n",
      "        [-0.2857],\n",
      "        [-0.2829],\n",
      "        [-0.2860],\n",
      "        [-0.2780],\n",
      "        [-0.2702],\n",
      "        [-0.2811],\n",
      "        [-0.2788],\n",
      "        [-0.2790],\n",
      "        [-0.2706],\n",
      "        [-0.2784],\n",
      "        [-0.2785],\n",
      "        [-0.2731],\n",
      "        [-0.2850],\n",
      "        [-0.2745],\n",
      "        [-0.2683],\n",
      "        [-0.2762],\n",
      "        [-0.2866],\n",
      "        [-0.2729],\n",
      "        [-0.2754],\n",
      "        [-0.2763],\n",
      "        [-0.2773],\n",
      "        [-0.2882],\n",
      "        [-0.2744],\n",
      "        [-0.2748],\n",
      "        [-0.2727],\n",
      "        [-0.2796],\n",
      "        [-0.2830],\n",
      "        [-0.2751],\n",
      "        [-0.2729],\n",
      "        [-0.2704],\n",
      "        [-0.2804],\n",
      "        [-0.2825],\n",
      "        [-0.2801],\n",
      "        [-0.2793],\n",
      "        [-0.2737],\n",
      "        [-0.2750],\n",
      "        [-0.2771],\n",
      "        [-0.2774],\n",
      "        [-0.2892],\n",
      "        [-0.2746],\n",
      "        [-0.2798],\n",
      "        [-0.2718],\n",
      "        [-0.2772],\n",
      "        [-0.2764],\n",
      "        [-0.2699],\n",
      "        [-0.2747],\n",
      "        [-0.2799],\n",
      "        [-0.2777],\n",
      "        [-0.2769],\n",
      "        [-0.2752],\n",
      "        [-0.2742],\n",
      "        [-0.2746],\n",
      "        [-0.2625],\n",
      "        [-0.2733],\n",
      "        [-0.2795],\n",
      "        [-0.2749],\n",
      "        [-0.2695],\n",
      "        [-0.2768],\n",
      "        [-0.2809],\n",
      "        [-0.2819],\n",
      "        [-0.2806],\n",
      "        [-0.2741],\n",
      "        [-0.2779],\n",
      "        [-0.2715],\n",
      "        [-0.2786],\n",
      "        [-0.2682],\n",
      "        [-0.2797],\n",
      "        [-0.2721],\n",
      "        [-0.2867],\n",
      "        [-0.2757],\n",
      "        [-0.2758],\n",
      "        [-0.2772],\n",
      "        [-0.2771],\n",
      "        [-0.2731],\n",
      "        [-0.2773],\n",
      "        [-0.2794],\n",
      "        [-0.2882],\n",
      "        [-0.2795],\n",
      "        [-0.2830],\n",
      "        [-0.2570],\n",
      "        [-0.2873],\n",
      "        [-0.2756],\n",
      "        [-0.2739],\n",
      "        [-0.2771],\n",
      "        [-0.2819],\n",
      "        [-0.2869],\n",
      "        [-0.2869],\n",
      "        [-0.2734],\n",
      "        [-0.2748],\n",
      "        [-0.2845],\n",
      "        [-0.2834],\n",
      "        [-0.2769],\n",
      "        [-0.2718],\n",
      "        [-0.2785],\n",
      "        [-0.2732],\n",
      "        [-0.2830],\n",
      "        [-0.2770],\n",
      "        [-0.2683],\n",
      "        [-0.2725],\n",
      "        [-0.2632],\n",
      "        [-0.2724],\n",
      "        [-0.2791],\n",
      "        [-0.2697],\n",
      "        [-0.2766],\n",
      "        [-0.2814],\n",
      "        [-0.2729],\n",
      "        [-0.2712],\n",
      "        [-0.2853],\n",
      "        [-0.2723],\n",
      "        [-0.2698],\n",
      "        [-0.2842],\n",
      "        [-0.2803],\n",
      "        [-0.2751],\n",
      "        [-0.2805],\n",
      "        [-0.2790],\n",
      "        [-0.2754],\n",
      "        [-0.2750],\n",
      "        [-0.2726],\n",
      "        [-0.2802],\n",
      "        [-0.2629],\n",
      "        [-0.2777],\n",
      "        [-0.2752],\n",
      "        [-0.2769],\n",
      "        [-0.2765],\n",
      "        [-0.2743],\n",
      "        [-0.2767],\n",
      "        [-0.2706],\n",
      "        [-0.2787],\n",
      "        [-0.2752],\n",
      "        [-0.2760],\n",
      "        [-0.2717],\n",
      "        [-0.2736],\n",
      "        [-0.2710],\n",
      "        [-0.2774],\n",
      "        [-0.2778],\n",
      "        [-0.2771],\n",
      "        [-0.2849],\n",
      "        [-0.2852],\n",
      "        [-0.2770],\n",
      "        [-0.2829],\n",
      "        [-0.2834],\n",
      "        [-0.2681],\n",
      "        [-0.2732],\n",
      "        [-0.2792],\n",
      "        [-0.2793],\n",
      "        [-0.2828],\n",
      "        [-0.2677],\n",
      "        [-0.2710],\n",
      "        [-0.2729],\n",
      "        [-0.2792],\n",
      "        [-0.2824],\n",
      "        [-0.2758],\n",
      "        [-0.2733],\n",
      "        [-0.2816],\n",
      "        [-0.2758],\n",
      "        [-0.2753],\n",
      "        [-0.2761],\n",
      "        [-0.2757],\n",
      "        [-0.2787],\n",
      "        [-0.2842],\n",
      "        [-0.2659],\n",
      "        [-0.2794],\n",
      "        [-0.2860],\n",
      "        [-0.2769],\n",
      "        [-0.2780],\n",
      "        [-0.2750],\n",
      "        [-0.2852],\n",
      "        [-0.2859],\n",
      "        [-0.2858],\n",
      "        [-0.2857],\n",
      "        [-0.2822],\n",
      "        [-0.2722],\n",
      "        [-0.2767],\n",
      "        [-0.2709],\n",
      "        [-0.2824],\n",
      "        [-0.2743],\n",
      "        [-0.2810],\n",
      "        [-0.2842],\n",
      "        [-0.2735],\n",
      "        [-0.2783],\n",
      "        [-0.2756],\n",
      "        [-0.2749],\n",
      "        [-0.2794],\n",
      "        [-0.2722],\n",
      "        [-0.2806],\n",
      "        [-0.2837],\n",
      "        [-0.2808],\n",
      "        [-0.2711],\n",
      "        [-0.2739],\n",
      "        [-0.2830],\n",
      "        [-0.2756],\n",
      "        [-0.2742],\n",
      "        [-0.2805],\n",
      "        [-0.2808],\n",
      "        [-0.2785],\n",
      "        [-0.2734],\n",
      "        [-0.2838],\n",
      "        [-0.2798],\n",
      "        [-0.2725],\n",
      "        [-0.2768],\n",
      "        [-0.2720],\n",
      "        [-0.2716],\n",
      "        [-0.2774],\n",
      "        [-0.2732],\n",
      "        [-0.2778],\n",
      "        [-0.2751],\n",
      "        [-0.2784],\n",
      "        [-0.2797],\n",
      "        [-0.2737],\n",
      "        [-0.2694],\n",
      "        [-0.2830],\n",
      "        [-0.2725],\n",
      "        [-0.2817],\n",
      "        [-0.2742],\n",
      "        [-0.2747],\n",
      "        [-0.2746],\n",
      "        [-0.2702],\n",
      "        [-0.2809],\n",
      "        [-0.2829],\n",
      "        [-0.2771],\n",
      "        [-0.2751],\n",
      "        [-0.2797],\n",
      "        [-0.2694],\n",
      "        [-0.2905],\n",
      "        [-0.2717],\n",
      "        [-0.2699],\n",
      "        [-0.2781],\n",
      "        [-0.2767],\n",
      "        [-0.2710],\n",
      "        [-0.2776],\n",
      "        [-0.2794],\n",
      "        [-0.2842],\n",
      "        [-0.2771],\n",
      "        [-0.2759],\n",
      "        [-0.2749],\n",
      "        [-0.2805],\n",
      "        [-0.2749],\n",
      "        [-0.2761],\n",
      "        [-0.2734],\n",
      "        [-0.2758],\n",
      "        [-0.2772],\n",
      "        [-0.2847],\n",
      "        [-0.2898],\n",
      "        [-0.2800],\n",
      "        [-0.2691],\n",
      "        [-0.2820],\n",
      "        [-0.2857],\n",
      "        [-0.2801],\n",
      "        [-0.2785],\n",
      "        [-0.2769],\n",
      "        [-0.2825],\n",
      "        [-0.2663],\n",
      "        [-0.2884],\n",
      "        [-0.2794],\n",
      "        [-0.2814],\n",
      "        [-0.2711],\n",
      "        [-0.2804],\n",
      "        [-0.2739],\n",
      "        [-0.2694],\n",
      "        [-0.2732],\n",
      "        [-0.2761],\n",
      "        [-0.2552],\n",
      "        [-0.2771],\n",
      "        [-0.2806],\n",
      "        [-0.2758],\n",
      "        [-0.2688],\n",
      "        [-0.2763],\n",
      "        [-0.2814],\n",
      "        [-0.2797],\n",
      "        [-0.2760],\n",
      "        [-0.2737],\n",
      "        [-0.2728],\n",
      "        [-0.2772],\n",
      "        [-0.2910],\n",
      "        [-0.2862],\n",
      "        [-0.2824],\n",
      "        [-0.2670],\n",
      "        [-0.2787],\n",
      "        [-0.2853],\n",
      "        [-0.2778],\n",
      "        [-0.2746],\n",
      "        [-0.2720],\n",
      "        [-0.2756],\n",
      "        [-0.2759],\n",
      "        [-0.2752],\n",
      "        [-0.2817]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(3.1863e+15, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Regressor().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.001)\n",
    "\n",
    "output = model(train_batch[0])\n",
    "print(label_batch[0])\n",
    "loss = criterion(output, label_batch[0])\n",
    "#loss = criterion(output, label_batch[0])\n",
    "print(output)\n",
    "\n",
    "output = model(train_batch[1])\n",
    "loss = criterion(output, label_batch[i])\n",
    "#loss = criterion(output, label_batch[1])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000..  Training Loss: 11.8913..  Test Loss: 11.8650.. \n",
      "Epoch: 2/2000..  Training Loss: 10.0634..  Test Loss: 10.0631.. \n",
      "Epoch: 3/2000..  Training Loss: 9.0035..  Test Loss: 9.0122.. \n",
      "Epoch: 4/2000..  Training Loss: 8.2551..  Test Loss: 8.2682.. \n",
      "Epoch: 5/2000..  Training Loss: 7.6631..  Test Loss: 7.6783.. \n",
      "Epoch: 6/2000..  Training Loss: 7.1501..  Test Loss: 7.1666.. \n",
      "Epoch: 7/2000..  Training Loss: 6.6951..  Test Loss: 6.7129.. \n",
      "Epoch: 8/2000..  Training Loss: 6.3018..  Test Loss: 6.3208.. \n",
      "Epoch: 9/2000..  Training Loss: 5.9628..  Test Loss: 5.9828.. \n",
      "Epoch: 10/2000..  Training Loss: 5.6652..  Test Loss: 5.6859.. \n",
      "Epoch: 11/2000..  Training Loss: 5.3995..  Test Loss: 5.4208.. \n",
      "Epoch: 12/2000..  Training Loss: 5.1590..  Test Loss: 5.1808.. \n",
      "Epoch: 13/2000..  Training Loss: 4.9389..  Test Loss: 4.9611.. \n",
      "Epoch: 14/2000..  Training Loss: 4.7357..  Test Loss: 4.7582.. \n",
      "Epoch: 15/2000..  Training Loss: 4.5448..  Test Loss: 4.5675.. \n",
      "Epoch: 16/2000..  Training Loss: 4.3624..  Test Loss: 4.3854.. \n",
      "Epoch: 17/2000..  Training Loss: 4.1890..  Test Loss: 4.2121.. \n",
      "Epoch: 18/2000..  Training Loss: 4.0247..  Test Loss: 4.0481.. \n",
      "Epoch: 19/2000..  Training Loss: 3.8694..  Test Loss: 3.8930.. \n",
      "Epoch: 20/2000..  Training Loss: 3.7224..  Test Loss: 3.7461.. \n",
      "Epoch: 21/2000..  Training Loss: 3.5831..  Test Loss: 3.6070.. \n",
      "Epoch: 22/2000..  Training Loss: 3.4508..  Test Loss: 3.4748.. \n",
      "Epoch: 23/2000..  Training Loss: 3.3250..  Test Loss: 3.3491.. \n",
      "Epoch: 24/2000..  Training Loss: 3.2050..  Test Loss: 3.2292.. \n",
      "Epoch: 25/2000..  Training Loss: 3.0904..  Test Loss: 3.1148.. \n",
      "Epoch: 26/2000..  Training Loss: 2.9809..  Test Loss: 3.0053.. \n",
      "Epoch: 27/2000..  Training Loss: 2.8760..  Test Loss: 2.9005.. \n",
      "Epoch: 28/2000..  Training Loss: 2.7755..  Test Loss: 2.8000.. \n",
      "Epoch: 29/2000..  Training Loss: 2.6790..  Test Loss: 2.7036.. \n",
      "Epoch: 30/2000..  Training Loss: 2.5865..  Test Loss: 2.6111.. \n",
      "Epoch: 31/2000..  Training Loss: 2.4976..  Test Loss: 2.5222.. \n",
      "Epoch: 32/2000..  Training Loss: 2.4122..  Test Loss: 2.4368.. \n",
      "Epoch: 33/2000..  Training Loss: 2.3301..  Test Loss: 2.3548.. \n",
      "Epoch: 34/2000..  Training Loss: 2.2513..  Test Loss: 2.2759.. \n",
      "Epoch: 35/2000..  Training Loss: 2.1755..  Test Loss: 2.2002.. \n",
      "Epoch: 36/2000..  Training Loss: 2.1029..  Test Loss: 2.1274.. \n",
      "Epoch: 37/2000..  Training Loss: 2.0331..  Test Loss: 2.0576.. \n",
      "Epoch: 38/2000..  Training Loss: 1.9663..  Test Loss: 1.9907.. \n",
      "Epoch: 39/2000..  Training Loss: 1.9023..  Test Loss: 1.9266.. \n",
      "Epoch: 40/2000..  Training Loss: 1.8411..  Test Loss: 1.8653.. \n",
      "Epoch: 41/2000..  Training Loss: 1.7826..  Test Loss: 1.8067.. \n",
      "Epoch: 42/2000..  Training Loss: 1.7269..  Test Loss: 1.7509.. \n",
      "Epoch: 43/2000..  Training Loss: 1.6740..  Test Loss: 1.6977.. \n",
      "Epoch: 44/2000..  Training Loss: 1.6237..  Test Loss: 1.6472.. \n",
      "Epoch: 45/2000..  Training Loss: 1.5761..  Test Loss: 1.5994.. \n",
      "Epoch: 46/2000..  Training Loss: 1.5312..  Test Loss: 1.5542.. \n",
      "Epoch: 47/2000..  Training Loss: 1.4889..  Test Loss: 1.5116.. \n",
      "Epoch: 48/2000..  Training Loss: 1.4492..  Test Loss: 1.4717.. \n",
      "Epoch: 49/2000..  Training Loss: 1.4121..  Test Loss: 1.4342.. \n",
      "Epoch: 50/2000..  Training Loss: 1.3776..  Test Loss: 1.3993.. \n",
      "Epoch: 51/2000..  Training Loss: 1.3456..  Test Loss: 1.3669.. \n",
      "Epoch: 52/2000..  Training Loss: 1.3160..  Test Loss: 1.3369.. \n",
      "Epoch: 53/2000..  Training Loss: 1.2887..  Test Loss: 1.3092.. \n",
      "Epoch: 54/2000..  Training Loss: 1.2638..  Test Loss: 1.2838.. \n",
      "Epoch: 55/2000..  Training Loss: 1.2410..  Test Loss: 1.2606.. \n",
      "Epoch: 56/2000..  Training Loss: 1.2204..  Test Loss: 1.2394.. \n",
      "Epoch: 57/2000..  Training Loss: 1.2017..  Test Loss: 1.2203.. \n",
      "Epoch: 58/2000..  Training Loss: 1.1850..  Test Loss: 1.2030.. \n",
      "Epoch: 59/2000..  Training Loss: 1.1700..  Test Loss: 1.1875.. \n",
      "Epoch: 60/2000..  Training Loss: 1.1566..  Test Loss: 1.1736.. \n",
      "Epoch: 61/2000..  Training Loss: 1.1447..  Test Loss: 1.1612.. \n",
      "Epoch: 62/2000..  Training Loss: 1.1342..  Test Loss: 1.1502.. \n",
      "Epoch: 63/2000..  Training Loss: 1.1250..  Test Loss: 1.1405.. \n",
      "Epoch: 64/2000..  Training Loss: 1.1170..  Test Loss: 1.1319.. \n",
      "Epoch: 65/2000..  Training Loss: 1.1099..  Test Loss: 1.1244.. \n",
      "Epoch: 66/2000..  Training Loss: 1.1038..  Test Loss: 1.1178.. \n",
      "Epoch: 67/2000..  Training Loss: 1.0986..  Test Loss: 1.1121.. \n",
      "Epoch: 68/2000..  Training Loss: 1.0940..  Test Loss: 1.1071.. \n",
      "Epoch: 69/2000..  Training Loss: 1.0901..  Test Loss: 1.1028.. \n",
      "Epoch: 70/2000..  Training Loss: 1.0868..  Test Loss: 1.0991.. \n",
      "Epoch: 71/2000..  Training Loss: 1.0840..  Test Loss: 1.0958.. \n",
      "Epoch: 72/2000..  Training Loss: 1.0815..  Test Loss: 1.0931.. \n",
      "Epoch: 73/2000..  Training Loss: 1.0795..  Test Loss: 1.0907.. \n",
      "Epoch: 74/2000..  Training Loss: 1.0778..  Test Loss: 1.0886.. \n",
      "Epoch: 75/2000..  Training Loss: 1.0763..  Test Loss: 1.0869.. \n",
      "Epoch: 76/2000..  Training Loss: 1.0751..  Test Loss: 1.0854.. \n",
      "Epoch: 77/2000..  Training Loss: 1.0741..  Test Loss: 1.0841.. \n",
      "Epoch: 78/2000..  Training Loss: 1.0732..  Test Loss: 1.0829.. \n",
      "Epoch: 79/2000..  Training Loss: 1.0724..  Test Loss: 1.0820.. \n",
      "Epoch: 80/2000..  Training Loss: 1.0718..  Test Loss: 1.0811.. \n",
      "Epoch: 81/2000..  Training Loss: 1.0713..  Test Loss: 1.0803.. \n",
      "Epoch: 82/2000..  Training Loss: 1.0708..  Test Loss: 1.0797.. \n",
      "Epoch: 83/2000..  Training Loss: 1.0704..  Test Loss: 1.0791.. \n",
      "Epoch: 84/2000..  Training Loss: 1.0700..  Test Loss: 1.0785.. \n",
      "Epoch: 85/2000..  Training Loss: 1.0696..  Test Loss: 1.0780.. \n",
      "Epoch: 86/2000..  Training Loss: 1.0693..  Test Loss: 1.0776.. \n",
      "Epoch: 87/2000..  Training Loss: 1.0690..  Test Loss: 1.0771.. \n",
      "Epoch: 88/2000..  Training Loss: 1.0687..  Test Loss: 1.0767.. \n",
      "Epoch: 89/2000..  Training Loss: 1.0684..  Test Loss: 1.0763.. \n",
      "Epoch: 90/2000..  Training Loss: 1.0681..  Test Loss: 1.0759.. \n",
      "Epoch: 91/2000..  Training Loss: 1.0678..  Test Loss: 1.0755.. \n",
      "Epoch: 92/2000..  Training Loss: 1.0675..  Test Loss: 1.0751.. \n",
      "Epoch: 93/2000..  Training Loss: 1.0672..  Test Loss: 1.0747.. \n",
      "Epoch: 94/2000..  Training Loss: 1.0669..  Test Loss: 1.0743.. \n",
      "Epoch: 95/2000..  Training Loss: 1.0666..  Test Loss: 1.0739.. \n",
      "Epoch: 96/2000..  Training Loss: 1.0662..  Test Loss: 1.0735.. \n",
      "Epoch: 97/2000..  Training Loss: 1.0659..  Test Loss: 1.0731.. \n",
      "Epoch: 98/2000..  Training Loss: 1.0655..  Test Loss: 1.0727.. \n",
      "Epoch: 99/2000..  Training Loss: 1.0651..  Test Loss: 1.0723.. \n",
      "Epoch: 100/2000..  Training Loss: 1.0647..  Test Loss: 1.0718.. \n",
      "Epoch: 101/2000..  Training Loss: 1.0643..  Test Loss: 1.0714.. \n",
      "Epoch: 102/2000..  Training Loss: 1.0639..  Test Loss: 1.0709.. \n",
      "Epoch: 103/2000..  Training Loss: 1.0635..  Test Loss: 1.0704.. \n",
      "Epoch: 104/2000..  Training Loss: 1.0631..  Test Loss: 1.0700.. \n",
      "Epoch: 105/2000..  Training Loss: 1.0626..  Test Loss: 1.0695.. \n",
      "Epoch: 106/2000..  Training Loss: 1.0622..  Test Loss: 1.0690.. \n",
      "Epoch: 107/2000..  Training Loss: 1.0617..  Test Loss: 1.0685.. \n",
      "Epoch: 108/2000..  Training Loss: 1.0612..  Test Loss: 1.0679.. \n",
      "Epoch: 109/2000..  Training Loss: 1.0607..  Test Loss: 1.0674.. \n",
      "Epoch: 110/2000..  Training Loss: 1.0602..  Test Loss: 1.0669.. \n",
      "Epoch: 111/2000..  Training Loss: 1.0597..  Test Loss: 1.0663.. \n",
      "Epoch: 112/2000..  Training Loss: 1.0592..  Test Loss: 1.0657.. \n",
      "Epoch: 113/2000..  Training Loss: 1.0586..  Test Loss: 1.0651.. \n",
      "Epoch: 114/2000..  Training Loss: 1.0581..  Test Loss: 1.0646.. \n",
      "Epoch: 115/2000..  Training Loss: 1.0575..  Test Loss: 1.0639.. \n",
      "Epoch: 116/2000..  Training Loss: 1.0570..  Test Loss: 1.0633.. \n",
      "Epoch: 117/2000..  Training Loss: 1.0564..  Test Loss: 1.0627.. \n",
      "Epoch: 118/2000..  Training Loss: 1.0558..  Test Loss: 1.0621.. \n",
      "Epoch: 119/2000..  Training Loss: 1.0552..  Test Loss: 1.0614.. \n",
      "Epoch: 120/2000..  Training Loss: 1.0545..  Test Loss: 1.0607.. \n",
      "Epoch: 121/2000..  Training Loss: 1.0539..  Test Loss: 1.0600.. \n",
      "Epoch: 122/2000..  Training Loss: 1.0533..  Test Loss: 1.0593.. \n",
      "Epoch: 123/2000..  Training Loss: 1.0526..  Test Loss: 1.0586.. \n",
      "Epoch: 124/2000..  Training Loss: 1.0519..  Test Loss: 1.0579.. \n",
      "Epoch: 125/2000..  Training Loss: 1.0512..  Test Loss: 1.0572.. \n",
      "Epoch: 126/2000..  Training Loss: 1.0505..  Test Loss: 1.0564.. \n",
      "Epoch: 127/2000..  Training Loss: 1.0498..  Test Loss: 1.0556.. \n",
      "Epoch: 128/2000..  Training Loss: 1.0491..  Test Loss: 1.0548.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129/2000..  Training Loss: 1.0483..  Test Loss: 1.0540.. \n",
      "Epoch: 130/2000..  Training Loss: 1.0476..  Test Loss: 1.0532.. \n",
      "Epoch: 131/2000..  Training Loss: 1.0468..  Test Loss: 1.0524.. \n",
      "Epoch: 132/2000..  Training Loss: 1.0460..  Test Loss: 1.0515.. \n",
      "Epoch: 133/2000..  Training Loss: 1.0452..  Test Loss: 1.0507.. \n",
      "Epoch: 134/2000..  Training Loss: 1.0444..  Test Loss: 1.0498.. \n",
      "Epoch: 135/2000..  Training Loss: 1.0435..  Test Loss: 1.0489.. \n",
      "Epoch: 136/2000..  Training Loss: 1.0427..  Test Loss: 1.0480.. \n",
      "Epoch: 137/2000..  Training Loss: 1.0418..  Test Loss: 1.0470.. \n",
      "Epoch: 138/2000..  Training Loss: 1.0409..  Test Loss: 1.0461.. \n",
      "Epoch: 139/2000..  Training Loss: 1.0400..  Test Loss: 1.0451.. \n",
      "Epoch: 140/2000..  Training Loss: 1.0391..  Test Loss: 1.0441.. \n",
      "Epoch: 141/2000..  Training Loss: 1.0381..  Test Loss: 1.0431.. \n",
      "Epoch: 142/2000..  Training Loss: 1.0372..  Test Loss: 1.0420.. \n",
      "Epoch: 143/2000..  Training Loss: 1.0362..  Test Loss: 1.0410.. \n",
      "Epoch: 144/2000..  Training Loss: 1.0352..  Test Loss: 1.0399.. \n",
      "Epoch: 145/2000..  Training Loss: 1.0342..  Test Loss: 1.0388.. \n",
      "Epoch: 146/2000..  Training Loss: 1.0331..  Test Loss: 1.0377.. \n",
      "Epoch: 147/2000..  Training Loss: 1.0320..  Test Loss: 1.0365.. \n",
      "Epoch: 148/2000..  Training Loss: 1.0309..  Test Loss: 1.0354.. \n",
      "Epoch: 149/2000..  Training Loss: 1.0298..  Test Loss: 1.0342.. \n",
      "Epoch: 150/2000..  Training Loss: 1.0287..  Test Loss: 1.0329.. \n",
      "Epoch: 151/2000..  Training Loss: 1.0275..  Test Loss: 1.0317.. \n",
      "Epoch: 152/2000..  Training Loss: 1.0264..  Test Loss: 1.0304.. \n",
      "Epoch: 153/2000..  Training Loss: 1.0251..  Test Loss: 1.0292.. \n",
      "Epoch: 154/2000..  Training Loss: 1.0239..  Test Loss: 1.0278.. \n",
      "Epoch: 155/2000..  Training Loss: 1.0227..  Test Loss: 1.0265.. \n",
      "Epoch: 156/2000..  Training Loss: 1.0214..  Test Loss: 1.0251.. \n",
      "Epoch: 157/2000..  Training Loss: 1.0201..  Test Loss: 1.0237.. \n",
      "Epoch: 158/2000..  Training Loss: 1.0187..  Test Loss: 1.0223.. \n",
      "Epoch: 159/2000..  Training Loss: 1.0174..  Test Loss: 1.0209.. \n",
      "Epoch: 160/2000..  Training Loss: 1.0160..  Test Loss: 1.0194.. \n",
      "Epoch: 161/2000..  Training Loss: 1.0146..  Test Loss: 1.0179.. \n",
      "Epoch: 162/2000..  Training Loss: 1.0131..  Test Loss: 1.0164.. \n",
      "Epoch: 163/2000..  Training Loss: 1.0117..  Test Loss: 1.0149.. \n",
      "Epoch: 164/2000..  Training Loss: 1.0102..  Test Loss: 1.0133.. \n",
      "Epoch: 165/2000..  Training Loss: 1.0087..  Test Loss: 1.0117.. \n",
      "Epoch: 166/2000..  Training Loss: 1.0072..  Test Loss: 1.0101.. \n",
      "Epoch: 167/2000..  Training Loss: 1.0056..  Test Loss: 1.0085.. \n",
      "Epoch: 168/2000..  Training Loss: 1.0040..  Test Loss: 1.0068.. \n",
      "Epoch: 169/2000..  Training Loss: 1.0024..  Test Loss: 1.0052.. \n",
      "Epoch: 170/2000..  Training Loss: 1.0008..  Test Loss: 1.0035.. \n",
      "Epoch: 171/2000..  Training Loss: 0.9992..  Test Loss: 1.0017.. \n",
      "Epoch: 172/2000..  Training Loss: 0.9975..  Test Loss: 1.0000.. \n",
      "Epoch: 173/2000..  Training Loss: 0.9958..  Test Loss: 0.9982.. \n",
      "Epoch: 174/2000..  Training Loss: 0.9941..  Test Loss: 0.9964.. \n",
      "Epoch: 175/2000..  Training Loss: 0.9923..  Test Loss: 0.9946.. \n",
      "Epoch: 176/2000..  Training Loss: 0.9905..  Test Loss: 0.9926.. \n",
      "Epoch: 177/2000..  Training Loss: 0.9886..  Test Loss: 0.9907.. \n",
      "Epoch: 178/2000..  Training Loss: 0.9867..  Test Loss: 0.9887.. \n",
      "Epoch: 179/2000..  Training Loss: 0.9848..  Test Loss: 0.9866.. \n",
      "Epoch: 180/2000..  Training Loss: 0.9828..  Test Loss: 0.9845.. \n",
      "Epoch: 181/2000..  Training Loss: 0.9808..  Test Loss: 0.9824.. \n",
      "Epoch: 182/2000..  Training Loss: 0.9787..  Test Loss: 0.9802.. \n",
      "Epoch: 183/2000..  Training Loss: 0.9766..  Test Loss: 0.9779.. \n",
      "Epoch: 184/2000..  Training Loss: 0.9743..  Test Loss: 0.9756.. \n",
      "Epoch: 185/2000..  Training Loss: 0.9720..  Test Loss: 0.9732.. \n",
      "Epoch: 186/2000..  Training Loss: 0.9696..  Test Loss: 0.9707.. \n",
      "Epoch: 187/2000..  Training Loss: 0.9671..  Test Loss: 0.9681.. \n",
      "Epoch: 188/2000..  Training Loss: 0.9644..  Test Loss: 0.9654.. \n",
      "Epoch: 189/2000..  Training Loss: 0.9617..  Test Loss: 0.9626.. \n",
      "Epoch: 190/2000..  Training Loss: 0.9587..  Test Loss: 0.9596.. \n",
      "Epoch: 191/2000..  Training Loss: 0.9557..  Test Loss: 0.9564.. \n",
      "Epoch: 192/2000..  Training Loss: 0.9525..  Test Loss: 0.9531.. \n",
      "Epoch: 193/2000..  Training Loss: 0.9492..  Test Loss: 0.9497.. \n",
      "Epoch: 194/2000..  Training Loss: 0.9458..  Test Loss: 0.9461.. \n",
      "Epoch: 195/2000..  Training Loss: 0.9424..  Test Loss: 0.9425.. \n",
      "Epoch: 196/2000..  Training Loss: 0.9390..  Test Loss: 0.9388.. \n",
      "Epoch: 197/2000..  Training Loss: 0.9356..  Test Loss: 0.9352.. \n",
      "Epoch: 198/2000..  Training Loss: 0.9322..  Test Loss: 0.9315.. \n",
      "Epoch: 199/2000..  Training Loss: 0.9289..  Test Loss: 0.9279.. \n",
      "Epoch: 200/2000..  Training Loss: 0.9255..  Test Loss: 0.9242.. \n",
      "Epoch: 201/2000..  Training Loss: 0.9220..  Test Loss: 0.9206.. \n",
      "Epoch: 202/2000..  Training Loss: 0.9186..  Test Loss: 0.9169.. \n",
      "Epoch: 203/2000..  Training Loss: 0.9152..  Test Loss: 0.9132.. \n",
      "Epoch: 204/2000..  Training Loss: 0.9117..  Test Loss: 0.9096.. \n",
      "Epoch: 205/2000..  Training Loss: 0.9083..  Test Loss: 0.9060.. \n",
      "Epoch: 206/2000..  Training Loss: 0.9048..  Test Loss: 0.9024.. \n",
      "Epoch: 207/2000..  Training Loss: 0.9013..  Test Loss: 0.8987.. \n",
      "Epoch: 208/2000..  Training Loss: 0.8977..  Test Loss: 0.8951.. \n",
      "Epoch: 209/2000..  Training Loss: 0.8940..  Test Loss: 0.8914.. \n",
      "Epoch: 210/2000..  Training Loss: 0.8904..  Test Loss: 0.8877.. \n",
      "Epoch: 211/2000..  Training Loss: 0.8866..  Test Loss: 0.8839.. \n",
      "Epoch: 212/2000..  Training Loss: 0.8829..  Test Loss: 0.8801.. \n",
      "Epoch: 213/2000..  Training Loss: 0.8792..  Test Loss: 0.8764.. \n",
      "Epoch: 214/2000..  Training Loss: 0.8755..  Test Loss: 0.8726.. \n",
      "Epoch: 215/2000..  Training Loss: 0.8719..  Test Loss: 0.8689.. \n",
      "Epoch: 216/2000..  Training Loss: 0.8682..  Test Loss: 0.8652.. \n",
      "Epoch: 217/2000..  Training Loss: 0.8646..  Test Loss: 0.8616.. \n",
      "Epoch: 218/2000..  Training Loss: 0.8612..  Test Loss: 0.8580.. \n",
      "Epoch: 219/2000..  Training Loss: 0.8578..  Test Loss: 0.8546.. \n",
      "Epoch: 220/2000..  Training Loss: 0.8545..  Test Loss: 0.8511.. \n",
      "Epoch: 221/2000..  Training Loss: 0.8513..  Test Loss: 0.8478.. \n",
      "Epoch: 222/2000..  Training Loss: 0.8482..  Test Loss: 0.8445.. \n",
      "Epoch: 223/2000..  Training Loss: 0.8451..  Test Loss: 0.8411.. \n",
      "Epoch: 224/2000..  Training Loss: 0.8422..  Test Loss: 0.8380.. \n",
      "Epoch: 225/2000..  Training Loss: 0.8394..  Test Loss: 0.8350.. \n",
      "Epoch: 226/2000..  Training Loss: 0.8368..  Test Loss: 0.8321.. \n",
      "Epoch: 227/2000..  Training Loss: 0.8343..  Test Loss: 0.8292.. \n",
      "Epoch: 228/2000..  Training Loss: 0.8319..  Test Loss: 0.8265.. \n",
      "Epoch: 229/2000..  Training Loss: 0.8297..  Test Loss: 0.8238.. \n",
      "Epoch: 230/2000..  Training Loss: 0.8275..  Test Loss: 0.8213.. \n",
      "Epoch: 231/2000..  Training Loss: 0.8254..  Test Loss: 0.8188.. \n",
      "Epoch: 232/2000..  Training Loss: 0.8234..  Test Loss: 0.8163.. \n",
      "Epoch: 233/2000..  Training Loss: 0.8214..  Test Loss: 0.8139.. \n",
      "Epoch: 234/2000..  Training Loss: 0.8195..  Test Loss: 0.8116.. \n",
      "Epoch: 235/2000..  Training Loss: 0.8176..  Test Loss: 0.8094.. \n",
      "Epoch: 236/2000..  Training Loss: 0.8159..  Test Loss: 0.8072.. \n",
      "Epoch: 237/2000..  Training Loss: 0.8142..  Test Loss: 0.8051.. \n",
      "Epoch: 238/2000..  Training Loss: 0.8126..  Test Loss: 0.8031.. \n",
      "Epoch: 239/2000..  Training Loss: 0.8110..  Test Loss: 0.8011.. \n",
      "Epoch: 240/2000..  Training Loss: 0.8095..  Test Loss: 0.7991.. \n",
      "Epoch: 241/2000..  Training Loss: 0.8079..  Test Loss: 0.7972.. \n",
      "Epoch: 242/2000..  Training Loss: 0.8065..  Test Loss: 0.7954.. \n",
      "Epoch: 243/2000..  Training Loss: 0.8050..  Test Loss: 0.7936.. \n",
      "Epoch: 244/2000..  Training Loss: 0.8035..  Test Loss: 0.7919.. \n",
      "Epoch: 245/2000..  Training Loss: 0.8021..  Test Loss: 0.7902.. \n",
      "Epoch: 246/2000..  Training Loss: 0.8007..  Test Loss: 0.7885.. \n",
      "Epoch: 247/2000..  Training Loss: 0.7993..  Test Loss: 0.7869.. \n",
      "Epoch: 248/2000..  Training Loss: 0.7979..  Test Loss: 0.7853.. \n",
      "Epoch: 249/2000..  Training Loss: 0.7965..  Test Loss: 0.7838.. \n",
      "Epoch: 250/2000..  Training Loss: 0.7952..  Test Loss: 0.7822.. \n",
      "Epoch: 251/2000..  Training Loss: 0.7939..  Test Loss: 0.7807.. \n",
      "Epoch: 252/2000..  Training Loss: 0.7925..  Test Loss: 0.7792.. \n",
      "Epoch: 253/2000..  Training Loss: 0.7911..  Test Loss: 0.7778.. \n",
      "Epoch: 254/2000..  Training Loss: 0.7898..  Test Loss: 0.7763.. \n",
      "Epoch: 255/2000..  Training Loss: 0.7884..  Test Loss: 0.7749.. \n",
      "Epoch: 256/2000..  Training Loss: 0.7870..  Test Loss: 0.7735.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 257/2000..  Training Loss: 0.7856..  Test Loss: 0.7721.. \n",
      "Epoch: 258/2000..  Training Loss: 0.7843..  Test Loss: 0.7708.. \n",
      "Epoch: 259/2000..  Training Loss: 0.7829..  Test Loss: 0.7694.. \n",
      "Epoch: 260/2000..  Training Loss: 0.7815..  Test Loss: 0.7681.. \n",
      "Epoch: 261/2000..  Training Loss: 0.7802..  Test Loss: 0.7668.. \n",
      "Epoch: 262/2000..  Training Loss: 0.7789..  Test Loss: 0.7656.. \n",
      "Epoch: 263/2000..  Training Loss: 0.7776..  Test Loss: 0.7643.. \n",
      "Epoch: 264/2000..  Training Loss: 0.7763..  Test Loss: 0.7631.. \n",
      "Epoch: 265/2000..  Training Loss: 0.7750..  Test Loss: 0.7619.. \n",
      "Epoch: 266/2000..  Training Loss: 0.7738..  Test Loss: 0.7607.. \n",
      "Epoch: 267/2000..  Training Loss: 0.7725..  Test Loss: 0.7595.. \n",
      "Epoch: 268/2000..  Training Loss: 0.7713..  Test Loss: 0.7584.. \n",
      "Epoch: 269/2000..  Training Loss: 0.7701..  Test Loss: 0.7572.. \n",
      "Epoch: 270/2000..  Training Loss: 0.7689..  Test Loss: 0.7561.. \n",
      "Epoch: 271/2000..  Training Loss: 0.7677..  Test Loss: 0.7549.. \n",
      "Epoch: 272/2000..  Training Loss: 0.7665..  Test Loss: 0.7538.. \n",
      "Epoch: 273/2000..  Training Loss: 0.7653..  Test Loss: 0.7527.. \n",
      "Epoch: 274/2000..  Training Loss: 0.7642..  Test Loss: 0.7517.. \n",
      "Epoch: 275/2000..  Training Loss: 0.7631..  Test Loss: 0.7506.. \n",
      "Epoch: 276/2000..  Training Loss: 0.7619..  Test Loss: 0.7495.. \n",
      "Epoch: 277/2000..  Training Loss: 0.7608..  Test Loss: 0.7485.. \n",
      "Epoch: 278/2000..  Training Loss: 0.7597..  Test Loss: 0.7474.. \n",
      "Epoch: 279/2000..  Training Loss: 0.7586..  Test Loss: 0.7464.. \n",
      "Epoch: 280/2000..  Training Loss: 0.7575..  Test Loss: 0.7453.. \n",
      "Epoch: 281/2000..  Training Loss: 0.7564..  Test Loss: 0.7443.. \n",
      "Epoch: 282/2000..  Training Loss: 0.7553..  Test Loss: 0.7433.. \n",
      "Epoch: 283/2000..  Training Loss: 0.7542..  Test Loss: 0.7423.. \n",
      "Epoch: 284/2000..  Training Loss: 0.7532..  Test Loss: 0.7413.. \n",
      "Epoch: 285/2000..  Training Loss: 0.7520..  Test Loss: 0.7402.. \n",
      "Epoch: 286/2000..  Training Loss: 0.7509..  Test Loss: 0.7392.. \n",
      "Epoch: 287/2000..  Training Loss: 0.7498..  Test Loss: 0.7382.. \n",
      "Epoch: 288/2000..  Training Loss: 0.7486..  Test Loss: 0.7371.. \n",
      "Epoch: 289/2000..  Training Loss: 0.7475..  Test Loss: 0.7361.. \n",
      "Epoch: 290/2000..  Training Loss: 0.7464..  Test Loss: 0.7350.. \n",
      "Epoch: 291/2000..  Training Loss: 0.7452..  Test Loss: 0.7340.. \n",
      "Epoch: 292/2000..  Training Loss: 0.7440..  Test Loss: 0.7329.. \n",
      "Epoch: 293/2000..  Training Loss: 0.7428..  Test Loss: 0.7318.. \n",
      "Epoch: 294/2000..  Training Loss: 0.7417..  Test Loss: 0.7307.. \n",
      "Epoch: 295/2000..  Training Loss: 0.7405..  Test Loss: 0.7296.. \n",
      "Epoch: 296/2000..  Training Loss: 0.7393..  Test Loss: 0.7285.. \n",
      "Epoch: 297/2000..  Training Loss: 0.7381..  Test Loss: 0.7273.. \n",
      "Epoch: 298/2000..  Training Loss: 0.7368..  Test Loss: 0.7262.. \n",
      "Epoch: 299/2000..  Training Loss: 0.7355..  Test Loss: 0.7250.. \n",
      "Epoch: 300/2000..  Training Loss: 0.7343..  Test Loss: 0.7238.. \n",
      "Epoch: 301/2000..  Training Loss: 0.7329..  Test Loss: 0.7226.. \n",
      "Epoch: 302/2000..  Training Loss: 0.7317..  Test Loss: 0.7214.. \n",
      "Epoch: 303/2000..  Training Loss: 0.7304..  Test Loss: 0.7201.. \n",
      "Epoch: 304/2000..  Training Loss: 0.7290..  Test Loss: 0.7189.. \n",
      "Epoch: 305/2000..  Training Loss: 0.7277..  Test Loss: 0.7176.. \n",
      "Epoch: 306/2000..  Training Loss: 0.7263..  Test Loss: 0.7163.. \n",
      "Epoch: 307/2000..  Training Loss: 0.7249..  Test Loss: 0.7150.. \n",
      "Epoch: 308/2000..  Training Loss: 0.7235..  Test Loss: 0.7137.. \n",
      "Epoch: 309/2000..  Training Loss: 0.7221..  Test Loss: 0.7123.. \n",
      "Epoch: 310/2000..  Training Loss: 0.7207..  Test Loss: 0.7110.. \n",
      "Epoch: 311/2000..  Training Loss: 0.7192..  Test Loss: 0.7096.. \n",
      "Epoch: 312/2000..  Training Loss: 0.7177..  Test Loss: 0.7082.. \n",
      "Epoch: 313/2000..  Training Loss: 0.7163..  Test Loss: 0.7068.. \n",
      "Epoch: 314/2000..  Training Loss: 0.7148..  Test Loss: 0.7054.. \n",
      "Epoch: 315/2000..  Training Loss: 0.7133..  Test Loss: 0.7039.. \n",
      "Epoch: 316/2000..  Training Loss: 0.7118..  Test Loss: 0.7025.. \n",
      "Epoch: 317/2000..  Training Loss: 0.7102..  Test Loss: 0.7010.. \n",
      "Epoch: 318/2000..  Training Loss: 0.7087..  Test Loss: 0.6996.. \n",
      "Epoch: 319/2000..  Training Loss: 0.7071..  Test Loss: 0.6981.. \n",
      "Epoch: 320/2000..  Training Loss: 0.7056..  Test Loss: 0.6966.. \n",
      "Epoch: 321/2000..  Training Loss: 0.7040..  Test Loss: 0.6951.. \n",
      "Epoch: 322/2000..  Training Loss: 0.7025..  Test Loss: 0.6936.. \n",
      "Epoch: 323/2000..  Training Loss: 0.7009..  Test Loss: 0.6920.. \n",
      "Epoch: 324/2000..  Training Loss: 0.6993..  Test Loss: 0.6905.. \n",
      "Epoch: 325/2000..  Training Loss: 0.6977..  Test Loss: 0.6889.. \n",
      "Epoch: 326/2000..  Training Loss: 0.6961..  Test Loss: 0.6874.. \n",
      "Epoch: 327/2000..  Training Loss: 0.6946..  Test Loss: 0.6858.. \n",
      "Epoch: 328/2000..  Training Loss: 0.6930..  Test Loss: 0.6843.. \n",
      "Epoch: 329/2000..  Training Loss: 0.6914..  Test Loss: 0.6828.. \n",
      "Epoch: 330/2000..  Training Loss: 0.6898..  Test Loss: 0.6812.. \n",
      "Epoch: 331/2000..  Training Loss: 0.6883..  Test Loss: 0.6797.. \n",
      "Epoch: 332/2000..  Training Loss: 0.6869..  Test Loss: 0.6781.. \n",
      "Epoch: 333/2000..  Training Loss: 0.6854..  Test Loss: 0.6766.. \n",
      "Epoch: 334/2000..  Training Loss: 0.6839..  Test Loss: 0.6751.. \n",
      "Epoch: 335/2000..  Training Loss: 0.6824..  Test Loss: 0.6736.. \n",
      "Epoch: 336/2000..  Training Loss: 0.6808..  Test Loss: 0.6721.. \n",
      "Epoch: 337/2000..  Training Loss: 0.6794..  Test Loss: 0.6706.. \n",
      "Epoch: 338/2000..  Training Loss: 0.6779..  Test Loss: 0.6691.. \n",
      "Epoch: 339/2000..  Training Loss: 0.6765..  Test Loss: 0.6676.. \n",
      "Epoch: 340/2000..  Training Loss: 0.6751..  Test Loss: 0.6662.. \n",
      "Epoch: 341/2000..  Training Loss: 0.6737..  Test Loss: 0.6647.. \n",
      "Epoch: 342/2000..  Training Loss: 0.6723..  Test Loss: 0.6633.. \n",
      "Epoch: 343/2000..  Training Loss: 0.6710..  Test Loss: 0.6619.. \n",
      "Epoch: 344/2000..  Training Loss: 0.6697..  Test Loss: 0.6605.. \n",
      "Epoch: 345/2000..  Training Loss: 0.6684..  Test Loss: 0.6591.. \n",
      "Epoch: 346/2000..  Training Loss: 0.6671..  Test Loss: 0.6577.. \n",
      "Epoch: 347/2000..  Training Loss: 0.6658..  Test Loss: 0.6564.. \n",
      "Epoch: 348/2000..  Training Loss: 0.6645..  Test Loss: 0.6550.. \n",
      "Epoch: 349/2000..  Training Loss: 0.6633..  Test Loss: 0.6537.. \n",
      "Epoch: 350/2000..  Training Loss: 0.6621..  Test Loss: 0.6524.. \n",
      "Epoch: 351/2000..  Training Loss: 0.6609..  Test Loss: 0.6511.. \n",
      "Epoch: 352/2000..  Training Loss: 0.6597..  Test Loss: 0.6499.. \n",
      "Epoch: 353/2000..  Training Loss: 0.6586..  Test Loss: 0.6486.. \n",
      "Epoch: 354/2000..  Training Loss: 0.6574..  Test Loss: 0.6474.. \n",
      "Epoch: 355/2000..  Training Loss: 0.6563..  Test Loss: 0.6461.. \n",
      "Epoch: 356/2000..  Training Loss: 0.6551..  Test Loss: 0.6449.. \n",
      "Epoch: 357/2000..  Training Loss: 0.6540..  Test Loss: 0.6437.. \n",
      "Epoch: 358/2000..  Training Loss: 0.6528..  Test Loss: 0.6426.. \n",
      "Epoch: 359/2000..  Training Loss: 0.6517..  Test Loss: 0.6414.. \n",
      "Epoch: 360/2000..  Training Loss: 0.6506..  Test Loss: 0.6403.. \n",
      "Epoch: 361/2000..  Training Loss: 0.6495..  Test Loss: 0.6392.. \n",
      "Epoch: 362/2000..  Training Loss: 0.6484..  Test Loss: 0.6381.. \n",
      "Epoch: 363/2000..  Training Loss: 0.6473..  Test Loss: 0.6370.. \n",
      "Epoch: 364/2000..  Training Loss: 0.6463..  Test Loss: 0.6360.. \n",
      "Epoch: 365/2000..  Training Loss: 0.6452..  Test Loss: 0.6349.. \n",
      "Epoch: 366/2000..  Training Loss: 0.6442..  Test Loss: 0.6339.. \n",
      "Epoch: 367/2000..  Training Loss: 0.6433..  Test Loss: 0.6329.. \n",
      "Epoch: 368/2000..  Training Loss: 0.6423..  Test Loss: 0.6319.. \n",
      "Epoch: 369/2000..  Training Loss: 0.6413..  Test Loss: 0.6309.. \n",
      "Epoch: 370/2000..  Training Loss: 0.6404..  Test Loss: 0.6299.. \n",
      "Epoch: 371/2000..  Training Loss: 0.6395..  Test Loss: 0.6290.. \n",
      "Epoch: 372/2000..  Training Loss: 0.6386..  Test Loss: 0.6281.. \n",
      "Epoch: 373/2000..  Training Loss: 0.6378..  Test Loss: 0.6271.. \n",
      "Epoch: 374/2000..  Training Loss: 0.6369..  Test Loss: 0.6262.. \n",
      "Epoch: 375/2000..  Training Loss: 0.6360..  Test Loss: 0.6253.. \n",
      "Epoch: 376/2000..  Training Loss: 0.6352..  Test Loss: 0.6244.. \n",
      "Epoch: 377/2000..  Training Loss: 0.6344..  Test Loss: 0.6236.. \n",
      "Epoch: 378/2000..  Training Loss: 0.6335..  Test Loss: 0.6227.. \n",
      "Epoch: 379/2000..  Training Loss: 0.6327..  Test Loss: 0.6218.. \n",
      "Epoch: 380/2000..  Training Loss: 0.6319..  Test Loss: 0.6210.. \n",
      "Epoch: 381/2000..  Training Loss: 0.6311..  Test Loss: 0.6202.. \n",
      "Epoch: 382/2000..  Training Loss: 0.6304..  Test Loss: 0.6194.. \n",
      "Epoch: 383/2000..  Training Loss: 0.6296..  Test Loss: 0.6185.. \n",
      "Epoch: 384/2000..  Training Loss: 0.6289..  Test Loss: 0.6178.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 385/2000..  Training Loss: 0.6281..  Test Loss: 0.6170.. \n",
      "Epoch: 386/2000..  Training Loss: 0.6274..  Test Loss: 0.6162.. \n",
      "Epoch: 387/2000..  Training Loss: 0.6266..  Test Loss: 0.6154.. \n",
      "Epoch: 388/2000..  Training Loss: 0.6259..  Test Loss: 0.6146.. \n",
      "Epoch: 389/2000..  Training Loss: 0.6251..  Test Loss: 0.6138.. \n",
      "Epoch: 390/2000..  Training Loss: 0.6244..  Test Loss: 0.6131.. \n",
      "Epoch: 391/2000..  Training Loss: 0.6237..  Test Loss: 0.6123.. \n",
      "Epoch: 392/2000..  Training Loss: 0.6230..  Test Loss: 0.6116.. \n",
      "Epoch: 393/2000..  Training Loss: 0.6222..  Test Loss: 0.6108.. \n",
      "Epoch: 394/2000..  Training Loss: 0.6215..  Test Loss: 0.6101.. \n",
      "Epoch: 395/2000..  Training Loss: 0.6208..  Test Loss: 0.6093.. \n",
      "Epoch: 396/2000..  Training Loss: 0.6201..  Test Loss: 0.6086.. \n",
      "Epoch: 397/2000..  Training Loss: 0.6193..  Test Loss: 0.6079.. \n",
      "Epoch: 398/2000..  Training Loss: 0.6186..  Test Loss: 0.6071.. \n",
      "Epoch: 399/2000..  Training Loss: 0.6180..  Test Loss: 0.6064.. \n",
      "Epoch: 400/2000..  Training Loss: 0.6172..  Test Loss: 0.6056.. \n",
      "Epoch: 401/2000..  Training Loss: 0.6165..  Test Loss: 0.6049.. \n",
      "Epoch: 402/2000..  Training Loss: 0.6158..  Test Loss: 0.6042.. \n",
      "Epoch: 403/2000..  Training Loss: 0.6151..  Test Loss: 0.6034.. \n",
      "Epoch: 404/2000..  Training Loss: 0.6144..  Test Loss: 0.6027.. \n",
      "Epoch: 405/2000..  Training Loss: 0.6136..  Test Loss: 0.6019.. \n",
      "Epoch: 406/2000..  Training Loss: 0.6128..  Test Loss: 0.6012.. \n",
      "Epoch: 407/2000..  Training Loss: 0.6121..  Test Loss: 0.6004.. \n",
      "Epoch: 408/2000..  Training Loss: 0.6113..  Test Loss: 0.5997.. \n",
      "Epoch: 409/2000..  Training Loss: 0.6106..  Test Loss: 0.5989.. \n",
      "Epoch: 410/2000..  Training Loss: 0.6100..  Test Loss: 0.5982.. \n",
      "Epoch: 411/2000..  Training Loss: 0.6093..  Test Loss: 0.5975.. \n",
      "Epoch: 412/2000..  Training Loss: 0.6087..  Test Loss: 0.5967.. \n",
      "Epoch: 413/2000..  Training Loss: 0.6080..  Test Loss: 0.5960.. \n",
      "Epoch: 414/2000..  Training Loss: 0.6073..  Test Loss: 0.5953.. \n",
      "Epoch: 415/2000..  Training Loss: 0.6067..  Test Loss: 0.5946.. \n",
      "Epoch: 416/2000..  Training Loss: 0.6059..  Test Loss: 0.5938.. \n",
      "Epoch: 417/2000..  Training Loss: 0.6053..  Test Loss: 0.5931.. \n",
      "Epoch: 418/2000..  Training Loss: 0.6046..  Test Loss: 0.5924.. \n",
      "Epoch: 419/2000..  Training Loss: 0.6039..  Test Loss: 0.5918.. \n",
      "Epoch: 420/2000..  Training Loss: 0.6032..  Test Loss: 0.5911.. \n",
      "Epoch: 421/2000..  Training Loss: 0.6025..  Test Loss: 0.5904.. \n",
      "Epoch: 422/2000..  Training Loss: 0.6018..  Test Loss: 0.5898.. \n",
      "Epoch: 423/2000..  Training Loss: 0.6011..  Test Loss: 0.5892.. \n",
      "Epoch: 424/2000..  Training Loss: 0.6004..  Test Loss: 0.5887.. \n",
      "Epoch: 425/2000..  Training Loss: 0.5996..  Test Loss: 0.5883.. \n",
      "Epoch: 426/2000..  Training Loss: 0.5990..  Test Loss: nan.. \n",
      "Epoch: 427/2000..  Training Loss: 0.5983..  Test Loss: nan.. \n",
      "Epoch: 428/2000..  Training Loss: 0.5976..  Test Loss: nan.. \n",
      "Epoch: 429/2000..  Training Loss: 0.5969..  Test Loss: nan.. \n",
      "Epoch: 430/2000..  Training Loss: 0.5962..  Test Loss: nan.. \n",
      "Epoch: 431/2000..  Training Loss: 0.5955..  Test Loss: nan.. \n",
      "Epoch: 432/2000..  Training Loss: 0.5949..  Test Loss: nan.. \n",
      "Epoch: 433/2000..  Training Loss: 0.5942..  Test Loss: nan.. \n",
      "Epoch: 434/2000..  Training Loss: 0.5936..  Test Loss: nan.. \n",
      "Epoch: 435/2000..  Training Loss: 0.5929..  Test Loss: nan.. \n",
      "Epoch: 436/2000..  Training Loss: 0.5922..  Test Loss: nan.. \n",
      "Epoch: 437/2000..  Training Loss: 0.5914..  Test Loss: nan.. \n",
      "Epoch: 438/2000..  Training Loss: 0.5908..  Test Loss: nan.. \n",
      "Epoch: 439/2000..  Training Loss: 0.5900..  Test Loss: nan.. \n",
      "Epoch: 440/2000..  Training Loss: 0.5894..  Test Loss: nan.. \n",
      "Epoch: 441/2000..  Training Loss: 0.5887..  Test Loss: nan.. \n",
      "Epoch: 442/2000..  Training Loss: 0.5879..  Test Loss: nan.. \n",
      "Epoch: 443/2000..  Training Loss: 0.5871..  Test Loss: nan.. \n",
      "Epoch: 444/2000..  Training Loss: 0.5864..  Test Loss: nan.. \n",
      "Epoch: 445/2000..  Training Loss: 0.5856..  Test Loss: nan.. \n",
      "Epoch: 446/2000..  Training Loss: 0.5849..  Test Loss: nan.. \n",
      "Epoch: 447/2000..  Training Loss: 0.5842..  Test Loss: nan.. \n",
      "Epoch: 448/2000..  Training Loss: 0.5835..  Test Loss: nan.. \n",
      "Epoch: 449/2000..  Training Loss: 0.5827..  Test Loss: nan.. \n",
      "Epoch: 450/2000..  Training Loss: 0.5821..  Test Loss: nan.. \n",
      "Epoch: 451/2000..  Training Loss: 0.5814..  Test Loss: nan.. \n",
      "Epoch: 452/2000..  Training Loss: 0.5807..  Test Loss: nan.. \n",
      "Epoch: 453/2000..  Training Loss: 0.5800..  Test Loss: nan.. \n",
      "Epoch: 454/2000..  Training Loss: 0.5794..  Test Loss: nan.. \n",
      "Epoch: 455/2000..  Training Loss: 0.5787..  Test Loss: nan.. \n",
      "Epoch: 456/2000..  Training Loss: 0.5779..  Test Loss: nan.. \n",
      "Epoch: 457/2000..  Training Loss: 0.5772..  Test Loss: nan.. \n",
      "Epoch: 458/2000..  Training Loss: 0.5765..  Test Loss: nan.. \n",
      "Epoch: 459/2000..  Training Loss: 0.5758..  Test Loss: nan.. \n",
      "Epoch: 460/2000..  Training Loss: 0.5750..  Test Loss: nan.. \n",
      "Epoch: 461/2000..  Training Loss: 0.5742..  Test Loss: nan.. \n",
      "Epoch: 462/2000..  Training Loss: 0.5734..  Test Loss: nan.. \n",
      "Epoch: 463/2000..  Training Loss: 0.5727..  Test Loss: nan.. \n",
      "Epoch: 464/2000..  Training Loss: 0.5719..  Test Loss: nan.. \n",
      "Epoch: 465/2000..  Training Loss: 0.5711..  Test Loss: nan.. \n",
      "Epoch: 466/2000..  Training Loss: 0.5704..  Test Loss: nan.. \n",
      "Epoch: 467/2000..  Training Loss: 0.5696..  Test Loss: nan.. \n",
      "Epoch: 468/2000..  Training Loss: 0.5688..  Test Loss: nan.. \n",
      "Epoch: 469/2000..  Training Loss: 0.5681..  Test Loss: nan.. \n",
      "Epoch: 470/2000..  Training Loss: 0.5673..  Test Loss: nan.. \n",
      "Epoch: 471/2000..  Training Loss: 0.5665..  Test Loss: nan.. \n",
      "Epoch: 472/2000..  Training Loss: 0.5657..  Test Loss: nan.. \n",
      "Epoch: 473/2000..  Training Loss: 0.5649..  Test Loss: nan.. \n",
      "Epoch: 474/2000..  Training Loss: 0.5641..  Test Loss: nan.. \n",
      "Epoch: 475/2000..  Training Loss: 0.5633..  Test Loss: nan.. \n",
      "Epoch: 476/2000..  Training Loss: 0.5626..  Test Loss: nan.. \n",
      "Epoch: 477/2000..  Training Loss: 0.5619..  Test Loss: nan.. \n",
      "Epoch: 478/2000..  Training Loss: 0.5611..  Test Loss: nan.. \n",
      "Epoch: 479/2000..  Training Loss: 0.5604..  Test Loss: nan.. \n",
      "Epoch: 480/2000..  Training Loss: 0.5596..  Test Loss: nan.. \n",
      "Epoch: 481/2000..  Training Loss: 0.5589..  Test Loss: nan.. \n",
      "Epoch: 482/2000..  Training Loss: 0.5582..  Test Loss: nan.. \n",
      "Epoch: 483/2000..  Training Loss: 0.5574..  Test Loss: nan.. \n",
      "Epoch: 484/2000..  Training Loss: 0.5566..  Test Loss: nan.. \n",
      "Epoch: 485/2000..  Training Loss: 0.5558..  Test Loss: nan.. \n",
      "Epoch: 486/2000..  Training Loss: 0.5551..  Test Loss: nan.. \n",
      "Epoch: 487/2000..  Training Loss: 0.5542..  Test Loss: nan.. \n",
      "Epoch: 488/2000..  Training Loss: 0.5535..  Test Loss: nan.. \n",
      "Epoch: 489/2000..  Training Loss: 0.5528..  Test Loss: nan.. \n",
      "Epoch: 490/2000..  Training Loss: 0.5520..  Test Loss: nan.. \n",
      "Epoch: 491/2000..  Training Loss: 0.5513..  Test Loss: nan.. \n",
      "Epoch: 492/2000..  Training Loss: 0.5505..  Test Loss: nan.. \n",
      "Epoch: 493/2000..  Training Loss: 0.5498..  Test Loss: nan.. \n",
      "Epoch: 494/2000..  Training Loss: 0.5490..  Test Loss: nan.. \n",
      "Epoch: 495/2000..  Training Loss: 0.5483..  Test Loss: nan.. \n",
      "Epoch: 496/2000..  Training Loss: 0.5475..  Test Loss: nan.. \n",
      "Epoch: 497/2000..  Training Loss: 0.5468..  Test Loss: nan.. \n",
      "Epoch: 498/2000..  Training Loss: 0.5461..  Test Loss: nan.. \n",
      "Epoch: 499/2000..  Training Loss: 0.5453..  Test Loss: nan.. \n",
      "Epoch: 500/2000..  Training Loss: 0.5447..  Test Loss: nan.. \n",
      "Epoch: 501/2000..  Training Loss: 0.5440..  Test Loss: nan.. \n",
      "Epoch: 502/2000..  Training Loss: 0.5432..  Test Loss: nan.. \n",
      "Epoch: 503/2000..  Training Loss: 0.5426..  Test Loss: nan.. \n",
      "Epoch: 504/2000..  Training Loss: 0.5418..  Test Loss: nan.. \n",
      "Epoch: 505/2000..  Training Loss: 0.5411..  Test Loss: nan.. \n",
      "Epoch: 506/2000..  Training Loss: 0.5404..  Test Loss: nan.. \n",
      "Epoch: 507/2000..  Training Loss: 0.5396..  Test Loss: nan.. \n",
      "Epoch: 508/2000..  Training Loss: 0.5390..  Test Loss: nan.. \n",
      "Epoch: 509/2000..  Training Loss: 0.5383..  Test Loss: nan.. \n",
      "Epoch: 510/2000..  Training Loss: 0.5375..  Test Loss: nan.. \n",
      "Epoch: 511/2000..  Training Loss: 0.5369..  Test Loss: nan.. \n",
      "Epoch: 512/2000..  Training Loss: 0.5362..  Test Loss: nan.. \n",
      "Epoch: 513/2000..  Training Loss: 0.5354..  Test Loss: nan.. \n",
      "Epoch: 514/2000..  Training Loss: 0.5347..  Test Loss: nan.. \n",
      "Epoch: 515/2000..  Training Loss: 0.5340..  Test Loss: nan.. \n",
      "Epoch: 516/2000..  Training Loss: 0.5334..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 517/2000..  Training Loss: 0.5326..  Test Loss: nan.. \n",
      "Epoch: 518/2000..  Training Loss: 0.5320..  Test Loss: nan.. \n",
      "Epoch: 519/2000..  Training Loss: 0.5313..  Test Loss: nan.. \n",
      "Epoch: 520/2000..  Training Loss: 0.5306..  Test Loss: nan.. \n",
      "Epoch: 521/2000..  Training Loss: 0.5299..  Test Loss: nan.. \n",
      "Epoch: 522/2000..  Training Loss: 0.5291..  Test Loss: nan.. \n",
      "Epoch: 523/2000..  Training Loss: 0.5284..  Test Loss: nan.. \n",
      "Epoch: 524/2000..  Training Loss: 0.5277..  Test Loss: nan.. \n",
      "Epoch: 525/2000..  Training Loss: 0.5270..  Test Loss: nan.. \n",
      "Epoch: 526/2000..  Training Loss: 0.5262..  Test Loss: nan.. \n",
      "Epoch: 527/2000..  Training Loss: 0.5256..  Test Loss: nan.. \n",
      "Epoch: 528/2000..  Training Loss: 0.5249..  Test Loss: nan.. \n",
      "Epoch: 529/2000..  Training Loss: 0.5242..  Test Loss: nan.. \n",
      "Epoch: 530/2000..  Training Loss: 0.5236..  Test Loss: nan.. \n",
      "Epoch: 531/2000..  Training Loss: 0.5230..  Test Loss: nan.. \n",
      "Epoch: 532/2000..  Training Loss: 0.5224..  Test Loss: nan.. \n",
      "Epoch: 533/2000..  Training Loss: 0.5216..  Test Loss: nan.. \n",
      "Epoch: 534/2000..  Training Loss: 0.5210..  Test Loss: nan.. \n",
      "Epoch: 535/2000..  Training Loss: 0.5204..  Test Loss: nan.. \n",
      "Epoch: 536/2000..  Training Loss: 0.5197..  Test Loss: nan.. \n",
      "Epoch: 537/2000..  Training Loss: 0.5190..  Test Loss: nan.. \n",
      "Epoch: 538/2000..  Training Loss: 0.5184..  Test Loss: nan.. \n",
      "Epoch: 539/2000..  Training Loss: 0.5177..  Test Loss: nan.. \n",
      "Epoch: 540/2000..  Training Loss: 0.5171..  Test Loss: nan.. \n",
      "Epoch: 541/2000..  Training Loss: 0.5165..  Test Loss: nan.. \n",
      "Epoch: 542/2000..  Training Loss: 0.5157..  Test Loss: nan.. \n",
      "Epoch: 543/2000..  Training Loss: 0.5151..  Test Loss: nan.. \n",
      "Epoch: 544/2000..  Training Loss: 0.5145..  Test Loss: nan.. \n",
      "Epoch: 545/2000..  Training Loss: 0.5138..  Test Loss: nan.. \n",
      "Epoch: 546/2000..  Training Loss: 0.5132..  Test Loss: nan.. \n",
      "Epoch: 547/2000..  Training Loss: 0.5125..  Test Loss: nan.. \n",
      "Epoch: 548/2000..  Training Loss: 0.5118..  Test Loss: nan.. \n",
      "Epoch: 549/2000..  Training Loss: 0.5112..  Test Loss: nan.. \n",
      "Epoch: 550/2000..  Training Loss: 0.5105..  Test Loss: nan.. \n",
      "Epoch: 551/2000..  Training Loss: 0.5099..  Test Loss: nan.. \n",
      "Epoch: 552/2000..  Training Loss: 0.5092..  Test Loss: nan.. \n",
      "Epoch: 553/2000..  Training Loss: 0.5086..  Test Loss: nan.. \n",
      "Epoch: 554/2000..  Training Loss: 0.5080..  Test Loss: nan.. \n",
      "Epoch: 555/2000..  Training Loss: 0.5074..  Test Loss: nan.. \n",
      "Epoch: 556/2000..  Training Loss: 0.5067..  Test Loss: nan.. \n",
      "Epoch: 557/2000..  Training Loss: 0.5061..  Test Loss: nan.. \n",
      "Epoch: 558/2000..  Training Loss: 0.5054..  Test Loss: nan.. \n",
      "Epoch: 559/2000..  Training Loss: 0.5048..  Test Loss: nan.. \n",
      "Epoch: 560/2000..  Training Loss: 0.5043..  Test Loss: nan.. \n",
      "Epoch: 561/2000..  Training Loss: 0.5037..  Test Loss: nan.. \n",
      "Epoch: 562/2000..  Training Loss: 0.5030..  Test Loss: nan.. \n",
      "Epoch: 563/2000..  Training Loss: 0.5023..  Test Loss: nan.. \n",
      "Epoch: 564/2000..  Training Loss: 0.5017..  Test Loss: nan.. \n",
      "Epoch: 565/2000..  Training Loss: 0.5012..  Test Loss: nan.. \n",
      "Epoch: 566/2000..  Training Loss: 0.5006..  Test Loss: nan.. \n",
      "Epoch: 567/2000..  Training Loss: 0.5000..  Test Loss: nan.. \n",
      "Epoch: 568/2000..  Training Loss: 0.4993..  Test Loss: nan.. \n",
      "Epoch: 569/2000..  Training Loss: 0.4988..  Test Loss: nan.. \n",
      "Epoch: 570/2000..  Training Loss: 0.4982..  Test Loss: nan.. \n",
      "Epoch: 571/2000..  Training Loss: 0.4975..  Test Loss: nan.. \n",
      "Epoch: 572/2000..  Training Loss: 0.4969..  Test Loss: nan.. \n",
      "Epoch: 573/2000..  Training Loss: 0.4964..  Test Loss: nan.. \n",
      "Epoch: 574/2000..  Training Loss: 0.4958..  Test Loss: nan.. \n",
      "Epoch: 575/2000..  Training Loss: 0.4952..  Test Loss: nan.. \n",
      "Epoch: 576/2000..  Training Loss: 0.4946..  Test Loss: nan.. \n",
      "Epoch: 577/2000..  Training Loss: 0.4940..  Test Loss: nan.. \n",
      "Epoch: 578/2000..  Training Loss: 0.4934..  Test Loss: nan.. \n",
      "Epoch: 579/2000..  Training Loss: 0.4928..  Test Loss: nan.. \n",
      "Epoch: 580/2000..  Training Loss: 0.4923..  Test Loss: nan.. \n",
      "Epoch: 581/2000..  Training Loss: 0.4918..  Test Loss: nan.. \n",
      "Epoch: 582/2000..  Training Loss: 0.4913..  Test Loss: nan.. \n",
      "Epoch: 583/2000..  Training Loss: 0.4907..  Test Loss: nan.. \n",
      "Epoch: 584/2000..  Training Loss: 0.4902..  Test Loss: nan.. \n",
      "Epoch: 585/2000..  Training Loss: 0.4896..  Test Loss: nan.. \n",
      "Epoch: 586/2000..  Training Loss: 0.4891..  Test Loss: nan.. \n",
      "Epoch: 587/2000..  Training Loss: 0.4886..  Test Loss: nan.. \n",
      "Epoch: 588/2000..  Training Loss: 0.4880..  Test Loss: nan.. \n",
      "Epoch: 589/2000..  Training Loss: 0.4875..  Test Loss: nan.. \n",
      "Epoch: 590/2000..  Training Loss: 0.4869..  Test Loss: nan.. \n",
      "Epoch: 591/2000..  Training Loss: 0.4864..  Test Loss: nan.. \n",
      "Epoch: 592/2000..  Training Loss: 0.4859..  Test Loss: nan.. \n",
      "Epoch: 593/2000..  Training Loss: 0.4855..  Test Loss: nan.. \n",
      "Epoch: 594/2000..  Training Loss: 0.4849..  Test Loss: nan.. \n",
      "Epoch: 595/2000..  Training Loss: 0.4844..  Test Loss: nan.. \n",
      "Epoch: 596/2000..  Training Loss: 0.4837..  Test Loss: nan.. \n",
      "Epoch: 597/2000..  Training Loss: 0.4832..  Test Loss: nan.. \n",
      "Epoch: 598/2000..  Training Loss: 0.4826..  Test Loss: nan.. \n",
      "Epoch: 599/2000..  Training Loss: 0.4821..  Test Loss: nan.. \n",
      "Epoch: 600/2000..  Training Loss: 0.4816..  Test Loss: nan.. \n",
      "Epoch: 601/2000..  Training Loss: 0.4811..  Test Loss: nan.. \n",
      "Epoch: 602/2000..  Training Loss: 0.4806..  Test Loss: nan.. \n",
      "Epoch: 603/2000..  Training Loss: 0.4800..  Test Loss: nan.. \n",
      "Epoch: 604/2000..  Training Loss: 0.4795..  Test Loss: nan.. \n",
      "Epoch: 605/2000..  Training Loss: 0.4790..  Test Loss: nan.. \n",
      "Epoch: 606/2000..  Training Loss: 0.4785..  Test Loss: nan.. \n",
      "Epoch: 607/2000..  Training Loss: 0.4780..  Test Loss: nan.. \n",
      "Epoch: 608/2000..  Training Loss: 0.4774..  Test Loss: nan.. \n",
      "Epoch: 609/2000..  Training Loss: 0.4769..  Test Loss: nan.. \n",
      "Epoch: 610/2000..  Training Loss: 0.4764..  Test Loss: nan.. \n",
      "Epoch: 611/2000..  Training Loss: 0.4759..  Test Loss: nan.. \n",
      "Epoch: 612/2000..  Training Loss: 0.4754..  Test Loss: nan.. \n",
      "Epoch: 613/2000..  Training Loss: 0.4749..  Test Loss: nan.. \n",
      "Epoch: 614/2000..  Training Loss: 0.4744..  Test Loss: nan.. \n",
      "Epoch: 615/2000..  Training Loss: 0.4739..  Test Loss: nan.. \n",
      "Epoch: 616/2000..  Training Loss: 0.4734..  Test Loss: nan.. \n",
      "Epoch: 617/2000..  Training Loss: 0.4729..  Test Loss: nan.. \n",
      "Epoch: 618/2000..  Training Loss: 0.4723..  Test Loss: nan.. \n",
      "Epoch: 619/2000..  Training Loss: 0.4719..  Test Loss: nan.. \n",
      "Epoch: 620/2000..  Training Loss: 0.4714..  Test Loss: nan.. \n",
      "Epoch: 621/2000..  Training Loss: 0.4708..  Test Loss: nan.. \n",
      "Epoch: 622/2000..  Training Loss: 0.4704..  Test Loss: nan.. \n",
      "Epoch: 623/2000..  Training Loss: 0.4698..  Test Loss: nan.. \n",
      "Epoch: 624/2000..  Training Loss: 0.4694..  Test Loss: nan.. \n",
      "Epoch: 625/2000..  Training Loss: 0.4688..  Test Loss: nan.. \n",
      "Epoch: 626/2000..  Training Loss: 0.4684..  Test Loss: nan.. \n",
      "Epoch: 627/2000..  Training Loss: 0.4679..  Test Loss: nan.. \n",
      "Epoch: 628/2000..  Training Loss: 0.4675..  Test Loss: nan.. \n",
      "Epoch: 629/2000..  Training Loss: 0.4671..  Test Loss: nan.. \n",
      "Epoch: 630/2000..  Training Loss: 0.4666..  Test Loss: nan.. \n",
      "Epoch: 631/2000..  Training Loss: 0.4661..  Test Loss: nan.. \n",
      "Epoch: 632/2000..  Training Loss: 0.4656..  Test Loss: nan.. \n",
      "Epoch: 633/2000..  Training Loss: 0.4653..  Test Loss: nan.. \n",
      "Epoch: 634/2000..  Training Loss: 0.4647..  Test Loss: nan.. \n",
      "Epoch: 635/2000..  Training Loss: 0.4644..  Test Loss: nan.. \n",
      "Epoch: 636/2000..  Training Loss: 0.4639..  Test Loss: nan.. \n",
      "Epoch: 637/2000..  Training Loss: 0.4633..  Test Loss: nan.. \n",
      "Epoch: 638/2000..  Training Loss: 0.4628..  Test Loss: nan.. \n",
      "Epoch: 639/2000..  Training Loss: 0.4624..  Test Loss: nan.. \n",
      "Epoch: 640/2000..  Training Loss: 0.4619..  Test Loss: nan.. \n",
      "Epoch: 641/2000..  Training Loss: 0.4616..  Test Loss: nan.. \n",
      "Epoch: 642/2000..  Training Loss: 0.4611..  Test Loss: nan.. \n",
      "Epoch: 643/2000..  Training Loss: 0.4608..  Test Loss: nan.. \n",
      "Epoch: 644/2000..  Training Loss: 0.4602..  Test Loss: nan.. \n",
      "Epoch: 645/2000..  Training Loss: 0.4598..  Test Loss: nan.. \n",
      "Epoch: 646/2000..  Training Loss: 0.4593..  Test Loss: nan.. \n",
      "Epoch: 647/2000..  Training Loss: 0.4590..  Test Loss: nan.. \n",
      "Epoch: 648/2000..  Training Loss: 0.4585..  Test Loss: nan.. \n",
      "Epoch: 649/2000..  Training Loss: 0.4580..  Test Loss: nan.. \n",
      "Epoch: 650/2000..  Training Loss: 0.4576..  Test Loss: nan.. \n",
      "Epoch: 651/2000..  Training Loss: 0.4572..  Test Loss: nan.. \n",
      "Epoch: 652/2000..  Training Loss: 0.4567..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 653/2000..  Training Loss: 0.4563..  Test Loss: nan.. \n",
      "Epoch: 654/2000..  Training Loss: 0.4558..  Test Loss: nan.. \n",
      "Epoch: 655/2000..  Training Loss: 0.4554..  Test Loss: nan.. \n",
      "Epoch: 656/2000..  Training Loss: 0.4550..  Test Loss: nan.. \n",
      "Epoch: 657/2000..  Training Loss: 0.4544..  Test Loss: nan.. \n",
      "Epoch: 658/2000..  Training Loss: 0.4540..  Test Loss: nan.. \n",
      "Epoch: 659/2000..  Training Loss: 0.4535..  Test Loss: nan.. \n",
      "Epoch: 660/2000..  Training Loss: 0.4531..  Test Loss: nan.. \n",
      "Epoch: 661/2000..  Training Loss: 0.4527..  Test Loss: nan.. \n",
      "Epoch: 662/2000..  Training Loss: 0.4523..  Test Loss: nan.. \n",
      "Epoch: 663/2000..  Training Loss: 0.4518..  Test Loss: nan.. \n",
      "Epoch: 664/2000..  Training Loss: 0.4513..  Test Loss: nan.. \n",
      "Epoch: 665/2000..  Training Loss: 0.4509..  Test Loss: nan.. \n",
      "Epoch: 666/2000..  Training Loss: 0.4504..  Test Loss: nan.. \n",
      "Epoch: 667/2000..  Training Loss: 0.4500..  Test Loss: nan.. \n",
      "Epoch: 668/2000..  Training Loss: 0.4496..  Test Loss: nan.. \n",
      "Epoch: 669/2000..  Training Loss: 0.4492..  Test Loss: nan.. \n",
      "Epoch: 670/2000..  Training Loss: 0.4488..  Test Loss: nan.. \n",
      "Epoch: 671/2000..  Training Loss: 0.4484..  Test Loss: nan.. \n",
      "Epoch: 672/2000..  Training Loss: 0.4480..  Test Loss: nan.. \n",
      "Epoch: 673/2000..  Training Loss: 0.4476..  Test Loss: nan.. \n",
      "Epoch: 674/2000..  Training Loss: 0.4472..  Test Loss: nan.. \n",
      "Epoch: 675/2000..  Training Loss: 0.4468..  Test Loss: nan.. \n",
      "Epoch: 676/2000..  Training Loss: 0.4463..  Test Loss: nan.. \n",
      "Epoch: 677/2000..  Training Loss: 0.4459..  Test Loss: nan.. \n",
      "Epoch: 678/2000..  Training Loss: 0.4456..  Test Loss: nan.. \n",
      "Epoch: 679/2000..  Training Loss: 0.4452..  Test Loss: nan.. \n",
      "Epoch: 680/2000..  Training Loss: 0.4448..  Test Loss: nan.. \n",
      "Epoch: 681/2000..  Training Loss: 0.4445..  Test Loss: nan.. \n",
      "Epoch: 682/2000..  Training Loss: 0.4440..  Test Loss: nan.. \n",
      "Epoch: 683/2000..  Training Loss: 0.4436..  Test Loss: nan.. \n",
      "Epoch: 684/2000..  Training Loss: 0.4433..  Test Loss: nan.. \n",
      "Epoch: 685/2000..  Training Loss: 0.4429..  Test Loss: nan.. \n",
      "Epoch: 686/2000..  Training Loss: 0.4424..  Test Loss: nan.. \n",
      "Epoch: 687/2000..  Training Loss: 0.4421..  Test Loss: nan.. \n",
      "Epoch: 688/2000..  Training Loss: 0.4418..  Test Loss: nan.. \n",
      "Epoch: 689/2000..  Training Loss: 0.4414..  Test Loss: nan.. \n",
      "Epoch: 690/2000..  Training Loss: 0.4410..  Test Loss: nan.. \n",
      "Epoch: 691/2000..  Training Loss: 0.4406..  Test Loss: nan.. \n",
      "Epoch: 692/2000..  Training Loss: 0.4403..  Test Loss: nan.. \n",
      "Epoch: 693/2000..  Training Loss: 0.4399..  Test Loss: nan.. \n",
      "Epoch: 694/2000..  Training Loss: 0.4396..  Test Loss: nan.. \n",
      "Epoch: 695/2000..  Training Loss: 0.4392..  Test Loss: nan.. \n",
      "Epoch: 696/2000..  Training Loss: 0.4389..  Test Loss: nan.. \n",
      "Epoch: 697/2000..  Training Loss: 0.4386..  Test Loss: nan.. \n",
      "Epoch: 698/2000..  Training Loss: 0.4381..  Test Loss: nan.. \n",
      "Epoch: 699/2000..  Training Loss: 0.4379..  Test Loss: nan.. \n",
      "Epoch: 700/2000..  Training Loss: 0.4375..  Test Loss: nan.. \n",
      "Epoch: 701/2000..  Training Loss: 0.4371..  Test Loss: nan.. \n",
      "Epoch: 702/2000..  Training Loss: 0.4367..  Test Loss: nan.. \n",
      "Epoch: 703/2000..  Training Loss: 0.4363..  Test Loss: nan.. \n",
      "Epoch: 704/2000..  Training Loss: 0.4359..  Test Loss: nan.. \n",
      "Epoch: 705/2000..  Training Loss: 0.4356..  Test Loss: nan.. \n",
      "Epoch: 706/2000..  Training Loss: 0.4352..  Test Loss: nan.. \n",
      "Epoch: 707/2000..  Training Loss: 0.4349..  Test Loss: nan.. \n",
      "Epoch: 708/2000..  Training Loss: 0.4345..  Test Loss: nan.. \n",
      "Epoch: 709/2000..  Training Loss: 0.4342..  Test Loss: nan.. \n",
      "Epoch: 710/2000..  Training Loss: 0.4337..  Test Loss: nan.. \n",
      "Epoch: 711/2000..  Training Loss: 0.4334..  Test Loss: nan.. \n",
      "Epoch: 712/2000..  Training Loss: 0.4331..  Test Loss: nan.. \n",
      "Epoch: 713/2000..  Training Loss: 0.4327..  Test Loss: nan.. \n",
      "Epoch: 714/2000..  Training Loss: 0.4325..  Test Loss: nan.. \n",
      "Epoch: 715/2000..  Training Loss: 0.4322..  Test Loss: nan.. \n",
      "Epoch: 716/2000..  Training Loss: 0.4318..  Test Loss: nan.. \n",
      "Epoch: 717/2000..  Training Loss: 0.4315..  Test Loss: nan.. \n",
      "Epoch: 718/2000..  Training Loss: 0.4311..  Test Loss: nan.. \n",
      "Epoch: 719/2000..  Training Loss: 0.4306..  Test Loss: nan.. \n",
      "Epoch: 720/2000..  Training Loss: 0.4301..  Test Loss: nan.. \n",
      "Epoch: 721/2000..  Training Loss: 0.4299..  Test Loss: nan.. \n",
      "Epoch: 722/2000..  Training Loss: 0.4296..  Test Loss: nan.. \n",
      "Epoch: 723/2000..  Training Loss: 0.4292..  Test Loss: nan.. \n",
      "Epoch: 724/2000..  Training Loss: 0.4288..  Test Loss: nan.. \n",
      "Epoch: 725/2000..  Training Loss: 0.4286..  Test Loss: nan.. \n",
      "Epoch: 726/2000..  Training Loss: 0.4282..  Test Loss: nan.. \n",
      "Epoch: 727/2000..  Training Loss: 0.4279..  Test Loss: nan.. \n",
      "Epoch: 728/2000..  Training Loss: 0.4275..  Test Loss: nan.. \n",
      "Epoch: 729/2000..  Training Loss: 0.4271..  Test Loss: nan.. \n",
      "Epoch: 730/2000..  Training Loss: 0.4267..  Test Loss: nan.. \n",
      "Epoch: 731/2000..  Training Loss: 0.4263..  Test Loss: nan.. \n",
      "Epoch: 732/2000..  Training Loss: 0.4261..  Test Loss: nan.. \n",
      "Epoch: 733/2000..  Training Loss: 0.4257..  Test Loss: nan.. \n",
      "Epoch: 734/2000..  Training Loss: 0.4255..  Test Loss: nan.. \n",
      "Epoch: 735/2000..  Training Loss: 0.4251..  Test Loss: nan.. \n",
      "Epoch: 736/2000..  Training Loss: 0.4248..  Test Loss: nan.. \n",
      "Epoch: 737/2000..  Training Loss: 0.4244..  Test Loss: nan.. \n",
      "Epoch: 738/2000..  Training Loss: 0.4240..  Test Loss: nan.. \n",
      "Epoch: 739/2000..  Training Loss: 0.4237..  Test Loss: nan.. \n",
      "Epoch: 740/2000..  Training Loss: 0.4235..  Test Loss: nan.. \n",
      "Epoch: 741/2000..  Training Loss: 0.4231..  Test Loss: nan.. \n",
      "Epoch: 742/2000..  Training Loss: 0.4228..  Test Loss: nan.. \n",
      "Epoch: 743/2000..  Training Loss: 0.4225..  Test Loss: nan.. \n",
      "Epoch: 744/2000..  Training Loss: 0.4221..  Test Loss: nan.. \n",
      "Epoch: 745/2000..  Training Loss: 0.4217..  Test Loss: nan.. \n",
      "Epoch: 746/2000..  Training Loss: 0.4215..  Test Loss: nan.. \n",
      "Epoch: 747/2000..  Training Loss: 0.4212..  Test Loss: nan.. \n",
      "Epoch: 748/2000..  Training Loss: 0.4208..  Test Loss: nan.. \n",
      "Epoch: 749/2000..  Training Loss: 0.4205..  Test Loss: nan.. \n",
      "Epoch: 750/2000..  Training Loss: 0.4202..  Test Loss: nan.. \n",
      "Epoch: 751/2000..  Training Loss: 0.4199..  Test Loss: nan.. \n",
      "Epoch: 752/2000..  Training Loss: 0.4196..  Test Loss: nan.. \n",
      "Epoch: 753/2000..  Training Loss: 0.4192..  Test Loss: nan.. \n",
      "Epoch: 754/2000..  Training Loss: 0.4189..  Test Loss: nan.. \n",
      "Epoch: 755/2000..  Training Loss: 0.4186..  Test Loss: nan.. \n",
      "Epoch: 756/2000..  Training Loss: 0.4183..  Test Loss: nan.. \n",
      "Epoch: 757/2000..  Training Loss: 0.4180..  Test Loss: nan.. \n",
      "Epoch: 758/2000..  Training Loss: 0.4176..  Test Loss: nan.. \n",
      "Epoch: 759/2000..  Training Loss: 0.4173..  Test Loss: nan.. \n",
      "Epoch: 760/2000..  Training Loss: 0.4169..  Test Loss: nan.. \n",
      "Epoch: 761/2000..  Training Loss: 0.4166..  Test Loss: nan.. \n",
      "Epoch: 762/2000..  Training Loss: 0.4163..  Test Loss: nan.. \n",
      "Epoch: 763/2000..  Training Loss: 0.4158..  Test Loss: nan.. \n",
      "Epoch: 764/2000..  Training Loss: 0.4155..  Test Loss: nan.. \n",
      "Epoch: 765/2000..  Training Loss: 0.4152..  Test Loss: nan.. \n",
      "Epoch: 766/2000..  Training Loss: 0.4146..  Test Loss: nan.. \n",
      "Epoch: 767/2000..  Training Loss: 0.4144..  Test Loss: nan.. \n",
      "Epoch: 768/2000..  Training Loss: 0.4139..  Test Loss: nan.. \n",
      "Epoch: 769/2000..  Training Loss: 0.4136..  Test Loss: nan.. \n",
      "Epoch: 770/2000..  Training Loss: 0.4130..  Test Loss: nan.. \n",
      "Epoch: 771/2000..  Training Loss: 0.4127..  Test Loss: nan.. \n",
      "Epoch: 772/2000..  Training Loss: 0.4124..  Test Loss: nan.. \n",
      "Epoch: 773/2000..  Training Loss: 0.4120..  Test Loss: nan.. \n",
      "Epoch: 774/2000..  Training Loss: 0.4118..  Test Loss: nan.. \n",
      "Epoch: 775/2000..  Training Loss: 0.4115..  Test Loss: nan.. \n",
      "Epoch: 776/2000..  Training Loss: 0.4111..  Test Loss: nan.. \n",
      "Epoch: 777/2000..  Training Loss: 0.4107..  Test Loss: nan.. \n",
      "Epoch: 778/2000..  Training Loss: 0.4104..  Test Loss: nan.. \n",
      "Epoch: 779/2000..  Training Loss: 0.4100..  Test Loss: nan.. \n",
      "Epoch: 780/2000..  Training Loss: 0.4097..  Test Loss: nan.. \n",
      "Epoch: 781/2000..  Training Loss: 0.4093..  Test Loss: nan.. \n",
      "Epoch: 782/2000..  Training Loss: 0.4091..  Test Loss: nan.. \n",
      "Epoch: 783/2000..  Training Loss: 0.4088..  Test Loss: nan.. \n",
      "Epoch: 784/2000..  Training Loss: 0.4085..  Test Loss: nan.. \n",
      "Epoch: 785/2000..  Training Loss: 0.4081..  Test Loss: nan.. \n",
      "Epoch: 786/2000..  Training Loss: 0.4077..  Test Loss: nan.. \n",
      "Epoch: 787/2000..  Training Loss: 0.4073..  Test Loss: nan.. \n",
      "Epoch: 788/2000..  Training Loss: 0.4069..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 789/2000..  Training Loss: 0.4065..  Test Loss: nan.. \n",
      "Epoch: 790/2000..  Training Loss: 0.4062..  Test Loss: nan.. \n",
      "Epoch: 791/2000..  Training Loss: 0.4059..  Test Loss: nan.. \n",
      "Epoch: 792/2000..  Training Loss: 0.4057..  Test Loss: nan.. \n",
      "Epoch: 793/2000..  Training Loss: 0.4053..  Test Loss: nan.. \n",
      "Epoch: 794/2000..  Training Loss: 0.4051..  Test Loss: nan.. \n",
      "Epoch: 795/2000..  Training Loss: 0.4046..  Test Loss: nan.. \n",
      "Epoch: 796/2000..  Training Loss: 0.4044..  Test Loss: nan.. \n",
      "Epoch: 797/2000..  Training Loss: 0.4040..  Test Loss: nan.. \n",
      "Epoch: 798/2000..  Training Loss: 0.4036..  Test Loss: nan.. \n",
      "Epoch: 799/2000..  Training Loss: 0.4033..  Test Loss: nan.. \n",
      "Epoch: 800/2000..  Training Loss: 0.4029..  Test Loss: nan.. \n",
      "Epoch: 801/2000..  Training Loss: 0.4028..  Test Loss: nan.. \n",
      "Epoch: 802/2000..  Training Loss: 0.4024..  Test Loss: nan.. \n",
      "Epoch: 803/2000..  Training Loss: 0.4020..  Test Loss: nan.. \n",
      "Epoch: 804/2000..  Training Loss: 0.4015..  Test Loss: nan.. \n",
      "Epoch: 805/2000..  Training Loss: 0.4014..  Test Loss: nan.. \n",
      "Epoch: 806/2000..  Training Loss: 0.4009..  Test Loss: nan.. \n",
      "Epoch: 807/2000..  Training Loss: 0.4008..  Test Loss: nan.. \n",
      "Epoch: 808/2000..  Training Loss: 0.4005..  Test Loss: nan.. \n",
      "Epoch: 809/2000..  Training Loss: 0.4001..  Test Loss: nan.. \n",
      "Epoch: 810/2000..  Training Loss: 0.3999..  Test Loss: nan.. \n",
      "Epoch: 811/2000..  Training Loss: 0.3995..  Test Loss: nan.. \n",
      "Epoch: 812/2000..  Training Loss: 0.3992..  Test Loss: nan.. \n",
      "Epoch: 813/2000..  Training Loss: 0.3988..  Test Loss: nan.. \n",
      "Epoch: 814/2000..  Training Loss: 0.3984..  Test Loss: nan.. \n",
      "Epoch: 815/2000..  Training Loss: 0.3980..  Test Loss: nan.. \n",
      "Epoch: 816/2000..  Training Loss: 0.3977..  Test Loss: nan.. \n",
      "Epoch: 817/2000..  Training Loss: 0.3975..  Test Loss: nan.. \n",
      "Epoch: 818/2000..  Training Loss: 0.3973..  Test Loss: nan.. \n",
      "Epoch: 819/2000..  Training Loss: 0.3968..  Test Loss: nan.. \n",
      "Epoch: 820/2000..  Training Loss: 0.3966..  Test Loss: nan.. \n",
      "Epoch: 821/2000..  Training Loss: 0.3962..  Test Loss: nan.. \n",
      "Epoch: 822/2000..  Training Loss: 0.3959..  Test Loss: nan.. \n",
      "Epoch: 823/2000..  Training Loss: 0.3955..  Test Loss: nan.. \n",
      "Epoch: 824/2000..  Training Loss: 0.3952..  Test Loss: nan.. \n",
      "Epoch: 825/2000..  Training Loss: 0.3948..  Test Loss: nan.. \n",
      "Epoch: 826/2000..  Training Loss: 0.3946..  Test Loss: nan.. \n",
      "Epoch: 827/2000..  Training Loss: 0.3942..  Test Loss: nan.. \n",
      "Epoch: 828/2000..  Training Loss: 0.3938..  Test Loss: nan.. \n",
      "Epoch: 829/2000..  Training Loss: 0.3935..  Test Loss: nan.. \n",
      "Epoch: 830/2000..  Training Loss: 0.3932..  Test Loss: nan.. \n",
      "Epoch: 831/2000..  Training Loss: 0.3930..  Test Loss: nan.. \n",
      "Epoch: 832/2000..  Training Loss: 0.3926..  Test Loss: nan.. \n",
      "Epoch: 833/2000..  Training Loss: 0.3923..  Test Loss: nan.. \n",
      "Epoch: 834/2000..  Training Loss: 0.3920..  Test Loss: nan.. \n",
      "Epoch: 835/2000..  Training Loss: 0.3917..  Test Loss: nan.. \n",
      "Epoch: 836/2000..  Training Loss: 0.3914..  Test Loss: nan.. \n",
      "Epoch: 837/2000..  Training Loss: 0.3911..  Test Loss: nan.. \n",
      "Epoch: 838/2000..  Training Loss: 0.3907..  Test Loss: nan.. \n",
      "Epoch: 839/2000..  Training Loss: 0.3905..  Test Loss: nan.. \n",
      "Epoch: 840/2000..  Training Loss: 0.3902..  Test Loss: nan.. \n",
      "Epoch: 841/2000..  Training Loss: 0.3899..  Test Loss: nan.. \n",
      "Epoch: 842/2000..  Training Loss: 0.3896..  Test Loss: nan.. \n",
      "Epoch: 843/2000..  Training Loss: 0.3893..  Test Loss: nan.. \n",
      "Epoch: 844/2000..  Training Loss: 0.3890..  Test Loss: nan.. \n",
      "Epoch: 845/2000..  Training Loss: 0.3887..  Test Loss: nan.. \n",
      "Epoch: 846/2000..  Training Loss: 0.3884..  Test Loss: nan.. \n",
      "Epoch: 847/2000..  Training Loss: 0.3880..  Test Loss: nan.. \n",
      "Epoch: 848/2000..  Training Loss: 0.3877..  Test Loss: nan.. \n",
      "Epoch: 849/2000..  Training Loss: 0.3875..  Test Loss: nan.. \n",
      "Epoch: 850/2000..  Training Loss: 0.3871..  Test Loss: nan.. \n",
      "Epoch: 851/2000..  Training Loss: 0.3870..  Test Loss: nan.. \n",
      "Epoch: 852/2000..  Training Loss: 0.3867..  Test Loss: nan.. \n",
      "Epoch: 853/2000..  Training Loss: 0.3864..  Test Loss: nan.. \n",
      "Epoch: 854/2000..  Training Loss: 0.3861..  Test Loss: nan.. \n",
      "Epoch: 855/2000..  Training Loss: 0.3858..  Test Loss: nan.. \n",
      "Epoch: 856/2000..  Training Loss: 0.3856..  Test Loss: nan.. \n",
      "Epoch: 857/2000..  Training Loss: 0.3853..  Test Loss: nan.. \n",
      "Epoch: 858/2000..  Training Loss: 0.3851..  Test Loss: nan.. \n",
      "Epoch: 859/2000..  Training Loss: 0.3848..  Test Loss: nan.. \n",
      "Epoch: 860/2000..  Training Loss: 0.3844..  Test Loss: nan.. \n",
      "Epoch: 861/2000..  Training Loss: 0.3842..  Test Loss: nan.. \n",
      "Epoch: 862/2000..  Training Loss: 0.3839..  Test Loss: nan.. \n",
      "Epoch: 863/2000..  Training Loss: 0.3836..  Test Loss: nan.. \n",
      "Epoch: 864/2000..  Training Loss: 0.3834..  Test Loss: nan.. \n",
      "Epoch: 865/2000..  Training Loss: 0.3831..  Test Loss: nan.. \n",
      "Epoch: 866/2000..  Training Loss: 0.3828..  Test Loss: nan.. \n",
      "Epoch: 867/2000..  Training Loss: 0.3826..  Test Loss: nan.. \n",
      "Epoch: 868/2000..  Training Loss: 0.3823..  Test Loss: nan.. \n",
      "Epoch: 869/2000..  Training Loss: 0.3820..  Test Loss: nan.. \n",
      "Epoch: 870/2000..  Training Loss: 0.3818..  Test Loss: nan.. \n",
      "Epoch: 871/2000..  Training Loss: 0.3815..  Test Loss: nan.. \n",
      "Epoch: 872/2000..  Training Loss: 0.3812..  Test Loss: nan.. \n",
      "Epoch: 873/2000..  Training Loss: 0.3810..  Test Loss: nan.. \n",
      "Epoch: 874/2000..  Training Loss: 0.3808..  Test Loss: nan.. \n",
      "Epoch: 875/2000..  Training Loss: 0.3806..  Test Loss: nan.. \n",
      "Epoch: 876/2000..  Training Loss: 0.3803..  Test Loss: nan.. \n",
      "Epoch: 877/2000..  Training Loss: 0.3801..  Test Loss: nan.. \n",
      "Epoch: 878/2000..  Training Loss: 0.3798..  Test Loss: nan.. \n",
      "Epoch: 879/2000..  Training Loss: 0.3796..  Test Loss: nan.. \n",
      "Epoch: 880/2000..  Training Loss: 0.3792..  Test Loss: nan.. \n",
      "Epoch: 881/2000..  Training Loss: 0.3789..  Test Loss: nan.. \n",
      "Epoch: 882/2000..  Training Loss: 0.3787..  Test Loss: nan.. \n",
      "Epoch: 883/2000..  Training Loss: 0.3784..  Test Loss: nan.. \n",
      "Epoch: 884/2000..  Training Loss: 0.3783..  Test Loss: nan.. \n",
      "Epoch: 885/2000..  Training Loss: 0.3780..  Test Loss: nan.. \n",
      "Epoch: 886/2000..  Training Loss: 0.3780..  Test Loss: nan.. \n",
      "Epoch: 887/2000..  Training Loss: 0.3776..  Test Loss: nan.. \n",
      "Epoch: 888/2000..  Training Loss: 0.3776..  Test Loss: nan.. \n",
      "Epoch: 889/2000..  Training Loss: 0.3773..  Test Loss: nan.. \n",
      "Epoch: 890/2000..  Training Loss: 0.3771..  Test Loss: nan.. \n",
      "Epoch: 891/2000..  Training Loss: 0.3768..  Test Loss: nan.. \n",
      "Epoch: 892/2000..  Training Loss: 0.3765..  Test Loss: nan.. \n",
      "Epoch: 893/2000..  Training Loss: 0.3763..  Test Loss: nan.. \n",
      "Epoch: 894/2000..  Training Loss: 0.3761..  Test Loss: nan.. \n",
      "Epoch: 895/2000..  Training Loss: 0.3759..  Test Loss: nan.. \n",
      "Epoch: 896/2000..  Training Loss: 0.3756..  Test Loss: nan.. \n",
      "Epoch: 897/2000..  Training Loss: 0.3753..  Test Loss: nan.. \n",
      "Epoch: 898/2000..  Training Loss: 0.3751..  Test Loss: nan.. \n",
      "Epoch: 899/2000..  Training Loss: 0.3750..  Test Loss: nan.. \n",
      "Epoch: 900/2000..  Training Loss: 0.3745..  Test Loss: nan.. \n",
      "Epoch: 901/2000..  Training Loss: 0.3745..  Test Loss: nan.. \n",
      "Epoch: 902/2000..  Training Loss: 0.3741..  Test Loss: nan.. \n",
      "Epoch: 903/2000..  Training Loss: 0.3737..  Test Loss: nan.. \n",
      "Epoch: 904/2000..  Training Loss: 0.3736..  Test Loss: nan.. \n",
      "Epoch: 905/2000..  Training Loss: 0.3733..  Test Loss: nan.. \n",
      "Epoch: 906/2000..  Training Loss: 0.3732..  Test Loss: nan.. \n",
      "Epoch: 907/2000..  Training Loss: 0.3729..  Test Loss: nan.. \n",
      "Epoch: 908/2000..  Training Loss: 0.3726..  Test Loss: nan.. \n",
      "Epoch: 909/2000..  Training Loss: 0.3725..  Test Loss: nan.. \n",
      "Epoch: 910/2000..  Training Loss: 0.3722..  Test Loss: nan.. \n",
      "Epoch: 911/2000..  Training Loss: 0.3720..  Test Loss: nan.. \n",
      "Epoch: 912/2000..  Training Loss: 0.3717..  Test Loss: nan.. \n",
      "Epoch: 913/2000..  Training Loss: 0.3715..  Test Loss: nan.. \n",
      "Epoch: 914/2000..  Training Loss: 0.3713..  Test Loss: nan.. \n",
      "Epoch: 915/2000..  Training Loss: 0.3710..  Test Loss: nan.. \n",
      "Epoch: 916/2000..  Training Loss: 0.3709..  Test Loss: nan.. \n",
      "Epoch: 917/2000..  Training Loss: 0.3704..  Test Loss: nan.. \n",
      "Epoch: 918/2000..  Training Loss: 0.3702..  Test Loss: nan.. \n",
      "Epoch: 919/2000..  Training Loss: 0.3700..  Test Loss: nan.. \n",
      "Epoch: 920/2000..  Training Loss: 0.3697..  Test Loss: nan.. \n",
      "Epoch: 921/2000..  Training Loss: 0.3694..  Test Loss: nan.. \n",
      "Epoch: 922/2000..  Training Loss: 0.3694..  Test Loss: nan.. \n",
      "Epoch: 923/2000..  Training Loss: 0.3691..  Test Loss: nan.. \n",
      "Epoch: 924/2000..  Training Loss: 0.3687..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 925/2000..  Training Loss: 0.3686..  Test Loss: nan.. \n",
      "Epoch: 926/2000..  Training Loss: 0.3685..  Test Loss: nan.. \n",
      "Epoch: 927/2000..  Training Loss: 0.3680..  Test Loss: nan.. \n",
      "Epoch: 928/2000..  Training Loss: 0.3679..  Test Loss: nan.. \n",
      "Epoch: 929/2000..  Training Loss: 0.3676..  Test Loss: nan.. \n",
      "Epoch: 930/2000..  Training Loss: 0.3675..  Test Loss: nan.. \n",
      "Epoch: 931/2000..  Training Loss: 0.3673..  Test Loss: nan.. \n",
      "Epoch: 932/2000..  Training Loss: 0.3671..  Test Loss: nan.. \n",
      "Epoch: 933/2000..  Training Loss: 0.3669..  Test Loss: nan.. \n",
      "Epoch: 934/2000..  Training Loss: 0.3666..  Test Loss: nan.. \n",
      "Epoch: 935/2000..  Training Loss: 0.3664..  Test Loss: nan.. \n",
      "Epoch: 936/2000..  Training Loss: 0.3663..  Test Loss: nan.. \n",
      "Epoch: 937/2000..  Training Loss: 0.3661..  Test Loss: nan.. \n",
      "Epoch: 938/2000..  Training Loss: 0.3658..  Test Loss: nan.. \n",
      "Epoch: 939/2000..  Training Loss: 0.3656..  Test Loss: nan.. \n",
      "Epoch: 940/2000..  Training Loss: 0.3654..  Test Loss: nan.. \n",
      "Epoch: 941/2000..  Training Loss: 0.3650..  Test Loss: nan.. \n",
      "Epoch: 942/2000..  Training Loss: 0.3649..  Test Loss: nan.. \n",
      "Epoch: 943/2000..  Training Loss: 0.3647..  Test Loss: nan.. \n",
      "Epoch: 944/2000..  Training Loss: 0.3645..  Test Loss: nan.. \n",
      "Epoch: 945/2000..  Training Loss: 0.3643..  Test Loss: nan.. \n",
      "Epoch: 946/2000..  Training Loss: 0.3641..  Test Loss: nan.. \n",
      "Epoch: 947/2000..  Training Loss: 0.3639..  Test Loss: nan.. \n",
      "Epoch: 948/2000..  Training Loss: 0.3637..  Test Loss: nan.. \n",
      "Epoch: 949/2000..  Training Loss: 0.3635..  Test Loss: nan.. \n",
      "Epoch: 950/2000..  Training Loss: 0.3633..  Test Loss: nan.. \n",
      "Epoch: 951/2000..  Training Loss: 0.3631..  Test Loss: nan.. \n",
      "Epoch: 952/2000..  Training Loss: 0.3629..  Test Loss: nan.. \n",
      "Epoch: 953/2000..  Training Loss: 0.3626..  Test Loss: nan.. \n",
      "Epoch: 954/2000..  Training Loss: 0.3625..  Test Loss: nan.. \n",
      "Epoch: 955/2000..  Training Loss: 0.3623..  Test Loss: nan.. \n",
      "Epoch: 956/2000..  Training Loss: 0.3620..  Test Loss: nan.. \n",
      "Epoch: 957/2000..  Training Loss: 0.3619..  Test Loss: nan.. \n",
      "Epoch: 958/2000..  Training Loss: 0.3615..  Test Loss: nan.. \n",
      "Epoch: 959/2000..  Training Loss: 0.3616..  Test Loss: nan.. \n",
      "Epoch: 960/2000..  Training Loss: 0.3614..  Test Loss: nan.. \n",
      "Epoch: 961/2000..  Training Loss: 0.3611..  Test Loss: nan.. \n",
      "Epoch: 962/2000..  Training Loss: 0.3610..  Test Loss: nan.. \n",
      "Epoch: 963/2000..  Training Loss: 0.3606..  Test Loss: nan.. \n",
      "Epoch: 964/2000..  Training Loss: 0.3605..  Test Loss: nan.. \n",
      "Epoch: 965/2000..  Training Loss: 0.3603..  Test Loss: nan.. \n",
      "Epoch: 966/2000..  Training Loss: 0.3601..  Test Loss: nan.. \n",
      "Epoch: 967/2000..  Training Loss: 0.3599..  Test Loss: nan.. \n",
      "Epoch: 968/2000..  Training Loss: 0.3598..  Test Loss: nan.. \n",
      "Epoch: 969/2000..  Training Loss: 0.3596..  Test Loss: nan.. \n",
      "Epoch: 970/2000..  Training Loss: 0.3594..  Test Loss: nan.. \n",
      "Epoch: 971/2000..  Training Loss: 0.3590..  Test Loss: nan.. \n",
      "Epoch: 972/2000..  Training Loss: 0.3590..  Test Loss: nan.. \n",
      "Epoch: 973/2000..  Training Loss: 0.3587..  Test Loss: nan.. \n",
      "Epoch: 974/2000..  Training Loss: 0.3587..  Test Loss: nan.. \n",
      "Epoch: 975/2000..  Training Loss: 0.3584..  Test Loss: nan.. \n",
      "Epoch: 976/2000..  Training Loss: 0.3581..  Test Loss: nan.. \n",
      "Epoch: 977/2000..  Training Loss: 0.3580..  Test Loss: nan.. \n",
      "Epoch: 978/2000..  Training Loss: 0.3576..  Test Loss: nan.. \n",
      "Epoch: 979/2000..  Training Loss: 0.3575..  Test Loss: nan.. \n",
      "Epoch: 980/2000..  Training Loss: 0.3574..  Test Loss: nan.. \n",
      "Epoch: 981/2000..  Training Loss: 0.3572..  Test Loss: nan.. \n",
      "Epoch: 982/2000..  Training Loss: 0.3571..  Test Loss: nan.. \n",
      "Epoch: 983/2000..  Training Loss: 0.3570..  Test Loss: nan.. \n",
      "Epoch: 984/2000..  Training Loss: 0.3569..  Test Loss: nan.. \n",
      "Epoch: 985/2000..  Training Loss: 0.3566..  Test Loss: nan.. \n",
      "Epoch: 986/2000..  Training Loss: 0.3566..  Test Loss: nan.. \n",
      "Epoch: 987/2000..  Training Loss: 0.3562..  Test Loss: nan.. \n",
      "Epoch: 988/2000..  Training Loss: 0.3561..  Test Loss: nan.. \n",
      "Epoch: 989/2000..  Training Loss: 0.3560..  Test Loss: nan.. \n",
      "Epoch: 990/2000..  Training Loss: 0.3557..  Test Loss: nan.. \n",
      "Epoch: 991/2000..  Training Loss: 0.3558..  Test Loss: nan.. \n",
      "Epoch: 992/2000..  Training Loss: 0.3555..  Test Loss: nan.. \n",
      "Epoch: 993/2000..  Training Loss: 0.3555..  Test Loss: nan.. \n",
      "Epoch: 994/2000..  Training Loss: 0.3553..  Test Loss: nan.. \n",
      "Epoch: 995/2000..  Training Loss: 0.3551..  Test Loss: nan.. \n",
      "Epoch: 996/2000..  Training Loss: 0.3550..  Test Loss: nan.. \n",
      "Epoch: 997/2000..  Training Loss: 0.3548..  Test Loss: nan.. \n",
      "Epoch: 998/2000..  Training Loss: 0.3547..  Test Loss: nan.. \n",
      "Epoch: 999/2000..  Training Loss: 0.3544..  Test Loss: nan.. \n",
      "Epoch: 1000/2000..  Training Loss: 0.3543..  Test Loss: nan.. \n",
      "Epoch: 1001/2000..  Training Loss: 0.3541..  Test Loss: nan.. \n",
      "Epoch: 1002/2000..  Training Loss: 0.3539..  Test Loss: nan.. \n",
      "Epoch: 1003/2000..  Training Loss: 0.3537..  Test Loss: nan.. \n",
      "Epoch: 1004/2000..  Training Loss: 0.3535..  Test Loss: nan.. \n",
      "Epoch: 1005/2000..  Training Loss: 0.3535..  Test Loss: nan.. \n",
      "Epoch: 1006/2000..  Training Loss: 0.3532..  Test Loss: nan.. \n",
      "Epoch: 1007/2000..  Training Loss: 0.3531..  Test Loss: nan.. \n",
      "Epoch: 1008/2000..  Training Loss: 0.3529..  Test Loss: nan.. \n",
      "Epoch: 1009/2000..  Training Loss: 0.3527..  Test Loss: nan.. \n",
      "Epoch: 1010/2000..  Training Loss: 0.3524..  Test Loss: nan.. \n",
      "Epoch: 1011/2000..  Training Loss: 0.3525..  Test Loss: nan.. \n",
      "Epoch: 1012/2000..  Training Loss: 0.3522..  Test Loss: nan.. \n",
      "Epoch: 1013/2000..  Training Loss: 0.3521..  Test Loss: nan.. \n",
      "Epoch: 1014/2000..  Training Loss: 0.3518..  Test Loss: nan.. \n",
      "Epoch: 1015/2000..  Training Loss: 0.3514..  Test Loss: nan.. \n",
      "Epoch: 1016/2000..  Training Loss: 0.3514..  Test Loss: nan.. \n",
      "Epoch: 1017/2000..  Training Loss: 0.3512..  Test Loss: nan.. \n",
      "Epoch: 1018/2000..  Training Loss: 0.3508..  Test Loss: nan.. \n",
      "Epoch: 1019/2000..  Training Loss: 0.3506..  Test Loss: nan.. \n",
      "Epoch: 1020/2000..  Training Loss: 0.3507..  Test Loss: nan.. \n",
      "Epoch: 1021/2000..  Training Loss: 0.3503..  Test Loss: nan.. \n",
      "Epoch: 1022/2000..  Training Loss: 0.3503..  Test Loss: nan.. \n",
      "Epoch: 1023/2000..  Training Loss: 0.3500..  Test Loss: nan.. \n",
      "Epoch: 1024/2000..  Training Loss: 0.3499..  Test Loss: nan.. \n",
      "Epoch: 1025/2000..  Training Loss: 0.3496..  Test Loss: nan.. \n",
      "Epoch: 1026/2000..  Training Loss: 0.3495..  Test Loss: nan.. \n",
      "Epoch: 1027/2000..  Training Loss: 0.3495..  Test Loss: nan.. \n",
      "Epoch: 1028/2000..  Training Loss: 0.3495..  Test Loss: nan.. \n",
      "Epoch: 1029/2000..  Training Loss: 0.3491..  Test Loss: nan.. \n",
      "Epoch: 1030/2000..  Training Loss: 0.3488..  Test Loss: nan.. \n",
      "Epoch: 1031/2000..  Training Loss: 0.3487..  Test Loss: nan.. \n",
      "Epoch: 1032/2000..  Training Loss: 0.3484..  Test Loss: nan.. \n",
      "Epoch: 1033/2000..  Training Loss: 0.3482..  Test Loss: nan.. \n",
      "Epoch: 1034/2000..  Training Loss: 0.3481..  Test Loss: nan.. \n",
      "Epoch: 1035/2000..  Training Loss: 0.3479..  Test Loss: nan.. \n",
      "Epoch: 1036/2000..  Training Loss: 0.3477..  Test Loss: nan.. \n",
      "Epoch: 1037/2000..  Training Loss: 0.3474..  Test Loss: nan.. \n",
      "Epoch: 1038/2000..  Training Loss: 0.3474..  Test Loss: nan.. \n",
      "Epoch: 1039/2000..  Training Loss: 0.3471..  Test Loss: nan.. \n",
      "Epoch: 1040/2000..  Training Loss: 0.3470..  Test Loss: nan.. \n",
      "Epoch: 1041/2000..  Training Loss: 0.3468..  Test Loss: nan.. \n",
      "Epoch: 1042/2000..  Training Loss: 0.3467..  Test Loss: nan.. \n",
      "Epoch: 1043/2000..  Training Loss: 0.3464..  Test Loss: nan.. \n",
      "Epoch: 1044/2000..  Training Loss: 0.3466..  Test Loss: nan.. \n",
      "Epoch: 1045/2000..  Training Loss: 0.3463..  Test Loss: nan.. \n",
      "Epoch: 1046/2000..  Training Loss: 0.3459..  Test Loss: nan.. \n",
      "Epoch: 1047/2000..  Training Loss: 0.3457..  Test Loss: nan.. \n",
      "Epoch: 1048/2000..  Training Loss: 0.3456..  Test Loss: nan.. \n",
      "Epoch: 1049/2000..  Training Loss: 0.3454..  Test Loss: nan.. \n",
      "Epoch: 1050/2000..  Training Loss: 0.3453..  Test Loss: nan.. \n",
      "Epoch: 1051/2000..  Training Loss: 0.3451..  Test Loss: nan.. \n",
      "Epoch: 1052/2000..  Training Loss: 0.3449..  Test Loss: nan.. \n",
      "Epoch: 1053/2000..  Training Loss: 0.3448..  Test Loss: nan.. \n",
      "Epoch: 1054/2000..  Training Loss: 0.3447..  Test Loss: nan.. \n",
      "Epoch: 1055/2000..  Training Loss: 0.3444..  Test Loss: nan.. \n",
      "Epoch: 1056/2000..  Training Loss: 0.3442..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1057/2000..  Training Loss: 0.3440..  Test Loss: nan.. \n",
      "Epoch: 1058/2000..  Training Loss: 0.3439..  Test Loss: nan.. \n",
      "Epoch: 1059/2000..  Training Loss: 0.3438..  Test Loss: nan.. \n",
      "Epoch: 1060/2000..  Training Loss: 0.3435..  Test Loss: nan.. \n",
      "Epoch: 1061/2000..  Training Loss: 0.3433..  Test Loss: nan.. \n",
      "Epoch: 1062/2000..  Training Loss: 0.3433..  Test Loss: nan.. \n",
      "Epoch: 1063/2000..  Training Loss: 0.3429..  Test Loss: nan.. \n",
      "Epoch: 1064/2000..  Training Loss: 0.3430..  Test Loss: nan.. \n",
      "Epoch: 1065/2000..  Training Loss: 0.3427..  Test Loss: nan.. \n",
      "Epoch: 1066/2000..  Training Loss: 0.3425..  Test Loss: nan.. \n",
      "Epoch: 1067/2000..  Training Loss: 0.3424..  Test Loss: nan.. \n",
      "Epoch: 1068/2000..  Training Loss: 0.3423..  Test Loss: nan.. \n",
      "Epoch: 1069/2000..  Training Loss: 0.3418..  Test Loss: nan.. \n",
      "Epoch: 1070/2000..  Training Loss: 0.3419..  Test Loss: nan.. \n",
      "Epoch: 1071/2000..  Training Loss: 0.3417..  Test Loss: nan.. \n",
      "Epoch: 1072/2000..  Training Loss: 0.3414..  Test Loss: nan.. \n",
      "Epoch: 1073/2000..  Training Loss: 0.3414..  Test Loss: nan.. \n",
      "Epoch: 1074/2000..  Training Loss: 0.3412..  Test Loss: nan.. \n",
      "Epoch: 1075/2000..  Training Loss: 0.3410..  Test Loss: nan.. \n",
      "Epoch: 1076/2000..  Training Loss: 0.3408..  Test Loss: nan.. \n",
      "Epoch: 1077/2000..  Training Loss: 0.3407..  Test Loss: nan.. \n",
      "Epoch: 1078/2000..  Training Loss: 0.3404..  Test Loss: nan.. \n",
      "Epoch: 1079/2000..  Training Loss: 0.3400..  Test Loss: nan.. \n",
      "Epoch: 1080/2000..  Training Loss: 0.3400..  Test Loss: nan.. \n",
      "Epoch: 1081/2000..  Training Loss: 0.3396..  Test Loss: nan.. \n",
      "Epoch: 1082/2000..  Training Loss: 0.3396..  Test Loss: nan.. \n",
      "Epoch: 1083/2000..  Training Loss: 0.3394..  Test Loss: nan.. \n",
      "Epoch: 1084/2000..  Training Loss: 0.3393..  Test Loss: nan.. \n",
      "Epoch: 1085/2000..  Training Loss: 0.3388..  Test Loss: nan.. \n",
      "Epoch: 1086/2000..  Training Loss: 0.3389..  Test Loss: nan.. \n",
      "Epoch: 1087/2000..  Training Loss: 0.3385..  Test Loss: nan.. \n",
      "Epoch: 1088/2000..  Training Loss: 0.3384..  Test Loss: nan.. \n",
      "Epoch: 1089/2000..  Training Loss: 0.3381..  Test Loss: nan.. \n",
      "Epoch: 1090/2000..  Training Loss: 0.3382..  Test Loss: nan.. \n",
      "Epoch: 1091/2000..  Training Loss: 0.3377..  Test Loss: nan.. \n",
      "Epoch: 1092/2000..  Training Loss: 0.3379..  Test Loss: nan.. \n",
      "Epoch: 1093/2000..  Training Loss: 0.3375..  Test Loss: nan.. \n",
      "Epoch: 1094/2000..  Training Loss: 0.3374..  Test Loss: nan.. \n",
      "Epoch: 1095/2000..  Training Loss: 0.3374..  Test Loss: nan.. \n",
      "Epoch: 1096/2000..  Training Loss: 0.3370..  Test Loss: nan.. \n",
      "Epoch: 1097/2000..  Training Loss: 0.3371..  Test Loss: nan.. \n",
      "Epoch: 1098/2000..  Training Loss: 0.3367..  Test Loss: nan.. \n",
      "Epoch: 1099/2000..  Training Loss: 0.3366..  Test Loss: nan.. \n",
      "Epoch: 1100/2000..  Training Loss: 0.3363..  Test Loss: nan.. \n",
      "Epoch: 1101/2000..  Training Loss: 0.3362..  Test Loss: nan.. \n",
      "Epoch: 1102/2000..  Training Loss: 0.3361..  Test Loss: nan.. \n",
      "Epoch: 1103/2000..  Training Loss: 0.3357..  Test Loss: nan.. \n",
      "Epoch: 1104/2000..  Training Loss: 0.3358..  Test Loss: nan.. \n",
      "Epoch: 1105/2000..  Training Loss: 0.3354..  Test Loss: nan.. \n",
      "Epoch: 1106/2000..  Training Loss: 0.3355..  Test Loss: nan.. \n",
      "Epoch: 1107/2000..  Training Loss: 0.3351..  Test Loss: nan.. \n",
      "Epoch: 1108/2000..  Training Loss: 0.3352..  Test Loss: nan.. \n",
      "Epoch: 1109/2000..  Training Loss: 0.3349..  Test Loss: nan.. \n",
      "Epoch: 1110/2000..  Training Loss: 0.3347..  Test Loss: nan.. \n",
      "Epoch: 1111/2000..  Training Loss: 0.3345..  Test Loss: nan.. \n",
      "Epoch: 1112/2000..  Training Loss: 0.3343..  Test Loss: nan.. \n",
      "Epoch: 1113/2000..  Training Loss: 0.3342..  Test Loss: nan.. \n",
      "Epoch: 1114/2000..  Training Loss: 0.3341..  Test Loss: nan.. \n",
      "Epoch: 1115/2000..  Training Loss: 0.3340..  Test Loss: nan.. \n",
      "Epoch: 1116/2000..  Training Loss: 0.3337..  Test Loss: nan.. \n",
      "Epoch: 1117/2000..  Training Loss: 0.3337..  Test Loss: nan.. \n",
      "Epoch: 1118/2000..  Training Loss: 0.3334..  Test Loss: nan.. \n",
      "Epoch: 1119/2000..  Training Loss: 0.3331..  Test Loss: nan.. \n",
      "Epoch: 1120/2000..  Training Loss: 0.3330..  Test Loss: nan.. \n",
      "Epoch: 1121/2000..  Training Loss: 0.3331..  Test Loss: nan.. \n",
      "Epoch: 1122/2000..  Training Loss: 0.3331..  Test Loss: nan.. \n",
      "Epoch: 1123/2000..  Training Loss: 0.3332..  Test Loss: nan.. \n",
      "Epoch: 1124/2000..  Training Loss: 0.3326..  Test Loss: nan.. \n",
      "Epoch: 1125/2000..  Training Loss: 0.3326..  Test Loss: nan.. \n",
      "Epoch: 1126/2000..  Training Loss: 0.3324..  Test Loss: nan.. \n",
      "Epoch: 1127/2000..  Training Loss: 0.3324..  Test Loss: nan.. \n",
      "Epoch: 1128/2000..  Training Loss: 0.3319..  Test Loss: nan.. \n",
      "Epoch: 1129/2000..  Training Loss: 0.3322..  Test Loss: nan.. \n",
      "Epoch: 1130/2000..  Training Loss: 0.3315..  Test Loss: nan.. \n",
      "Epoch: 1131/2000..  Training Loss: 0.3320..  Test Loss: nan.. \n",
      "Epoch: 1132/2000..  Training Loss: 0.3312..  Test Loss: nan.. \n",
      "Epoch: 1133/2000..  Training Loss: 0.3314..  Test Loss: nan.. \n",
      "Epoch: 1134/2000..  Training Loss: 0.3311..  Test Loss: nan.. \n",
      "Epoch: 1135/2000..  Training Loss: 0.3310..  Test Loss: nan.. \n",
      "Epoch: 1136/2000..  Training Loss: 0.3307..  Test Loss: nan.. \n",
      "Epoch: 1137/2000..  Training Loss: 0.3308..  Test Loss: nan.. \n",
      "Epoch: 1138/2000..  Training Loss: 0.3305..  Test Loss: nan.. \n",
      "Epoch: 1139/2000..  Training Loss: 0.3303..  Test Loss: nan.. \n",
      "Epoch: 1140/2000..  Training Loss: 0.3301..  Test Loss: nan.. \n",
      "Epoch: 1141/2000..  Training Loss: 0.3301..  Test Loss: nan.. \n",
      "Epoch: 1142/2000..  Training Loss: 0.3297..  Test Loss: nan.. \n",
      "Epoch: 1143/2000..  Training Loss: 0.3298..  Test Loss: nan.. \n",
      "Epoch: 1144/2000..  Training Loss: 0.3295..  Test Loss: nan.. \n",
      "Epoch: 1145/2000..  Training Loss: 0.3295..  Test Loss: nan.. \n",
      "Epoch: 1146/2000..  Training Loss: 0.3292..  Test Loss: nan.. \n",
      "Epoch: 1147/2000..  Training Loss: 0.3294..  Test Loss: nan.. \n",
      "Epoch: 1148/2000..  Training Loss: 0.3291..  Test Loss: nan.. \n",
      "Epoch: 1149/2000..  Training Loss: 0.3289..  Test Loss: nan.. \n",
      "Epoch: 1150/2000..  Training Loss: 0.3286..  Test Loss: nan.. \n",
      "Epoch: 1151/2000..  Training Loss: 0.3286..  Test Loss: nan.. \n",
      "Epoch: 1152/2000..  Training Loss: 0.3284..  Test Loss: nan.. \n",
      "Epoch: 1153/2000..  Training Loss: 0.3283..  Test Loss: nan.. \n",
      "Epoch: 1154/2000..  Training Loss: 0.3281..  Test Loss: nan.. \n",
      "Epoch: 1155/2000..  Training Loss: 0.3279..  Test Loss: nan.. \n",
      "Epoch: 1156/2000..  Training Loss: 0.3279..  Test Loss: nan.. \n",
      "Epoch: 1157/2000..  Training Loss: 0.3276..  Test Loss: nan.. \n",
      "Epoch: 1158/2000..  Training Loss: 0.3274..  Test Loss: nan.. \n",
      "Epoch: 1159/2000..  Training Loss: 0.3274..  Test Loss: nan.. \n",
      "Epoch: 1160/2000..  Training Loss: 0.3272..  Test Loss: nan.. \n",
      "Epoch: 1161/2000..  Training Loss: 0.3269..  Test Loss: nan.. \n",
      "Epoch: 1162/2000..  Training Loss: 0.3271..  Test Loss: nan.. \n",
      "Epoch: 1163/2000..  Training Loss: 0.3267..  Test Loss: nan.. \n",
      "Epoch: 1164/2000..  Training Loss: 0.3268..  Test Loss: nan.. \n",
      "Epoch: 1165/2000..  Training Loss: 0.3265..  Test Loss: nan.. \n",
      "Epoch: 1166/2000..  Training Loss: 0.3264..  Test Loss: nan.. \n",
      "Epoch: 1167/2000..  Training Loss: 0.3260..  Test Loss: nan.. \n",
      "Epoch: 1168/2000..  Training Loss: 0.3261..  Test Loss: nan.. \n",
      "Epoch: 1169/2000..  Training Loss: 0.3257..  Test Loss: nan.. \n",
      "Epoch: 1170/2000..  Training Loss: 0.3260..  Test Loss: nan.. \n",
      "Epoch: 1171/2000..  Training Loss: 0.3254..  Test Loss: nan.. \n",
      "Epoch: 1172/2000..  Training Loss: 0.3255..  Test Loss: nan.. \n",
      "Epoch: 1173/2000..  Training Loss: 0.3252..  Test Loss: nan.. \n",
      "Epoch: 1174/2000..  Training Loss: 0.3253..  Test Loss: nan.. \n",
      "Epoch: 1175/2000..  Training Loss: 0.3249..  Test Loss: nan.. \n",
      "Epoch: 1176/2000..  Training Loss: 0.3249..  Test Loss: nan.. \n",
      "Epoch: 1177/2000..  Training Loss: 0.3246..  Test Loss: nan.. \n",
      "Epoch: 1178/2000..  Training Loss: 0.3245..  Test Loss: nan.. \n",
      "Epoch: 1179/2000..  Training Loss: 0.3242..  Test Loss: nan.. \n",
      "Epoch: 1180/2000..  Training Loss: 0.3242..  Test Loss: nan.. \n",
      "Epoch: 1181/2000..  Training Loss: 0.3240..  Test Loss: nan.. \n",
      "Epoch: 1182/2000..  Training Loss: 0.3239..  Test Loss: nan.. \n",
      "Epoch: 1183/2000..  Training Loss: 0.3236..  Test Loss: nan.. \n",
      "Epoch: 1184/2000..  Training Loss: 0.3238..  Test Loss: nan.. \n",
      "Epoch: 1185/2000..  Training Loss: 0.3234..  Test Loss: nan.. \n",
      "Epoch: 1186/2000..  Training Loss: 0.3236..  Test Loss: nan.. \n",
      "Epoch: 1187/2000..  Training Loss: 0.3231..  Test Loss: nan.. \n",
      "Epoch: 1188/2000..  Training Loss: 0.3233..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1189/2000..  Training Loss: 0.3228..  Test Loss: nan.. \n",
      "Epoch: 1190/2000..  Training Loss: 0.3229..  Test Loss: nan.. \n",
      "Epoch: 1191/2000..  Training Loss: 0.3224..  Test Loss: nan.. \n",
      "Epoch: 1192/2000..  Training Loss: 0.3224..  Test Loss: nan.. \n",
      "Epoch: 1193/2000..  Training Loss: 0.3222..  Test Loss: nan.. \n",
      "Epoch: 1194/2000..  Training Loss: 0.3222..  Test Loss: nan.. \n",
      "Epoch: 1195/2000..  Training Loss: 0.3219..  Test Loss: nan.. \n",
      "Epoch: 1196/2000..  Training Loss: 0.3220..  Test Loss: nan.. \n",
      "Epoch: 1197/2000..  Training Loss: 0.3218..  Test Loss: nan.. \n",
      "Epoch: 1198/2000..  Training Loss: 0.3218..  Test Loss: nan.. \n",
      "Epoch: 1199/2000..  Training Loss: 0.3214..  Test Loss: nan.. \n",
      "Epoch: 1200/2000..  Training Loss: 0.3214..  Test Loss: nan.. \n",
      "Epoch: 1201/2000..  Training Loss: 0.3211..  Test Loss: nan.. \n",
      "Epoch: 1202/2000..  Training Loss: 0.3211..  Test Loss: nan.. \n",
      "Epoch: 1203/2000..  Training Loss: 0.3209..  Test Loss: nan.. \n",
      "Epoch: 1204/2000..  Training Loss: 0.3209..  Test Loss: nan.. \n",
      "Epoch: 1205/2000..  Training Loss: 0.3206..  Test Loss: nan.. \n",
      "Epoch: 1206/2000..  Training Loss: 0.3209..  Test Loss: nan.. \n",
      "Epoch: 1207/2000..  Training Loss: 0.3207..  Test Loss: nan.. \n",
      "Epoch: 1208/2000..  Training Loss: 0.3207..  Test Loss: nan.. \n",
      "Epoch: 1209/2000..  Training Loss: 0.3204..  Test Loss: nan.. \n",
      "Epoch: 1210/2000..  Training Loss: 0.3203..  Test Loss: nan.. \n",
      "Epoch: 1211/2000..  Training Loss: 0.3202..  Test Loss: nan.. \n",
      "Epoch: 1212/2000..  Training Loss: 0.3199..  Test Loss: nan.. \n",
      "Epoch: 1213/2000..  Training Loss: 0.3198..  Test Loss: nan.. \n",
      "Epoch: 1214/2000..  Training Loss: 0.3196..  Test Loss: nan.. \n",
      "Epoch: 1215/2000..  Training Loss: 0.3196..  Test Loss: nan.. \n",
      "Epoch: 1216/2000..  Training Loss: 0.3194..  Test Loss: nan.. \n",
      "Epoch: 1217/2000..  Training Loss: 0.3193..  Test Loss: nan.. \n",
      "Epoch: 1218/2000..  Training Loss: 0.3190..  Test Loss: nan.. \n",
      "Epoch: 1219/2000..  Training Loss: 0.3188..  Test Loss: nan.. \n",
      "Epoch: 1220/2000..  Training Loss: 0.3185..  Test Loss: nan.. \n",
      "Epoch: 1221/2000..  Training Loss: 0.3183..  Test Loss: nan.. \n",
      "Epoch: 1222/2000..  Training Loss: 0.3182..  Test Loss: nan.. \n",
      "Epoch: 1223/2000..  Training Loss: 0.3180..  Test Loss: nan.. \n",
      "Epoch: 1224/2000..  Training Loss: 0.3181..  Test Loss: nan.. \n",
      "Epoch: 1225/2000..  Training Loss: 0.3178..  Test Loss: nan.. \n",
      "Epoch: 1226/2000..  Training Loss: 0.3177..  Test Loss: nan.. \n",
      "Epoch: 1227/2000..  Training Loss: 0.3173..  Test Loss: nan.. \n",
      "Epoch: 1228/2000..  Training Loss: 0.3173..  Test Loss: nan.. \n",
      "Epoch: 1229/2000..  Training Loss: 0.3170..  Test Loss: nan.. \n",
      "Epoch: 1230/2000..  Training Loss: 0.3167..  Test Loss: nan.. \n",
      "Epoch: 1231/2000..  Training Loss: 0.3166..  Test Loss: nan.. \n",
      "Epoch: 1232/2000..  Training Loss: 0.3165..  Test Loss: nan.. \n",
      "Epoch: 1233/2000..  Training Loss: 0.3164..  Test Loss: nan.. \n",
      "Epoch: 1234/2000..  Training Loss: 0.3159..  Test Loss: nan.. \n",
      "Epoch: 1235/2000..  Training Loss: 0.3161..  Test Loss: nan.. \n",
      "Epoch: 1236/2000..  Training Loss: 0.3157..  Test Loss: nan.. \n",
      "Epoch: 1237/2000..  Training Loss: 0.3157..  Test Loss: nan.. \n",
      "Epoch: 1238/2000..  Training Loss: 0.3153..  Test Loss: nan.. \n",
      "Epoch: 1239/2000..  Training Loss: 0.3151..  Test Loss: nan.. \n",
      "Epoch: 1240/2000..  Training Loss: 0.3151..  Test Loss: nan.. \n",
      "Epoch: 1241/2000..  Training Loss: 0.3148..  Test Loss: nan.. \n",
      "Epoch: 1242/2000..  Training Loss: 0.3146..  Test Loss: nan.. \n",
      "Epoch: 1243/2000..  Training Loss: 0.3145..  Test Loss: nan.. \n",
      "Epoch: 1244/2000..  Training Loss: 0.3142..  Test Loss: nan.. \n",
      "Epoch: 1245/2000..  Training Loss: 0.3142..  Test Loss: nan.. \n",
      "Epoch: 1246/2000..  Training Loss: 0.3137..  Test Loss: nan.. \n",
      "Epoch: 1247/2000..  Training Loss: 0.3140..  Test Loss: nan.. \n",
      "Epoch: 1248/2000..  Training Loss: 0.3135..  Test Loss: nan.. \n",
      "Epoch: 1249/2000..  Training Loss: 0.3135..  Test Loss: nan.. \n",
      "Epoch: 1250/2000..  Training Loss: 0.3135..  Test Loss: nan.. \n",
      "Epoch: 1251/2000..  Training Loss: 0.3135..  Test Loss: nan.. \n",
      "Epoch: 1252/2000..  Training Loss: 0.3130..  Test Loss: nan.. \n",
      "Epoch: 1253/2000..  Training Loss: 0.3133..  Test Loss: nan.. \n",
      "Epoch: 1254/2000..  Training Loss: 0.3131..  Test Loss: nan.. \n",
      "Epoch: 1255/2000..  Training Loss: 0.3130..  Test Loss: nan.. \n",
      "Epoch: 1256/2000..  Training Loss: 0.3130..  Test Loss: nan.. \n",
      "Epoch: 1257/2000..  Training Loss: 0.3130..  Test Loss: nan.. \n",
      "Epoch: 1258/2000..  Training Loss: 0.3128..  Test Loss: nan.. \n",
      "Epoch: 1259/2000..  Training Loss: 0.3130..  Test Loss: nan.. \n",
      "Epoch: 1260/2000..  Training Loss: 0.3128..  Test Loss: nan.. \n",
      "Epoch: 1261/2000..  Training Loss: 0.3130..  Test Loss: nan.. \n",
      "Epoch: 1262/2000..  Training Loss: 0.3128..  Test Loss: nan.. \n",
      "Epoch: 1263/2000..  Training Loss: 0.3130..  Test Loss: nan.. \n",
      "Epoch: 1264/2000..  Training Loss: 0.3128..  Test Loss: nan.. \n",
      "Epoch: 1265/2000..  Training Loss: 0.3130..  Test Loss: nan.. \n",
      "Epoch: 1266/2000..  Training Loss: 0.3130..  Test Loss: nan.. \n",
      "Epoch: 1267/2000..  Training Loss: 0.3129..  Test Loss: nan.. \n",
      "Epoch: 1268/2000..  Training Loss: 0.3126..  Test Loss: nan.. \n",
      "Epoch: 1269/2000..  Training Loss: 0.3125..  Test Loss: nan.. \n",
      "Epoch: 1270/2000..  Training Loss: 0.3119..  Test Loss: nan.. \n",
      "Epoch: 1271/2000..  Training Loss: 0.3121..  Test Loss: nan.. \n",
      "Epoch: 1272/2000..  Training Loss: 0.3116..  Test Loss: nan.. \n",
      "Epoch: 1273/2000..  Training Loss: 0.3115..  Test Loss: nan.. \n",
      "Epoch: 1274/2000..  Training Loss: 0.3112..  Test Loss: nan.. \n",
      "Epoch: 1275/2000..  Training Loss: 0.3111..  Test Loss: nan.. \n",
      "Epoch: 1276/2000..  Training Loss: 0.3108..  Test Loss: nan.. \n",
      "Epoch: 1277/2000..  Training Loss: 0.3106..  Test Loss: nan.. \n",
      "Epoch: 1278/2000..  Training Loss: 0.3102..  Test Loss: nan.. \n",
      "Epoch: 1279/2000..  Training Loss: 0.3102..  Test Loss: nan.. \n",
      "Epoch: 1280/2000..  Training Loss: 0.3095..  Test Loss: nan.. \n",
      "Epoch: 1281/2000..  Training Loss: 0.3098..  Test Loss: nan.. \n",
      "Epoch: 1282/2000..  Training Loss: 0.3092..  Test Loss: nan.. \n",
      "Epoch: 1283/2000..  Training Loss: 0.3092..  Test Loss: nan.. \n",
      "Epoch: 1284/2000..  Training Loss: 0.3087..  Test Loss: nan.. \n",
      "Epoch: 1285/2000..  Training Loss: 0.3086..  Test Loss: nan.. \n",
      "Epoch: 1286/2000..  Training Loss: 0.3084..  Test Loss: nan.. \n",
      "Epoch: 1287/2000..  Training Loss: 0.3083..  Test Loss: nan.. \n",
      "Epoch: 1288/2000..  Training Loss: 0.3079..  Test Loss: nan.. \n",
      "Epoch: 1289/2000..  Training Loss: 0.3079..  Test Loss: nan.. \n",
      "Epoch: 1290/2000..  Training Loss: 0.3075..  Test Loss: nan.. \n",
      "Epoch: 1291/2000..  Training Loss: 0.3075..  Test Loss: nan.. \n",
      "Epoch: 1292/2000..  Training Loss: 0.3070..  Test Loss: nan.. \n",
      "Epoch: 1293/2000..  Training Loss: 0.3070..  Test Loss: nan.. \n",
      "Epoch: 1294/2000..  Training Loss: 0.3070..  Test Loss: nan.. \n",
      "Epoch: 1295/2000..  Training Loss: 0.3069..  Test Loss: nan.. \n",
      "Epoch: 1296/2000..  Training Loss: 0.3066..  Test Loss: nan.. \n",
      "Epoch: 1297/2000..  Training Loss: 0.3067..  Test Loss: nan.. \n",
      "Epoch: 1298/2000..  Training Loss: 0.3063..  Test Loss: nan.. \n",
      "Epoch: 1299/2000..  Training Loss: 0.3065..  Test Loss: nan.. \n",
      "Epoch: 1300/2000..  Training Loss: 0.3064..  Test Loss: nan.. \n",
      "Epoch: 1301/2000..  Training Loss: 0.3063..  Test Loss: nan.. \n",
      "Epoch: 1302/2000..  Training Loss: 0.3062..  Test Loss: nan.. \n",
      "Epoch: 1303/2000..  Training Loss: 0.3064..  Test Loss: nan.. \n",
      "Epoch: 1304/2000..  Training Loss: 0.3064..  Test Loss: nan.. \n",
      "Epoch: 1305/2000..  Training Loss: 0.3066..  Test Loss: nan.. \n",
      "Epoch: 1306/2000..  Training Loss: 0.3067..  Test Loss: nan.. \n",
      "Epoch: 1307/2000..  Training Loss: 0.3068..  Test Loss: nan.. \n",
      "Epoch: 1308/2000..  Training Loss: 0.3071..  Test Loss: nan.. \n",
      "Epoch: 1309/2000..  Training Loss: 0.3075..  Test Loss: nan.. \n",
      "Epoch: 1310/2000..  Training Loss: 0.3077..  Test Loss: nan.. \n",
      "Epoch: 1311/2000..  Training Loss: 0.3084..  Test Loss: nan.. \n",
      "Epoch: 1312/2000..  Training Loss: 0.3078..  Test Loss: nan.. \n",
      "Epoch: 1313/2000..  Training Loss: 0.3080..  Test Loss: 0.3633.. \n",
      "Epoch: 1314/2000..  Training Loss: 0.3071..  Test Loss: 0.3632.. \n",
      "Epoch: 1315/2000..  Training Loss: 0.3072..  Test Loss: 0.3630.. \n",
      "Epoch: 1316/2000..  Training Loss: 0.3064..  Test Loss: 0.3628.. \n",
      "Epoch: 1317/2000..  Training Loss: 0.3063..  Test Loss: 0.3630.. \n",
      "Epoch: 1318/2000..  Training Loss: 0.3057..  Test Loss: 0.3631.. \n",
      "Epoch: 1319/2000..  Training Loss: 0.3058..  Test Loss: 0.3646.. \n",
      "Epoch: 1320/2000..  Training Loss: 0.3053..  Test Loss: 0.3662.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1321/2000..  Training Loss: 0.3054..  Test Loss: nan.. \n",
      "Epoch: 1322/2000..  Training Loss: 0.3047..  Test Loss: nan.. \n",
      "Epoch: 1323/2000..  Training Loss: 0.3048..  Test Loss: nan.. \n",
      "Epoch: 1324/2000..  Training Loss: 0.3041..  Test Loss: nan.. \n",
      "Epoch: 1325/2000..  Training Loss: 0.3044..  Test Loss: nan.. \n",
      "Epoch: 1326/2000..  Training Loss: 0.3037..  Test Loss: nan.. \n",
      "Epoch: 1327/2000..  Training Loss: 0.3038..  Test Loss: nan.. \n",
      "Epoch: 1328/2000..  Training Loss: 0.3030..  Test Loss: nan.. \n",
      "Epoch: 1329/2000..  Training Loss: 0.3033..  Test Loss: nan.. \n",
      "Epoch: 1330/2000..  Training Loss: 0.3029..  Test Loss: nan.. \n",
      "Epoch: 1331/2000..  Training Loss: 0.3029..  Test Loss: nan.. \n",
      "Epoch: 1332/2000..  Training Loss: 0.3024..  Test Loss: nan.. \n",
      "Epoch: 1333/2000..  Training Loss: 0.3021..  Test Loss: nan.. \n",
      "Epoch: 1334/2000..  Training Loss: 0.3020..  Test Loss: nan.. \n",
      "Epoch: 1335/2000..  Training Loss: 0.3016..  Test Loss: nan.. \n",
      "Epoch: 1336/2000..  Training Loss: 0.3017..  Test Loss: nan.. \n",
      "Epoch: 1337/2000..  Training Loss: 0.3012..  Test Loss: nan.. \n",
      "Epoch: 1338/2000..  Training Loss: 0.3012..  Test Loss: nan.. \n",
      "Epoch: 1339/2000..  Training Loss: 0.3010..  Test Loss: nan.. \n",
      "Epoch: 1340/2000..  Training Loss: 0.3007..  Test Loss: nan.. \n",
      "Epoch: 1341/2000..  Training Loss: 0.3006..  Test Loss: nan.. \n",
      "Epoch: 1342/2000..  Training Loss: 0.3003..  Test Loss: nan.. \n",
      "Epoch: 1343/2000..  Training Loss: 0.3003..  Test Loss: nan.. \n",
      "Epoch: 1344/2000..  Training Loss: 0.3000..  Test Loss: nan.. \n",
      "Epoch: 1345/2000..  Training Loss: 0.2998..  Test Loss: nan.. \n",
      "Epoch: 1346/2000..  Training Loss: 0.2997..  Test Loss: nan.. \n",
      "Epoch: 1347/2000..  Training Loss: 0.2994..  Test Loss: nan.. \n",
      "Epoch: 1348/2000..  Training Loss: 0.2992..  Test Loss: nan.. \n",
      "Epoch: 1349/2000..  Training Loss: 0.2994..  Test Loss: nan.. \n",
      "Epoch: 1350/2000..  Training Loss: 0.2989..  Test Loss: nan.. \n",
      "Epoch: 1351/2000..  Training Loss: 0.2990..  Test Loss: nan.. \n",
      "Epoch: 1352/2000..  Training Loss: 0.2988..  Test Loss: nan.. \n",
      "Epoch: 1353/2000..  Training Loss: 0.2988..  Test Loss: nan.. \n",
      "Epoch: 1354/2000..  Training Loss: 0.2984..  Test Loss: nan.. \n",
      "Epoch: 1355/2000..  Training Loss: 0.2987..  Test Loss: nan.. \n",
      "Epoch: 1356/2000..  Training Loss: 0.2981..  Test Loss: nan.. \n",
      "Epoch: 1357/2000..  Training Loss: 0.2983..  Test Loss: nan.. \n",
      "Epoch: 1358/2000..  Training Loss: 0.2981..  Test Loss: nan.. \n",
      "Epoch: 1359/2000..  Training Loss: 0.2980..  Test Loss: nan.. \n",
      "Epoch: 1360/2000..  Training Loss: 0.2977..  Test Loss: nan.. \n",
      "Epoch: 1361/2000..  Training Loss: 0.2975..  Test Loss: nan.. \n",
      "Epoch: 1362/2000..  Training Loss: 0.2975..  Test Loss: nan.. \n",
      "Epoch: 1363/2000..  Training Loss: 0.2974..  Test Loss: nan.. \n",
      "Epoch: 1364/2000..  Training Loss: 0.2973..  Test Loss: nan.. \n",
      "Epoch: 1365/2000..  Training Loss: 0.2975..  Test Loss: nan.. \n",
      "Epoch: 1366/2000..  Training Loss: 0.2972..  Test Loss: nan.. \n",
      "Epoch: 1367/2000..  Training Loss: 0.2976..  Test Loss: nan.. \n",
      "Epoch: 1368/2000..  Training Loss: 0.2974..  Test Loss: nan.. \n",
      "Epoch: 1369/2000..  Training Loss: 0.2981..  Test Loss: nan.. \n",
      "Epoch: 1370/2000..  Training Loss: 0.2982..  Test Loss: nan.. \n",
      "Epoch: 1371/2000..  Training Loss: 0.2996..  Test Loss: nan.. \n",
      "Epoch: 1372/2000..  Training Loss: 0.3009..  Test Loss: nan.. \n",
      "Epoch: 1373/2000..  Training Loss: 0.3031..  Test Loss: nan.. \n",
      "Epoch: 1374/2000..  Training Loss: 0.3027..  Test Loss: nan.. \n",
      "Epoch: 1375/2000..  Training Loss: 0.3016..  Test Loss: nan.. \n",
      "Epoch: 1376/2000..  Training Loss: 0.3007..  Test Loss: nan.. \n",
      "Epoch: 1377/2000..  Training Loss: 0.3002..  Test Loss: nan.. \n",
      "Epoch: 1378/2000..  Training Loss: 0.2997..  Test Loss: nan.. \n",
      "Epoch: 1379/2000..  Training Loss: 0.2994..  Test Loss: nan.. \n",
      "Epoch: 1380/2000..  Training Loss: 0.2993..  Test Loss: nan.. \n",
      "Epoch: 1381/2000..  Training Loss: 0.2990..  Test Loss: nan.. \n",
      "Epoch: 1382/2000..  Training Loss: 0.2989..  Test Loss: nan.. \n",
      "Epoch: 1383/2000..  Training Loss: 0.2988..  Test Loss: nan.. \n",
      "Epoch: 1384/2000..  Training Loss: 0.2985..  Test Loss: nan.. \n",
      "Epoch: 1385/2000..  Training Loss: 0.2983..  Test Loss: nan.. \n",
      "Epoch: 1386/2000..  Training Loss: 0.2981..  Test Loss: nan.. \n",
      "Epoch: 1387/2000..  Training Loss: 0.2978..  Test Loss: nan.. \n",
      "Epoch: 1388/2000..  Training Loss: 0.2980..  Test Loss: nan.. \n",
      "Epoch: 1389/2000..  Training Loss: 0.2977..  Test Loss: nan.. \n",
      "Epoch: 1390/2000..  Training Loss: 0.2974..  Test Loss: nan.. \n",
      "Epoch: 1391/2000..  Training Loss: 0.2973..  Test Loss: nan.. \n",
      "Epoch: 1392/2000..  Training Loss: 0.2972..  Test Loss: nan.. \n",
      "Epoch: 1393/2000..  Training Loss: 0.2971..  Test Loss: nan.. \n",
      "Epoch: 1394/2000..  Training Loss: 0.2969..  Test Loss: nan.. \n",
      "Epoch: 1395/2000..  Training Loss: 0.2968..  Test Loss: nan.. \n",
      "Epoch: 1396/2000..  Training Loss: 0.2967..  Test Loss: nan.. \n",
      "Epoch: 1397/2000..  Training Loss: 0.2963..  Test Loss: nan.. \n",
      "Epoch: 1398/2000..  Training Loss: 0.2966..  Test Loss: nan.. \n",
      "Epoch: 1399/2000..  Training Loss: 0.2961..  Test Loss: nan.. \n",
      "Epoch: 1400/2000..  Training Loss: 0.2964..  Test Loss: nan.. \n",
      "Epoch: 1401/2000..  Training Loss: 0.2959..  Test Loss: nan.. \n",
      "Epoch: 1402/2000..  Training Loss: 0.2961..  Test Loss: nan.. \n",
      "Epoch: 1403/2000..  Training Loss: 0.2957..  Test Loss: nan.. \n",
      "Epoch: 1404/2000..  Training Loss: 0.2959..  Test Loss: nan.. \n",
      "Epoch: 1405/2000..  Training Loss: 0.2953..  Test Loss: nan.. \n",
      "Epoch: 1406/2000..  Training Loss: 0.2952..  Test Loss: nan.. \n",
      "Epoch: 1407/2000..  Training Loss: 0.2952..  Test Loss: nan.. \n",
      "Epoch: 1408/2000..  Training Loss: 0.2950..  Test Loss: nan.. \n",
      "Epoch: 1409/2000..  Training Loss: 0.2950..  Test Loss: nan.. \n",
      "Epoch: 1410/2000..  Training Loss: 0.2952..  Test Loss: nan.. \n",
      "Epoch: 1411/2000..  Training Loss: 0.2948..  Test Loss: nan.. \n",
      "Epoch: 1412/2000..  Training Loss: 0.2949..  Test Loss: nan.. \n",
      "Epoch: 1413/2000..  Training Loss: 0.2948..  Test Loss: nan.. \n",
      "Epoch: 1414/2000..  Training Loss: 0.2949..  Test Loss: nan.. \n",
      "Epoch: 1415/2000..  Training Loss: 0.2947..  Test Loss: nan.. \n",
      "Epoch: 1416/2000..  Training Loss: 0.2946..  Test Loss: nan.. \n",
      "Epoch: 1417/2000..  Training Loss: 0.2946..  Test Loss: nan.. \n",
      "Epoch: 1418/2000..  Training Loss: 0.2948..  Test Loss: nan.. \n",
      "Epoch: 1419/2000..  Training Loss: 0.2949..  Test Loss: nan.. \n",
      "Epoch: 1420/2000..  Training Loss: 0.2948..  Test Loss: nan.. \n",
      "Epoch: 1421/2000..  Training Loss: 0.2948..  Test Loss: nan.. \n",
      "Epoch: 1422/2000..  Training Loss: 0.2949..  Test Loss: nan.. \n",
      "Epoch: 1423/2000..  Training Loss: 0.2950..  Test Loss: nan.. \n",
      "Epoch: 1424/2000..  Training Loss: 0.2949..  Test Loss: nan.. \n",
      "Epoch: 1425/2000..  Training Loss: 0.2948..  Test Loss: nan.. \n",
      "Epoch: 1426/2000..  Training Loss: 0.2947..  Test Loss: nan.. \n",
      "Epoch: 1427/2000..  Training Loss: 0.2945..  Test Loss: nan.. \n",
      "Epoch: 1428/2000..  Training Loss: 0.2944..  Test Loss: nan.. \n",
      "Epoch: 1429/2000..  Training Loss: 0.2941..  Test Loss: nan.. \n",
      "Epoch: 1430/2000..  Training Loss: 0.2939..  Test Loss: nan.. \n",
      "Epoch: 1431/2000..  Training Loss: 0.2935..  Test Loss: nan.. \n",
      "Epoch: 1432/2000..  Training Loss: 0.2934..  Test Loss: nan.. \n",
      "Epoch: 1433/2000..  Training Loss: 0.2932..  Test Loss: nan.. \n",
      "Epoch: 1434/2000..  Training Loss: 0.2928..  Test Loss: nan.. \n",
      "Epoch: 1435/2000..  Training Loss: 0.2924..  Test Loss: nan.. \n",
      "Epoch: 1436/2000..  Training Loss: 0.2925..  Test Loss: nan.. \n",
      "Epoch: 1437/2000..  Training Loss: 0.2918..  Test Loss: nan.. \n",
      "Epoch: 1438/2000..  Training Loss: 0.2918..  Test Loss: nan.. \n",
      "Epoch: 1439/2000..  Training Loss: 0.2914..  Test Loss: nan.. \n",
      "Epoch: 1440/2000..  Training Loss: 0.2912..  Test Loss: nan.. \n",
      "Epoch: 1441/2000..  Training Loss: 0.2907..  Test Loss: nan.. \n",
      "Epoch: 1442/2000..  Training Loss: 0.2905..  Test Loss: nan.. \n",
      "Epoch: 1443/2000..  Training Loss: 0.2901..  Test Loss: nan.. \n",
      "Epoch: 1444/2000..  Training Loss: 0.2900..  Test Loss: nan.. \n",
      "Epoch: 1445/2000..  Training Loss: 0.2896..  Test Loss: nan.. \n",
      "Epoch: 1446/2000..  Training Loss: 0.2896..  Test Loss: nan.. \n",
      "Epoch: 1447/2000..  Training Loss: 0.2893..  Test Loss: nan.. \n",
      "Epoch: 1448/2000..  Training Loss: 0.2890..  Test Loss: nan.. \n",
      "Epoch: 1449/2000..  Training Loss: 0.2888..  Test Loss: nan.. \n",
      "Epoch: 1450/2000..  Training Loss: 0.2888..  Test Loss: nan.. \n",
      "Epoch: 1451/2000..  Training Loss: 0.2885..  Test Loss: nan.. \n",
      "Epoch: 1452/2000..  Training Loss: 0.2886..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1453/2000..  Training Loss: 0.2882..  Test Loss: nan.. \n",
      "Epoch: 1454/2000..  Training Loss: 0.2878..  Test Loss: nan.. \n",
      "Epoch: 1455/2000..  Training Loss: 0.2877..  Test Loss: nan.. \n",
      "Epoch: 1456/2000..  Training Loss: 0.2875..  Test Loss: nan.. \n",
      "Epoch: 1457/2000..  Training Loss: 0.2875..  Test Loss: nan.. \n",
      "Epoch: 1458/2000..  Training Loss: 0.2873..  Test Loss: nan.. \n",
      "Epoch: 1459/2000..  Training Loss: 0.2870..  Test Loss: nan.. \n",
      "Epoch: 1460/2000..  Training Loss: 0.2868..  Test Loss: nan.. \n",
      "Epoch: 1461/2000..  Training Loss: 0.2867..  Test Loss: nan.. \n",
      "Epoch: 1462/2000..  Training Loss: 0.2866..  Test Loss: nan.. \n",
      "Epoch: 1463/2000..  Training Loss: 0.2864..  Test Loss: nan.. \n",
      "Epoch: 1464/2000..  Training Loss: 0.2864..  Test Loss: nan.. \n",
      "Epoch: 1465/2000..  Training Loss: 0.2863..  Test Loss: nan.. \n",
      "Epoch: 1466/2000..  Training Loss: 0.2860..  Test Loss: nan.. \n",
      "Epoch: 1467/2000..  Training Loss: 0.2859..  Test Loss: nan.. \n",
      "Epoch: 1468/2000..  Training Loss: 0.2857..  Test Loss: nan.. \n",
      "Epoch: 1469/2000..  Training Loss: 0.2855..  Test Loss: nan.. \n",
      "Epoch: 1470/2000..  Training Loss: 0.2854..  Test Loss: nan.. \n",
      "Epoch: 1471/2000..  Training Loss: 0.2853..  Test Loss: nan.. \n",
      "Epoch: 1472/2000..  Training Loss: 0.2852..  Test Loss: nan.. \n",
      "Epoch: 1473/2000..  Training Loss: 0.2851..  Test Loss: nan.. \n",
      "Epoch: 1474/2000..  Training Loss: 0.2850..  Test Loss: nan.. \n",
      "Epoch: 1475/2000..  Training Loss: 0.2849..  Test Loss: nan.. \n",
      "Epoch: 1476/2000..  Training Loss: 0.2849..  Test Loss: nan.. \n",
      "Epoch: 1477/2000..  Training Loss: 0.2846..  Test Loss: nan.. \n",
      "Epoch: 1478/2000..  Training Loss: 0.2847..  Test Loss: nan.. \n",
      "Epoch: 1479/2000..  Training Loss: 0.2846..  Test Loss: nan.. \n",
      "Epoch: 1480/2000..  Training Loss: 0.2845..  Test Loss: nan.. \n",
      "Epoch: 1481/2000..  Training Loss: 0.2841..  Test Loss: nan.. \n",
      "Epoch: 1482/2000..  Training Loss: 0.2841..  Test Loss: nan.. \n",
      "Epoch: 1483/2000..  Training Loss: 0.2839..  Test Loss: nan.. \n",
      "Epoch: 1484/2000..  Training Loss: 0.2838..  Test Loss: nan.. \n",
      "Epoch: 1485/2000..  Training Loss: 0.2836..  Test Loss: nan.. \n",
      "Epoch: 1486/2000..  Training Loss: 0.2836..  Test Loss: nan.. \n",
      "Epoch: 1487/2000..  Training Loss: 0.2835..  Test Loss: nan.. \n",
      "Epoch: 1488/2000..  Training Loss: 0.2834..  Test Loss: nan.. \n",
      "Epoch: 1489/2000..  Training Loss: 0.2833..  Test Loss: nan.. \n",
      "Epoch: 1490/2000..  Training Loss: 0.2833..  Test Loss: nan.. \n",
      "Epoch: 1491/2000..  Training Loss: 0.2831..  Test Loss: nan.. \n",
      "Epoch: 1492/2000..  Training Loss: 0.2830..  Test Loss: nan.. \n",
      "Epoch: 1493/2000..  Training Loss: 0.2829..  Test Loss: nan.. \n",
      "Epoch: 1494/2000..  Training Loss: 0.2828..  Test Loss: nan.. \n",
      "Epoch: 1495/2000..  Training Loss: 0.2827..  Test Loss: nan.. \n",
      "Epoch: 1496/2000..  Training Loss: 0.2826..  Test Loss: nan.. \n",
      "Epoch: 1497/2000..  Training Loss: 0.2824..  Test Loss: nan.. \n",
      "Epoch: 1498/2000..  Training Loss: 0.2824..  Test Loss: nan.. \n",
      "Epoch: 1499/2000..  Training Loss: 0.2823..  Test Loss: nan.. \n",
      "Epoch: 1500/2000..  Training Loss: 0.2822..  Test Loss: nan.. \n",
      "Epoch: 1501/2000..  Training Loss: 0.2822..  Test Loss: nan.. \n",
      "Epoch: 1502/2000..  Training Loss: 0.2821..  Test Loss: nan.. \n",
      "Epoch: 1503/2000..  Training Loss: 0.2818..  Test Loss: nan.. \n",
      "Epoch: 1504/2000..  Training Loss: 0.2820..  Test Loss: nan.. \n",
      "Epoch: 1505/2000..  Training Loss: 0.2817..  Test Loss: nan.. \n",
      "Epoch: 1506/2000..  Training Loss: 0.2816..  Test Loss: nan.. \n",
      "Epoch: 1507/2000..  Training Loss: 0.2814..  Test Loss: nan.. \n",
      "Epoch: 1508/2000..  Training Loss: 0.2815..  Test Loss: nan.. \n",
      "Epoch: 1509/2000..  Training Loss: 0.2812..  Test Loss: nan.. \n",
      "Epoch: 1510/2000..  Training Loss: 0.2811..  Test Loss: nan.. \n",
      "Epoch: 1511/2000..  Training Loss: 0.2810..  Test Loss: nan.. \n",
      "Epoch: 1512/2000..  Training Loss: 0.2807..  Test Loss: nan.. \n",
      "Epoch: 1513/2000..  Training Loss: 0.2806..  Test Loss: nan.. \n",
      "Epoch: 1514/2000..  Training Loss: 0.2806..  Test Loss: nan.. \n",
      "Epoch: 1515/2000..  Training Loss: 0.2805..  Test Loss: nan.. \n",
      "Epoch: 1516/2000..  Training Loss: 0.2805..  Test Loss: nan.. \n",
      "Epoch: 1517/2000..  Training Loss: 0.2805..  Test Loss: nan.. \n",
      "Epoch: 1518/2000..  Training Loss: 0.2802..  Test Loss: nan.. \n",
      "Epoch: 1519/2000..  Training Loss: 0.2802..  Test Loss: nan.. \n",
      "Epoch: 1520/2000..  Training Loss: 0.2800..  Test Loss: nan.. \n",
      "Epoch: 1521/2000..  Training Loss: 0.2801..  Test Loss: nan.. \n",
      "Epoch: 1522/2000..  Training Loss: 0.2798..  Test Loss: nan.. \n",
      "Epoch: 1523/2000..  Training Loss: 0.2800..  Test Loss: nan.. \n",
      "Epoch: 1524/2000..  Training Loss: 0.2797..  Test Loss: nan.. \n",
      "Epoch: 1525/2000..  Training Loss: 0.2796..  Test Loss: nan.. \n",
      "Epoch: 1526/2000..  Training Loss: 0.2796..  Test Loss: nan.. \n",
      "Epoch: 1527/2000..  Training Loss: 0.2794..  Test Loss: nan.. \n",
      "Epoch: 1528/2000..  Training Loss: 0.2792..  Test Loss: nan.. \n",
      "Epoch: 1529/2000..  Training Loss: 0.2790..  Test Loss: nan.. \n",
      "Epoch: 1530/2000..  Training Loss: 0.2789..  Test Loss: nan.. \n",
      "Epoch: 1531/2000..  Training Loss: 0.2787..  Test Loss: nan.. \n",
      "Epoch: 1532/2000..  Training Loss: 0.2787..  Test Loss: nan.. \n",
      "Epoch: 1533/2000..  Training Loss: 0.2784..  Test Loss: nan.. \n",
      "Epoch: 1534/2000..  Training Loss: 0.2786..  Test Loss: nan.. \n",
      "Epoch: 1535/2000..  Training Loss: 0.2784..  Test Loss: nan.. \n",
      "Epoch: 1536/2000..  Training Loss: 0.2784..  Test Loss: nan.. \n",
      "Epoch: 1537/2000..  Training Loss: 0.2782..  Test Loss: nan.. \n",
      "Epoch: 1538/2000..  Training Loss: 0.2783..  Test Loss: nan.. \n",
      "Epoch: 1539/2000..  Training Loss: 0.2779..  Test Loss: nan.. \n",
      "Epoch: 1540/2000..  Training Loss: 0.2781..  Test Loss: nan.. \n",
      "Epoch: 1541/2000..  Training Loss: 0.2780..  Test Loss: nan.. \n",
      "Epoch: 1542/2000..  Training Loss: 0.2778..  Test Loss: nan.. \n",
      "Epoch: 1543/2000..  Training Loss: 0.2777..  Test Loss: nan.. \n",
      "Epoch: 1544/2000..  Training Loss: 0.2777..  Test Loss: nan.. \n",
      "Epoch: 1545/2000..  Training Loss: 0.2776..  Test Loss: nan.. \n",
      "Epoch: 1546/2000..  Training Loss: 0.2776..  Test Loss: nan.. \n",
      "Epoch: 1547/2000..  Training Loss: 0.2775..  Test Loss: nan.. \n",
      "Epoch: 1548/2000..  Training Loss: 0.2773..  Test Loss: nan.. \n",
      "Epoch: 1549/2000..  Training Loss: 0.2774..  Test Loss: nan.. \n",
      "Epoch: 1550/2000..  Training Loss: 0.2772..  Test Loss: nan.. \n",
      "Epoch: 1551/2000..  Training Loss: 0.2772..  Test Loss: nan.. \n",
      "Epoch: 1552/2000..  Training Loss: 0.2768..  Test Loss: nan.. \n",
      "Epoch: 1553/2000..  Training Loss: 0.2771..  Test Loss: nan.. \n",
      "Epoch: 1554/2000..  Training Loss: 0.2769..  Test Loss: nan.. \n",
      "Epoch: 1555/2000..  Training Loss: 0.2766..  Test Loss: nan.. \n",
      "Epoch: 1556/2000..  Training Loss: 0.2766..  Test Loss: nan.. \n",
      "Epoch: 1557/2000..  Training Loss: 0.2764..  Test Loss: nan.. \n",
      "Epoch: 1558/2000..  Training Loss: 0.2766..  Test Loss: nan.. \n",
      "Epoch: 1559/2000..  Training Loss: 0.2761..  Test Loss: nan.. \n",
      "Epoch: 1560/2000..  Training Loss: 0.2763..  Test Loss: nan.. \n",
      "Epoch: 1561/2000..  Training Loss: 0.2760..  Test Loss: nan.. \n",
      "Epoch: 1562/2000..  Training Loss: 0.2760..  Test Loss: nan.. \n",
      "Epoch: 1563/2000..  Training Loss: 0.2760..  Test Loss: nan.. \n",
      "Epoch: 1564/2000..  Training Loss: 0.2759..  Test Loss: nan.. \n",
      "Epoch: 1565/2000..  Training Loss: 0.2759..  Test Loss: nan.. \n",
      "Epoch: 1566/2000..  Training Loss: 0.2757..  Test Loss: nan.. \n",
      "Epoch: 1567/2000..  Training Loss: 0.2755..  Test Loss: nan.. \n",
      "Epoch: 1568/2000..  Training Loss: 0.2755..  Test Loss: nan.. \n",
      "Epoch: 1569/2000..  Training Loss: 0.2753..  Test Loss: nan.. \n",
      "Epoch: 1570/2000..  Training Loss: 0.2752..  Test Loss: nan.. \n",
      "Epoch: 1571/2000..  Training Loss: 0.2750..  Test Loss: nan.. \n",
      "Epoch: 1572/2000..  Training Loss: 0.2751..  Test Loss: nan.. \n",
      "Epoch: 1573/2000..  Training Loss: 0.2748..  Test Loss: nan.. \n",
      "Epoch: 1574/2000..  Training Loss: 0.2748..  Test Loss: nan.. \n",
      "Epoch: 1575/2000..  Training Loss: 0.2747..  Test Loss: nan.. \n",
      "Epoch: 1576/2000..  Training Loss: 0.2746..  Test Loss: nan.. \n",
      "Epoch: 1577/2000..  Training Loss: 0.2746..  Test Loss: nan.. \n",
      "Epoch: 1578/2000..  Training Loss: 0.2744..  Test Loss: nan.. \n",
      "Epoch: 1579/2000..  Training Loss: 0.2742..  Test Loss: nan.. \n",
      "Epoch: 1580/2000..  Training Loss: 0.2741..  Test Loss: nan.. \n",
      "Epoch: 1581/2000..  Training Loss: 0.2739..  Test Loss: nan.. \n",
      "Epoch: 1582/2000..  Training Loss: 0.2738..  Test Loss: nan.. \n",
      "Epoch: 1583/2000..  Training Loss: 0.2737..  Test Loss: nan.. \n",
      "Epoch: 1584/2000..  Training Loss: 0.2738..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1585/2000..  Training Loss: 0.2735..  Test Loss: nan.. \n",
      "Epoch: 1586/2000..  Training Loss: 0.2735..  Test Loss: nan.. \n",
      "Epoch: 1587/2000..  Training Loss: 0.2733..  Test Loss: nan.. \n",
      "Epoch: 1588/2000..  Training Loss: 0.2734..  Test Loss: nan.. \n",
      "Epoch: 1589/2000..  Training Loss: 0.2733..  Test Loss: nan.. \n",
      "Epoch: 1590/2000..  Training Loss: 0.2733..  Test Loss: nan.. \n",
      "Epoch: 1591/2000..  Training Loss: 0.2732..  Test Loss: nan.. \n",
      "Epoch: 1592/2000..  Training Loss: 0.2732..  Test Loss: nan.. \n",
      "Epoch: 1593/2000..  Training Loss: 0.2729..  Test Loss: nan.. \n",
      "Epoch: 1594/2000..  Training Loss: 0.2728..  Test Loss: nan.. \n",
      "Epoch: 1595/2000..  Training Loss: 0.2727..  Test Loss: nan.. \n",
      "Epoch: 1596/2000..  Training Loss: 0.2725..  Test Loss: nan.. \n",
      "Epoch: 1597/2000..  Training Loss: 0.2724..  Test Loss: nan.. \n",
      "Epoch: 1598/2000..  Training Loss: 0.2725..  Test Loss: nan.. \n",
      "Epoch: 1599/2000..  Training Loss: 0.2723..  Test Loss: nan.. \n",
      "Epoch: 1600/2000..  Training Loss: 0.2721..  Test Loss: nan.. \n",
      "Epoch: 1601/2000..  Training Loss: 0.2721..  Test Loss: nan.. \n",
      "Epoch: 1602/2000..  Training Loss: 0.2721..  Test Loss: nan.. \n",
      "Epoch: 1603/2000..  Training Loss: 0.2721..  Test Loss: nan.. \n",
      "Epoch: 1604/2000..  Training Loss: 0.2719..  Test Loss: nan.. \n",
      "Epoch: 1605/2000..  Training Loss: 0.2717..  Test Loss: nan.. \n",
      "Epoch: 1606/2000..  Training Loss: 0.2717..  Test Loss: nan.. \n",
      "Epoch: 1607/2000..  Training Loss: 0.2715..  Test Loss: nan.. \n",
      "Epoch: 1608/2000..  Training Loss: 0.2714..  Test Loss: nan.. \n",
      "Epoch: 1609/2000..  Training Loss: 0.2712..  Test Loss: nan.. \n",
      "Epoch: 1610/2000..  Training Loss: 0.2711..  Test Loss: nan.. \n",
      "Epoch: 1611/2000..  Training Loss: 0.2710..  Test Loss: nan.. \n",
      "Epoch: 1612/2000..  Training Loss: 0.2710..  Test Loss: nan.. \n",
      "Epoch: 1613/2000..  Training Loss: 0.2710..  Test Loss: nan.. \n",
      "Epoch: 1614/2000..  Training Loss: 0.2709..  Test Loss: nan.. \n",
      "Epoch: 1615/2000..  Training Loss: 0.2708..  Test Loss: nan.. \n",
      "Epoch: 1616/2000..  Training Loss: 0.2708..  Test Loss: nan.. \n",
      "Epoch: 1617/2000..  Training Loss: 0.2707..  Test Loss: nan.. \n",
      "Epoch: 1618/2000..  Training Loss: 0.2705..  Test Loss: nan.. \n",
      "Epoch: 1619/2000..  Training Loss: 0.2706..  Test Loss: nan.. \n",
      "Epoch: 1620/2000..  Training Loss: 0.2704..  Test Loss: nan.. \n",
      "Epoch: 1621/2000..  Training Loss: 0.2703..  Test Loss: nan.. \n",
      "Epoch: 1622/2000..  Training Loss: 0.2701..  Test Loss: nan.. \n",
      "Epoch: 1623/2000..  Training Loss: 0.2701..  Test Loss: nan.. \n",
      "Epoch: 1624/2000..  Training Loss: 0.2699..  Test Loss: nan.. \n",
      "Epoch: 1625/2000..  Training Loss: 0.2700..  Test Loss: nan.. \n",
      "Epoch: 1626/2000..  Training Loss: 0.2698..  Test Loss: nan.. \n",
      "Epoch: 1627/2000..  Training Loss: 0.2698..  Test Loss: nan.. \n",
      "Epoch: 1628/2000..  Training Loss: 0.2695..  Test Loss: nan.. \n"
     ]
    }
   ],
   "source": [
    "model = Regressor().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.001)\n",
    "\n",
    "epochs = 2000\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(train_batch)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_batch[i])\n",
    "        loss = torch.sqrt(criterion(torch.log(output), torch.log(label_batch[i])))\n",
    "        #loss = criterion(output, label_batch[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = model(X_val)\n",
    "            test_loss += torch.sqrt(criterion(torch.log(predictions), torch.log(y_val)))\n",
    "            #test_loss = criterion(predictions, y_val)\n",
    "                \n",
    "        train_losses.append(train_loss/len(train_batch))\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.4f}.. \".format(loss),\n",
    "              \"Test Loss: {:.4f}.. \".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe7fe2c5940>"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.from_numpy(test.values).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model.forward(test)\n",
    "\n",
    "output.shape\n",
    "output.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./dataset-0510/submit_test.csv')\n",
    "submission['total_price'] = output.cpu().numpy()\n",
    "submission.to_csv('submission/DNN_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
