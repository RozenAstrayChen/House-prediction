{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building_id\n",
    "#columns = X.columns\n",
    "data_train = pd.read_csv('./dataset-0510/train.csv')\n",
    "X_test = pd.read_csv('./dataset-0510/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "data = data_train.append(X_test, ignore_index=True, sort=False)\n",
    "data = data.drop(['building_id'], axis=1)\n",
    "print(data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# fill the nan values\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "sale_price = data['total_price'].values\n",
    "data = data.drop('total_price', axis=1)\n",
    "\n",
    "columns = data.columns\n",
    "# check has any nan value in data\n",
    "data.isnull().values.any()\n",
    "print(type(data))\n",
    "#sale_price = pd.DataFrame(sale_price, columns=['total_price'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       total_price\n",
      "0        -0.218563\n",
      "1        -0.166338\n",
      "2        -0.044276\n",
      "3         0.046433\n",
      "4        -0.216315\n",
      "5        -0.223469\n",
      "6        -0.064991\n",
      "7        -0.198430\n",
      "8        -0.211934\n",
      "9        -0.000436\n",
      "10       -0.081933\n",
      "11       -0.011663\n",
      "12       -0.168146\n",
      "13       -0.137322\n",
      "14       -0.084860\n",
      "15       -0.025807\n",
      "16       -0.175608\n",
      "17       -0.150328\n",
      "18       -0.103845\n",
      "19       -0.112993\n",
      "20        0.034442\n",
      "21       -0.126267\n",
      "22       -0.128854\n",
      "23       -0.142278\n",
      "24       -0.176988\n",
      "25       -0.121903\n",
      "26        0.803788\n",
      "27       -0.209851\n",
      "28       -0.173925\n",
      "29       -0.112993\n",
      "...            ...\n",
      "69970    -0.128856\n",
      "69971    -0.128856\n",
      "69972    -0.128856\n",
      "69973    -0.128856\n",
      "69974    -0.128856\n",
      "69975    -0.128856\n",
      "69976    -0.128856\n",
      "69977    -0.128856\n",
      "69978    -0.128856\n",
      "69979    -0.128856\n",
      "69980    -0.128856\n",
      "69981    -0.128856\n",
      "69982    -0.128856\n",
      "69983    -0.128856\n",
      "69984    -0.128856\n",
      "69985    -0.128856\n",
      "69986    -0.128856\n",
      "69987    -0.128856\n",
      "69988    -0.128856\n",
      "69989    -0.128856\n",
      "69990    -0.128856\n",
      "69991    -0.128856\n",
      "69992    -0.128856\n",
      "69993    -0.128856\n",
      "69994    -0.128856\n",
      "69995    -0.128856\n",
      "69996    -0.128856\n",
      "69997    -0.128856\n",
      "69998    -0.128856\n",
      "69999    -0.128856\n",
      "\n",
      "[70000 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/islab/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "scaling_data = x_scaler.fit_transform(data)\n",
    "scaling_sale_price = y_scaler.fit_transform(sale_price.reshape(-1, 1))\n",
    "\n",
    "data = pd.DataFrame(scaling_data, columns = columns)\n",
    "sale_price = pd.DataFrame(scaling_sale_price, columns=['total_price'])\n",
    "print(sale_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_x = data.iloc[:60000]\n",
    "test_x  = data.iloc[60000:]\n",
    "\n",
    "train_y = sale_price.iloc[:60000]\n",
    "test_y = sale_price.iloc[60000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(233, 256)\n",
    "        self.norm1 = nn.BatchNorm1d(256, momentum=0.5)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.norm2 = nn.BatchNorm1d(512, momentum=0.5)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.norm3 = nn.BatchNorm1d(512, momentum=0.5)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.norm4 = nn.BatchNorm1d(256, momentum=0.5)\n",
    "        self.fc5 = nn.Linear(256, 1)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.tanh(self.norm1(self.dropout(self.fc1(x))))\n",
    "        x = torch.tanh(self.norm2(self.dropout(self.fc2(x))))\n",
    "        x = torch.tanh(self.norm3(self.dropout(self.fc3(x))))\n",
    "        x = torch.tanh(self.norm4(self.dropout(self.fc4(x))))\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "# takes in a module and applies the specified weight initialization\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "## takes in a module and applies the specified weight initialization\n",
    "def weights_init_normal(m):\n",
    "    '''Takes in a module and initializes all linear layers with weight\n",
    "       values taken from a normal distribution.'''\n",
    "\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model\n",
    "    if classname.find('Linear') != -1:\n",
    "        y = m.in_features\n",
    "    # m.weight.data shoud be taken from a normal distribution\n",
    "        m.weight.data.normal_(0.0,1/np.sqrt(y))\n",
    "    # m.bias.data should be 0\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None: \n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training(X_train, y_train):\n",
    "    train_batch = np.array_split(X_train, 50)\n",
    "    label_batch = np.array_split(y_train, 50)\n",
    "\n",
    "    for i in range(len(train_batch)):\n",
    "        train_batch[i] = torch.from_numpy(train_batch[i].values).float().to(device)\n",
    "    for i in range(len(label_batch)):\n",
    "        label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1).to(device)\n",
    "    return train_batch, label_batch\n",
    "\n",
    "#train_batch = torch.from_numpy(X_train.values).float().to(device)\n",
    "#label_batch = torch.from_numpy(y_train).float().view(-1, 1).to(device)\n",
    "train_batch, label_batch = batch_training(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = torch.from_numpy(X_val.values).float().to(device)\n",
    "y_val = torch.from_numpy(y_val.values).float().view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Regressor().to(device)\n",
    "ps = model(train_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, data_x, data_y, pct_close):\n",
    "    # data_x and data_y are numpy array-of-arrays matrices\n",
    "    n_feat = len(data_x[0])  # number features\n",
    "    n_items = len(data_x)    # number items\n",
    "    n_correct = 0; n_wrong = 0\n",
    "    for i in range(n_items):\n",
    "        #X = torch.Tensor(data_x[i])\n",
    "        X = data_x[i].reshape(1, -1)\n",
    "        # Y = T.Tensor(data_y[i])  # not needed\n",
    "        oupt = model(X)            # Tensor\n",
    "        pred_y = oupt.cpu().item()       # scalar\n",
    "\n",
    "        if np.abs(pred_y - data_y[i].cpu()) < \\\n",
    "          np.abs(pct_close * data_y[i].cpu()):\n",
    "          n_correct += 1\n",
    "        else:\n",
    "          n_wrong += 1\n",
    "    \n",
    "    print(n_correct, n_wrong)\n",
    "    return (n_correct ) / (n_correct + n_wrong)\n",
    "\n",
    "def accuracy2(pred, y, threshold=0.5):\n",
    "    pred = pred >= threshold\n",
    "    acc = np.mean(pred == y)\n",
    "    return acc * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094 18706\n",
      "Epoch: 1/50000..  Training Loss: 1.12882506..  Test Loss: 1.19771385..  Accurary:   0.055252525252525254\n",
      "8 19792\n",
      "Epoch: 51/50000..  Training Loss: 0.80215847..  Test Loss: 9.67003632..  Accurary:   0.00040404040404040404\n",
      "18 19782\n",
      "Epoch: 101/50000..  Training Loss: 0.60831948..  Test Loss: 13.17505074..  Accurary:   0.0009090909090909091\n",
      "12 19788\n",
      "Epoch: 151/50000..  Training Loss: 0.47974061..  Test Loss: 23.88561630..  Accurary:   0.0006060606060606061\n",
      "20 19780\n",
      "Epoch: 201/50000..  Training Loss: 0.41805742..  Test Loss: 9.20618820..  Accurary:   0.00101010101010101\n",
      "14 19786\n",
      "Epoch: 251/50000..  Training Loss: 0.28930524..  Test Loss: 9.46547222..  Accurary:   0.0007070707070707071\n",
      "9 19791\n",
      "Epoch: 301/50000..  Training Loss: 0.27763356..  Test Loss: 7.44520140..  Accurary:   0.00045454545454545455\n",
      "9 19791\n",
      "Epoch: 351/50000..  Training Loss: 0.22722285..  Test Loss: 7.79245090..  Accurary:   0.00045454545454545455\n",
      "9 19791\n",
      "Epoch: 401/50000..  Training Loss: 0.26407222..  Test Loss: 5.72727013..  Accurary:   0.00045454545454545455\n",
      "10 19790\n",
      "Epoch: 451/50000..  Training Loss: 0.18230313..  Test Loss: 3.08052945..  Accurary:   0.000505050505050505\n",
      "20 19780\n",
      "Epoch: 501/50000..  Training Loss: 0.19011941..  Test Loss: 2.39395881..  Accurary:   0.00101010101010101\n",
      "32 19768\n",
      "Epoch: 551/50000..  Training Loss: 0.14965371..  Test Loss: 1.44836545..  Accurary:   0.0016161616161616162\n",
      "16 19784\n",
      "Epoch: 601/50000..  Training Loss: 0.16035838..  Test Loss: 1.54987001..  Accurary:   0.0008080808080808081\n",
      "26 19774\n",
      "Epoch: 651/50000..  Training Loss: 0.15700320..  Test Loss: 1.08233821..  Accurary:   0.0013131313131313131\n",
      "87 19713\n",
      "Epoch: 701/50000..  Training Loss: 0.20582035..  Test Loss: 0.79863918..  Accurary:   0.004393939393939394\n",
      "83 19717\n",
      "Epoch: 751/50000..  Training Loss: 0.15900995..  Test Loss: 0.71524489..  Accurary:   0.004191919191919192\n",
      "518 19282\n",
      "Epoch: 801/50000..  Training Loss: 0.15063180..  Test Loss: 0.52728105..  Accurary:   0.02616161616161616\n",
      "864 18936\n",
      "Epoch: 851/50000..  Training Loss: 0.14070501..  Test Loss: 0.49576774..  Accurary:   0.04363636363636364\n",
      "203 19597\n",
      "Epoch: 901/50000..  Training Loss: 0.12026692..  Test Loss: 0.55165547..  Accurary:   0.010252525252525253\n",
      "2514 17286\n",
      "Epoch: 951/50000..  Training Loss: 0.12442200..  Test Loss: 0.49931610..  Accurary:   0.12696969696969698\n",
      "943 18857\n",
      "Epoch: 1001/50000..  Training Loss: 0.10893336..  Test Loss: 0.43707564..  Accurary:   0.04762626262626263\n",
      "330 19470\n",
      "Epoch: 1051/50000..  Training Loss: 0.12146160..  Test Loss: 0.50159425..  Accurary:   0.016666666666666666\n",
      "944 18856\n",
      "Epoch: 1101/50000..  Training Loss: 0.10945630..  Test Loss: 0.47142005..  Accurary:   0.04767676767676768\n",
      "687 19113\n",
      "Epoch: 1151/50000..  Training Loss: 0.17526964..  Test Loss: 0.50561577..  Accurary:   0.0346969696969697\n",
      "360 19440\n",
      "Epoch: 1201/50000..  Training Loss: 0.12085474..  Test Loss: 0.48400387..  Accurary:   0.01818181818181818\n",
      "359 19441\n",
      "Epoch: 1251/50000..  Training Loss: 0.11606361..  Test Loss: 0.49397248..  Accurary:   0.01813131313131313\n",
      "825 18975\n",
      "Epoch: 1301/50000..  Training Loss: 0.15221355..  Test Loss: 0.46004304..  Accurary:   0.041666666666666664\n",
      "315 19485\n",
      "Epoch: 1351/50000..  Training Loss: 0.10059427..  Test Loss: 0.45119467..  Accurary:   0.015909090909090907\n",
      "1305 18495\n",
      "Epoch: 1401/50000..  Training Loss: 0.11251808..  Test Loss: 0.45498157..  Accurary:   0.0659090909090909\n",
      "421 19379\n",
      "Epoch: 1451/50000..  Training Loss: 0.11572491..  Test Loss: 0.45377263..  Accurary:   0.021262626262626264\n",
      "274 19526\n",
      "Epoch: 1501/50000..  Training Loss: 0.09384767..  Test Loss: 0.51087713..  Accurary:   0.013838383838383839\n",
      "201 19599\n",
      "Epoch: 1551/50000..  Training Loss: 0.11460889..  Test Loss: 0.49882570..  Accurary:   0.010151515151515151\n",
      "4723 15077\n",
      "Epoch: 1601/50000..  Training Loss: 0.10864579..  Test Loss: 0.46137601..  Accurary:   0.23853535353535354\n",
      "1549 18251\n",
      "Epoch: 1651/50000..  Training Loss: 0.10726119..  Test Loss: 0.44694722..  Accurary:   0.07823232323232324\n",
      "132 19668\n",
      "Epoch: 1701/50000..  Training Loss: 0.10762677..  Test Loss: 0.48816496..  Accurary:   0.006666666666666667\n",
      "4970 14830\n",
      "Epoch: 1751/50000..  Training Loss: 0.12939690..  Test Loss: 0.70151258..  Accurary:   0.251010101010101\n",
      "233 19567\n",
      "Epoch: 1801/50000..  Training Loss: 0.09707234..  Test Loss: 0.49481195..  Accurary:   0.011767676767676767\n",
      "297 19503\n",
      "Epoch: 1851/50000..  Training Loss: 0.12432478..  Test Loss: 0.47351709..  Accurary:   0.015\n",
      "215 19585\n",
      "Epoch: 1901/50000..  Training Loss: 0.14108284..  Test Loss: 0.45225686..  Accurary:   0.010858585858585859\n",
      "292 19508\n",
      "Epoch: 1951/50000..  Training Loss: 0.09049826..  Test Loss: 0.45408538..  Accurary:   0.014747474747474747\n",
      "125 19675\n",
      "Epoch: 2001/50000..  Training Loss: 0.11973797..  Test Loss: 0.50298256..  Accurary:   0.006313131313131313\n",
      "246 19554\n",
      "Epoch: 2051/50000..  Training Loss: 0.17051830..  Test Loss: 0.48729217..  Accurary:   0.012424242424242424\n",
      "181 19619\n",
      "Epoch: 2101/50000..  Training Loss: 0.11195440..  Test Loss: 0.52407771..  Accurary:   0.009141414141414141\n",
      "94 19706\n",
      "Epoch: 2151/50000..  Training Loss: 0.10132244..  Test Loss: 0.54219854..  Accurary:   0.004747474747474748\n",
      "353 19447\n",
      "Epoch: 2201/50000..  Training Loss: 0.12229549..  Test Loss: 0.47075093..  Accurary:   0.01782828282828283\n",
      "428 19372\n",
      "Epoch: 2251/50000..  Training Loss: 0.08696396..  Test Loss: 0.45778212..  Accurary:   0.021616161616161617\n",
      "256 19544\n",
      "Epoch: 2301/50000..  Training Loss: 0.09795104..  Test Loss: 0.47196308..  Accurary:   0.01292929292929293\n",
      "161 19639\n",
      "Epoch: 2351/50000..  Training Loss: 0.07911795..  Test Loss: 0.50478274..  Accurary:   0.008131313131313132\n",
      "256 19544\n",
      "Epoch: 2401/50000..  Training Loss: 0.09902899..  Test Loss: 0.47991827..  Accurary:   0.01292929292929293\n",
      "178 19622\n",
      "Epoch: 2451/50000..  Training Loss: 0.08707431..  Test Loss: 0.58424622..  Accurary:   0.00898989898989899\n",
      "176 19624\n",
      "Epoch: 2501/50000..  Training Loss: 0.09826416..  Test Loss: 0.53673726..  Accurary:   0.008888888888888889\n",
      "216 19584\n",
      "Epoch: 2551/50000..  Training Loss: 0.10087694..  Test Loss: 0.49874496..  Accurary:   0.01090909090909091\n",
      "318 19482\n",
      "Epoch: 2601/50000..  Training Loss: 0.10533990..  Test Loss: 0.45398057..  Accurary:   0.01606060606060606\n",
      "516 19284\n",
      "Epoch: 2651/50000..  Training Loss: 0.07220779..  Test Loss: 0.45283335..  Accurary:   0.026060606060606062\n",
      "423 19377\n",
      "Epoch: 2701/50000..  Training Loss: 0.08055590..  Test Loss: 0.45811385..  Accurary:   0.021363636363636362\n",
      "115 19685\n",
      "Epoch: 2751/50000..  Training Loss: 0.16751951..  Test Loss: 0.54929101..  Accurary:   0.005808080808080808\n",
      "153 19647\n",
      "Epoch: 2801/50000..  Training Loss: 0.08976790..  Test Loss: 0.52107406..  Accurary:   0.007727272727272728\n",
      "268 19532\n",
      "Epoch: 2851/50000..  Training Loss: 0.06706854..  Test Loss: 0.53766733..  Accurary:   0.013535353535353536\n",
      "348 19452\n",
      "Epoch: 2901/50000..  Training Loss: 0.08888113..  Test Loss: 0.42644393..  Accurary:   0.017575757575757574\n",
      "112 19688\n",
      "Epoch: 2951/50000..  Training Loss: 0.08258310..  Test Loss: 0.50297040..  Accurary:   0.0056565656565656566\n",
      "214 19586\n",
      "Epoch: 3001/50000..  Training Loss: 0.10800476..  Test Loss: 0.51604927..  Accurary:   0.010808080808080808\n",
      "317 19483\n",
      "Epoch: 3051/50000..  Training Loss: 0.06940082..  Test Loss: 0.44342369..  Accurary:   0.01601010101010101\n",
      "198 19602\n",
      "Epoch: 3101/50000..  Training Loss: 0.08062782..  Test Loss: 0.49385497..  Accurary:   0.01\n",
      "281 19519\n",
      "Epoch: 3151/50000..  Training Loss: 0.07858767..  Test Loss: 0.45963711..  Accurary:   0.014191919191919191\n",
      "227 19573\n",
      "Epoch: 3201/50000..  Training Loss: 0.06033229..  Test Loss: 0.46411827..  Accurary:   0.011464646464646464\n",
      "391 19409\n",
      "Epoch: 3251/50000..  Training Loss: 0.07222482..  Test Loss: 0.44196033..  Accurary:   0.019747474747474746\n",
      "277 19523\n",
      "Epoch: 3301/50000..  Training Loss: 0.08209055..  Test Loss: 0.44651303..  Accurary:   0.01398989898989899\n",
      "257 19543\n",
      "Epoch: 3351/50000..  Training Loss: 0.06479654..  Test Loss: 0.45068330..  Accurary:   0.01297979797979798\n",
      "846 18954\n",
      "Epoch: 3401/50000..  Training Loss: 0.15042114..  Test Loss: 0.55453002..  Accurary:   0.042727272727272725\n",
      "392 19408\n",
      "Epoch: 3451/50000..  Training Loss: 0.06695866..  Test Loss: 0.44217059..  Accurary:   0.019797979797979797\n",
      "141 19659\n",
      "Epoch: 3501/50000..  Training Loss: 0.05875573..  Test Loss: 0.49308664..  Accurary:   0.007121212121212121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 19599\n",
      "Epoch: 3551/50000..  Training Loss: 0.06531730..  Test Loss: 0.49563062..  Accurary:   0.010151515151515151\n",
      "364 19436\n",
      "Epoch: 3601/50000..  Training Loss: 0.07586390..  Test Loss: 0.42254913..  Accurary:   0.018383838383838384\n",
      "180 19620\n",
      "Epoch: 3651/50000..  Training Loss: 0.07360595..  Test Loss: 0.47617620..  Accurary:   0.00909090909090909\n",
      "278 19522\n",
      "Epoch: 3701/50000..  Training Loss: 0.08361509..  Test Loss: 0.45491689..  Accurary:   0.01404040404040404\n",
      "357 19443\n",
      "Epoch: 3751/50000..  Training Loss: 0.06657573..  Test Loss: 0.42547047..  Accurary:   0.018030303030303032\n",
      "209 19591\n",
      "Epoch: 3801/50000..  Training Loss: 0.06301530..  Test Loss: 0.46028250..  Accurary:   0.010555555555555556\n",
      "604 19196\n",
      "Epoch: 3851/50000..  Training Loss: 0.08725680..  Test Loss: 0.47963777..  Accurary:   0.030505050505050504\n",
      "204 19596\n",
      "Epoch: 3901/50000..  Training Loss: 0.08500947..  Test Loss: 0.45185438..  Accurary:   0.010303030303030303\n",
      "312 19488\n",
      "Epoch: 3951/50000..  Training Loss: 0.06223525..  Test Loss: 0.42654252..  Accurary:   0.01575757575757576\n",
      "461 19339\n",
      "Epoch: 4001/50000..  Training Loss: 0.06411597..  Test Loss: 0.43813890..  Accurary:   0.023282828282828284\n",
      "197 19603\n",
      "Epoch: 4051/50000..  Training Loss: 0.07291574..  Test Loss: 0.45659703..  Accurary:   0.00994949494949495\n",
      "223 19577\n",
      "Epoch: 4101/50000..  Training Loss: 0.05803494..  Test Loss: 0.44699925..  Accurary:   0.011262626262626262\n",
      "253 19547\n",
      "Epoch: 4151/50000..  Training Loss: 0.07168872..  Test Loss: 0.45313209..  Accurary:   0.012777777777777779\n",
      "409 19391\n",
      "Epoch: 4201/50000..  Training Loss: 0.06289902..  Test Loss: 0.42774779..  Accurary:   0.020656565656565658\n",
      "174 19626\n",
      "Epoch: 4251/50000..  Training Loss: 0.06253140..  Test Loss: 0.44043279..  Accurary:   0.008787878787878787\n",
      "292 19508\n",
      "Epoch: 4301/50000..  Training Loss: 0.06534743..  Test Loss: 0.44168374..  Accurary:   0.014747474747474747\n",
      "183 19617\n",
      "Epoch: 4351/50000..  Training Loss: 0.05920428..  Test Loss: 0.48638189..  Accurary:   0.009242424242424243\n",
      "209 19591\n",
      "Epoch: 4401/50000..  Training Loss: 0.05784567..  Test Loss: 0.46338785..  Accurary:   0.010555555555555556\n",
      "443 19357\n",
      "Epoch: 4451/50000..  Training Loss: 0.04506726..  Test Loss: 0.41229671..  Accurary:   0.022373737373737372\n",
      "456 19344\n",
      "Epoch: 4501/50000..  Training Loss: 0.06567188..  Test Loss: 0.41827404..  Accurary:   0.02303030303030303\n",
      "374 19426\n",
      "Epoch: 4551/50000..  Training Loss: 0.04946918..  Test Loss: 0.41593340..  Accurary:   0.01888888888888889\n",
      "269 19531\n",
      "Epoch: 4601/50000..  Training Loss: 0.05573884..  Test Loss: 0.44513258..  Accurary:   0.013585858585858587\n",
      "163 19637\n",
      "Epoch: 4651/50000..  Training Loss: 0.05903551..  Test Loss: 0.48766029..  Accurary:   0.008232323232323232\n",
      "201 19599\n",
      "Epoch: 4701/50000..  Training Loss: 0.05735886..  Test Loss: 0.45751375..  Accurary:   0.010151515151515151\n",
      "126 19674\n",
      "Epoch: 4751/50000..  Training Loss: 0.06035514..  Test Loss: 0.49942073..  Accurary:   0.006363636363636364\n",
      "225 19575\n",
      "Epoch: 4801/50000..  Training Loss: 0.04230428..  Test Loss: 0.44562146..  Accurary:   0.011363636363636364\n",
      "376 19424\n",
      "Epoch: 4851/50000..  Training Loss: 0.04500394..  Test Loss: 0.43515143..  Accurary:   0.01898989898989899\n",
      "327 19473\n",
      "Epoch: 4901/50000..  Training Loss: 0.05629635..  Test Loss: 0.43770820..  Accurary:   0.016515151515151514\n",
      "481 19319\n",
      "Epoch: 4951/50000..  Training Loss: 0.06456264..  Test Loss: 0.41798413..  Accurary:   0.024292929292929293\n",
      "153 19647\n",
      "Epoch: 5001/50000..  Training Loss: 0.08743323..  Test Loss: 0.58337492..  Accurary:   0.007727272727272728\n",
      "379 19421\n",
      "Epoch: 5051/50000..  Training Loss: 0.06399319..  Test Loss: 0.43231046..  Accurary:   0.01914141414141414\n",
      "187 19613\n",
      "Epoch: 5101/50000..  Training Loss: 0.04830179..  Test Loss: 0.46038678..  Accurary:   0.009444444444444445\n",
      "323 19477\n",
      "Epoch: 5151/50000..  Training Loss: 0.05664270..  Test Loss: 0.42213482..  Accurary:   0.016313131313131314\n",
      "258 19542\n",
      "Epoch: 5201/50000..  Training Loss: 0.05008759..  Test Loss: 0.42958531..  Accurary:   0.013030303030303031\n",
      "345 19455\n",
      "Epoch: 5251/50000..  Training Loss: 0.06009122..  Test Loss: 0.42611206..  Accurary:   0.017424242424242425\n",
      "418 19382\n",
      "Epoch: 5301/50000..  Training Loss: 0.05086418..  Test Loss: 0.43583298..  Accurary:   0.021111111111111112\n",
      "460 19340\n",
      "Epoch: 5351/50000..  Training Loss: 0.07014252..  Test Loss: 0.42643127..  Accurary:   0.023232323232323233\n",
      "270 19530\n",
      "Epoch: 5401/50000..  Training Loss: 0.05739235..  Test Loss: 0.44590268..  Accurary:   0.013636363636363636\n",
      "332 19468\n",
      "Epoch: 5451/50000..  Training Loss: 0.06602760..  Test Loss: 0.43646139..  Accurary:   0.016767676767676768\n",
      "340 19460\n",
      "Epoch: 5501/50000..  Training Loss: 0.04782780..  Test Loss: 0.42448643..  Accurary:   0.01717171717171717\n",
      "201 19599\n",
      "Epoch: 5551/50000..  Training Loss: 0.13791661..  Test Loss: 0.47933120..  Accurary:   0.010151515151515151\n",
      "276 19524\n",
      "Epoch: 5601/50000..  Training Loss: 0.07422052..  Test Loss: 0.44533217..  Accurary:   0.013939393939393939\n",
      "377 19423\n",
      "Epoch: 5651/50000..  Training Loss: 0.05590547..  Test Loss: 0.43686238..  Accurary:   0.01904040404040404\n",
      "303 19497\n",
      "Epoch: 5701/50000..  Training Loss: 0.04923169..  Test Loss: 0.43137157..  Accurary:   0.015303030303030303\n",
      "167 19633\n",
      "Epoch: 5751/50000..  Training Loss: 0.05712997..  Test Loss: 0.45358250..  Accurary:   0.008434343434343435\n",
      "220 19580\n",
      "Epoch: 5801/50000..  Training Loss: 0.04808741..  Test Loss: 0.44025761..  Accurary:   0.011111111111111112\n",
      "443 19357\n",
      "Epoch: 5851/50000..  Training Loss: 0.07136342..  Test Loss: 0.40873033..  Accurary:   0.022373737373737372\n",
      "630 19170\n",
      "Epoch: 5901/50000..  Training Loss: 0.05072823..  Test Loss: 0.40489036..  Accurary:   0.031818181818181815\n",
      "209 19591\n",
      "Epoch: 5951/50000..  Training Loss: 0.07276060..  Test Loss: 0.45045897..  Accurary:   0.010555555555555556\n",
      "208 19592\n",
      "Epoch: 6001/50000..  Training Loss: 0.06220665..  Test Loss: 0.44546765..  Accurary:   0.010505050505050505\n",
      "314 19486\n",
      "Epoch: 6051/50000..  Training Loss: 0.07107220..  Test Loss: 0.41434789..  Accurary:   0.01585858585858586\n",
      "315 19485\n",
      "Epoch: 6101/50000..  Training Loss: 0.05016461..  Test Loss: 0.42122790..  Accurary:   0.015909090909090907\n",
      "412 19388\n",
      "Epoch: 6151/50000..  Training Loss: 0.04726756..  Test Loss: 0.41620961..  Accurary:   0.020808080808080807\n",
      "314 19486\n",
      "Epoch: 6201/50000..  Training Loss: 0.05628807..  Test Loss: 0.45426807..  Accurary:   0.01585858585858586\n",
      "306 19494\n",
      "Epoch: 6251/50000..  Training Loss: 0.05117708..  Test Loss: 0.43675417..  Accurary:   0.015454545454545455\n",
      "136 19664\n",
      "Epoch: 6301/50000..  Training Loss: 0.04982737..  Test Loss: 0.47819838..  Accurary:   0.006868686868686869\n",
      "282 19518\n",
      "Epoch: 6351/50000..  Training Loss: 0.08147571..  Test Loss: 0.42650086..  Accurary:   0.014242424242424242\n",
      "237 19563\n",
      "Epoch: 6401/50000..  Training Loss: 0.05859993..  Test Loss: 0.45058274..  Accurary:   0.01196969696969697\n",
      "188 19612\n",
      "Epoch: 6451/50000..  Training Loss: 0.03937227..  Test Loss: 0.44723850..  Accurary:   0.009494949494949495\n",
      "864 18936\n",
      "Epoch: 6501/50000..  Training Loss: 0.05761073..  Test Loss: 0.41509435..  Accurary:   0.04363636363636364\n",
      "412 19388\n",
      "Epoch: 6551/50000..  Training Loss: 0.08167950..  Test Loss: 0.43703738..  Accurary:   0.020808080808080807\n",
      "225 19575\n",
      "Epoch: 6601/50000..  Training Loss: 0.04589334..  Test Loss: 0.45611951..  Accurary:   0.011363636363636364\n",
      "130 19670\n",
      "Epoch: 6651/50000..  Training Loss: 0.04663439..  Test Loss: 0.50765574..  Accurary:   0.0065656565656565654\n",
      "190 19610\n",
      "Epoch: 6701/50000..  Training Loss: 0.04179043..  Test Loss: 0.45060548..  Accurary:   0.009595959595959595\n",
      "169 19631\n",
      "Epoch: 6751/50000..  Training Loss: 0.04965724..  Test Loss: 0.45001706..  Accurary:   0.008535353535353535\n",
      "266 19534\n",
      "Epoch: 6801/50000..  Training Loss: 0.05045179..  Test Loss: 0.40707955..  Accurary:   0.013434343434343434\n",
      "374 19426\n",
      "Epoch: 6851/50000..  Training Loss: 0.04804951..  Test Loss: 0.41867527..  Accurary:   0.01888888888888889\n",
      "413 19387\n",
      "Epoch: 6901/50000..  Training Loss: 0.04720944..  Test Loss: 0.43500078..  Accurary:   0.020858585858585858\n",
      "263 19537\n",
      "Epoch: 6951/50000..  Training Loss: 0.04235600..  Test Loss: 0.45708117..  Accurary:   0.013282828282828283\n",
      "280 19520\n",
      "Epoch: 7001/50000..  Training Loss: 0.04752456..  Test Loss: 0.42703909..  Accurary:   0.014141414141414142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 19580\n",
      "Epoch: 7051/50000..  Training Loss: 0.05864017..  Test Loss: 0.45239407..  Accurary:   0.011111111111111112\n",
      "526 19274\n",
      "Epoch: 7101/50000..  Training Loss: 0.07826193..  Test Loss: 0.41138983..  Accurary:   0.026565656565656567\n",
      "186 19614\n",
      "Epoch: 7151/50000..  Training Loss: 0.04291523..  Test Loss: 0.46217996..  Accurary:   0.009393939393939394\n",
      "352 19448\n",
      "Epoch: 7201/50000..  Training Loss: 0.03671922..  Test Loss: 0.43826872..  Accurary:   0.017777777777777778\n",
      "246 19554\n",
      "Epoch: 7251/50000..  Training Loss: 0.04890201..  Test Loss: 0.44728053..  Accurary:   0.012424242424242424\n",
      "414 19386\n",
      "Epoch: 7301/50000..  Training Loss: 0.04083510..  Test Loss: 0.43892762..  Accurary:   0.02090909090909091\n",
      "466 19334\n",
      "Epoch: 7351/50000..  Training Loss: 0.04156130..  Test Loss: 0.42759264..  Accurary:   0.023535353535353534\n",
      "259 19541\n",
      "Epoch: 7401/50000..  Training Loss: 0.05173809..  Test Loss: 0.43567029..  Accurary:   0.01308080808080808\n",
      "269 19531\n",
      "Epoch: 7451/50000..  Training Loss: 0.03900084..  Test Loss: 0.42728060..  Accurary:   0.013585858585858587\n",
      "212 19588\n",
      "Epoch: 7501/50000..  Training Loss: 0.04065241..  Test Loss: 0.43322635..  Accurary:   0.010707070707070707\n",
      "163 19637\n",
      "Epoch: 7551/50000..  Training Loss: 0.04670096..  Test Loss: 0.46268031..  Accurary:   0.008232323232323232\n",
      "389 19411\n",
      "Epoch: 7601/50000..  Training Loss: 0.05324889..  Test Loss: 0.42524138..  Accurary:   0.019646464646464648\n",
      "336 19464\n",
      "Epoch: 7651/50000..  Training Loss: 0.05623291..  Test Loss: 0.46116671..  Accurary:   0.01696969696969697\n",
      "464 19336\n",
      "Epoch: 7701/50000..  Training Loss: 0.05355876..  Test Loss: 0.43574721..  Accurary:   0.023434343434343436\n",
      "384 19416\n",
      "Epoch: 7751/50000..  Training Loss: 0.04371289..  Test Loss: 0.41530657..  Accurary:   0.019393939393939394\n",
      "310 19490\n",
      "Epoch: 7801/50000..  Training Loss: 0.03859620..  Test Loss: 0.43231535..  Accurary:   0.015656565656565657\n",
      "151 19649\n",
      "Epoch: 7851/50000..  Training Loss: 0.04944560..  Test Loss: 0.44554964..  Accurary:   0.007626262626262626\n",
      "444 19356\n",
      "Epoch: 7901/50000..  Training Loss: 0.16606635..  Test Loss: 0.44629642..  Accurary:   0.022424242424242423\n",
      "615 19185\n",
      "Epoch: 7951/50000..  Training Loss: 0.03679530..  Test Loss: 0.42287377..  Accurary:   0.03106060606060606\n",
      "326 19474\n",
      "Epoch: 8001/50000..  Training Loss: 0.04878401..  Test Loss: 0.43062469..  Accurary:   0.016464646464646463\n",
      "360 19440\n",
      "Epoch: 8051/50000..  Training Loss: 0.03997302..  Test Loss: 0.43059808..  Accurary:   0.01818181818181818\n",
      "418 19382\n",
      "Epoch: 8101/50000..  Training Loss: 0.06134491..  Test Loss: 0.42655018..  Accurary:   0.021111111111111112\n",
      "177 19623\n",
      "Epoch: 8151/50000..  Training Loss: 0.04463242..  Test Loss: 0.44738853..  Accurary:   0.00893939393939394\n",
      "311 19489\n",
      "Epoch: 8201/50000..  Training Loss: 0.04351262..  Test Loss: 0.43185502..  Accurary:   0.015707070707070708\n",
      "418 19382\n",
      "Epoch: 8251/50000..  Training Loss: 0.04678887..  Test Loss: 0.42413574..  Accurary:   0.021111111111111112\n",
      "540 19260\n",
      "Epoch: 8301/50000..  Training Loss: 0.05698467..  Test Loss: 0.43160766..  Accurary:   0.02727272727272727\n",
      "221 19579\n",
      "Epoch: 8351/50000..  Training Loss: 0.06349125..  Test Loss: 0.43384382..  Accurary:   0.011161616161616162\n",
      "566 19234\n",
      "Epoch: 8401/50000..  Training Loss: 0.07662671..  Test Loss: 0.41533771..  Accurary:   0.028585858585858586\n",
      "224 19576\n",
      "Epoch: 8451/50000..  Training Loss: 0.05293251..  Test Loss: 0.45235905..  Accurary:   0.011313131313131313\n",
      "294 19506\n",
      "Epoch: 8501/50000..  Training Loss: 0.05080859..  Test Loss: 0.42626426..  Accurary:   0.014848484848484849\n",
      "308 19492\n",
      "Epoch: 8551/50000..  Training Loss: 0.05023668..  Test Loss: 0.43793911..  Accurary:   0.015555555555555555\n",
      "485 19315\n",
      "Epoch: 8601/50000..  Training Loss: 0.04132320..  Test Loss: 0.43174946..  Accurary:   0.024494949494949497\n",
      "326 19474\n",
      "Epoch: 8651/50000..  Training Loss: 0.04763848..  Test Loss: 0.42984843..  Accurary:   0.016464646464646463\n",
      "245 19555\n",
      "Epoch: 8701/50000..  Training Loss: 0.05084785..  Test Loss: 0.42332312..  Accurary:   0.012373737373737374\n",
      "294 19506\n",
      "Epoch: 8751/50000..  Training Loss: 0.03727657..  Test Loss: 0.41384163..  Accurary:   0.014848484848484849\n",
      "384 19416\n",
      "Epoch: 8801/50000..  Training Loss: 0.04244414..  Test Loss: 0.43623176..  Accurary:   0.019393939393939394\n",
      "242 19558\n",
      "Epoch: 8851/50000..  Training Loss: 0.05109691..  Test Loss: 0.45687184..  Accurary:   0.012222222222222223\n",
      "267 19533\n",
      "Epoch: 8901/50000..  Training Loss: 0.06337877..  Test Loss: 0.43094549..  Accurary:   0.013484848484848485\n",
      "353 19447\n",
      "Epoch: 8951/50000..  Training Loss: 0.03823240..  Test Loss: 0.44287705..  Accurary:   0.01782828282828283\n",
      "244 19556\n",
      "Epoch: 9001/50000..  Training Loss: 0.03734827..  Test Loss: 0.44077000..  Accurary:   0.012323232323232323\n",
      "191 19609\n",
      "Epoch: 9051/50000..  Training Loss: 0.04750903..  Test Loss: 0.42988685..  Accurary:   0.009646464646464646\n",
      "678 19122\n",
      "Epoch: 9101/50000..  Training Loss: 0.03561062..  Test Loss: 0.43972498..  Accurary:   0.03424242424242424\n",
      "297 19503\n",
      "Epoch: 9151/50000..  Training Loss: 0.03769036..  Test Loss: 0.45261309..  Accurary:   0.015\n",
      "225 19575\n",
      "Epoch: 9201/50000..  Training Loss: 0.04198007..  Test Loss: 0.44244748..  Accurary:   0.011363636363636364\n",
      "633 19167\n",
      "Epoch: 9251/50000..  Training Loss: 0.03782294..  Test Loss: 0.42926103..  Accurary:   0.03196969696969697\n",
      "213 19587\n",
      "Epoch: 9301/50000..  Training Loss: 0.05034928..  Test Loss: 0.43748525..  Accurary:   0.010757575757575757\n",
      "229 19571\n",
      "Epoch: 9351/50000..  Training Loss: 0.03922008..  Test Loss: 0.43882683..  Accurary:   0.011565656565656566\n",
      "554 19246\n",
      "Epoch: 9401/50000..  Training Loss: 0.08664587..  Test Loss: 0.40956026..  Accurary:   0.02797979797979798\n",
      "204 19596\n",
      "Epoch: 9451/50000..  Training Loss: 0.05194798..  Test Loss: 0.45173502..  Accurary:   0.010303030303030303\n",
      "263 19537\n",
      "Epoch: 9501/50000..  Training Loss: 0.03584579..  Test Loss: 0.44878927..  Accurary:   0.013282828282828283\n",
      "183 19617\n",
      "Epoch: 9551/50000..  Training Loss: 0.04702743..  Test Loss: 0.45521194..  Accurary:   0.009242424242424243\n",
      "654 19146\n",
      "Epoch: 9601/50000..  Training Loss: 0.05440026..  Test Loss: 0.41996366..  Accurary:   0.03303030303030303\n",
      "177 19623\n",
      "Epoch: 9651/50000..  Training Loss: 0.04698556..  Test Loss: 0.46030229..  Accurary:   0.00893939393939394\n",
      "260 19540\n",
      "Epoch: 9701/50000..  Training Loss: 0.04830867..  Test Loss: 0.44757387..  Accurary:   0.013131313131313131\n",
      "465 19335\n",
      "Epoch: 9751/50000..  Training Loss: 0.05056374..  Test Loss: 0.42683959..  Accurary:   0.023484848484848483\n",
      "665 19135\n",
      "Epoch: 9801/50000..  Training Loss: 0.05658686..  Test Loss: 0.43337280..  Accurary:   0.03358585858585859\n",
      "413 19387\n",
      "Epoch: 9851/50000..  Training Loss: 0.04123779..  Test Loss: 0.44031990..  Accurary:   0.020858585858585858\n",
      "437 19363\n",
      "Epoch: 9901/50000..  Training Loss: 0.04115291..  Test Loss: 0.42811123..  Accurary:   0.02207070707070707\n",
      "681 19119\n",
      "Epoch: 9951/50000..  Training Loss: 0.03798774..  Test Loss: 0.42423522..  Accurary:   0.03439393939393939\n",
      "277 19523\n",
      "Epoch: 10001/50000..  Training Loss: 0.03477553..  Test Loss: 0.43813100..  Accurary:   0.01398989898989899\n",
      "197 19603\n",
      "Epoch: 10051/50000..  Training Loss: 0.05613228..  Test Loss: 0.45420572..  Accurary:   0.00994949494949495\n",
      "262 19538\n",
      "Epoch: 10101/50000..  Training Loss: 0.03326367..  Test Loss: 0.44092807..  Accurary:   0.013232323232323233\n",
      "433 19367\n",
      "Epoch: 10151/50000..  Training Loss: 0.04013515..  Test Loss: 0.45673087..  Accurary:   0.021868686868686867\n",
      "245 19555\n",
      "Epoch: 10201/50000..  Training Loss: 0.04302973..  Test Loss: 0.44899440..  Accurary:   0.012373737373737374\n",
      "399 19401\n",
      "Epoch: 10251/50000..  Training Loss: 0.05376717..  Test Loss: 0.44485599..  Accurary:   0.020151515151515153\n",
      "528 19272\n",
      "Epoch: 10301/50000..  Training Loss: 0.06560178..  Test Loss: 0.42359301..  Accurary:   0.02666666666666667\n",
      "540 19260\n",
      "Epoch: 10351/50000..  Training Loss: 0.03928517..  Test Loss: 0.42714155..  Accurary:   0.02727272727272727\n",
      "569 19231\n",
      "Epoch: 10401/50000..  Training Loss: 0.03693833..  Test Loss: 0.43032452..  Accurary:   0.02873737373737374\n",
      "366 19434\n",
      "Epoch: 10451/50000..  Training Loss: 0.04706324..  Test Loss: 0.44876045..  Accurary:   0.018484848484848486\n",
      "615 19185\n",
      "Epoch: 10501/50000..  Training Loss: 0.03608363..  Test Loss: 0.41946840..  Accurary:   0.03106060606060606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322 19478\n",
      "Epoch: 10551/50000..  Training Loss: 0.04526660..  Test Loss: 0.42855135..  Accurary:   0.016262626262626263\n",
      "424 19376\n",
      "Epoch: 10601/50000..  Training Loss: 0.03659463..  Test Loss: 0.42432648..  Accurary:   0.021414141414141413\n",
      "340 19460\n",
      "Epoch: 10651/50000..  Training Loss: 0.05142141..  Test Loss: 0.44142824..  Accurary:   0.01717171717171717\n",
      "289 19511\n",
      "Epoch: 10701/50000..  Training Loss: 0.03924005..  Test Loss: 0.43671861..  Accurary:   0.014595959595959596\n",
      "742 19058\n",
      "Epoch: 10751/50000..  Training Loss: 0.04651314..  Test Loss: 0.42721242..  Accurary:   0.03747474747474747\n",
      "724 19076\n",
      "Epoch: 10801/50000..  Training Loss: 0.03814695..  Test Loss: 0.45214248..  Accurary:   0.036565656565656565\n",
      "258 19542\n",
      "Epoch: 10851/50000..  Training Loss: 0.05905739..  Test Loss: 0.43392372..  Accurary:   0.013030303030303031\n",
      "453 19347\n",
      "Epoch: 10901/50000..  Training Loss: 0.04942689..  Test Loss: 0.43923602..  Accurary:   0.02287878787878788\n",
      "282 19518\n",
      "Epoch: 10951/50000..  Training Loss: 0.04319551..  Test Loss: 0.45771158..  Accurary:   0.014242424242424242\n",
      "528 19272\n",
      "Epoch: 11001/50000..  Training Loss: 0.04248864..  Test Loss: 0.44299597..  Accurary:   0.02666666666666667\n",
      "539 19261\n",
      "Epoch: 11051/50000..  Training Loss: 0.03502780..  Test Loss: 0.45819443..  Accurary:   0.02722222222222222\n",
      "440 19360\n",
      "Epoch: 11101/50000..  Training Loss: 0.04623902..  Test Loss: 0.46434698..  Accurary:   0.022222222222222223\n",
      "413 19387\n",
      "Epoch: 11151/50000..  Training Loss: 0.05071575..  Test Loss: 0.43217137..  Accurary:   0.020858585858585858\n",
      "298 19502\n",
      "Epoch: 11201/50000..  Training Loss: 0.03774396..  Test Loss: 0.43424898..  Accurary:   0.01505050505050505\n",
      "217 19583\n",
      "Epoch: 11251/50000..  Training Loss: 0.05240784..  Test Loss: 0.46053159..  Accurary:   0.010959595959595959\n",
      "167 19633\n",
      "Epoch: 11301/50000..  Training Loss: 0.03413444..  Test Loss: 0.44958481..  Accurary:   0.008434343434343435\n",
      "387 19413\n",
      "Epoch: 11351/50000..  Training Loss: 0.04464749..  Test Loss: 0.43626904..  Accurary:   0.019545454545454546\n",
      "276 19524\n",
      "Epoch: 11401/50000..  Training Loss: 0.04205796..  Test Loss: 0.44242364..  Accurary:   0.013939393939393939\n",
      "356 19444\n",
      "Epoch: 11451/50000..  Training Loss: 0.04305167..  Test Loss: 0.43842274..  Accurary:   0.01797979797979798\n",
      "521 19279\n",
      "Epoch: 11501/50000..  Training Loss: 0.04762200..  Test Loss: 0.43479148..  Accurary:   0.026313131313131313\n",
      "230 19570\n",
      "Epoch: 11551/50000..  Training Loss: 0.04578873..  Test Loss: 0.44518864..  Accurary:   0.011616161616161616\n",
      "233 19567\n",
      "Epoch: 11601/50000..  Training Loss: 0.05457788..  Test Loss: 0.44347084..  Accurary:   0.011767676767676767\n",
      "243 19557\n",
      "Epoch: 11651/50000..  Training Loss: 0.03892448..  Test Loss: 0.45701575..  Accurary:   0.012272727272727272\n",
      "612 19188\n",
      "Epoch: 11701/50000..  Training Loss: 0.04205516..  Test Loss: 0.44678834..  Accurary:   0.03090909090909091\n",
      "629 19171\n",
      "Epoch: 11751/50000..  Training Loss: 0.05108187..  Test Loss: 0.41815731..  Accurary:   0.03176767676767677\n",
      "995 18805\n",
      "Epoch: 11801/50000..  Training Loss: 0.04606611..  Test Loss: 0.42023596..  Accurary:   0.05025252525252525\n",
      "304 19496\n",
      "Epoch: 11851/50000..  Training Loss: 0.04162489..  Test Loss: 0.44816485..  Accurary:   0.015353535353535354\n",
      "286 19514\n",
      "Epoch: 11901/50000..  Training Loss: 0.03796924..  Test Loss: 0.43559492..  Accurary:   0.014444444444444444\n",
      "532 19268\n",
      "Epoch: 11951/50000..  Training Loss: 0.03465315..  Test Loss: 0.43102220..  Accurary:   0.026868686868686868\n",
      "386 19414\n",
      "Epoch: 12001/50000..  Training Loss: 0.03813312..  Test Loss: 0.42895582..  Accurary:   0.019494949494949496\n",
      "216 19584\n",
      "Epoch: 12051/50000..  Training Loss: 0.04300133..  Test Loss: 0.46353516..  Accurary:   0.01090909090909091\n",
      "414 19386\n",
      "Epoch: 12101/50000..  Training Loss: 0.05836349..  Test Loss: 0.45641869..  Accurary:   0.02090909090909091\n",
      "1755 18045\n",
      "Epoch: 12151/50000..  Training Loss: 0.03941912..  Test Loss: 0.42129341..  Accurary:   0.08863636363636364\n",
      "457 19343\n",
      "Epoch: 12201/50000..  Training Loss: 0.04406576..  Test Loss: 0.43053147..  Accurary:   0.02308080808080808\n",
      "1375 18425\n",
      "Epoch: 12251/50000..  Training Loss: 0.05793976..  Test Loss: 0.44590396..  Accurary:   0.06944444444444445\n",
      "339 19461\n",
      "Epoch: 12301/50000..  Training Loss: 0.03613071..  Test Loss: 0.43466458..  Accurary:   0.01712121212121212\n",
      "400 19400\n",
      "Epoch: 12351/50000..  Training Loss: 0.04358200..  Test Loss: 0.42352745..  Accurary:   0.020202020202020204\n",
      "548 19252\n",
      "Epoch: 12401/50000..  Training Loss: 0.04030005..  Test Loss: 0.42690939..  Accurary:   0.027676767676767678\n",
      "375 19425\n",
      "Epoch: 12451/50000..  Training Loss: 0.04844576..  Test Loss: 0.42416993..  Accurary:   0.01893939393939394\n",
      "781 19019\n",
      "Epoch: 12501/50000..  Training Loss: 0.04406500..  Test Loss: 0.43933955..  Accurary:   0.03944444444444444\n",
      "287 19513\n",
      "Epoch: 12551/50000..  Training Loss: 0.04325363..  Test Loss: 0.43667486..  Accurary:   0.014494949494949495\n",
      "363 19437\n",
      "Epoch: 12951/50000..  Training Loss: 0.04910207..  Test Loss: 0.43069261..  Accurary:   0.018333333333333333\n",
      "1525 18275\n",
      "Epoch: 13001/50000..  Training Loss: 0.03972591..  Test Loss: 0.42902595..  Accurary:   0.07702020202020202\n",
      "1322 18478\n",
      "Epoch: 13051/50000..  Training Loss: 0.04762466..  Test Loss: 0.42948809..  Accurary:   0.06676767676767677\n",
      "344 19456\n",
      "Epoch: 13101/50000..  Training Loss: 0.03604622..  Test Loss: 0.43275943..  Accurary:   0.017373737373737375\n",
      "364 19436\n",
      "Epoch: 13151/50000..  Training Loss: 0.03591408..  Test Loss: 0.44075224..  Accurary:   0.018383838383838384\n",
      "704 19096\n",
      "Epoch: 13201/50000..  Training Loss: 0.05864237..  Test Loss: 0.42107624..  Accurary:   0.035555555555555556\n",
      "617 19183\n",
      "Epoch: 13251/50000..  Training Loss: 0.06044796..  Test Loss: 0.41157681..  Accurary:   0.03116161616161616\n",
      "1170 18630\n",
      "Epoch: 13301/50000..  Training Loss: 0.03629759..  Test Loss: 0.40733734..  Accurary:   0.05909090909090909\n",
      "425 19375\n",
      "Epoch: 13351/50000..  Training Loss: 0.04605433..  Test Loss: 0.43159387..  Accurary:   0.021464646464646464\n",
      "350 19450\n",
      "Epoch: 13401/50000..  Training Loss: 0.03812459..  Test Loss: 0.42785752..  Accurary:   0.017676767676767676\n",
      "476 19324\n",
      "Epoch: 13451/50000..  Training Loss: 0.04511378..  Test Loss: 0.44635752..  Accurary:   0.02404040404040404\n",
      "927 18873\n",
      "Epoch: 13501/50000..  Training Loss: 0.03535903..  Test Loss: 0.41569453..  Accurary:   0.04681818181818182\n",
      "1140 18660\n",
      "Epoch: 13551/50000..  Training Loss: 0.03541784..  Test Loss: 0.42107147..  Accurary:   0.05757575757575758\n",
      "471 19329\n",
      "Epoch: 13601/50000..  Training Loss: 0.03446705..  Test Loss: 0.42397836..  Accurary:   0.02378787878787879\n",
      "365 19435\n",
      "Epoch: 13651/50000..  Training Loss: 0.06593972..  Test Loss: 0.44224364..  Accurary:   0.018434343434343435\n",
      "738 19062\n",
      "Epoch: 13701/50000..  Training Loss: 0.05947233..  Test Loss: 0.44001910..  Accurary:   0.03727272727272727\n",
      "1063 18737\n",
      "Epoch: 13751/50000..  Training Loss: 0.03354334..  Test Loss: 0.42924604..  Accurary:   0.053686868686868686\n",
      "842 18958\n",
      "Epoch: 13801/50000..  Training Loss: 0.04308785..  Test Loss: 0.43091881..  Accurary:   0.04252525252525253\n",
      "355 19445\n",
      "Epoch: 13851/50000..  Training Loss: 0.04960736..  Test Loss: 0.42216039..  Accurary:   0.01792929292929293\n",
      "1414 18386\n",
      "Epoch: 13901/50000..  Training Loss: 0.05668029..  Test Loss: 0.41579595..  Accurary:   0.07141414141414142\n",
      "1315 18485\n",
      "Epoch: 13951/50000..  Training Loss: 0.03486136..  Test Loss: 0.42362434..  Accurary:   0.06641414141414141\n",
      "1206 18594\n",
      "Epoch: 14001/50000..  Training Loss: 0.03504972..  Test Loss: 0.41853142..  Accurary:   0.060909090909090906\n",
      "379 19421\n",
      "Epoch: 14051/50000..  Training Loss: 0.03468011..  Test Loss: 0.43240184..  Accurary:   0.01914141414141414\n",
      "466 19334\n",
      "Epoch: 14101/50000..  Training Loss: 0.05269396..  Test Loss: 0.41186687..  Accurary:   0.023535353535353534\n",
      "515 19285\n",
      "Epoch: 14151/50000..  Training Loss: 0.06756703..  Test Loss: 0.41030431..  Accurary:   0.02601010101010101\n",
      "728 19072\n",
      "Epoch: 14201/50000..  Training Loss: 0.04042690..  Test Loss: 0.42438674..  Accurary:   0.03676767676767677\n",
      "5817 13983\n",
      "Epoch: 14251/50000..  Training Loss: 0.03869821..  Test Loss: 0.43484810..  Accurary:   0.29378787878787876\n",
      "1106 18694\n",
      "Epoch: 14301/50000..  Training Loss: 0.03539390..  Test Loss: 0.41940597..  Accurary:   0.05585858585858586\n",
      "950 18850\n",
      "Epoch: 14351/50000..  Training Loss: 0.03652437..  Test Loss: 0.42674518..  Accurary:   0.047979797979797977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1104 18696\n",
      "Epoch: 14401/50000..  Training Loss: 0.05398326..  Test Loss: 0.42164651..  Accurary:   0.055757575757575756\n",
      "568 19232\n",
      "Epoch: 14451/50000..  Training Loss: 0.03552222..  Test Loss: 0.42609286..  Accurary:   0.028686868686868688\n",
      "1159 18641\n",
      "Epoch: 14501/50000..  Training Loss: 0.05083480..  Test Loss: 0.43832332..  Accurary:   0.05853535353535354\n",
      "539 19261\n",
      "Epoch: 14551/50000..  Training Loss: 0.04319671..  Test Loss: 0.42843670..  Accurary:   0.02722222222222222\n",
      "422 19378\n",
      "Epoch: 14601/50000..  Training Loss: 0.05892983..  Test Loss: 0.42372173..  Accurary:   0.02131313131313131\n",
      "787 19013\n",
      "Epoch: 14651/50000..  Training Loss: 0.05735371..  Test Loss: 0.42125759..  Accurary:   0.03974747474747475\n",
      "560 19240\n",
      "Epoch: 14701/50000..  Training Loss: 0.03522003..  Test Loss: 0.44067165..  Accurary:   0.028282828282828285\n",
      "1005 18795\n",
      "Epoch: 14751/50000..  Training Loss: 0.03114342..  Test Loss: 0.41459018..  Accurary:   0.05075757575757576\n",
      "427 19373\n",
      "Epoch: 14801/50000..  Training Loss: 0.05684850..  Test Loss: 0.42855519..  Accurary:   0.021565656565656566\n",
      "601 19199\n",
      "Epoch: 14851/50000..  Training Loss: 0.03807523..  Test Loss: 0.44228980..  Accurary:   0.030353535353535355\n",
      "585 19215\n",
      "Epoch: 14901/50000..  Training Loss: 0.03452794..  Test Loss: 0.42779192..  Accurary:   0.029545454545454545\n",
      "306 19494\n",
      "Epoch: 14951/50000..  Training Loss: 0.04569880..  Test Loss: 0.44441929..  Accurary:   0.015454545454545455\n",
      "601 19199\n",
      "Epoch: 15001/50000..  Training Loss: 0.04285679..  Test Loss: 0.42791602..  Accurary:   0.030353535353535355\n",
      "419 19381\n",
      "Epoch: 15051/50000..  Training Loss: 0.05489378..  Test Loss: 0.42673725..  Accurary:   0.021161616161616163\n",
      "669 19131\n",
      "Epoch: 15101/50000..  Training Loss: 0.04557498..  Test Loss: 0.44895938..  Accurary:   0.03378787878787879\n",
      "296 19504\n",
      "Epoch: 15151/50000..  Training Loss: 0.04583739..  Test Loss: 0.43101749..  Accurary:   0.01494949494949495\n",
      "1827 17973\n",
      "Epoch: 15201/50000..  Training Loss: 0.03182317..  Test Loss: 0.43187618..  Accurary:   0.09227272727272727\n",
      "659 19141\n",
      "Epoch: 15251/50000..  Training Loss: 0.03572311..  Test Loss: 0.45054820..  Accurary:   0.03328282828282828\n",
      "429 19371\n",
      "Epoch: 15301/50000..  Training Loss: 0.03434215..  Test Loss: 0.44541144..  Accurary:   0.021666666666666667\n",
      "314 19486\n",
      "Epoch: 15351/50000..  Training Loss: 0.03953326..  Test Loss: 0.43663895..  Accurary:   0.01585858585858586\n",
      "749 19051\n",
      "Epoch: 15401/50000..  Training Loss: 0.04014448..  Test Loss: 0.43844047..  Accurary:   0.03782828282828283\n",
      "1450 18350\n",
      "Epoch: 15451/50000..  Training Loss: 0.05439928..  Test Loss: 0.41854718..  Accurary:   0.07323232323232323\n",
      "547 19253\n",
      "Epoch: 15501/50000..  Training Loss: 0.03630543..  Test Loss: 0.41766608..  Accurary:   0.027626262626262627\n",
      "666 19134\n",
      "Epoch: 15551/50000..  Training Loss: 0.03493408..  Test Loss: 0.42500755..  Accurary:   0.03363636363636364\n",
      "586 19214\n",
      "Epoch: 15601/50000..  Training Loss: 0.04218673..  Test Loss: 0.42112339..  Accurary:   0.029595959595959596\n",
      "344 19456\n",
      "Epoch: 15651/50000..  Training Loss: 0.04510532..  Test Loss: 0.42840481..  Accurary:   0.017373737373737375\n",
      "366 19434\n",
      "Epoch: 15701/50000..  Training Loss: 0.03880305..  Test Loss: 0.47424573..  Accurary:   0.018484848484848486\n",
      "325 19475\n",
      "Epoch: 15751/50000..  Training Loss: 0.04297320..  Test Loss: 0.44104326..  Accurary:   0.016414141414141416\n",
      "911 18889\n",
      "Epoch: 15801/50000..  Training Loss: 0.03762132..  Test Loss: 0.41645887..  Accurary:   0.04601010101010101\n",
      "549 19251\n",
      "Epoch: 15851/50000..  Training Loss: 0.03968021..  Test Loss: 0.44200996..  Accurary:   0.02772727272727273\n",
      "686 19114\n",
      "Epoch: 15901/50000..  Training Loss: 0.04252727..  Test Loss: 0.42004088..  Accurary:   0.03464646464646465\n",
      "748 19052\n",
      "Epoch: 15951/50000..  Training Loss: 0.03583490..  Test Loss: 0.41507679..  Accurary:   0.03777777777777778\n",
      "802 18998\n",
      "Epoch: 16001/50000..  Training Loss: 0.03649505..  Test Loss: 0.43891531..  Accurary:   0.0405050505050505\n",
      "437 19363\n",
      "Epoch: 16051/50000..  Training Loss: 0.04993456..  Test Loss: 0.45001775..  Accurary:   0.02207070707070707\n",
      "618 19182\n",
      "Epoch: 16101/50000..  Training Loss: 0.04112515..  Test Loss: 0.42645094..  Accurary:   0.031212121212121212\n",
      "1572 18228\n",
      "Epoch: 16151/50000..  Training Loss: 0.03834918..  Test Loss: 0.42423344..  Accurary:   0.07939393939393939\n",
      "479 19321\n",
      "Epoch: 16201/50000..  Training Loss: 0.03306955..  Test Loss: 0.44022608..  Accurary:   0.02419191919191919\n",
      "698 19102\n",
      "Epoch: 16251/50000..  Training Loss: 0.05822802..  Test Loss: 0.41711575..  Accurary:   0.03525252525252525\n",
      "637 19163\n",
      "Epoch: 16301/50000..  Training Loss: 0.06106095..  Test Loss: 0.40939996..  Accurary:   0.03217171717171717\n",
      "331 19469\n",
      "Epoch: 16351/50000..  Training Loss: 0.03831183..  Test Loss: 0.44293532..  Accurary:   0.016717171717171717\n",
      "1016 18784\n",
      "Epoch: 16401/50000..  Training Loss: 0.04938133..  Test Loss: 0.43880853..  Accurary:   0.05131313131313131\n",
      "261 19539\n",
      "Epoch: 16451/50000..  Training Loss: 0.03869843..  Test Loss: 0.44916150..  Accurary:   0.013181818181818182\n",
      "1027 18773\n",
      "Epoch: 16501/50000..  Training Loss: 0.03148040..  Test Loss: 0.41058037..  Accurary:   0.05186868686868687\n",
      "671 19129\n",
      "Epoch: 16551/50000..  Training Loss: 0.06049804..  Test Loss: 0.40866578..  Accurary:   0.03388888888888889\n",
      "849 18951\n",
      "Epoch: 16601/50000..  Training Loss: 0.04464585..  Test Loss: 0.41877303..  Accurary:   0.04287878787878788\n",
      "441 19359\n",
      "Epoch: 16651/50000..  Training Loss: 0.04535782..  Test Loss: 0.42668062..  Accurary:   0.022272727272727274\n",
      "402 19398\n",
      "Epoch: 16701/50000..  Training Loss: 0.06240991..  Test Loss: 0.44199371..  Accurary:   0.020303030303030302\n",
      "1087 18713\n",
      "Epoch: 16751/50000..  Training Loss: 0.02929844..  Test Loss: 0.43741792..  Accurary:   0.0548989898989899\n",
      "418 19382\n",
      "Epoch: 16801/50000..  Training Loss: 0.04141152..  Test Loss: 0.43276533..  Accurary:   0.021111111111111112\n",
      "630 19170\n",
      "Epoch: 16851/50000..  Training Loss: 0.03678325..  Test Loss: 0.43246153..  Accurary:   0.031818181818181815\n",
      "635 19165\n",
      "Epoch: 16901/50000..  Training Loss: 0.06629529..  Test Loss: 0.42329165..  Accurary:   0.03207070707070707\n",
      "379 19421\n",
      "Epoch: 16951/50000..  Training Loss: 0.03995392..  Test Loss: 0.42949939..  Accurary:   0.01914141414141414\n",
      "439 19361\n",
      "Epoch: 17001/50000..  Training Loss: 0.03839605..  Test Loss: 0.44689748..  Accurary:   0.022171717171717172\n",
      "403 19397\n",
      "Epoch: 17351/50000..  Training Loss: 0.03737646..  Test Loss: 0.43750247..  Accurary:   0.020353535353535353\n",
      "497 19303\n",
      "Epoch: 17401/50000..  Training Loss: 0.04250241..  Test Loss: 0.43542910..  Accurary:   0.0251010101010101\n",
      "442 19358\n",
      "Epoch: 17451/50000..  Training Loss: 0.03548490..  Test Loss: 0.44096822..  Accurary:   0.022323232323232325\n",
      "529 19271\n",
      "Epoch: 17501/50000..  Training Loss: 0.03460901..  Test Loss: 0.43116695..  Accurary:   0.026717171717171716\n",
      "1340 18460\n",
      "Epoch: 17551/50000..  Training Loss: 0.03568873..  Test Loss: 0.42255440..  Accurary:   0.06767676767676768\n",
      "590 19210\n",
      "Epoch: 17601/50000..  Training Loss: 0.04200998..  Test Loss: 0.42872661..  Accurary:   0.0297979797979798\n",
      "766 19034\n",
      "Epoch: 17651/50000..  Training Loss: 0.03519086..  Test Loss: 0.42313778..  Accurary:   0.038686868686868686\n",
      "587 19213\n",
      "Epoch: 17701/50000..  Training Loss: 0.04511567..  Test Loss: 0.43143114..  Accurary:   0.029646464646464647\n",
      "443 19357\n",
      "Epoch: 17751/50000..  Training Loss: 0.03948072..  Test Loss: 0.43683106..  Accurary:   0.022373737373737372\n",
      "558 19242\n",
      "Epoch: 17801/50000..  Training Loss: 0.04241741..  Test Loss: 0.42141706..  Accurary:   0.028181818181818183\n",
      "1002 18798\n",
      "Epoch: 17851/50000..  Training Loss: 0.04207742..  Test Loss: 0.41112790..  Accurary:   0.050606060606060606\n",
      "697 19103\n",
      "Epoch: 17901/50000..  Training Loss: 0.03528301..  Test Loss: 0.43660921..  Accurary:   0.0352020202020202\n",
      "672 19128\n",
      "Epoch: 17951/50000..  Training Loss: 0.04136115..  Test Loss: 0.43264300..  Accurary:   0.03393939393939394\n",
      "697 19103\n",
      "Epoch: 18001/50000..  Training Loss: 0.05059911..  Test Loss: 0.42414853..  Accurary:   0.0352020202020202\n",
      "359 19441\n",
      "Epoch: 18051/50000..  Training Loss: 0.07969625..  Test Loss: 0.43573552..  Accurary:   0.01813131313131313\n",
      "337 19463\n",
      "Epoch: 18101/50000..  Training Loss: 0.02998169..  Test Loss: 0.44945461..  Accurary:   0.01702020202020202\n",
      "982 18818\n",
      "Epoch: 18151/50000..  Training Loss: 0.03398028..  Test Loss: 0.42631766..  Accurary:   0.049595959595959596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652 19148\n",
      "Epoch: 18201/50000..  Training Loss: 0.03774908..  Test Loss: 0.44112131..  Accurary:   0.032929292929292926\n",
      "484 19316\n",
      "Epoch: 18251/50000..  Training Loss: 0.03846674..  Test Loss: 0.42886880..  Accurary:   0.024444444444444446\n",
      "656 19144\n",
      "Epoch: 18301/50000..  Training Loss: 0.04635530..  Test Loss: 0.43212584..  Accurary:   0.03313131313131313\n",
      "2348 17452\n",
      "Epoch: 18351/50000..  Training Loss: 0.04595910..  Test Loss: 0.42523557..  Accurary:   0.11858585858585859\n",
      "1173 18627\n",
      "Epoch: 18401/50000..  Training Loss: 0.03018750..  Test Loss: 0.41803667..  Accurary:   0.05924242424242424\n",
      "439 19361\n",
      "Epoch: 18451/50000..  Training Loss: 0.03975994..  Test Loss: 0.42031398..  Accurary:   0.022171717171717172\n",
      "626 19174\n",
      "Epoch: 18501/50000..  Training Loss: 0.03427328..  Test Loss: 0.43668732..  Accurary:   0.03161616161616162\n",
      "1971 17829\n",
      "Epoch: 18551/50000..  Training Loss: 0.03259943..  Test Loss: 0.42497745..  Accurary:   0.09954545454545455\n",
      "552 19248\n",
      "Epoch: 18601/50000..  Training Loss: 0.03927969..  Test Loss: 0.44043434..  Accurary:   0.027878787878787878\n",
      "281 19519\n",
      "Epoch: 18651/50000..  Training Loss: 0.03605617..  Test Loss: 0.43294927..  Accurary:   0.014191919191919191\n",
      "544 19256\n",
      "Epoch: 18701/50000..  Training Loss: 0.04283152..  Test Loss: 0.44131258..  Accurary:   0.027474747474747475\n",
      "442 19358\n",
      "Epoch: 18751/50000..  Training Loss: 0.04174566..  Test Loss: 0.44055304..  Accurary:   0.022323232323232325\n",
      "281 19519\n",
      "Epoch: 18801/50000..  Training Loss: 0.05676841..  Test Loss: 0.44620764..  Accurary:   0.014191919191919191\n",
      "720 19080\n",
      "Epoch: 18851/50000..  Training Loss: 0.04270796..  Test Loss: 0.44876400..  Accurary:   0.03636363636363636\n",
      "348 19452\n",
      "Epoch: 18901/50000..  Training Loss: 0.07185136..  Test Loss: 0.44646642..  Accurary:   0.017575757575757574\n",
      "438 19362\n",
      "Epoch: 18951/50000..  Training Loss: 0.03214275..  Test Loss: 0.43099806..  Accurary:   0.02212121212121212\n",
      "817 18983\n",
      "Epoch: 19001/50000..  Training Loss: 0.05059190..  Test Loss: 0.42556477..  Accurary:   0.041262626262626265\n",
      "359 19441\n",
      "Epoch: 19051/50000..  Training Loss: 0.04614318..  Test Loss: 0.42625859..  Accurary:   0.01813131313131313\n",
      "831 18969\n",
      "Epoch: 19101/50000..  Training Loss: 0.04326950..  Test Loss: 0.43342429..  Accurary:   0.04196969696969697\n",
      "691 19109\n",
      "Epoch: 19401/50000..  Training Loss: 0.07405199..  Test Loss: 0.42940554..  Accurary:   0.0348989898989899\n",
      "2128 17672\n",
      "Epoch: 19451/50000..  Training Loss: 0.02957213..  Test Loss: 0.42813793..  Accurary:   0.10747474747474747\n",
      "1306 18494\n",
      "Epoch: 19501/50000..  Training Loss: 0.03003576..  Test Loss: 0.42972818..  Accurary:   0.06595959595959595\n",
      "2149 17651\n",
      "Epoch: 19551/50000..  Training Loss: 0.05526644..  Test Loss: 0.40641487..  Accurary:   0.10853535353535354\n",
      "515 19285\n",
      "Epoch: 19601/50000..  Training Loss: 0.04402679..  Test Loss: 0.42752591..  Accurary:   0.02601010101010101\n",
      "880 18920\n",
      "Epoch: 19651/50000..  Training Loss: 0.04455271..  Test Loss: 0.42657828..  Accurary:   0.044444444444444446\n",
      "615 19185\n",
      "Epoch: 19701/50000..  Training Loss: 0.02809088..  Test Loss: 0.43244940..  Accurary:   0.03106060606060606\n",
      "974 18826\n",
      "Epoch: 19751/50000..  Training Loss: 0.03056894..  Test Loss: 0.41701201..  Accurary:   0.04919191919191919\n",
      "779 19021\n",
      "Epoch: 19801/50000..  Training Loss: 0.03143481..  Test Loss: 0.42900589..  Accurary:   0.03934343434343434\n",
      "1476 18324\n",
      "Epoch: 19851/50000..  Training Loss: 0.05115642..  Test Loss: 0.41912583..  Accurary:   0.07454545454545454\n",
      "1230 18570\n",
      "Epoch: 19901/50000..  Training Loss: 0.05184900..  Test Loss: 0.43461198..  Accurary:   0.06212121212121212\n",
      "705 19095\n",
      "Epoch: 19951/50000..  Training Loss: 0.03768964..  Test Loss: 0.42696017..  Accurary:   0.035606060606060606\n",
      "1854 17946\n",
      "Epoch: 20001/50000..  Training Loss: 0.03454211..  Test Loss: 0.42088625..  Accurary:   0.09363636363636364\n",
      "904 18896\n",
      "Epoch: 20051/50000..  Training Loss: 0.02731339..  Test Loss: 0.42768800..  Accurary:   0.04565656565656566\n",
      "919 18881\n",
      "Epoch: 20101/50000..  Training Loss: 0.03474934..  Test Loss: 0.43365544..  Accurary:   0.046414141414141415\n",
      "1077 18723\n",
      "Epoch: 20151/50000..  Training Loss: 0.03707744..  Test Loss: 0.42732975..  Accurary:   0.0543939393939394\n",
      "4505 15295\n",
      "Epoch: 20201/50000..  Training Loss: 0.03395768..  Test Loss: 0.41826141..  Accurary:   0.22752525252525252\n",
      "749 19051\n",
      "Epoch: 20251/50000..  Training Loss: 0.03481843..  Test Loss: 0.42354733..  Accurary:   0.03782828282828283\n",
      "915 18885\n",
      "Epoch: 20301/50000..  Training Loss: 0.03739193..  Test Loss: 0.42815933..  Accurary:   0.04621212121212121\n",
      "749 19051\n",
      "Epoch: 20351/50000..  Training Loss: 0.05128913..  Test Loss: 0.41401032..  Accurary:   0.03782828282828283\n",
      "458 19342\n",
      "Epoch: 20401/50000..  Training Loss: 0.03761932..  Test Loss: 0.43280184..  Accurary:   0.02313131313131313\n",
      "1415 18385\n",
      "Epoch: 20451/50000..  Training Loss: 0.03259227..  Test Loss: 0.41836712..  Accurary:   0.07146464646464647\n",
      "1110 18690\n",
      "Epoch: 20501/50000..  Training Loss: 0.03695920..  Test Loss: 0.42752352..  Accurary:   0.05606060606060606\n",
      "478 19322\n",
      "Epoch: 20551/50000..  Training Loss: 0.05814174..  Test Loss: 0.41661024..  Accurary:   0.02414141414141414\n",
      "935 18865\n",
      "Epoch: 20601/50000..  Training Loss: 0.03844259..  Test Loss: 0.42372307..  Accurary:   0.04722222222222222\n",
      "701 19099\n",
      "Epoch: 20651/50000..  Training Loss: 0.03034630..  Test Loss: 0.42647535..  Accurary:   0.0354040404040404\n",
      "388 19412\n",
      "Epoch: 20701/50000..  Training Loss: 0.04558786..  Test Loss: 0.44990036..  Accurary:   0.019595959595959597\n",
      "502 19298\n",
      "Epoch: 20751/50000..  Training Loss: 0.03018916..  Test Loss: 0.43854856..  Accurary:   0.025353535353535354\n",
      "728 19072\n",
      "Epoch: 20801/50000..  Training Loss: 0.03897383..  Test Loss: 0.41489431..  Accurary:   0.03676767676767677\n",
      "1866 17934\n",
      "Epoch: 20851/50000..  Training Loss: 0.03357278..  Test Loss: 0.42631826..  Accurary:   0.09424242424242424\n",
      "1009 18791\n",
      "Epoch: 20901/50000..  Training Loss: 0.03437914..  Test Loss: 0.42322838..  Accurary:   0.05095959595959596\n",
      "4757 15043\n",
      "Epoch: 20951/50000..  Training Loss: 0.04991018..  Test Loss: 0.41526091..  Accurary:   0.24025252525252525\n",
      "1878 17922\n",
      "Epoch: 21001/50000..  Training Loss: 0.03711429..  Test Loss: 0.41200003..  Accurary:   0.09484848484848485\n",
      "466 19334\n",
      "Epoch: 21051/50000..  Training Loss: 0.03257914..  Test Loss: 0.43558410..  Accurary:   0.023535353535353534\n",
      "693 19107\n",
      "Epoch: 21101/50000..  Training Loss: 0.04944008..  Test Loss: 0.43271938..  Accurary:   0.035\n",
      "1134 18666\n",
      "Epoch: 21151/50000..  Training Loss: 0.03101202..  Test Loss: 0.43094012..  Accurary:   0.057272727272727274\n",
      "606 19194\n",
      "Epoch: 21201/50000..  Training Loss: 0.02817493..  Test Loss: 0.42442417..  Accurary:   0.030606060606060605\n",
      "467 19333\n",
      "Epoch: 21251/50000..  Training Loss: 0.03116436..  Test Loss: 0.43210444..  Accurary:   0.023585858585858585\n",
      "2466 17334\n",
      "Epoch: 21301/50000..  Training Loss: 0.03859413..  Test Loss: 0.42467907..  Accurary:   0.12454545454545454\n",
      "2108 17692\n",
      "Epoch: 21351/50000..  Training Loss: 0.03827942..  Test Loss: 0.43650958..  Accurary:   0.10646464646464647\n",
      "324 19476\n",
      "Epoch: 21401/50000..  Training Loss: 0.05710605..  Test Loss: 0.46206912..  Accurary:   0.016363636363636365\n",
      "286 19514\n",
      "Epoch: 21451/50000..  Training Loss: 0.04910702..  Test Loss: 0.44091880..  Accurary:   0.014444444444444444\n",
      "427 19373\n",
      "Epoch: 21501/50000..  Training Loss: 0.03109677..  Test Loss: 0.43792647..  Accurary:   0.021565656565656566\n",
      "1582 18218\n",
      "Epoch: 21551/50000..  Training Loss: 0.03330844..  Test Loss: 0.42560300..  Accurary:   0.0798989898989899\n",
      "980 18820\n",
      "Epoch: 21601/50000..  Training Loss: 0.04760565..  Test Loss: 0.44779819..  Accurary:   0.049494949494949494\n",
      "938 18862\n",
      "Epoch: 21651/50000..  Training Loss: 0.03134902..  Test Loss: 0.42141029..  Accurary:   0.047373737373737373\n",
      "1099 18701\n",
      "Epoch: 21701/50000..  Training Loss: 0.03232097..  Test Loss: 0.41516286..  Accurary:   0.0555050505050505\n",
      "1866 17934\n",
      "Epoch: 21751/50000..  Training Loss: 0.07409601..  Test Loss: 0.42621282..  Accurary:   0.09424242424242424\n",
      "1109 18691\n",
      "Epoch: 21801/50000..  Training Loss: 0.04406186..  Test Loss: 0.41726056..  Accurary:   0.05601010101010101\n",
      "392 19408\n",
      "Epoch: 21851/50000..  Training Loss: 0.03986061..  Test Loss: 0.42279530..  Accurary:   0.019797979797979797\n",
      "690 19110\n",
      "Epoch: 21901/50000..  Training Loss: 0.04079365..  Test Loss: 0.42992800..  Accurary:   0.03484848484848485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951 18849\n",
      "Epoch: 21951/50000..  Training Loss: 0.03043482..  Test Loss: 0.40950653..  Accurary:   0.04803030303030303\n",
      "1317 18483\n",
      "Epoch: 22001/50000..  Training Loss: 0.03941273..  Test Loss: 0.43105271..  Accurary:   0.06651515151515151\n",
      "408 19392\n",
      "Epoch: 22051/50000..  Training Loss: 0.05109714..  Test Loss: 0.44219372..  Accurary:   0.020606060606060607\n",
      "916 18884\n",
      "Epoch: 22101/50000..  Training Loss: 0.04140901..  Test Loss: 0.43060353..  Accurary:   0.04626262626262626\n",
      "3092 16708\n",
      "Epoch: 22151/50000..  Training Loss: 0.04801594..  Test Loss: 0.41098440..  Accurary:   0.15616161616161617\n",
      "433 19367\n",
      "Epoch: 22201/50000..  Training Loss: 0.03005056..  Test Loss: 0.43267745..  Accurary:   0.021868686868686867\n",
      "1265 18535\n",
      "Epoch: 22251/50000..  Training Loss: 0.04185420..  Test Loss: 0.41961065..  Accurary:   0.06388888888888888\n",
      "472 19328\n",
      "Epoch: 22301/50000..  Training Loss: 0.05480330..  Test Loss: 0.43858477..  Accurary:   0.02383838383838384\n",
      "437 19363\n",
      "Epoch: 22351/50000..  Training Loss: 0.05645911..  Test Loss: 0.42496020..  Accurary:   0.02207070707070707\n",
      "1393 18407\n",
      "Epoch: 22401/50000..  Training Loss: 0.05596787..  Test Loss: 0.40797645..  Accurary:   0.07035353535353535\n",
      "2639 17161\n",
      "Epoch: 22451/50000..  Training Loss: 0.04917798..  Test Loss: 0.41721803..  Accurary:   0.13328282828282828\n",
      "1065 18735\n",
      "Epoch: 22501/50000..  Training Loss: 0.04833264..  Test Loss: 0.42625478..  Accurary:   0.05378787878787879\n",
      "1814 17986\n",
      "Epoch: 22551/50000..  Training Loss: 0.03608897..  Test Loss: 0.42018747..  Accurary:   0.09161616161616161\n",
      "2394 17406\n",
      "Epoch: 22601/50000..  Training Loss: 0.05764423..  Test Loss: 0.43992808..  Accurary:   0.12090909090909091\n",
      "380 19420\n",
      "Epoch: 22651/50000..  Training Loss: 0.03405893..  Test Loss: 0.43330044..  Accurary:   0.01919191919191919\n",
      "1734 18066\n",
      "Epoch: 22701/50000..  Training Loss: 0.05019349..  Test Loss: 0.43543458..  Accurary:   0.08757575757575757\n",
      "2065 17735\n",
      "Epoch: 22751/50000..  Training Loss: 0.04488607..  Test Loss: 0.42207327..  Accurary:   0.1042929292929293\n",
      "2347 17453\n",
      "Epoch: 22801/50000..  Training Loss: 0.03322215..  Test Loss: 0.41553268..  Accurary:   0.11853535353535354\n",
      "1630 18170\n",
      "Epoch: 22851/50000..  Training Loss: 0.02790928..  Test Loss: 0.42304090..  Accurary:   0.08232323232323233\n",
      "1484 18316\n",
      "Epoch: 22901/50000..  Training Loss: 0.04263979..  Test Loss: 0.45347628..  Accurary:   0.07494949494949495\n",
      "926 18874\n",
      "Epoch: 22951/50000..  Training Loss: 0.03848638..  Test Loss: 0.44464621..  Accurary:   0.04676767676767677\n",
      "503 19297\n",
      "Epoch: 23001/50000..  Training Loss: 0.03480119..  Test Loss: 0.43068138..  Accurary:   0.025404040404040405\n",
      "1000 18800\n",
      "Epoch: 23051/50000..  Training Loss: 0.02856965..  Test Loss: 0.41459167..  Accurary:   0.050505050505050504\n",
      "944 18856\n",
      "Epoch: 23101/50000..  Training Loss: 0.02992924..  Test Loss: 0.43377131..  Accurary:   0.04767676767676768\n",
      "594 19206\n",
      "Epoch: 23151/50000..  Training Loss: 0.05058718..  Test Loss: 0.43374270..  Accurary:   0.03\n",
      "1185 18615\n",
      "Epoch: 23201/50000..  Training Loss: 0.04187684..  Test Loss: 0.42847854..  Accurary:   0.059848484848484845\n",
      "1337 18463\n",
      "Epoch: 23251/50000..  Training Loss: 0.03743013..  Test Loss: 0.42157695..  Accurary:   0.06752525252525253\n",
      "2627 17173\n",
      "Epoch: 23301/50000..  Training Loss: 0.03256736..  Test Loss: 0.41578406..  Accurary:   0.13267676767676767\n",
      "2545 17255\n",
      "Epoch: 23351/50000..  Training Loss: 0.03606247..  Test Loss: 0.43048590..  Accurary:   0.12853535353535353\n",
      "3399 16401\n",
      "Epoch: 23401/50000..  Training Loss: 0.02832528..  Test Loss: 0.42495137..  Accurary:   0.17166666666666666\n",
      "1797 18003\n",
      "Epoch: 23451/50000..  Training Loss: 0.04084008..  Test Loss: 0.41350624..  Accurary:   0.09075757575757576\n",
      "3026 16774\n",
      "Epoch: 23501/50000..  Training Loss: 0.03220817..  Test Loss: 0.42261708..  Accurary:   0.15282828282828284\n",
      "1462 18338\n",
      "Epoch: 23551/50000..  Training Loss: 0.04269111..  Test Loss: 0.43887833..  Accurary:   0.07383838383838384\n",
      "826 18974\n",
      "Epoch: 23601/50000..  Training Loss: 0.03769964..  Test Loss: 0.42587480..  Accurary:   0.041717171717171715\n",
      "662 19138\n",
      "Epoch: 23651/50000..  Training Loss: 0.04177039..  Test Loss: 0.42651200..  Accurary:   0.033434343434343435\n",
      "503 19297\n",
      "Epoch: 23701/50000..  Training Loss: 0.06190582..  Test Loss: 0.42104843..  Accurary:   0.025404040404040405\n",
      "549 19251\n",
      "Epoch: 23751/50000..  Training Loss: 0.03679197..  Test Loss: 0.41463345..  Accurary:   0.02772727272727273\n",
      "494 19306\n",
      "Epoch: 23801/50000..  Training Loss: 0.03414659..  Test Loss: 0.42515492..  Accurary:   0.02494949494949495\n",
      "1566 18234\n",
      "Epoch: 23851/50000..  Training Loss: 0.04605542..  Test Loss: 0.42628774..  Accurary:   0.07909090909090909\n",
      "1114 18686\n",
      "Epoch: 23901/50000..  Training Loss: 0.03598468..  Test Loss: 0.42016938..  Accurary:   0.056262626262626264\n",
      "909 18891\n",
      "Epoch: 23951/50000..  Training Loss: 0.03887883..  Test Loss: 0.42466289..  Accurary:   0.045909090909090906\n",
      "3009 16791\n",
      "Epoch: 24001/50000..  Training Loss: 0.04057764..  Test Loss: 0.43093026..  Accurary:   0.15196969696969698\n",
      "851 18949\n",
      "Epoch: 24051/50000..  Training Loss: 0.03821348..  Test Loss: 0.44174889..  Accurary:   0.04297979797979798\n",
      "2051 17749\n",
      "Epoch: 24101/50000..  Training Loss: 0.03523519..  Test Loss: 0.42217413..  Accurary:   0.10358585858585859\n",
      "1318 18482\n",
      "Epoch: 24151/50000..  Training Loss: 0.03992277..  Test Loss: 0.41079378..  Accurary:   0.06656565656565656\n",
      "1016 18784\n",
      "Epoch: 24201/50000..  Training Loss: 0.03659696..  Test Loss: 0.45539719..  Accurary:   0.05131313131313131\n",
      "921 18879\n",
      "Epoch: 24251/50000..  Training Loss: 0.03067786..  Test Loss: 0.42937785..  Accurary:   0.046515151515151516\n",
      "656 19144\n",
      "Epoch: 24301/50000..  Training Loss: 0.03705752..  Test Loss: 0.43400335..  Accurary:   0.03313131313131313\n",
      "1560 18240\n",
      "Epoch: 24351/50000..  Training Loss: 0.04220856..  Test Loss: 0.42576981..  Accurary:   0.07878787878787878\n",
      "387 19413\n",
      "Epoch: 24401/50000..  Training Loss: 0.04653574..  Test Loss: 0.44423819..  Accurary:   0.019545454545454546\n",
      "1384 18416\n",
      "Epoch: 24451/50000..  Training Loss: 0.04008138..  Test Loss: 0.42322999..  Accurary:   0.0698989898989899\n",
      "1512 18288\n",
      "Epoch: 24501/50000..  Training Loss: 0.02946437..  Test Loss: 0.41674232..  Accurary:   0.07636363636363637\n",
      "841 18959\n",
      "Epoch: 24551/50000..  Training Loss: 0.03499427..  Test Loss: 0.41831917..  Accurary:   0.04247474747474748\n",
      "500 19300\n",
      "Epoch: 24601/50000..  Training Loss: 0.05239717..  Test Loss: 0.42719737..  Accurary:   0.025252525252525252\n",
      "1201 18599\n",
      "Epoch: 24651/50000..  Training Loss: 0.02907102..  Test Loss: 0.43873230..  Accurary:   0.06065656565656566\n",
      "1450 18350\n",
      "Epoch: 24701/50000..  Training Loss: 0.05463476..  Test Loss: 0.42009845..  Accurary:   0.07323232323232323\n",
      "1014 18786\n",
      "Epoch: 24751/50000..  Training Loss: 0.03542437..  Test Loss: 0.44070649..  Accurary:   0.05121212121212121\n",
      "879 18921\n",
      "Epoch: 24801/50000..  Training Loss: 0.04809049..  Test Loss: 0.43967408..  Accurary:   0.044393939393939395\n",
      "1683 18117\n",
      "Epoch: 24851/50000..  Training Loss: 0.03137070..  Test Loss: 0.42128289..  Accurary:   0.085\n",
      "1163 18637\n",
      "Epoch: 24901/50000..  Training Loss: 0.02192590..  Test Loss: 0.42659461..  Accurary:   0.058737373737373734\n",
      "444 19356\n",
      "Epoch: 24951/50000..  Training Loss: 0.05222434..  Test Loss: 0.44915634..  Accurary:   0.022424242424242423\n",
      "2583 17217\n",
      "Epoch: 25001/50000..  Training Loss: 0.04358962..  Test Loss: 0.40930247..  Accurary:   0.13045454545454546\n",
      "1736 18064\n",
      "Epoch: 25051/50000..  Training Loss: 0.06399616..  Test Loss: 0.43294913..  Accurary:   0.08767676767676767\n",
      "829 18971\n",
      "Epoch: 25101/50000..  Training Loss: 0.03556221..  Test Loss: 0.42822063..  Accurary:   0.04186868686868687\n",
      "1446 18354\n",
      "Epoch: 25151/50000..  Training Loss: 0.03267292..  Test Loss: 0.41412690..  Accurary:   0.07303030303030303\n",
      "1096 18704\n",
      "Epoch: 25201/50000..  Training Loss: 0.03265251..  Test Loss: 0.43502057..  Accurary:   0.055353535353535356\n",
      "735 19065\n",
      "Epoch: 25251/50000..  Training Loss: 0.03215264..  Test Loss: 0.42909181..  Accurary:   0.037121212121212124\n",
      "1214 18586\n",
      "Epoch: 25301/50000..  Training Loss: 0.04286093..  Test Loss: 0.44069597..  Accurary:   0.06131313131313131\n",
      "385 19415\n",
      "Epoch: 25351/50000..  Training Loss: 0.03684995..  Test Loss: 0.41890693..  Accurary:   0.019444444444444445\n",
      "586 19214\n",
      "Epoch: 25401/50000..  Training Loss: 0.05826407..  Test Loss: 0.42713523..  Accurary:   0.029595959595959596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 18200\n",
      "Epoch: 25451/50000..  Training Loss: 0.04903983..  Test Loss: 0.44936198..  Accurary:   0.08080808080808081\n",
      "1798 18002\n",
      "Epoch: 25501/50000..  Training Loss: 0.03063482..  Test Loss: 0.42692885..  Accurary:   0.09080808080808081\n",
      "1254 18546\n",
      "Epoch: 25551/50000..  Training Loss: 0.05408298..  Test Loss: 0.42702791..  Accurary:   0.06333333333333334\n",
      "610 19190\n",
      "Epoch: 25601/50000..  Training Loss: 0.05210299..  Test Loss: 0.43489406..  Accurary:   0.03080808080808081\n",
      "967 18833\n",
      "Epoch: 25651/50000..  Training Loss: 0.03972039..  Test Loss: 0.43427196..  Accurary:   0.04883838383838384\n",
      "1794 18006\n",
      "Epoch: 25701/50000..  Training Loss: 0.04127069..  Test Loss: 0.42213699..  Accurary:   0.0906060606060606\n",
      "1301 18499\n",
      "Epoch: 25751/50000..  Training Loss: 0.04762459..  Test Loss: 0.42909652..  Accurary:   0.06570707070707071\n",
      "1558 18242\n",
      "Epoch: 25801/50000..  Training Loss: 0.02708438..  Test Loss: 0.42635792..  Accurary:   0.07868686868686868\n",
      "3089 16711\n",
      "Epoch: 25851/50000..  Training Loss: 0.03673591..  Test Loss: 0.43239489..  Accurary:   0.15601010101010102\n",
      "1456 18344\n",
      "Epoch: 25901/50000..  Training Loss: 0.04713061..  Test Loss: 0.42449531..  Accurary:   0.07353535353535354\n",
      "606 19194\n",
      "Epoch: 25951/50000..  Training Loss: 0.03230884..  Test Loss: 0.42462343..  Accurary:   0.030606060606060605\n",
      "980 18820\n",
      "Epoch: 26001/50000..  Training Loss: 0.02953131..  Test Loss: 0.42142272..  Accurary:   0.049494949494949494\n",
      "773 19027\n",
      "Epoch: 26051/50000..  Training Loss: 0.04154482..  Test Loss: 0.43234825..  Accurary:   0.03904040404040404\n",
      "603 19197\n",
      "Epoch: 26101/50000..  Training Loss: 0.08177505..  Test Loss: 0.43522877..  Accurary:   0.030454545454545453\n",
      "1913 17887\n",
      "Epoch: 26151/50000..  Training Loss: 0.04963772..  Test Loss: 0.41514733..  Accurary:   0.09661616161616161\n",
      "955 18845\n",
      "Epoch: 26201/50000..  Training Loss: 0.03407392..  Test Loss: 0.42418101..  Accurary:   0.04823232323232323\n",
      "1487 18313\n",
      "Epoch: 26251/50000..  Training Loss: 0.03157746..  Test Loss: 0.43825802..  Accurary:   0.0751010101010101\n",
      "402 19398\n",
      "Epoch: 26301/50000..  Training Loss: 0.05704203..  Test Loss: 0.43586355..  Accurary:   0.020303030303030302\n",
      "1630 18170\n",
      "Epoch: 26351/50000..  Training Loss: 0.04249688..  Test Loss: 0.41483632..  Accurary:   0.08232323232323233\n",
      "483 19317\n",
      "Epoch: 26401/50000..  Training Loss: 0.02868137..  Test Loss: 0.42419887..  Accurary:   0.024393939393939395\n",
      "1656 18144\n",
      "Epoch: 26451/50000..  Training Loss: 0.03386229..  Test Loss: 0.42185798..  Accurary:   0.08363636363636363\n",
      "1849 17951\n",
      "Epoch: 26501/50000..  Training Loss: 0.03430287..  Test Loss: 0.43128571..  Accurary:   0.09338383838383839\n",
      "890 18910\n",
      "Epoch: 26551/50000..  Training Loss: 0.03534002..  Test Loss: 0.41916844..  Accurary:   0.04494949494949495\n",
      "714 19086\n",
      "Epoch: 26601/50000..  Training Loss: 0.02486729..  Test Loss: 0.43182868..  Accurary:   0.036060606060606064\n",
      "901 18899\n",
      "Epoch: 26651/50000..  Training Loss: 0.06517247..  Test Loss: 0.43564236..  Accurary:   0.04550505050505051\n",
      "1337 18463\n",
      "Epoch: 26701/50000..  Training Loss: 0.05130503..  Test Loss: 0.41570422..  Accurary:   0.06752525252525253\n",
      "6523 13277\n",
      "Epoch: 26751/50000..  Training Loss: 0.03314064..  Test Loss: 0.43017617..  Accurary:   0.32944444444444443\n",
      "625 19175\n",
      "Epoch: 26801/50000..  Training Loss: 0.02902607..  Test Loss: 0.42874232..  Accurary:   0.03156565656565657\n",
      "967 18833\n",
      "Epoch: 26851/50000..  Training Loss: 0.04118721..  Test Loss: 0.43055639..  Accurary:   0.04883838383838384\n",
      "3525 16275\n",
      "Epoch: 26901/50000..  Training Loss: 0.03227777..  Test Loss: 0.42640772..  Accurary:   0.17803030303030304\n",
      "441 19359\n",
      "Epoch: 26951/50000..  Training Loss: 0.03394040..  Test Loss: 0.42723000..  Accurary:   0.022272727272727274\n",
      "1370 18430\n",
      "Epoch: 27001/50000..  Training Loss: 0.02930318..  Test Loss: 0.42691430..  Accurary:   0.0691919191919192\n",
      "859 18941\n",
      "Epoch: 27051/50000..  Training Loss: 0.03317998..  Test Loss: 0.41807473..  Accurary:   0.043383838383838386\n",
      "701 19099\n",
      "Epoch: 27101/50000..  Training Loss: 0.03560948..  Test Loss: 0.42376894..  Accurary:   0.0354040404040404\n",
      "387 19413\n",
      "Epoch: 27151/50000..  Training Loss: 0.04523591..  Test Loss: 0.41493377..  Accurary:   0.019545454545454546\n",
      "894 18906\n",
      "Epoch: 27201/50000..  Training Loss: 0.03649478..  Test Loss: 0.41582599..  Accurary:   0.04515151515151515\n",
      "4488 15312\n",
      "Epoch: 27251/50000..  Training Loss: 0.04241146..  Test Loss: 0.40913266..  Accurary:   0.22666666666666666\n",
      "2714 17086\n",
      "Epoch: 27301/50000..  Training Loss: 0.03795000..  Test Loss: 0.41998559..  Accurary:   0.13707070707070707\n",
      "2570 17230\n",
      "Epoch: 27351/50000..  Training Loss: 0.03864417..  Test Loss: 0.41809249..  Accurary:   0.1297979797979798\n",
      "2451 17349\n",
      "Epoch: 27401/50000..  Training Loss: 0.03279183..  Test Loss: 0.41571096..  Accurary:   0.1237878787878788\n",
      "1863 17937\n",
      "Epoch: 27451/50000..  Training Loss: 0.03809001..  Test Loss: 0.43545637..  Accurary:   0.09409090909090909\n",
      "693 19107\n",
      "Epoch: 27501/50000..  Training Loss: 0.03793344..  Test Loss: 0.43027648..  Accurary:   0.035\n",
      "904 18896\n",
      "Epoch: 27551/50000..  Training Loss: 0.03302772..  Test Loss: 0.42977464..  Accurary:   0.04565656565656566\n",
      "938 18862\n",
      "Epoch: 27601/50000..  Training Loss: 0.03641361..  Test Loss: 0.42437062..  Accurary:   0.047373737373737373\n",
      "1513 18287\n",
      "Epoch: 27651/50000..  Training Loss: 0.04040276..  Test Loss: 0.42811143..  Accurary:   0.07641414141414142\n",
      "871 18929\n",
      "Epoch: 27701/50000..  Training Loss: 0.04211354..  Test Loss: 0.41886216..  Accurary:   0.04398989898989899\n",
      "1355 18445\n",
      "Epoch: 27751/50000..  Training Loss: 0.02865169..  Test Loss: 0.41989177..  Accurary:   0.06843434343434343\n",
      "1516 18284\n",
      "Epoch: 27801/50000..  Training Loss: 0.04337664..  Test Loss: 0.41501895..  Accurary:   0.07656565656565656\n",
      "2078 17722\n",
      "Epoch: 27851/50000..  Training Loss: 0.04075321..  Test Loss: 0.41804984..  Accurary:   0.10494949494949495\n",
      "1457 18343\n",
      "Epoch: 27901/50000..  Training Loss: 0.04015322..  Test Loss: 0.42359892..  Accurary:   0.07358585858585859\n",
      "594 19206\n",
      "Epoch: 27951/50000..  Training Loss: 0.02917034..  Test Loss: 0.43006006..  Accurary:   0.03\n",
      "2331 17469\n",
      "Epoch: 28001/50000..  Training Loss: 0.03391134..  Test Loss: 0.41932711..  Accurary:   0.11772727272727272\n",
      "2319 17481\n",
      "Epoch: 28051/50000..  Training Loss: 0.03395453..  Test Loss: 0.41214722..  Accurary:   0.11712121212121213\n",
      "5234 14566\n",
      "Epoch: 28101/50000..  Training Loss: 0.04801248..  Test Loss: 0.43008083..  Accurary:   0.2643434343434343\n",
      "1087 18713\n",
      "Epoch: 28151/50000..  Training Loss: 0.03775196..  Test Loss: 0.42103958..  Accurary:   0.0548989898989899\n",
      "1391 18409\n",
      "Epoch: 28201/50000..  Training Loss: 0.03663404..  Test Loss: 0.42921135..  Accurary:   0.07025252525252525\n",
      "7628 12172\n",
      "Epoch: 28251/50000..  Training Loss: 0.06100970..  Test Loss: 0.41201225..  Accurary:   0.38525252525252524\n",
      "2403 17397\n",
      "Epoch: 28301/50000..  Training Loss: 0.06493784..  Test Loss: 0.41189334..  Accurary:   0.12136363636363637\n",
      "4878 14922\n",
      "Epoch: 28351/50000..  Training Loss: 0.03382698..  Test Loss: 0.43217665..  Accurary:   0.24636363636363637\n",
      "2097 17703\n",
      "Epoch: 28401/50000..  Training Loss: 0.03882273..  Test Loss: 0.41820988..  Accurary:   0.10590909090909091\n",
      "996 18804\n",
      "Epoch: 28451/50000..  Training Loss: 0.03552111..  Test Loss: 0.41710347..  Accurary:   0.0503030303030303\n",
      "1249 18551\n",
      "Epoch: 28501/50000..  Training Loss: 0.03569807..  Test Loss: 0.42648974..  Accurary:   0.06308080808080808\n",
      "412 19388\n",
      "Epoch: 28551/50000..  Training Loss: 0.06232047..  Test Loss: 0.44107980..  Accurary:   0.020808080808080807\n",
      "1012 18788\n",
      "Epoch: 28601/50000..  Training Loss: 0.03607270..  Test Loss: 0.44036922..  Accurary:   0.051111111111111114\n",
      "1743 18057\n",
      "Epoch: 28651/50000..  Training Loss: 0.03831910..  Test Loss: 0.41352791..  Accurary:   0.08803030303030303\n",
      "618 19182\n",
      "Epoch: 28701/50000..  Training Loss: 0.06840524..  Test Loss: 0.43643618..  Accurary:   0.031212121212121212\n",
      "5539 14261\n",
      "Epoch: 28751/50000..  Training Loss: 0.03081482..  Test Loss: 0.42391795..  Accurary:   0.27974747474747474\n",
      "475 19325\n",
      "Epoch: 28801/50000..  Training Loss: 0.03784252..  Test Loss: 0.43054864..  Accurary:   0.023989898989898988\n",
      "1293 18507\n",
      "Epoch: 28851/50000..  Training Loss: 0.03058644..  Test Loss: 0.41431832..  Accurary:   0.06530303030303031\n",
      "1329 18471\n",
      "Epoch: 28901/50000..  Training Loss: 0.03294576..  Test Loss: 0.43272585..  Accurary:   0.06712121212121212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466 19334\n",
      "Epoch: 28951/50000..  Training Loss: 0.03924905..  Test Loss: 0.42882833..  Accurary:   0.023535353535353534\n",
      "3968 15832\n",
      "Epoch: 29001/50000..  Training Loss: 0.04442991..  Test Loss: 0.41406882..  Accurary:   0.20040404040404042\n",
      "2495 17305\n",
      "Epoch: 29051/50000..  Training Loss: 0.03469485..  Test Loss: 0.40628704..  Accurary:   0.12601010101010102\n",
      "1714 18086\n",
      "Epoch: 29101/50000..  Training Loss: 0.02441276..  Test Loss: 0.43602046..  Accurary:   0.08656565656565657\n",
      "1202 18598\n",
      "Epoch: 29151/50000..  Training Loss: 0.03500573..  Test Loss: 0.41885689..  Accurary:   0.06070707070707071\n",
      "1391 18409\n",
      "Epoch: 29201/50000..  Training Loss: 0.03770868..  Test Loss: 0.41853458..  Accurary:   0.07025252525252525\n",
      "678 19122\n",
      "Epoch: 29251/50000..  Training Loss: 0.04044971..  Test Loss: 0.41993520..  Accurary:   0.03424242424242424\n",
      "1380 18420\n",
      "Epoch: 29301/50000..  Training Loss: 0.03678623..  Test Loss: 0.41089854..  Accurary:   0.0696969696969697\n",
      "504 19296\n",
      "Epoch: 29351/50000..  Training Loss: 0.04819138..  Test Loss: 0.43041083..  Accurary:   0.025454545454545455\n",
      "970 18830\n",
      "Epoch: 29401/50000..  Training Loss: 0.02836443..  Test Loss: 0.42458540..  Accurary:   0.04898989898989899\n",
      "580 19220\n",
      "Epoch: 29451/50000..  Training Loss: 0.03803824..  Test Loss: 0.42750490..  Accurary:   0.029292929292929294\n",
      "1045 18755\n",
      "Epoch: 29501/50000..  Training Loss: 0.03203424..  Test Loss: 0.42722085..  Accurary:   0.05277777777777778\n",
      "1917 17883\n",
      "Epoch: 29551/50000..  Training Loss: 0.04186682..  Test Loss: 0.42374128..  Accurary:   0.09681818181818182\n",
      "1218 18582\n",
      "Epoch: 29601/50000..  Training Loss: 0.03985841..  Test Loss: 0.43786004..  Accurary:   0.061515151515151516\n",
      "1155 18645\n",
      "Epoch: 29651/50000..  Training Loss: 0.03225612..  Test Loss: 0.43542707..  Accurary:   0.058333333333333334\n",
      "988 18812\n",
      "Epoch: 29701/50000..  Training Loss: 0.03994289..  Test Loss: 0.44219336..  Accurary:   0.0498989898989899\n",
      "1193 18607\n",
      "Epoch: 29751/50000..  Training Loss: 0.03385379..  Test Loss: 0.42085695..  Accurary:   0.06025252525252525\n",
      "3141 16659\n",
      "Epoch: 29801/50000..  Training Loss: 0.03382011..  Test Loss: 0.41939408..  Accurary:   0.15863636363636363\n",
      "547 19253\n",
      "Epoch: 29851/50000..  Training Loss: 0.03391217..  Test Loss: 0.42259589..  Accurary:   0.027626262626262627\n",
      "1188 18612\n",
      "Epoch: 29901/50000..  Training Loss: 0.03053257..  Test Loss: 0.43738154..  Accurary:   0.06\n",
      "937 18863\n",
      "Epoch: 29951/50000..  Training Loss: 0.03566922..  Test Loss: 0.44689324..  Accurary:   0.04732323232323232\n",
      "990 18810\n",
      "Epoch: 30001/50000..  Training Loss: 0.04643754..  Test Loss: 0.43263435..  Accurary:   0.05\n",
      "611 19189\n",
      "Epoch: 30051/50000..  Training Loss: 0.02989035..  Test Loss: 0.44058067..  Accurary:   0.03085858585858586\n",
      "1007 18793\n",
      "Epoch: 30101/50000..  Training Loss: 0.04242351..  Test Loss: 0.43707895..  Accurary:   0.05085858585858586\n",
      "633 19167\n",
      "Epoch: 30151/50000..  Training Loss: 0.03712400..  Test Loss: 0.46259680..  Accurary:   0.03196969696969697\n",
      "3279 16521\n",
      "Epoch: 30201/50000..  Training Loss: 0.02905211..  Test Loss: 0.42618293..  Accurary:   0.16560606060606062\n",
      "817 18983\n",
      "Epoch: 30251/50000..  Training Loss: 0.03953705..  Test Loss: 0.43757319..  Accurary:   0.041262626262626265\n",
      "2387 17413\n",
      "Epoch: 30301/50000..  Training Loss: 0.03873278..  Test Loss: 0.43204021..  Accurary:   0.12055555555555555\n",
      "352 19448\n",
      "Epoch: 30351/50000..  Training Loss: 0.03861130..  Test Loss: 0.45260268..  Accurary:   0.017777777777777778\n",
      "2247 17553\n",
      "Epoch: 30401/50000..  Training Loss: 0.04882164..  Test Loss: 0.42006516..  Accurary:   0.11348484848484848\n",
      "1092 18708\n",
      "Epoch: 30451/50000..  Training Loss: 0.03417900..  Test Loss: 0.42676932..  Accurary:   0.05515151515151515\n",
      "2279 17521\n",
      "Epoch: 30501/50000..  Training Loss: 0.02954568..  Test Loss: 0.42743480..  Accurary:   0.1151010101010101\n",
      "484 19316\n",
      "Epoch: 30551/50000..  Training Loss: 0.03195679..  Test Loss: 0.43684733..  Accurary:   0.024444444444444446\n",
      "806 18994\n",
      "Epoch: 30601/50000..  Training Loss: 0.03314011..  Test Loss: 0.42279434..  Accurary:   0.040707070707070706\n",
      "580 19220\n",
      "Epoch: 30651/50000..  Training Loss: 0.02976153..  Test Loss: 0.42601356..  Accurary:   0.029292929292929294\n",
      "3482 16318\n",
      "Epoch: 30701/50000..  Training Loss: 0.03893318..  Test Loss: 0.42076311..  Accurary:   0.17585858585858585\n",
      "1059 18741\n",
      "Epoch: 30751/50000..  Training Loss: 0.03464001..  Test Loss: 0.42332485..  Accurary:   0.05348484848484848\n",
      "3372 16428\n",
      "Epoch: 30801/50000..  Training Loss: 0.04581471..  Test Loss: 0.43065479..  Accurary:   0.1703030303030303\n",
      "2607 17193\n",
      "Epoch: 30851/50000..  Training Loss: 0.05019796..  Test Loss: 0.41891423..  Accurary:   0.13166666666666665\n",
      "4331 15469\n",
      "Epoch: 30901/50000..  Training Loss: 0.06120690..  Test Loss: 0.40425637..  Accurary:   0.21873737373737373\n",
      "480 19320\n",
      "Epoch: 30951/50000..  Training Loss: 0.04982804..  Test Loss: 0.43288964..  Accurary:   0.024242424242424242\n",
      "777 19023\n",
      "Epoch: 31001/50000..  Training Loss: 0.03449847..  Test Loss: 0.42716196..  Accurary:   0.039242424242424245\n",
      "695 19105\n",
      "Epoch: 31051/50000..  Training Loss: 0.03448354..  Test Loss: 0.42273340..  Accurary:   0.0351010101010101\n",
      "1829 17971\n",
      "Epoch: 31101/50000..  Training Loss: 0.02746144..  Test Loss: 0.43243125..  Accurary:   0.09237373737373737\n",
      "1332 18468\n",
      "Epoch: 31151/50000..  Training Loss: 0.03758960..  Test Loss: 0.43150944..  Accurary:   0.06727272727272728\n",
      "3161 16639\n",
      "Epoch: 31201/50000..  Training Loss: 0.03242641..  Test Loss: 0.43384054..  Accurary:   0.15964646464646465\n",
      "468 19332\n",
      "Epoch: 31251/50000..  Training Loss: 0.03214187..  Test Loss: 0.43660402..  Accurary:   0.023636363636363636\n",
      "2655 17145\n",
      "Epoch: 31301/50000..  Training Loss: 0.03801430..  Test Loss: 0.43622988..  Accurary:   0.1340909090909091\n",
      "1426 18374\n",
      "Epoch: 31351/50000..  Training Loss: 0.03175814..  Test Loss: 0.42324865..  Accurary:   0.07202020202020203\n",
      "363 19437\n",
      "Epoch: 31401/50000..  Training Loss: 0.03139897..  Test Loss: 0.43564084..  Accurary:   0.018333333333333333\n",
      "1377 18423\n",
      "Epoch: 31451/50000..  Training Loss: 0.03160623..  Test Loss: 0.43246302..  Accurary:   0.06954545454545455\n",
      "513 19287\n",
      "Epoch: 31501/50000..  Training Loss: 0.03451908..  Test Loss: 0.42709345..  Accurary:   0.02590909090909091\n",
      "7261 12539\n",
      "Epoch: 31551/50000..  Training Loss: 0.03066128..  Test Loss: 0.43000335..  Accurary:   0.3667171717171717\n",
      "1071 18729\n",
      "Epoch: 31601/50000..  Training Loss: 0.04990549..  Test Loss: 0.43813875..  Accurary:   0.05409090909090909\n",
      "2382 17418\n",
      "Epoch: 31651/50000..  Training Loss: 0.04089254..  Test Loss: 0.42799163..  Accurary:   0.1203030303030303\n",
      "2337 17463\n",
      "Epoch: 31701/50000..  Training Loss: 0.03934246..  Test Loss: 0.42455330..  Accurary:   0.11803030303030303\n",
      "853 18947\n",
      "Epoch: 31751/50000..  Training Loss: 0.04607088..  Test Loss: 0.44105184..  Accurary:   0.04308080808080808\n",
      "2282 17518\n",
      "Epoch: 31801/50000..  Training Loss: 0.04216234..  Test Loss: 0.42677605..  Accurary:   0.11525252525252526\n",
      "860 18940\n",
      "Epoch: 31851/50000..  Training Loss: 0.02708433..  Test Loss: 0.43981299..  Accurary:   0.043434343434343436\n",
      "1200 18600\n",
      "Epoch: 31901/50000..  Training Loss: 0.03641506..  Test Loss: 0.43128693..  Accurary:   0.06060606060606061\n",
      "2747 17053\n",
      "Epoch: 31951/50000..  Training Loss: 0.03031554..  Test Loss: 0.42907679..  Accurary:   0.13873737373737374\n",
      "1308 18492\n",
      "Epoch: 32001/50000..  Training Loss: 0.03473786..  Test Loss: 0.42828515..  Accurary:   0.06606060606060606\n",
      "3437 16363\n",
      "Epoch: 32051/50000..  Training Loss: 0.04161977..  Test Loss: 0.41944927..  Accurary:   0.1735858585858586\n",
      "1047 18753\n",
      "Epoch: 32101/50000..  Training Loss: 0.02975929..  Test Loss: 0.43114942..  Accurary:   0.05287878787878788\n",
      "1848 17952\n",
      "Epoch: 32151/50000..  Training Loss: 0.03557077..  Test Loss: 0.42638010..  Accurary:   0.09333333333333334\n",
      "933 18867\n",
      "Epoch: 32201/50000..  Training Loss: 0.02548539..  Test Loss: 0.43133211..  Accurary:   0.04712121212121212\n",
      "4699 15101\n",
      "Epoch: 32251/50000..  Training Loss: 0.03287620..  Test Loss: 0.42177659..  Accurary:   0.23732323232323232\n",
      "1012 18788\n",
      "Epoch: 32301/50000..  Training Loss: 0.03844286..  Test Loss: 0.43722725..  Accurary:   0.051111111111111114\n",
      "881 18919\n",
      "Epoch: 32351/50000..  Training Loss: 0.03449426..  Test Loss: 0.43169916..  Accurary:   0.0444949494949495\n",
      "1679 18121\n",
      "Epoch: 32401/50000..  Training Loss: 0.02845917..  Test Loss: 0.43505469..  Accurary:   0.0847979797979798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2286 17514\n",
      "Epoch: 32451/50000..  Training Loss: 0.04685724..  Test Loss: 0.40653241..  Accurary:   0.11545454545454545\n",
      "1028 18772\n",
      "Epoch: 32501/50000..  Training Loss: 0.05732694..  Test Loss: 0.43094346..  Accurary:   0.05191919191919192\n",
      "1518 18282\n",
      "Epoch: 32551/50000..  Training Loss: 0.03045563..  Test Loss: 0.42443901..  Accurary:   0.07666666666666666\n",
      "2680 17120\n",
      "Epoch: 32601/50000..  Training Loss: 0.03601288..  Test Loss: 0.42822355..  Accurary:   0.13535353535353536\n",
      "778 19022\n",
      "Epoch: 32651/50000..  Training Loss: 0.02779524..  Test Loss: 0.44216105..  Accurary:   0.039292929292929296\n",
      "2134 17666\n",
      "Epoch: 32701/50000..  Training Loss: 0.04553694..  Test Loss: 0.42648330..  Accurary:   0.10777777777777778\n",
      "2171 17629\n",
      "Epoch: 32751/50000..  Training Loss: 0.03189895..  Test Loss: 0.43808255..  Accurary:   0.10964646464646464\n",
      "2313 17487\n",
      "Epoch: 32801/50000..  Training Loss: 0.03162351..  Test Loss: 0.43070173..  Accurary:   0.11681818181818182\n",
      "1436 18364\n",
      "Epoch: 32851/50000..  Training Loss: 0.03302971..  Test Loss: 0.44388336..  Accurary:   0.07252525252525252\n",
      "3020 16780\n",
      "Epoch: 32901/50000..  Training Loss: 0.02815798..  Test Loss: 0.43169469..  Accurary:   0.15252525252525254\n",
      "1676 18124\n",
      "Epoch: 32951/50000..  Training Loss: 0.02744994..  Test Loss: 0.44079718..  Accurary:   0.08464646464646465\n",
      "347 19453\n",
      "Epoch: 33001/50000..  Training Loss: 0.03251327..  Test Loss: 0.43615398..  Accurary:   0.017525252525252524\n",
      "4224 15576\n",
      "Epoch: 33051/50000..  Training Loss: 0.03098423..  Test Loss: 0.41918728..  Accurary:   0.21333333333333335\n",
      "2687 17113\n",
      "Epoch: 33101/50000..  Training Loss: 0.02446727..  Test Loss: 0.43002555..  Accurary:   0.13570707070707072\n",
      "1135 18665\n",
      "Epoch: 33151/50000..  Training Loss: 0.03226016..  Test Loss: 0.42141661..  Accurary:   0.057323232323232325\n",
      "1130 18670\n",
      "Epoch: 33201/50000..  Training Loss: 0.02487796..  Test Loss: 0.44333205..  Accurary:   0.05707070707070707\n",
      "537 19263\n",
      "Epoch: 33251/50000..  Training Loss: 0.02250517..  Test Loss: 0.42552415..  Accurary:   0.027121212121212122\n",
      "2403 17397\n",
      "Epoch: 33301/50000..  Training Loss: 0.04201586..  Test Loss: 0.44643593..  Accurary:   0.12136363636363637\n",
      "929 18871\n",
      "Epoch: 33351/50000..  Training Loss: 0.04128384..  Test Loss: 0.43691126..  Accurary:   0.046919191919191916\n",
      "1213 18587\n",
      "Epoch: 33401/50000..  Training Loss: 0.04632422..  Test Loss: 0.41580206..  Accurary:   0.06126262626262626\n",
      "1929 17871\n",
      "Epoch: 33451/50000..  Training Loss: 0.02773161..  Test Loss: 0.42764875..  Accurary:   0.09742424242424243\n",
      "4346 15454\n",
      "Epoch: 33501/50000..  Training Loss: 0.03465750..  Test Loss: 0.42384145..  Accurary:   0.2194949494949495\n",
      "407 19393\n",
      "Epoch: 33551/50000..  Training Loss: 0.03365337..  Test Loss: 0.44851869..  Accurary:   0.020555555555555556\n",
      "527 19273\n",
      "Epoch: 33601/50000..  Training Loss: 0.03299373..  Test Loss: 0.43876067..  Accurary:   0.026616161616161618\n",
      "585 19215\n",
      "Epoch: 33651/50000..  Training Loss: 0.03685530..  Test Loss: 0.42660955..  Accurary:   0.029545454545454545\n",
      "2644 17156\n",
      "Epoch: 33701/50000..  Training Loss: 0.03406001..  Test Loss: 0.42263937..  Accurary:   0.13353535353535353\n",
      "870 18930\n",
      "Epoch: 33751/50000..  Training Loss: 0.03065066..  Test Loss: 0.44775617..  Accurary:   0.04393939393939394\n",
      "1152 18648\n",
      "Epoch: 33801/50000..  Training Loss: 0.02994311..  Test Loss: 0.43642193..  Accurary:   0.05818181818181818\n",
      "724 19076\n",
      "Epoch: 33851/50000..  Training Loss: 0.02656799..  Test Loss: 0.43898931..  Accurary:   0.036565656565656565\n",
      "4016 15784\n",
      "Epoch: 33901/50000..  Training Loss: 0.03992014..  Test Loss: 0.42194021..  Accurary:   0.20282828282828283\n",
      "1450 18350\n",
      "Epoch: 33951/50000..  Training Loss: 0.03372249..  Test Loss: 0.42688477..  Accurary:   0.07323232323232323\n",
      "918 18882\n",
      "Epoch: 34001/50000..  Training Loss: 0.03064274..  Test Loss: 0.44199973..  Accurary:   0.046363636363636364\n",
      "4603 15197\n",
      "Epoch: 34051/50000..  Training Loss: 0.05011390..  Test Loss: 0.42456311..  Accurary:   0.23247474747474747\n",
      "1169 18631\n",
      "Epoch: 34101/50000..  Training Loss: 0.04457403..  Test Loss: 0.44181335..  Accurary:   0.05904040404040404\n",
      "1708 18092\n",
      "Epoch: 34151/50000..  Training Loss: 0.04577765..  Test Loss: 0.44042590..  Accurary:   0.08626262626262626\n",
      "1266 18534\n",
      "Epoch: 34201/50000..  Training Loss: 0.03235813..  Test Loss: 0.42808786..  Accurary:   0.06393939393939393\n",
      "1385 18415\n",
      "Epoch: 34251/50000..  Training Loss: 0.03168485..  Test Loss: 0.42230573..  Accurary:   0.06994949494949496\n",
      "2322 17478\n",
      "Epoch: 34301/50000..  Training Loss: 0.04359441..  Test Loss: 0.43568477..  Accurary:   0.11727272727272728\n",
      "838 18962\n",
      "Epoch: 34351/50000..  Training Loss: 0.04021913..  Test Loss: 0.43085542..  Accurary:   0.042323232323232325\n",
      "2857 16943\n",
      "Epoch: 34401/50000..  Training Loss: 0.03107932..  Test Loss: 0.42716551..  Accurary:   0.1442929292929293\n",
      "493 19307\n",
      "Epoch: 34451/50000..  Training Loss: 0.03855826..  Test Loss: 0.43333182..  Accurary:   0.0248989898989899\n",
      "3683 16117\n",
      "Epoch: 34501/50000..  Training Loss: 0.02645124..  Test Loss: 0.42696130..  Accurary:   0.18601010101010101\n",
      "881 18919\n",
      "Epoch: 34551/50000..  Training Loss: 0.04419630..  Test Loss: 0.43464315..  Accurary:   0.0444949494949495\n",
      "976 18824\n",
      "Epoch: 34601/50000..  Training Loss: 0.02755990..  Test Loss: 0.42064098..  Accurary:   0.04929292929292929\n",
      "2554 17246\n",
      "Epoch: 34651/50000..  Training Loss: 0.04040828..  Test Loss: 0.41079357..  Accurary:   0.128989898989899\n",
      "918 18882\n",
      "Epoch: 34701/50000..  Training Loss: 0.04278379..  Test Loss: 0.43670863..  Accurary:   0.046363636363636364\n",
      "4347 15453\n",
      "Epoch: 34751/50000..  Training Loss: 0.02781694..  Test Loss: 0.43522269..  Accurary:   0.21954545454545454\n",
      "3670 16130\n",
      "Epoch: 34801/50000..  Training Loss: 0.03751140..  Test Loss: 0.41255736..  Accurary:   0.18535353535353535\n",
      "4441 15359\n",
      "Epoch: 34851/50000..  Training Loss: 0.03243670..  Test Loss: 0.42464069..  Accurary:   0.2242929292929293\n",
      "2023 17777\n",
      "Epoch: 34901/50000..  Training Loss: 0.03263131..  Test Loss: 0.43744436..  Accurary:   0.10217171717171718\n",
      "2021 17779\n",
      "Epoch: 34951/50000..  Training Loss: 0.03037876..  Test Loss: 0.42181790..  Accurary:   0.10207070707070708\n",
      "1103 18697\n",
      "Epoch: 35001/50000..  Training Loss: 0.02796015..  Test Loss: 0.42907107..  Accurary:   0.055707070707070705\n",
      "1443 18357\n",
      "Epoch: 35051/50000..  Training Loss: 0.03679027..  Test Loss: 0.42500177..  Accurary:   0.07287878787878788\n",
      "1721 18079\n",
      "Epoch: 35101/50000..  Training Loss: 0.04043505..  Test Loss: 0.43884015..  Accurary:   0.08691919191919192\n",
      "4044 15756\n",
      "Epoch: 35151/50000..  Training Loss: 0.02694896..  Test Loss: 0.42679742..  Accurary:   0.20424242424242425\n",
      "628 19172\n",
      "Epoch: 35201/50000..  Training Loss: 0.02970587..  Test Loss: 0.43460608..  Accurary:   0.03171717171717172\n",
      "1801 17999\n",
      "Epoch: 35251/50000..  Training Loss: 0.03188724..  Test Loss: 0.44071120..  Accurary:   0.09095959595959596\n",
      "1769 18031\n",
      "Epoch: 35301/50000..  Training Loss: 0.03345365..  Test Loss: 0.45566356..  Accurary:   0.08934343434343435\n",
      "647 19153\n",
      "Epoch: 35351/50000..  Training Loss: 0.04078795..  Test Loss: 0.41735479..  Accurary:   0.03267676767676768\n",
      "2352 17448\n",
      "Epoch: 35401/50000..  Training Loss: 0.03293218..  Test Loss: 0.42531496..  Accurary:   0.11878787878787879\n",
      "4099 15701\n",
      "Epoch: 35451/50000..  Training Loss: 0.02734977..  Test Loss: 0.43312529..  Accurary:   0.20702020202020202\n",
      "1159 18641\n",
      "Epoch: 35501/50000..  Training Loss: 0.03120220..  Test Loss: 0.43220860..  Accurary:   0.05853535353535354\n",
      "1211 18589\n",
      "Epoch: 35551/50000..  Training Loss: 0.03222620..  Test Loss: 0.41243505..  Accurary:   0.06116161616161616\n",
      "1577 18223\n",
      "Epoch: 35601/50000..  Training Loss: 0.03212228..  Test Loss: 0.41719040..  Accurary:   0.07964646464646465\n",
      "2262 17538\n",
      "Epoch: 35651/50000..  Training Loss: 0.02777013..  Test Loss: 0.42619535..  Accurary:   0.11424242424242424\n",
      "757 19043\n",
      "Epoch: 35701/50000..  Training Loss: 0.03591074..  Test Loss: 0.42449248..  Accurary:   0.038232323232323236\n",
      "1250 18550\n",
      "Epoch: 35751/50000..  Training Loss: 0.03956026..  Test Loss: 0.42201892..  Accurary:   0.06313131313131314\n",
      "7744 12056\n",
      "Epoch: 35801/50000..  Training Loss: 0.04703214..  Test Loss: 0.41827554..  Accurary:   0.39111111111111113\n",
      "3198 16602\n",
      "Epoch: 35851/50000..  Training Loss: 0.04992156..  Test Loss: 0.41578895..  Accurary:   0.16151515151515153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674 18126\n",
      "Epoch: 35901/50000..  Training Loss: 0.04165621..  Test Loss: 0.42729104..  Accurary:   0.08454545454545455\n",
      "1002 18798\n",
      "Epoch: 35951/50000..  Training Loss: 0.03722727..  Test Loss: 0.42762908..  Accurary:   0.050606060606060606\n",
      "3180 16620\n",
      "Epoch: 36001/50000..  Training Loss: 0.03941303..  Test Loss: 0.43280628..  Accurary:   0.1606060606060606\n",
      "3443 16357\n",
      "Epoch: 36051/50000..  Training Loss: 0.03830453..  Test Loss: 0.42391768..  Accurary:   0.1738888888888889\n",
      "1158 18642\n",
      "Epoch: 36101/50000..  Training Loss: 0.03668927..  Test Loss: 0.43679124..  Accurary:   0.05848484848484849\n",
      "4666 15134\n",
      "Epoch: 36151/50000..  Training Loss: 0.02980845..  Test Loss: 0.42496684..  Accurary:   0.23565656565656565\n",
      "2062 17738\n",
      "Epoch: 36201/50000..  Training Loss: 0.02885743..  Test Loss: 0.43120059..  Accurary:   0.10414141414141415\n",
      "1530 18270\n",
      "Epoch: 36251/50000..  Training Loss: 0.03547852..  Test Loss: 0.42713952..  Accurary:   0.07727272727272727\n",
      "1559 18241\n",
      "Epoch: 36301/50000..  Training Loss: 0.03707192..  Test Loss: 0.44156259..  Accurary:   0.07873737373737373\n",
      "1147 18653\n",
      "Epoch: 36351/50000..  Training Loss: 0.03066754..  Test Loss: 0.43119287..  Accurary:   0.05792929292929293\n",
      "1964 17836\n",
      "Epoch: 36401/50000..  Training Loss: 0.04055813..  Test Loss: 0.42561150..  Accurary:   0.09919191919191919\n",
      "4139 15661\n",
      "Epoch: 36451/50000..  Training Loss: 0.03223930..  Test Loss: 0.42781320..  Accurary:   0.20904040404040405\n",
      "1628 18172\n",
      "Epoch: 36501/50000..  Training Loss: 0.03100554..  Test Loss: 0.42511496..  Accurary:   0.08222222222222222\n",
      "2355 17445\n",
      "Epoch: 36551/50000..  Training Loss: 0.04608310..  Test Loss: 0.43380401..  Accurary:   0.11893939393939394\n",
      "506 19294\n",
      "Epoch: 36601/50000..  Training Loss: 0.03549290..  Test Loss: 0.42175308..  Accurary:   0.025555555555555557\n",
      "586 19214\n",
      "Epoch: 36651/50000..  Training Loss: 0.03507128..  Test Loss: 0.43401146..  Accurary:   0.029595959595959596\n",
      "1803 17997\n",
      "Epoch: 36701/50000..  Training Loss: 0.02965761..  Test Loss: 0.42690301..  Accurary:   0.09106060606060606\n",
      "1696 18104\n",
      "Epoch: 36751/50000..  Training Loss: 0.05950048..  Test Loss: 0.43367779..  Accurary:   0.08565656565656565\n",
      "1129 18671\n",
      "Epoch: 36801/50000..  Training Loss: 0.02703486..  Test Loss: 0.41668364..  Accurary:   0.05702020202020202\n",
      "1783 18017\n",
      "Epoch: 36851/50000..  Training Loss: 0.03168624..  Test Loss: 0.42201754..  Accurary:   0.09005050505050505\n",
      "1293 18507\n",
      "Epoch: 36901/50000..  Training Loss: 0.02417376..  Test Loss: 0.44435802..  Accurary:   0.06530303030303031\n",
      "740 19060\n",
      "Epoch: 36951/50000..  Training Loss: 0.03917546..  Test Loss: 0.42679763..  Accurary:   0.03737373737373737\n",
      "4727 15073\n",
      "Epoch: 37001/50000..  Training Loss: 0.02897689..  Test Loss: 0.42362088..  Accurary:   0.23873737373737375\n",
      "1345 18455\n",
      "Epoch: 37051/50000..  Training Loss: 0.02934402..  Test Loss: 0.44006413..  Accurary:   0.06792929292929292\n",
      "1606 18194\n",
      "Epoch: 37101/50000..  Training Loss: 0.03144700..  Test Loss: 0.43276161..  Accurary:   0.0811111111111111\n",
      "4351 15449\n",
      "Epoch: 37151/50000..  Training Loss: 0.03219037..  Test Loss: 0.43363848..  Accurary:   0.21974747474747475\n",
      "1900 17900\n",
      "Epoch: 37201/50000..  Training Loss: 0.02969029..  Test Loss: 0.43208325..  Accurary:   0.09595959595959595\n",
      "2847 16953\n",
      "Epoch: 37251/50000..  Training Loss: 0.04509053..  Test Loss: 0.43118790..  Accurary:   0.1437878787878788\n",
      "917 18883\n",
      "Epoch: 37301/50000..  Training Loss: 0.04381867..  Test Loss: 0.44107890..  Accurary:   0.04631313131313131\n",
      "978 18822\n",
      "Epoch: 37351/50000..  Training Loss: 0.03551558..  Test Loss: 0.42137763..  Accurary:   0.04939393939393939\n",
      "1207 18593\n",
      "Epoch: 37401/50000..  Training Loss: 0.05924967..  Test Loss: 0.43775588..  Accurary:   0.06095959595959596\n",
      "925 18875\n",
      "Epoch: 37451/50000..  Training Loss: 0.03146626..  Test Loss: 0.43583995..  Accurary:   0.04671717171717172\n",
      "1673 18127\n",
      "Epoch: 37501/50000..  Training Loss: 0.03166292..  Test Loss: 0.42585868..  Accurary:   0.0844949494949495\n",
      "1248 18552\n",
      "Epoch: 37551/50000..  Training Loss: 0.04212574..  Test Loss: 0.42091003..  Accurary:   0.06303030303030303\n",
      "1691 18109\n",
      "Epoch: 37601/50000..  Training Loss: 0.02700865..  Test Loss: 0.42268416..  Accurary:   0.0854040404040404\n",
      "1728 18072\n",
      "Epoch: 37651/50000..  Training Loss: 0.02741407..  Test Loss: 0.42052200..  Accurary:   0.08727272727272728\n",
      "900 18900\n",
      "Epoch: 37701/50000..  Training Loss: 0.05942247..  Test Loss: 0.42813793..  Accurary:   0.045454545454545456\n",
      "1244 18556\n",
      "Epoch: 37751/50000..  Training Loss: 0.02703963..  Test Loss: 0.43237320..  Accurary:   0.06282828282828283\n",
      "984 18816\n",
      "Epoch: 37801/50000..  Training Loss: 0.03246679..  Test Loss: 0.41456267..  Accurary:   0.0496969696969697\n",
      "2663 17137\n",
      "Epoch: 37851/50000..  Training Loss: 0.03879664..  Test Loss: 0.43574575..  Accurary:   0.1344949494949495\n",
      "1520 18280\n",
      "Epoch: 37901/50000..  Training Loss: 0.02743618..  Test Loss: 0.41541094..  Accurary:   0.07676767676767676\n",
      "1851 17949\n",
      "Epoch: 37951/50000..  Training Loss: 0.03230653..  Test Loss: 0.42848244..  Accurary:   0.09348484848484849\n",
      "581 19219\n",
      "Epoch: 38001/50000..  Training Loss: 0.02996991..  Test Loss: 0.42945874..  Accurary:   0.029343434343434345\n",
      "1046 18754\n",
      "Epoch: 38051/50000..  Training Loss: 0.03025860..  Test Loss: 0.42006722..  Accurary:   0.05282828282828283\n",
      "2030 17770\n",
      "Epoch: 38101/50000..  Training Loss: 0.04530112..  Test Loss: 0.43899193..  Accurary:   0.10252525252525252\n",
      "1544 18256\n",
      "Epoch: 38151/50000..  Training Loss: 0.03465663..  Test Loss: 0.42550653..  Accurary:   0.07797979797979798\n",
      "2605 17195\n",
      "Epoch: 38201/50000..  Training Loss: 0.02799424..  Test Loss: 0.41279548..  Accurary:   0.13156565656565655\n",
      "1071 18729\n",
      "Epoch: 38251/50000..  Training Loss: 0.02639278..  Test Loss: 0.41871795..  Accurary:   0.05409090909090909\n",
      "2003 17797\n",
      "Epoch: 38301/50000..  Training Loss: 0.03836744..  Test Loss: 0.43007308..  Accurary:   0.10116161616161616\n",
      "2546 17254\n",
      "Epoch: 38351/50000..  Training Loss: 0.02978774..  Test Loss: 0.42993572..  Accurary:   0.12858585858585858\n",
      "492 19308\n",
      "Epoch: 38401/50000..  Training Loss: 0.04755018..  Test Loss: 0.43715832..  Accurary:   0.02484848484848485\n",
      "997 18803\n",
      "Epoch: 38451/50000..  Training Loss: 0.05948550..  Test Loss: 0.43043715..  Accurary:   0.05035353535353535\n",
      "831 18969\n",
      "Epoch: 38501/50000..  Training Loss: 0.03019136..  Test Loss: 0.43141207..  Accurary:   0.04196969696969697\n",
      "4947 14853\n",
      "Epoch: 38551/50000..  Training Loss: 0.04093957..  Test Loss: 0.43033665..  Accurary:   0.24984848484848485\n",
      "1609 18191\n",
      "Epoch: 38601/50000..  Training Loss: 0.02103478..  Test Loss: 0.42449829..  Accurary:   0.08126262626262626\n",
      "2155 17645\n",
      "Epoch: 38651/50000..  Training Loss: 0.02670234..  Test Loss: 0.43015182..  Accurary:   0.10883838383838383\n",
      "915 18885\n",
      "Epoch: 38701/50000..  Training Loss: 0.04099772..  Test Loss: 0.42651382..  Accurary:   0.04621212121212121\n",
      "900 18900\n",
      "Epoch: 38751/50000..  Training Loss: 0.04078183..  Test Loss: 0.42503431..  Accurary:   0.045454545454545456\n",
      "1655 18145\n",
      "Epoch: 38801/50000..  Training Loss: 0.03577260..  Test Loss: 0.42312980..  Accurary:   0.08358585858585858\n",
      "1057 18743\n",
      "Epoch: 38851/50000..  Training Loss: 0.02841751..  Test Loss: 0.44184738..  Accurary:   0.05338383838383838\n",
      "1257 18543\n",
      "Epoch: 38901/50000..  Training Loss: 0.03203866..  Test Loss: 0.42355031..  Accurary:   0.06348484848484849\n",
      "2241 17559\n",
      "Epoch: 38951/50000..  Training Loss: 0.02702420..  Test Loss: 0.43215609..  Accurary:   0.11318181818181818\n",
      "1162 18638\n",
      "Epoch: 39001/50000..  Training Loss: 0.02916571..  Test Loss: 0.44530407..  Accurary:   0.05868686868686869\n",
      "2386 17414\n",
      "Epoch: 39051/50000..  Training Loss: 0.03141194..  Test Loss: 0.44640949..  Accurary:   0.1205050505050505\n",
      "992 18808\n",
      "Epoch: 39101/50000..  Training Loss: 0.02602080..  Test Loss: 0.43959400..  Accurary:   0.050101010101010104\n",
      "3805 15995\n",
      "Epoch: 39151/50000..  Training Loss: 0.02545164..  Test Loss: 0.42710716..  Accurary:   0.19217171717171716\n",
      "4085 15715\n",
      "Epoch: 39201/50000..  Training Loss: 0.03894013..  Test Loss: 0.41609439..  Accurary:   0.2063131313131313\n",
      "949 18851\n",
      "Epoch: 39251/50000..  Training Loss: 0.03699966..  Test Loss: 0.42590830..  Accurary:   0.04792929292929293\n",
      "1707 18093\n",
      "Epoch: 39301/50000..  Training Loss: 0.03465746..  Test Loss: 0.43030298..  Accurary:   0.08621212121212121\n",
      "4816 14984\n",
      "Epoch: 39351/50000..  Training Loss: 0.02511707..  Test Loss: 0.44035226..  Accurary:   0.24323232323232324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2346 17454\n",
      "Epoch: 39401/50000..  Training Loss: 0.03652155..  Test Loss: 0.41799736..  Accurary:   0.11848484848484848\n",
      "1253 18547\n",
      "Epoch: 39451/50000..  Training Loss: 0.02967007..  Test Loss: 0.43337670..  Accurary:   0.06328282828282829\n",
      "3435 16365\n",
      "Epoch: 39501/50000..  Training Loss: 0.03199469..  Test Loss: 0.42525727..  Accurary:   0.1734848484848485\n",
      "1327 18473\n",
      "Epoch: 39551/50000..  Training Loss: 0.04314578..  Test Loss: 0.43148503..  Accurary:   0.06702020202020202\n",
      "2297 17503\n",
      "Epoch: 39601/50000..  Training Loss: 0.02869008..  Test Loss: 0.42767870..  Accurary:   0.11601010101010101\n",
      "2138 17662\n",
      "Epoch: 39651/50000..  Training Loss: 0.03955121..  Test Loss: 0.42875859..  Accurary:   0.10797979797979798\n",
      "900 18900\n",
      "Epoch: 39701/50000..  Training Loss: 0.03064575..  Test Loss: 0.44039521..  Accurary:   0.045454545454545456\n",
      "903 18897\n",
      "Epoch: 39751/50000..  Training Loss: 0.03160359..  Test Loss: 0.43803039..  Accurary:   0.04560606060606061\n",
      "1929 17871\n",
      "Epoch: 39801/50000..  Training Loss: 0.03649927..  Test Loss: 0.43679664..  Accurary:   0.09742424242424243\n",
      "1743 18057\n",
      "Epoch: 39851/50000..  Training Loss: 0.04009404..  Test Loss: 0.42017281..  Accurary:   0.08803030303030303\n",
      "3490 16310\n",
      "Epoch: 39901/50000..  Training Loss: 0.04063087..  Test Loss: 0.41155601..  Accurary:   0.17626262626262626\n",
      "1720 18080\n",
      "Epoch: 39951/50000..  Training Loss: 0.03064465..  Test Loss: 0.42531088..  Accurary:   0.08686868686868687\n",
      "2984 16816\n",
      "Epoch: 40001/50000..  Training Loss: 0.05489127..  Test Loss: 0.42134061..  Accurary:   0.1507070707070707\n",
      "1847 17953\n",
      "Epoch: 40051/50000..  Training Loss: 0.02698748..  Test Loss: 0.42712224..  Accurary:   0.09328282828282829\n",
      "3580 16220\n",
      "Epoch: 40101/50000..  Training Loss: 0.02795735..  Test Loss: 0.42841521..  Accurary:   0.1808080808080808\n",
      "2217 17583\n",
      "Epoch: 40151/50000..  Training Loss: 0.02450900..  Test Loss: 0.42000952..  Accurary:   0.11196969696969697\n",
      "2638 17162\n",
      "Epoch: 40201/50000..  Training Loss: 0.03551026..  Test Loss: 0.42095339..  Accurary:   0.13323232323232323\n",
      "880 18920\n",
      "Epoch: 40251/50000..  Training Loss: 0.03526027..  Test Loss: 0.44187203..  Accurary:   0.044444444444444446\n",
      "2329 17471\n",
      "Epoch: 40301/50000..  Training Loss: 0.02696520..  Test Loss: 0.42843238..  Accurary:   0.11762626262626262\n",
      "2596 17204\n",
      "Epoch: 40351/50000..  Training Loss: 0.03506557..  Test Loss: 0.44114727..  Accurary:   0.13111111111111112\n",
      "3506 16294\n",
      "Epoch: 40401/50000..  Training Loss: 0.02297836..  Test Loss: 0.42493367..  Accurary:   0.17707070707070707\n",
      "2928 16872\n",
      "Epoch: 40451/50000..  Training Loss: 0.03344227..  Test Loss: 0.41614193..  Accurary:   0.1478787878787879\n",
      "1521 18279\n",
      "Epoch: 40501/50000..  Training Loss: 0.03510675..  Test Loss: 0.41579881..  Accurary:   0.07681818181818181\n",
      "474 19326\n",
      "Epoch: 40551/50000..  Training Loss: 0.05108628..  Test Loss: 0.43660146..  Accurary:   0.02393939393939394\n",
      "1555 18245\n",
      "Epoch: 40601/50000..  Training Loss: 0.03480465..  Test Loss: 0.43887439..  Accurary:   0.07853535353535354\n",
      "2399 17401\n",
      "Epoch: 40651/50000..  Training Loss: 0.03727557..  Test Loss: 0.44255811..  Accurary:   0.12116161616161616\n",
      "1697 18103\n",
      "Epoch: 40701/50000..  Training Loss: 0.03803072..  Test Loss: 0.43410704..  Accurary:   0.0857070707070707\n",
      "1909 17891\n",
      "Epoch: 40751/50000..  Training Loss: 0.03004523..  Test Loss: 0.43014777..  Accurary:   0.09641414141414141\n",
      "2506 17294\n",
      "Epoch: 40801/50000..  Training Loss: 0.04466868..  Test Loss: 0.44018307..  Accurary:   0.12656565656565658\n",
      "2229 17571\n",
      "Epoch: 40851/50000..  Training Loss: 0.03257313..  Test Loss: 0.42927045..  Accurary:   0.11257575757575758\n",
      "6431 13369\n",
      "Epoch: 40901/50000..  Training Loss: 0.03276758..  Test Loss: 0.42439607..  Accurary:   0.3247979797979798\n",
      "1943 17857\n",
      "Epoch: 40951/50000..  Training Loss: 0.04544068..  Test Loss: 0.44929144..  Accurary:   0.09813131313131312\n",
      "1396 18404\n",
      "Epoch: 41001/50000..  Training Loss: 0.03917312..  Test Loss: 0.42803937..  Accurary:   0.0705050505050505\n",
      "848 18952\n",
      "Epoch: 41051/50000..  Training Loss: 0.02922844..  Test Loss: 0.43656108..  Accurary:   0.042828282828282827\n",
      "628 19172\n",
      "Epoch: 41101/50000..  Training Loss: 0.03959656..  Test Loss: 0.44237128..  Accurary:   0.03171717171717172\n",
      "1521 18279\n",
      "Epoch: 41151/50000..  Training Loss: 0.03217455..  Test Loss: 0.44700199..  Accurary:   0.07681818181818181\n",
      "1241 18559\n",
      "Epoch: 41201/50000..  Training Loss: 0.02936305..  Test Loss: 0.41614845..  Accurary:   0.06267676767676768\n",
      "3040 16760\n",
      "Epoch: 41251/50000..  Training Loss: 0.02693537..  Test Loss: 0.43185812..  Accurary:   0.15353535353535352\n",
      "3471 16329\n",
      "Epoch: 41301/50000..  Training Loss: 0.02644856..  Test Loss: 0.42517167..  Accurary:   0.1753030303030303\n",
      "1963 17837\n",
      "Epoch: 41351/50000..  Training Loss: 0.02308517..  Test Loss: 0.43356267..  Accurary:   0.09914141414141414\n",
      "2007 17793\n",
      "Epoch: 41401/50000..  Training Loss: 0.02825100..  Test Loss: 0.43381363..  Accurary:   0.10136363636363636\n",
      "2446 17354\n",
      "Epoch: 41451/50000..  Training Loss: 0.03049235..  Test Loss: 0.41764751..  Accurary:   0.12353535353535354\n",
      "2800 17000\n",
      "Epoch: 41501/50000..  Training Loss: 0.02789702..  Test Loss: 0.41469264..  Accurary:   0.1414141414141414\n",
      "1232 18568\n",
      "Epoch: 41551/50000..  Training Loss: 0.03196137..  Test Loss: 0.42168117..  Accurary:   0.06222222222222222\n",
      "694 19106\n",
      "Epoch: 41601/50000..  Training Loss: 0.04024575..  Test Loss: 0.43403947..  Accurary:   0.03505050505050505\n",
      "2566 17234\n",
      "Epoch: 41651/50000..  Training Loss: 0.02602832..  Test Loss: 0.43574923..  Accurary:   0.1295959595959596\n",
      "3743 16057\n",
      "Epoch: 41701/50000..  Training Loss: 0.04817308..  Test Loss: 0.41384041..  Accurary:   0.18904040404040404\n",
      "1702 18098\n",
      "Epoch: 41751/50000..  Training Loss: 0.03437749..  Test Loss: 0.41809294..  Accurary:   0.08595959595959596\n",
      "1562 18238\n",
      "Epoch: 41801/50000..  Training Loss: 0.03589718..  Test Loss: 0.41807163..  Accurary:   0.07888888888888888\n",
      "1536 18264\n",
      "Epoch: 41851/50000..  Training Loss: 0.02855583..  Test Loss: 0.42402431..  Accurary:   0.07757575757575758\n",
      "702 19098\n",
      "Epoch: 41901/50000..  Training Loss: 0.02883247..  Test Loss: 0.42900404..  Accurary:   0.035454545454545454\n",
      "3401 16399\n",
      "Epoch: 41951/50000..  Training Loss: 0.03699686..  Test Loss: 0.42247766..  Accurary:   0.17176767676767676\n",
      "4179 15621\n",
      "Epoch: 42001/50000..  Training Loss: 0.04334343..  Test Loss: 0.43020847..  Accurary:   0.21106060606060606\n",
      "2099 17701\n",
      "Epoch: 42051/50000..  Training Loss: 0.04187912..  Test Loss: 0.43109497..  Accurary:   0.10601010101010101\n",
      "1655 18145\n",
      "Epoch: 42101/50000..  Training Loss: 0.02910354..  Test Loss: 0.44078353..  Accurary:   0.08358585858585858\n",
      "925 18875\n",
      "Epoch: 42151/50000..  Training Loss: 0.02622881..  Test Loss: 0.44452527..  Accurary:   0.04671717171717172\n",
      "2940 16860\n",
      "Epoch: 42201/50000..  Training Loss: 0.03329820..  Test Loss: 0.41849965..  Accurary:   0.1484848484848485\n",
      "2664 17136\n",
      "Epoch: 42251/50000..  Training Loss: 0.02927639..  Test Loss: 0.42795736..  Accurary:   0.13454545454545455\n",
      "1398 18402\n",
      "Epoch: 42301/50000..  Training Loss: 0.03645786..  Test Loss: 0.42292854..  Accurary:   0.0706060606060606\n",
      "4144 15656\n",
      "Epoch: 42351/50000..  Training Loss: 0.02761043..  Test Loss: 0.42557064..  Accurary:   0.20929292929292928\n",
      "1289 18511\n",
      "Epoch: 42401/50000..  Training Loss: 0.04738766..  Test Loss: 0.42376509..  Accurary:   0.0651010101010101\n",
      "2435 17365\n",
      "Epoch: 42451/50000..  Training Loss: 0.03549127..  Test Loss: 0.41939265..  Accurary:   0.12297979797979798\n",
      "906 18894\n",
      "Epoch: 42501/50000..  Training Loss: 0.03039220..  Test Loss: 0.42659864..  Accurary:   0.04575757575757576\n",
      "1207 18593\n",
      "Epoch: 42551/50000..  Training Loss: 0.03835426..  Test Loss: 0.41943201..  Accurary:   0.06095959595959596\n",
      "1090 18710\n",
      "Epoch: 42601/50000..  Training Loss: 0.03826927..  Test Loss: 0.42218524..  Accurary:   0.05505050505050505\n",
      "2061 17739\n",
      "Epoch: 42651/50000..  Training Loss: 0.02833001..  Test Loss: 0.42010334..  Accurary:   0.1040909090909091\n",
      "567 19233\n",
      "Epoch: 42701/50000..  Training Loss: 0.04322547..  Test Loss: 0.44093868..  Accurary:   0.028636363636363637\n",
      "2082 17718\n",
      "Epoch: 42751/50000..  Training Loss: 0.04783473..  Test Loss: 0.41154099..  Accurary:   0.10515151515151515\n",
      "1940 17860\n",
      "Epoch: 42801/50000..  Training Loss: 0.04070256..  Test Loss: 0.42630243..  Accurary:   0.09797979797979799\n",
      "851 18949\n",
      "Epoch: 42851/50000..  Training Loss: 0.03928967..  Test Loss: 0.44268698..  Accurary:   0.04297979797979798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1423 18377\n",
      "Epoch: 42901/50000..  Training Loss: 0.02721200..  Test Loss: 0.42809314..  Accurary:   0.07186868686868687\n",
      "1131 18669\n",
      "Epoch: 42951/50000..  Training Loss: 0.03106892..  Test Loss: 0.44151309..  Accurary:   0.05712121212121212\n",
      "2555 17245\n",
      "Epoch: 43001/50000..  Training Loss: 0.03198070..  Test Loss: 0.43574485..  Accurary:   0.12904040404040404\n",
      "1433 18367\n",
      "Epoch: 43051/50000..  Training Loss: 0.03722310..  Test Loss: 0.44200826..  Accurary:   0.07237373737373737\n",
      "2044 17756\n",
      "Epoch: 43101/50000..  Training Loss: 0.02530791..  Test Loss: 0.43161684..  Accurary:   0.10323232323232323\n",
      "1353 18447\n",
      "Epoch: 43151/50000..  Training Loss: 0.02532237..  Test Loss: 0.42126787..  Accurary:   0.06833333333333333\n",
      "5182 14618\n",
      "Epoch: 43201/50000..  Training Loss: 0.03425950..  Test Loss: 0.42595157..  Accurary:   0.26171717171717174\n",
      "1096 18704\n",
      "Epoch: 43251/50000..  Training Loss: 0.03717738..  Test Loss: 0.42133462..  Accurary:   0.055353535353535356\n",
      "3677 16123\n",
      "Epoch: 43301/50000..  Training Loss: 0.02971648..  Test Loss: 0.42954639..  Accurary:   0.1857070707070707\n",
      "4708 15092\n",
      "Epoch: 43351/50000..  Training Loss: 0.02350838..  Test Loss: 0.43441924..  Accurary:   0.23777777777777778\n",
      "1775 18025\n",
      "Epoch: 43401/50000..  Training Loss: 0.04360992..  Test Loss: 0.43508592..  Accurary:   0.08964646464646464\n",
      "905 18895\n",
      "Epoch: 43451/50000..  Training Loss: 0.02714317..  Test Loss: 0.44207346..  Accurary:   0.04570707070707071\n",
      "1108 18692\n",
      "Epoch: 43501/50000..  Training Loss: 0.03989671..  Test Loss: 0.43221068..  Accurary:   0.05595959595959596\n",
      "2996 16804\n",
      "Epoch: 43551/50000..  Training Loss: 0.03668352..  Test Loss: 0.42220905..  Accurary:   0.15131313131313132\n",
      "929 18871\n",
      "Epoch: 43601/50000..  Training Loss: 0.02822844..  Test Loss: 0.42659950..  Accurary:   0.046919191919191916\n",
      "850 18950\n",
      "Epoch: 43651/50000..  Training Loss: 0.06339712..  Test Loss: 0.43361688..  Accurary:   0.04292929292929293\n",
      "1677 18123\n",
      "Epoch: 43701/50000..  Training Loss: 0.02941256..  Test Loss: 0.42746553..  Accurary:   0.0846969696969697\n",
      "1010 18790\n",
      "Epoch: 43751/50000..  Training Loss: 0.04040568..  Test Loss: 0.42915112..  Accurary:   0.05101010101010101\n",
      "1543 18257\n",
      "Epoch: 43801/50000..  Training Loss: 0.05660064..  Test Loss: 0.43244603..  Accurary:   0.07792929292929293\n",
      "396 19404\n",
      "Epoch: 43851/50000..  Training Loss: 0.04465609..  Test Loss: 0.42519641..  Accurary:   0.02\n",
      "5019 14781\n",
      "Epoch: 43901/50000..  Training Loss: 0.02949648..  Test Loss: 0.43534067..  Accurary:   0.2534848484848485\n",
      "3473 16327\n",
      "Epoch: 43951/50000..  Training Loss: 0.04671088..  Test Loss: 0.41928819..  Accurary:   0.1754040404040404\n",
      "3627 16173\n",
      "Epoch: 44001/50000..  Training Loss: 0.02235490..  Test Loss: 0.43100175..  Accurary:   0.1831818181818182\n",
      "996 18804\n",
      "Epoch: 44051/50000..  Training Loss: 0.06733707..  Test Loss: 0.42869514..  Accurary:   0.0503030303030303\n",
      "1012 18788\n",
      "Epoch: 44101/50000..  Training Loss: 0.03343526..  Test Loss: 0.43765634..  Accurary:   0.051111111111111114\n",
      "929 18871\n",
      "Epoch: 44151/50000..  Training Loss: 0.03109917..  Test Loss: 0.43246478..  Accurary:   0.046919191919191916\n",
      "863 18937\n",
      "Epoch: 44201/50000..  Training Loss: 0.07793143..  Test Loss: 0.42325228..  Accurary:   0.04358585858585859\n",
      "1358 18442\n",
      "Epoch: 44251/50000..  Training Loss: 0.02981550..  Test Loss: 0.43295959..  Accurary:   0.06858585858585858\n",
      "938 18862\n",
      "Epoch: 44301/50000..  Training Loss: 0.02419503..  Test Loss: 0.43492448..  Accurary:   0.047373737373737373\n",
      "3142 16658\n",
      "Epoch: 44351/50000..  Training Loss: 0.02996028..  Test Loss: 0.42547542..  Accurary:   0.15868686868686868\n",
      "1449 18351\n",
      "Epoch: 44401/50000..  Training Loss: 0.03229624..  Test Loss: 0.43607387..  Accurary:   0.07318181818181818\n",
      "1655 18145\n",
      "Epoch: 44451/50000..  Training Loss: 0.03932881..  Test Loss: 0.42929584..  Accurary:   0.08358585858585858\n",
      "1668 18132\n",
      "Epoch: 44501/50000..  Training Loss: 0.03167250..  Test Loss: 0.43922901..  Accurary:   0.08424242424242424\n",
      "3262 16538\n",
      "Epoch: 44551/50000..  Training Loss: 0.03205439..  Test Loss: 0.44777146..  Accurary:   0.16474747474747475\n",
      "1398 18402\n",
      "Epoch: 44601/50000..  Training Loss: 0.04048453..  Test Loss: 0.43721867..  Accurary:   0.0706060606060606\n",
      "2123 17677\n",
      "Epoch: 44651/50000..  Training Loss: 0.03307644..  Test Loss: 0.42320520..  Accurary:   0.10722222222222222\n",
      "1088 18712\n",
      "Epoch: 44701/50000..  Training Loss: 0.02862809..  Test Loss: 0.44473884..  Accurary:   0.05494949494949495\n",
      "2420 17380\n",
      "Epoch: 44751/50000..  Training Loss: 0.02783546..  Test Loss: 0.43076548..  Accurary:   0.12222222222222222\n",
      "814 18986\n",
      "Epoch: 44801/50000..  Training Loss: 0.02531397..  Test Loss: 0.43265507..  Accurary:   0.04111111111111111\n",
      "1104 18696\n",
      "Epoch: 44851/50000..  Training Loss: 0.03245118..  Test Loss: 0.43314266..  Accurary:   0.055757575757575756\n",
      "1801 17999\n",
      "Epoch: 44901/50000..  Training Loss: 0.03476039..  Test Loss: 0.43493578..  Accurary:   0.09095959595959596\n",
      "1363 18437\n",
      "Epoch: 44951/50000..  Training Loss: 0.03056932..  Test Loss: 0.42964470..  Accurary:   0.06883838383838384\n",
      "4082 15718\n",
      "Epoch: 45001/50000..  Training Loss: 0.03321442..  Test Loss: 0.41429412..  Accurary:   0.20616161616161616\n",
      "2793 17007\n",
      "Epoch: 45051/50000..  Training Loss: 0.04800793..  Test Loss: 0.42259982..  Accurary:   0.14106060606060605\n",
      "1484 18316\n",
      "Epoch: 45101/50000..  Training Loss: 0.04554674..  Test Loss: 0.40777442..  Accurary:   0.07494949494949495\n",
      "2437 17363\n",
      "Epoch: 45151/50000..  Training Loss: 0.03017016..  Test Loss: 0.43001410..  Accurary:   0.12308080808080808\n",
      "932 18868\n",
      "Epoch: 45201/50000..  Training Loss: 0.04249068..  Test Loss: 0.43572077..  Accurary:   0.04707070707070707\n",
      "877 18923\n",
      "Epoch: 45251/50000..  Training Loss: 0.03088901..  Test Loss: 0.42058623..  Accurary:   0.044292929292929294\n",
      "3203 16597\n",
      "Epoch: 45301/50000..  Training Loss: 0.03595420..  Test Loss: 0.44065973..  Accurary:   0.16176767676767675\n",
      "1094 18706\n",
      "Epoch: 45351/50000..  Training Loss: 0.03934753..  Test Loss: 0.41755486..  Accurary:   0.055252525252525254\n",
      "1317 18483\n",
      "Epoch: 45401/50000..  Training Loss: 0.02578967..  Test Loss: 0.42645258..  Accurary:   0.06651515151515151\n",
      "1134 18666\n",
      "Epoch: 45451/50000..  Training Loss: 0.03330854..  Test Loss: 0.42836031..  Accurary:   0.057272727272727274\n",
      "613 19187\n",
      "Epoch: 45501/50000..  Training Loss: 0.02359105..  Test Loss: 0.42251375..  Accurary:   0.03095959595959596\n",
      "2443 17357\n",
      "Epoch: 45551/50000..  Training Loss: 0.02448548..  Test Loss: 0.40727982..  Accurary:   0.12338383838383839\n",
      "1570 18230\n",
      "Epoch: 45601/50000..  Training Loss: 0.03401001..  Test Loss: 0.41120395..  Accurary:   0.07929292929292929\n",
      "1520 18280\n",
      "Epoch: 45651/50000..  Training Loss: 0.02781908..  Test Loss: 0.42568991..  Accurary:   0.07676767676767676\n",
      "1045 18755\n",
      "Epoch: 45701/50000..  Training Loss: 0.03606482..  Test Loss: 0.43003240..  Accurary:   0.05277777777777778\n",
      "2181 17619\n",
      "Epoch: 45751/50000..  Training Loss: 0.02860842..  Test Loss: 0.42415982..  Accurary:   0.11015151515151515\n",
      "881 18919\n",
      "Epoch: 45801/50000..  Training Loss: 0.03384940..  Test Loss: 0.43677449..  Accurary:   0.0444949494949495\n",
      "333 19467\n",
      "Epoch: 45851/50000..  Training Loss: 0.03128364..  Test Loss: 0.42830878..  Accurary:   0.01681818181818182\n",
      "2646 17154\n",
      "Epoch: 45901/50000..  Training Loss: 0.03059825..  Test Loss: 0.42248431..  Accurary:   0.13363636363636364\n",
      "501 19299\n",
      "Epoch: 45951/50000..  Training Loss: 0.05303850..  Test Loss: 0.44988641..  Accurary:   0.025303030303030303\n",
      "3353 16447\n",
      "Epoch: 46001/50000..  Training Loss: 0.03291677..  Test Loss: 0.42485785..  Accurary:   0.16934343434343435\n",
      "2278 17522\n",
      "Epoch: 46051/50000..  Training Loss: 0.03604925..  Test Loss: 0.42347026..  Accurary:   0.11505050505050506\n",
      "1987 17813\n",
      "Epoch: 46101/50000..  Training Loss: 0.02642001..  Test Loss: 0.45787495..  Accurary:   0.10035353535353535\n",
      "1614 18186\n",
      "Epoch: 46151/50000..  Training Loss: 0.04836457..  Test Loss: 0.42945570..  Accurary:   0.08151515151515151\n",
      "4606 15194\n",
      "Epoch: 46201/50000..  Training Loss: 0.04464449..  Test Loss: 0.41957489..  Accurary:   0.23262626262626263\n",
      "2566 17234\n",
      "Epoch: 46251/50000..  Training Loss: 0.03609346..  Test Loss: 0.41318759..  Accurary:   0.1295959595959596\n",
      "2250 17550\n",
      "Epoch: 46301/50000..  Training Loss: 0.02710075..  Test Loss: 0.42495385..  Accurary:   0.11363636363636363\n",
      "1164 18636\n",
      "Epoch: 46351/50000..  Training Loss: 0.02579184..  Test Loss: 0.42320427..  Accurary:   0.058787878787878785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1073 18727\n",
      "Epoch: 46401/50000..  Training Loss: 0.03357795..  Test Loss: 0.45449257..  Accurary:   0.054191919191919194\n",
      "728 19072\n",
      "Epoch: 46451/50000..  Training Loss: 0.02662950..  Test Loss: 0.43301502..  Accurary:   0.03676767676767677\n",
      "3238 16562\n",
      "Epoch: 46501/50000..  Training Loss: 0.04308429..  Test Loss: 0.44491774..  Accurary:   0.16353535353535353\n",
      "1088 18712\n",
      "Epoch: 46551/50000..  Training Loss: 0.03840401..  Test Loss: 0.43817076..  Accurary:   0.05494949494949495\n",
      "942 18858\n",
      "Epoch: 46601/50000..  Training Loss: 0.03195612..  Test Loss: 0.44179684..  Accurary:   0.04757575757575758\n",
      "2289 17511\n",
      "Epoch: 46651/50000..  Training Loss: 0.03199528..  Test Loss: 0.43177506..  Accurary:   0.1156060606060606\n",
      "5853 13947\n",
      "Epoch: 46701/50000..  Training Loss: 0.02859766..  Test Loss: 0.42932153..  Accurary:   0.2956060606060606\n",
      "1698 18102\n",
      "Epoch: 46751/50000..  Training Loss: 0.03073881..  Test Loss: 0.43552557..  Accurary:   0.08575757575757575\n",
      "1339 18461\n",
      "Epoch: 46801/50000..  Training Loss: 0.03140891..  Test Loss: 0.42598164..  Accurary:   0.06762626262626263\n",
      "3389 16411\n",
      "Epoch: 46851/50000..  Training Loss: 0.03066013..  Test Loss: 0.42926240..  Accurary:   0.17116161616161615\n",
      "1178 18622\n",
      "Epoch: 46901/50000..  Training Loss: 0.03032914..  Test Loss: 0.44117936..  Accurary:   0.059494949494949496\n",
      "1107 18693\n",
      "Epoch: 46951/50000..  Training Loss: 0.03900699..  Test Loss: 0.43123162..  Accurary:   0.05590909090909091\n",
      "1385 18415\n",
      "Epoch: 47001/50000..  Training Loss: 0.03847572..  Test Loss: 0.41462317..  Accurary:   0.06994949494949496\n",
      "1150 18650\n",
      "Epoch: 47051/50000..  Training Loss: 0.03801795..  Test Loss: 0.42811295..  Accurary:   0.05808080808080808\n",
      "2876 16924\n",
      "Epoch: 47101/50000..  Training Loss: 0.03906455..  Test Loss: 0.43445918..  Accurary:   0.14525252525252524\n",
      "714 19086\n",
      "Epoch: 47151/50000..  Training Loss: 0.03797336..  Test Loss: 0.41668946..  Accurary:   0.036060606060606064\n",
      "895 18905\n",
      "Epoch: 47201/50000..  Training Loss: 0.02532248..  Test Loss: 0.42598963..  Accurary:   0.0452020202020202\n",
      "1617 18183\n",
      "Epoch: 47251/50000..  Training Loss: 0.02683533..  Test Loss: 0.43758366..  Accurary:   0.08166666666666667\n",
      "1535 18265\n",
      "Epoch: 47301/50000..  Training Loss: 0.03267199..  Test Loss: 0.43452647..  Accurary:   0.07752525252525252\n",
      "1177 18623\n",
      "Epoch: 47351/50000..  Training Loss: 0.02172419..  Test Loss: 0.42098692..  Accurary:   0.059444444444444446\n",
      "873 18927\n",
      "Epoch: 47401/50000..  Training Loss: 0.04889141..  Test Loss: 0.41676387..  Accurary:   0.04409090909090909\n",
      "1125 18675\n",
      "Epoch: 47451/50000..  Training Loss: 0.02865740..  Test Loss: 0.43381807..  Accurary:   0.056818181818181816\n",
      "862 18938\n",
      "Epoch: 47501/50000..  Training Loss: 0.03742352..  Test Loss: 0.42218554..  Accurary:   0.04353535353535354\n",
      "1162 18638\n",
      "Epoch: 47551/50000..  Training Loss: 0.03428583..  Test Loss: 0.43667608..  Accurary:   0.05868686868686869\n",
      "1161 18639\n",
      "Epoch: 47601/50000..  Training Loss: 0.04239191..  Test Loss: 0.42318612..  Accurary:   0.05863636363636364\n",
      "3927 15873\n",
      "Epoch: 47651/50000..  Training Loss: 0.03457100..  Test Loss: 0.42837796..  Accurary:   0.19833333333333333\n",
      "1700 18100\n",
      "Epoch: 47701/50000..  Training Loss: 0.03303663..  Test Loss: 0.42285377..  Accurary:   0.08585858585858586\n",
      "1079 18721\n",
      "Epoch: 47751/50000..  Training Loss: 0.03950732..  Test Loss: 0.43138298..  Accurary:   0.05449494949494949\n",
      "435 19365\n",
      "Epoch: 47801/50000..  Training Loss: 0.02946835..  Test Loss: 0.42443803..  Accurary:   0.02196969696969697\n",
      "2554 17246\n",
      "Epoch: 47851/50000..  Training Loss: 0.02820486..  Test Loss: 0.43122545..  Accurary:   0.128989898989899\n",
      "1638 18162\n",
      "Epoch: 47901/50000..  Training Loss: 0.06579920..  Test Loss: 0.42526940..  Accurary:   0.08272727272727273\n",
      "1244 18556\n",
      "Epoch: 47951/50000..  Training Loss: 0.02883463..  Test Loss: 0.43770990..  Accurary:   0.06282828282828283\n",
      "595 19205\n",
      "Epoch: 48001/50000..  Training Loss: 0.03429116..  Test Loss: 0.41890788..  Accurary:   0.03005050505050505\n",
      "3231 16569\n",
      "Epoch: 48051/50000..  Training Loss: 0.02972361..  Test Loss: 0.43183869..  Accurary:   0.16318181818181818\n",
      "1462 18338\n",
      "Epoch: 48101/50000..  Training Loss: 0.02516977..  Test Loss: 0.42732728..  Accurary:   0.07383838383838384\n",
      "647 19153\n",
      "Epoch: 48151/50000..  Training Loss: 0.03288373..  Test Loss: 0.43392864..  Accurary:   0.03267676767676768\n",
      "1552 18248\n",
      "Epoch: 48201/50000..  Training Loss: 0.04059278..  Test Loss: 0.43098217..  Accurary:   0.07838383838383839\n",
      "1051 18749\n",
      "Epoch: 48251/50000..  Training Loss: 0.02577175..  Test Loss: 0.42700344..  Accurary:   0.05308080808080808\n",
      "1769 18031\n",
      "Epoch: 48301/50000..  Training Loss: 0.02646597..  Test Loss: 0.43536848..  Accurary:   0.08934343434343435\n",
      "2387 17413\n",
      "Epoch: 48351/50000..  Training Loss: 0.03543970..  Test Loss: 0.41913787..  Accurary:   0.12055555555555555\n",
      "771 19029\n",
      "Epoch: 48401/50000..  Training Loss: 0.03235606..  Test Loss: 0.43244535..  Accurary:   0.03893939393939394\n",
      "2500 17300\n",
      "Epoch: 48451/50000..  Training Loss: 0.04896482..  Test Loss: 0.42086810..  Accurary:   0.12626262626262627\n",
      "2056 17744\n",
      "Epoch: 48501/50000..  Training Loss: 0.03345065..  Test Loss: 0.44483784..  Accurary:   0.10383838383838384\n",
      "1656 18144\n",
      "Epoch: 48551/50000..  Training Loss: 0.02886895..  Test Loss: 0.42107561..  Accurary:   0.08363636363636363\n",
      "2530 17270\n",
      "Epoch: 48601/50000..  Training Loss: 0.02677464..  Test Loss: 0.42439842..  Accurary:   0.12777777777777777\n",
      "1054 18746\n",
      "Epoch: 48651/50000..  Training Loss: 0.07407676..  Test Loss: 0.44248086..  Accurary:   0.053232323232323235\n",
      "3839 15961\n",
      "Epoch: 48701/50000..  Training Loss: 0.06489293..  Test Loss: 0.42065933..  Accurary:   0.1938888888888889\n",
      "1455 18345\n",
      "Epoch: 48751/50000..  Training Loss: 0.03849473..  Test Loss: 0.42365679..  Accurary:   0.07348484848484849\n",
      "1822 17978\n",
      "Epoch: 48801/50000..  Training Loss: 0.03717090..  Test Loss: 0.42601365..  Accurary:   0.09202020202020202\n",
      "1921 17879\n",
      "Epoch: 48851/50000..  Training Loss: 0.03026614..  Test Loss: 0.41757715..  Accurary:   0.09702020202020202\n",
      "796 19004\n",
      "Epoch: 48901/50000..  Training Loss: 0.02486547..  Test Loss: 0.43684027..  Accurary:   0.040202020202020204\n",
      "1502 18298\n",
      "Epoch: 48951/50000..  Training Loss: 0.03785756..  Test Loss: 0.42471907..  Accurary:   0.07585858585858586\n",
      "827 18973\n",
      "Epoch: 49001/50000..  Training Loss: 0.05463387..  Test Loss: 0.43334675..  Accurary:   0.041767676767676766\n",
      "1855 17945\n",
      "Epoch: 49051/50000..  Training Loss: 0.03026238..  Test Loss: 0.42608550..  Accurary:   0.0936868686868687\n",
      "1232 18568\n",
      "Epoch: 49101/50000..  Training Loss: 0.02414636..  Test Loss: 0.41747504..  Accurary:   0.06222222222222222\n",
      "4168 15632\n",
      "Epoch: 49151/50000..  Training Loss: 0.02763520..  Test Loss: 0.42904657..  Accurary:   0.2105050505050505\n",
      "1596 18204\n",
      "Epoch: 49201/50000..  Training Loss: 0.02669482..  Test Loss: 0.42396292..  Accurary:   0.08060606060606061\n",
      "1072 18728\n",
      "Epoch: 49251/50000..  Training Loss: 0.03283808..  Test Loss: 0.42428109..  Accurary:   0.05414141414141414\n",
      "1370 18430\n",
      "Epoch: 49301/50000..  Training Loss: 0.02478470..  Test Loss: 0.42545956..  Accurary:   0.0691919191919192\n",
      "4443 15357\n",
      "Epoch: 49351/50000..  Training Loss: 0.02935139..  Test Loss: 0.41925228..  Accurary:   0.2243939393939394\n",
      "2103 17697\n",
      "Epoch: 49401/50000..  Training Loss: 0.03068407..  Test Loss: 0.42457762..  Accurary:   0.10621212121212122\n",
      "1805 17995\n",
      "Epoch: 49451/50000..  Training Loss: 0.03446527..  Test Loss: 0.42182726..  Accurary:   0.09116161616161617\n",
      "1512 18288\n",
      "Epoch: 49501/50000..  Training Loss: 0.02696810..  Test Loss: 0.43408254..  Accurary:   0.07636363636363637\n",
      "2433 17367\n",
      "Epoch: 49551/50000..  Training Loss: 0.02920890..  Test Loss: 0.43990648..  Accurary:   0.12287878787878788\n",
      "2578 17222\n",
      "Epoch: 49601/50000..  Training Loss: 0.02936251..  Test Loss: 0.41716382..  Accurary:   0.1302020202020202\n",
      "1119 18681\n",
      "Epoch: 49651/50000..  Training Loss: 0.04220204..  Test Loss: 0.41748855..  Accurary:   0.05651515151515152\n",
      "2062 17738\n",
      "Epoch: 49701/50000..  Training Loss: 0.03190493..  Test Loss: 0.43185896..  Accurary:   0.10414141414141415\n",
      "8073 11727\n",
      "Epoch: 49751/50000..  Training Loss: 0.03483683..  Test Loss: 0.42208645..  Accurary:   0.4077272727272727\n",
      "2127 17673\n",
      "Epoch: 49801/50000..  Training Loss: 0.03080730..  Test Loss: 0.41768909..  Accurary:   0.10742424242424242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2434 17366\n",
      "Epoch: 49851/50000..  Training Loss: 0.03804083..  Test Loss: 0.43303800..  Accurary:   0.12292929292929293\n",
      "1585 18215\n",
      "Epoch: 49901/50000..  Training Loss: 0.02580497..  Test Loss: 0.41433939..  Accurary:   0.08005050505050505\n",
      "5925 13875\n",
      "Epoch: 49951/50000..  Training Loss: 0.04161072..  Test Loss: 0.41873628..  Accurary:   0.29924242424242425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Regressor().to(device)\n",
    "#model.apply(weights_init)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.003)\n",
    "\n",
    "epochs = 50000\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(train_batch)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_batch[i])\n",
    "        #MSE(output, label_batch[i])\n",
    "        #print(output)\n",
    "        #loss = torch.sqrt(criterion(torch.log(output), torch.log(label_batch[i])))\n",
    "        loss = criterion(output, label_batch[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    if e%50 == 0:\n",
    "        test_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = model(X_val)\n",
    "            #test_loss += torch.sqrt(criterion(torch.log(predictions), torch.log(y_val)))\n",
    "            test_loss += criterion(predictions, y_val)\n",
    "                \n",
    "        train_losses.append(train_loss/len(train_batch))\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.8f}.. \".format(train_loss/len(train_batch)),\n",
    "              \"Test Loss: {:.8f}.. \".format(test_loss),\n",
    "              \"Accurary:  \", accuracy(model, X_val, y_val, 0.15))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8bad3929b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "# loss 0.24 -> 2800\n",
    "# loss 0.14 -> 3300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.from_numpy(test_x.values).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model.forward(test_x)\n",
    "    \n",
    "\n",
    "output.shape\n",
    "output = output.cpu().numpy()\n",
    "#output_col = pd.DataFrame(test, columns = columns)\n",
    "\n",
    "output = y_scaler.inverse_transform(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./dataset-0510/submit_test.csv')\n",
    "submission['total_price'] = output\n",
    "submission.to_csv('submission/DNN_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
