{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building_id\n",
    "#columns = X.columns\n",
    "data_train = pd.read_csv('./dataset-0510/train.csv')\n",
    "X_test = pd.read_csv('./dataset-0510/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "data = data_train.append(X_test, ignore_index=True, sort=False)\n",
    "data = data.drop(['building_id'], axis=1)\n",
    "print(data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# fill the nan values\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "sale_price = data['total_price'].values\n",
    "data = data.drop('total_price', axis=1)\n",
    "\n",
    "columns = data.columns\n",
    "# check has any nan value in data\n",
    "data.isnull().values.any()\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/islab/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "scaling_data = x_scaler.fit_transform(data)\n",
    "scaling_sale_price = y_scaler.fit_transform(sale_price.reshape(-1, 1))\n",
    "\n",
    "data = pd.DataFrame(scaling_data, columns = columns)\n",
    "sale_price = pd.DataFrame(sale_price, columns=['total_price'])\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_x = data.iloc[:60000]\n",
    "test_x  = data.iloc[60000:]\n",
    "\n",
    "train_y = sale_price.iloc[:60000]\n",
    "test_y = sale_price.iloc[60000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(233, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(self.fc4(x))\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "# takes in a module and applies the specified weight initialization\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "## takes in a module and applies the specified weight initialization\n",
    "def weights_init_normal(m):\n",
    "    '''Takes in a module and initializes all linear layers with weight\n",
    "       values taken from a normal distribution.'''\n",
    "\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model\n",
    "    if classname.find('Linear') != -1:\n",
    "        y = m.in_features\n",
    "    # m.weight.data shoud be taken from a normal distribution\n",
    "        m.weight.data.normal_(0.0,1/np.sqrt(y))\n",
    "    # m.bias.data should be 0\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None: \n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training(X_train, y_train):\n",
    "    train_batch = np.array_split(X_train, 50)\n",
    "    label_batch = np.array_split(y_train, 50)\n",
    "\n",
    "    for i in range(len(train_batch)):\n",
    "        train_batch[i] = torch.from_numpy(train_batch[i].values).float().to(device)\n",
    "    for i in range(len(label_batch)):\n",
    "        label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1).to(device)\n",
    "    return train_batch, label_batch\n",
    "\n",
    "#train_batch = torch.from_numpy(X_train.values).float().to(device)\n",
    "#label_batch = torch.from_numpy(y_train).float().view(-1, 1).to(device)\n",
    "train_batch, label_batch = batch_training(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = torch.from_numpy(X_val.values).float().to(device)\n",
    "y_val = torch.from_numpy(y_val.values).float().view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Regressor().to(device)\n",
    "ps = model(train_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 51/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 1951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 2951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 3951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 4951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 5951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6701/50000..  Training Loss: nan..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 6951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 7951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 8951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 9851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 11401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 11451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 11501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 11551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 11601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 11651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 11701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 11751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 11801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 11851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 11901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 11951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 12951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 13951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 14901/50000..  Training Loss: nan..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 15951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 16951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 17951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 18951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 19951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 20951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21551/50000..  Training Loss: nan..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 21951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 22951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 23951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 24951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 25951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 26951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 27951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28201/50000..  Training Loss: nan..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 28951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 29951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 30951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 31651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 32951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 33951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 34951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 35951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36101/50000..  Training Loss: nan..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 36951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 37951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 38951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 39951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 40951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 41951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42751/50000..  Training Loss: nan..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 42951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 43951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 44951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 45951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 46951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 47951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48401/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 48951/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49001/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49051/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49101/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49151/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49201/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49251/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49301/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49351/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49401/50000..  Training Loss: nan..  Test Loss: nan.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49451/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49501/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49551/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49601/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49651/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49701/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49751/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49801/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49851/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49901/50000..  Training Loss: nan..  Test Loss: nan.. \n",
      "Epoch: 49951/50000..  Training Loss: nan..  Test Loss: nan.. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Regressor().to(device)\n",
    "#model.apply(weights_init)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.001)\n",
    "\n",
    "epochs = 50000\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(train_batch)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_batch[i])\n",
    "        #MSE(output, label_batch[i])\n",
    "        loss = torch.sqrt(criterion(torch.log(output), torch.log(label_batch[i])))\n",
    "        #loss = criterion(output, label_batch[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    if e%50 == 0:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = model(X_val)\n",
    "            test_loss += torch.sqrt(criterion(torch.log(predictions), torch.log(y_val)))\n",
    "            #test_loss += torch.sqrt(criterion(predictions, y_val))\n",
    "                \n",
    "        train_losses.append(train_loss/len(train_batch))\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.8f}.. \".format(train_loss/len(train_batch)),\n",
    "              \"Test Loss: {:.8f}.. \".format(test_loss))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8bad3929b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "# loss 0.24 -> 2800\n",
    "# loss 0.14 -> 3300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.from_numpy(test_x.values).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model.forward(test_x)\n",
    "    \n",
    "\n",
    "output.shape\n",
    "output = output.cpu().numpy()\n",
    "#output_col = pd.DataFrame(test, columns = columns)\n",
    "\n",
    "output = y_scaler.inverse_transform(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./dataset-0510/submit_test.csv')\n",
    "submission['total_price'] = output\n",
    "submission.to_csv('submission/DNN_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
